[data]
input_size = 224
second_input_size = 112 #Images input size for discrete vae
train_interpolation = 'bicubic' #Training interpolation (random, bilinear, bicubic default: "bicubic")
second_interpolation = 'lanczos' #Interpolation for discrete vae (random, bilinear, bicubic default: "lanczos")
mean = [0.5118, 0.5118, 0.5118] #S1 mean greyscale
std = [0.2717, 0.2717, 0.2717] #S1 std greyscale
train_path = '/scratch/SDF25/DeepCube_UC4a/S1/Train/' # enter your path for the S1 train set
output_dir = '/home/conradb/git/ifg-ssl/S1_C1/outputs/'

[resume]
auto_resume = false
resume = ''
start_epoch = 0
# val_path = '/scratch/SDF25/DeepCube_UC4a/S1/Test/'
# c1_val = true
# c1_val_path = '/scratch/SDF25/DeepCube_UC4a/C1/'
# mean = [0.5118, 0.5118, 0.5118] #S1 mean greyscale
# std = [0.2717, 0.2717, 0.2717] #S1 std greyscale

# [crops]
# global_crops_scale = [0.4, 1.0]
# local_crops_scale = [0.05, 0.4]
# local_crops_number = 8

[model]
# model = 'beit_base_patch16_224.in22k_ft_in22k'
model = 'beit_base_patch16_224_8k_vocab'
discrete_vae_type = 'dall-e'
# discrete_vae_type = 'customized'
discrete_vae_weight_path = '/home/conradb/git/ifg-ssl/BEiT/Dall-E_weights'
num_mask_patches = 75 #Number of the visual tokens/patches need be masked
min_mask_patches_per_block = 16
rel_pos_bias = true
abs_pos_emb = false
layer_scale_init_value = 0.1
drop_path = 0.1
checkpoint_path = 'model_checkpoints/BEiT_VitB_224_no_oversampling.pth'

[optimizer]
opt = 'adamw'
weight_decay = 0.05
lr = 3.75e-4
min_lr = 1e-5
epochs = 501
warmup_epochs = 5
warmup_steps = 2000
opt_eps = 1e-8
opt_betas = [0.9, 0.999]




# model = 'vit_small' # embed_dim=384, depth=12, num_heads=8
# patch_size = 16
# out_dim = 16384 # complex and large datasets values like 65k work well
# use_bn_head = false # use batch norm in head
# norm_last_layer = true #  normalize the last layer of the DINO head, typically set this paramater to False with vit_small and True with vit_base


# [DINO]
# warmup_teacher_temp = 0.01 # Initial value for the teacher temperature, Try decreasing it if the training loss does not decrease
# teacher_temp = 0.03
# warmup_teacher_temp_epochs = 30 # 30 is default?
# momentum_teacher = 0.9995 # Base EMA parameter for teacher update. The value is increased to 1 during training with cosine schedule. We recommend setting a higher value with small batches: for example use 0.9995 with batch size of 256.""")
# oversampling = false

# [linear]
# n_last_blocks = 4
# avgpool_patchtokens = false
# num_labels = 2
# lr = 0.0005

[train]
device = 'cuda'
# epochs = 801 #500 for DINO, 100 for linear
# lr = 1e-3
# min_lr=1e-4
# warmup_epochs = 10
# weight_decay = 0.04
# weight_decay_end = 0.4
batch_size = 128
# train_eval_batch_size = 1 #training set batch size for knn eval (maximise sample count)
# val_batch_size = 32 #val batch size for knn eval and linear eval
# c1_val_batch_size = 1
num_workers = 32
pin_memory = true
# use_fp16 = true
# clip_grad = 3.0
# freeze_last_layer = 1