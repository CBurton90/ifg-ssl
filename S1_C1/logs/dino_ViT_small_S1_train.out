/share/home/conradb/git/ifg-ssl/S1_C1
/home/conradb/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
/share/home/conradb/git/ifg-ssl/S1_C1/training/dino_S1_train.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  fp16_scaler = torch.cuda.amp.GradScaler()
Using cuda
Commencing training
Epoch 0/501
  0%|          | 0/30 [00:00<?, ?it/s]/share/home/conradb/git/ifg-ssl/S1_C1/training/dino_S1_train.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(fp16_scaler is not None):
  3%|▎         | 1/30 [01:02<30:08, 62.36s/it]  7%|▋         | 2/30 [01:07<13:29, 28.89s/it] 10%|█         | 3/30 [01:08<07:13, 16.05s/it] 13%|█▎        | 4/30 [01:09<04:21, 10.04s/it] 17%|█▋        | 5/30 [01:10<02:47,  6.69s/it] 20%|██        | 6/30 [01:10<01:52,  4.67s/it] 23%|██▎       | 7/30 [01:11<01:17,  3.39s/it] 27%|██▋       | 8/30 [01:12<00:56,  2.55s/it] 30%|███       | 9/30 [01:13<00:41,  1.99s/it] 33%|███▎      | 10/30 [01:13<00:32,  1.60s/it] 37%|███▋      | 11/30 [01:14<00:25,  1.34s/it] 40%|████      | 12/30 [01:15<00:20,  1.16s/it] 43%|████▎     | 13/30 [01:16<00:17,  1.04s/it] 47%|████▋     | 14/30 [01:16<00:15,  1.05it/s] 50%|█████     | 15/30 [01:17<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:18<00:11,  1.18it/s] 57%|█████▋    | 17/30 [01:19<00:10,  1.22it/s] 60%|██████    | 18/30 [01:19<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:20<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:21<00:07,  1.30it/s] 70%|███████   | 21/30 [01:22<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:22<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:23<00:05,  1.32it/s] 80%|████████  | 24/30 [01:24<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:25<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:25<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:26<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:27<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:28<00:00,  1.34it/s]100%|██████████| 30/30 [01:28<00:00,  1.34it/s]100%|██████████| 30/30 [01:29<00:00,  2.97s/it]
/share/home/conradb/git/ifg-ssl/S1_C1/training/dino_S1_train.py:210: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(config.model.checkpoint_path, map_location="cpu")
Epoch loss is 8.118204132715862
saving checkpoint
Extracting features for train set...
/home/conradb/git/ifg-ssl/dino/knn_evaluation.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels).long()
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0838, -0.0267,  0.0747,  ..., -0.0071,  0.0106,  0.0606],
        [ 0.0515, -0.0384,  0.0595,  ...,  0.0100,  0.0323,  0.0233],
        [ 0.0210, -0.0191,  0.0772,  ...,  0.0313,  0.0240,  0.0760],
        ...,
        [-0.0052,  0.0208,  0.0499,  ...,  0.0447,  0.0382,  0.0850],
        [ 0.0191,  0.0084,  0.0622,  ...,  0.0420,  0.0289,  0.0984],
        [ 0.0274,  0.0077,  0.0526,  ...,  0.0355,  0.0537,  0.0398]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9739, 0.9684, 0.9600, 0.9595, 0.9496, 0.9494, 0.9453, 0.9442, 0.9418,
         0.9418],
        [0.9834, 0.9748, 0.9728, 0.9702, 0.9701, 0.9701, 0.9696, 0.9680, 0.9670,
         0.9668],
        [0.9862, 0.9862, 0.9838, 0.9832, 0.9815, 0.9807, 0.9797, 0.9796, 0.9791,
         0.9789],
        [0.9856, 0.9839, 0.9792, 0.9780, 0.9773, 0.9767, 0.9763, 0.9752, 0.9712,
         0.9712],
        [0.9806, 0.9771, 0.9763, 0.9740, 0.9717, 0.9701, 0.9657, 0.9645, 0.9638,
         0.9636],
        [0.9956, 0.9925, 0.9917, 0.9912, 0.9912, 0.9902, 0.9901, 0.9899, 0.9895,
         0.9893],
        [0.9961, 0.9956, 0.9955, 0.9955, 0.9953, 0.9947, 0.9939, 0.9938, 0.9934,
         0.9934],
        [0.9942, 0.9931, 0.9930, 0.9926, 0.9926, 0.9926, 0.9923, 0.9921, 0.9919,
         0.9917],
        [0.9847, 0.9829, 0.9821, 0.9773, 0.9742, 0.9739, 0.9736, 0.9726, 0.9712,
         0.9707],
        [0.9746, 0.9631, 0.9571, 0.9556, 0.9519, 0.9518, 0.9517, 0.9516, 0.9502,
         0.9500],
        [0.9824, 0.9819, 0.9811, 0.9808, 0.9807, 0.9796, 0.9789, 0.9738, 0.9734,
         0.9729],
        [0.9946, 0.9931, 0.9906, 0.9904, 0.9900, 0.9900, 0.9899, 0.9891, 0.9885,
         0.9880],
        [0.9910, 0.9864, 0.9859, 0.9827, 0.9732, 0.9672, 0.9670, 0.9670, 0.9668,
         0.9647],
        [0.9742, 0.9711, 0.9699, 0.9691, 0.9684, 0.9684, 0.9673, 0.9664, 0.9660,
         0.9622],
        [0.9659, 0.9601, 0.9526, 0.9524, 0.9523, 0.9515, 0.9510, 0.9503, 0.9502,
         0.9499],
        [0.9658, 0.9625, 0.9613, 0.9610, 0.9606, 0.9564, 0.9564, 0.9534, 0.9516,
         0.9516],
        [0.9854, 0.9836, 0.9777, 0.9767, 0.9758, 0.9750, 0.9741, 0.9730, 0.9700,
         0.9678],
        [0.9899, 0.9824, 0.9792, 0.9742, 0.9739, 0.9735, 0.9734, 0.9729, 0.9705,
         0.9702],
        [0.9914, 0.9910, 0.9906, 0.9896, 0.9889, 0.9887, 0.9886, 0.9871, 0.9869,
         0.9867],
        [0.9851, 0.9792, 0.9776, 0.9747, 0.9739, 0.9732, 0.9687, 0.9662, 0.9655,
         0.9655],
        [0.9775, 0.9679, 0.9668, 0.9653, 0.9648, 0.9641, 0.9625, 0.9623, 0.9614,
         0.9613],
        [0.9883, 0.9877, 0.9859, 0.9841, 0.9828, 0.9824, 0.9805, 0.9802, 0.9797,
         0.9795],
        [0.9544, 0.9437, 0.9360, 0.9352, 0.9350, 0.9349, 0.9343, 0.9342, 0.9337,
         0.9330],
        [0.9617, 0.9597, 0.9592, 0.9550, 0.9525, 0.9524, 0.9521, 0.9518, 0.9488,
         0.9475],
        [0.9904, 0.9789, 0.9738, 0.9719, 0.9704, 0.9686, 0.9683, 0.9676, 0.9670,
         0.9667],
        [0.9539, 0.9522, 0.9485, 0.9463, 0.9463, 0.9444, 0.9437, 0.9429, 0.9420,
         0.9416],
        [0.9513, 0.9421, 0.9398, 0.9379, 0.9362, 0.9360, 0.9335, 0.9332, 0.9327,
         0.9321],
        [0.9439, 0.9389, 0.9346, 0.9327, 0.9327, 0.9325, 0.9316, 0.9302, 0.9299,
         0.9293],
        [0.9565, 0.9559, 0.9548, 0.9506, 0.9499, 0.9491, 0.9486, 0.9465, 0.9461,
         0.9460],
        [0.9678, 0.9588, 0.9511, 0.9493, 0.9472, 0.9450, 0.9438, 0.9432, 0.9392,
         0.9385],
        [0.9527, 0.9491, 0.9490, 0.9474, 0.9474, 0.9472, 0.9468, 0.9451, 0.9445,
         0.9440],
        [0.9602, 0.9568, 0.9566, 0.9512, 0.9505, 0.9498, 0.9451, 0.9444, 0.9439,
         0.9436],
        [0.9670, 0.9644, 0.9634, 0.9626, 0.9620, 0.9598, 0.9594, 0.9589, 0.9588,
         0.9585],
        [0.9857, 0.9796, 0.9793, 0.9778, 0.9778, 0.9777, 0.9772, 0.9753, 0.9752,
         0.9747],
        [0.9736, 0.9713, 0.9700, 0.9687, 0.9684, 0.9682, 0.9603, 0.9592, 0.9587,
         0.9577],
        [0.9835, 0.9773, 0.9773, 0.9772, 0.9765, 0.9759, 0.9756, 0.9754, 0.9754,
         0.9734],
        [0.9858, 0.9856, 0.9851, 0.9813, 0.9806, 0.9792, 0.9789, 0.9764, 0.9753,
         0.9742],
        [0.9797, 0.9790, 0.9780, 0.9766, 0.9746, 0.9734, 0.9707, 0.9659, 0.9638,
         0.9631],
        [0.9806, 0.9804, 0.9746, 0.9737, 0.9718, 0.9708, 0.9704, 0.9692, 0.9677,
         0.9668],
        [0.9875, 0.9852, 0.9848, 0.9790, 0.9783, 0.9781, 0.9719, 0.9715, 0.9706,
         0.9678],
        [0.9609, 0.9609, 0.9545, 0.9532, 0.9517, 0.9510, 0.9504, 0.9503, 0.9499,
         0.9489],
        [0.9596, 0.9544, 0.9543, 0.9526, 0.9502, 0.9496, 0.9496, 0.9488, 0.9484,
         0.9480],
        [0.9632, 0.9622, 0.9614, 0.9602, 0.9600, 0.9596, 0.9584, 0.9584, 0.9576,
         0.9554],
        [0.9577, 0.9525, 0.9504, 0.9478, 0.9469, 0.9430, 0.9424, 0.9417, 0.9401,
         0.9400],
        [0.9325, 0.9324, 0.9306, 0.9305, 0.9301, 0.9290, 0.9286, 0.9276, 0.9244,
         0.9240],
        [0.9445, 0.9392, 0.9340, 0.9316, 0.9285, 0.9285, 0.9277, 0.9277, 0.9265,
         0.9256],
        [0.9413, 0.9395, 0.9375, 0.9369, 0.9367, 0.9345, 0.9341, 0.9334, 0.9333,
         0.9326],
        [0.9799, 0.9778, 0.9755, 0.9751, 0.9702, 0.9702, 0.9695, 0.9690, 0.9674,
         0.9670],
        [0.9769, 0.9737, 0.9708, 0.9703, 0.9674, 0.9669, 0.9668, 0.9663, 0.9648,
         0.9648],
        [0.9929, 0.9919, 0.9917, 0.9913, 0.9909, 0.9902, 0.9886, 0.9880, 0.9879,
         0.9874],
        [0.9878, 0.9847, 0.9846, 0.9838, 0.9830, 0.9823, 0.9818, 0.9818, 0.9803,
         0.9799],
        [0.9514, 0.9512, 0.9508, 0.9490, 0.9490, 0.9471, 0.9464, 0.9452, 0.9443,
         0.9440],
        [0.9423, 0.9393, 0.9374, 0.9370, 0.9357, 0.9328, 0.9321, 0.9317, 0.9316,
         0.9304],
        [0.9520, 0.9455, 0.9437, 0.9423, 0.9421, 0.9413, 0.9413, 0.9400, 0.9393,
         0.9390],
        [0.9696, 0.9660, 0.9629, 0.9628, 0.9599, 0.9596, 0.9585, 0.9583, 0.9581,
         0.9581],
        [0.9638, 0.9509, 0.9474, 0.9429, 0.9418, 0.9401, 0.9396, 0.9378, 0.9374,
         0.9360],
        [0.9529, 0.9448, 0.9419, 0.9418, 0.9418, 0.9408, 0.9408, 0.9392, 0.9382,
         0.9378],
        [0.9492, 0.9465, 0.9462, 0.9453, 0.9453, 0.9443, 0.9443, 0.9441, 0.9439,
         0.9430],
        [0.9283, 0.9270, 0.9261, 0.9247, 0.9230, 0.9219, 0.9213, 0.9212, 0.9211,
         0.9204],
        [0.9569, 0.9564, 0.9546, 0.9519, 0.9514, 0.9491, 0.9487, 0.9480, 0.9476,
         0.9467],
        [0.9784, 0.9770, 0.9766, 0.9753, 0.9751, 0.9748, 0.9748, 0.9746, 0.9737,
         0.9737],
        [0.9420, 0.9383, 0.9381, 0.9340, 0.9288, 0.9285, 0.9281, 0.9273, 0.9270,
         0.9266],
        [0.9452, 0.9437, 0.9403, 0.9382, 0.9344, 0.9339, 0.9335, 0.9335, 0.9332,
         0.9325],
        [0.9380, 0.9349, 0.9284, 0.9259, 0.9254, 0.9237, 0.9214, 0.9197, 0.9190,
         0.9189]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[1101753.5000, 1019541.5625,  903382.6875,  896903.1250,  779448.8750,
          776418.2500,  732495.3750,  721019.4375,  696542.5625,  696420.3125],
        [1263283.5000, 1115740.5000, 1085102.8750, 1045244.3750, 1043843.6875,
         1043257.5000, 1037245.6875, 1013775.6250,  998068.9375,  995321.9375],
        [1314844.1250, 1313479.2500, 1269306.0000, 1259251.6250, 1228818.2500,
         1215360.7500, 1197895.2500, 1195084.7500, 1187738.0000, 1183429.0000],
        [1302809.0000, 1272146.5000, 1189591.5000, 1168048.8750, 1157487.7500,
         1147925.8750, 1139887.1250, 1123597.2500, 1060102.8750, 1060102.8750],
        [1212348.5000, 1153478.7500, 1141499.2500, 1104343.5000, 1067589.3750,
         1044660.3125,  979939.5000,  964334.8125,  954722.0625,  951974.5000],
        [1503596.7500, 1437215.1250, 1421009.0000, 1411651.2500, 1411266.2500,
         1390333.0000, 1389806.7500, 1385723.7500, 1377704.3750, 1374337.0000],
        [1514405.7500, 1502702.2500, 1501269.8750, 1499638.5000, 1497060.7500,
         1483657.7500, 1467251.0000, 1465602.2500, 1456279.8750, 1455793.8750],
        [1473656.8750, 1450527.7500, 1448980.6250, 1440542.7500, 1439724.1250,
         1439235.5000, 1433956.7500, 1429119.7500, 1425497.6250, 1421320.7500],
        [1285760.7500, 1253969.1250, 1239381.2500, 1156800.2500, 1106237.6250,
         1101897.5000, 1097495.8750, 1082691.2500, 1061222.6250, 1052663.0000],
        [1113306.5000,  945017.5625,  867328.3750,  849030.0000,  805135.1250,
          804134.5000,  802646.5625,  801980.8750,  785789.1250,  783247.6250],
        [1244088.3750, 1235556.2500, 1222217.2500, 1216493.6250, 1215145.1250,
         1195752.7500, 1184474.6250, 1100250.0000, 1093671.7500, 1086129.8750],
        [1481559.3750, 1449174.1250, 1399979.2500, 1394884.3750, 1387622.8750,
         1387323.8750, 1385579.7500, 1370132.2500, 1358667.5000, 1348862.0000],
        [1406721.8750, 1317271.3750, 1308665.3750, 1249453.2500, 1091570.1250,
         1001380.1250,  998774.5000,  998586.8750,  996521.5625,  966303.0625],
        [1107449.5000, 1058730.8750, 1040568.7500, 1028716.5000, 1019494.8750,
         1018663.9375, 1003326.3750,  990037.5625,  984779.2500,  932750.9375],
        [ 983105.1875,  905343.0625,  812728.5000,  810724.2500,  809133.8750,
          800151.9375,  794518.9375,  786905.0000,  785309.6875,  782531.5625],
        [ 981299.2500,  936076.6250,  921245.9375,  916264.0000,  911711.3125,
          858789.0625,  858478.6875,  822255.3125,  801768.2500,  801055.1875],
        [1299006.5000, 1266924.6250, 1163967.7500, 1146558.3750, 1132354.7500,
         1120202.3750, 1104624.6250, 1088633.1250, 1042385.3125, 1009918.6250],
        [1385627.3750, 1244395.7500, 1188320.3750, 1107278.3750, 1102428.2500,
         1095542.5000, 1095168.5000, 1086417.8750, 1049569.5000, 1044981.1875],
        [1415482.5000, 1407301.5000, 1399982.0000, 1379517.2500, 1365087.3750,
         1361696.3750, 1360050.7500, 1330953.3750, 1327743.5000, 1323803.6250],
        [1294305.0000, 1188355.3750, 1162346.0000, 1114707.7500, 1102492.3750,
         1090625.2500, 1023969.3125,  986798.6875,  977895.8750,  977344.8750],
        [1160919.1250, 1011999.2500,  996176.5625,  974857.5625,  968025.1250,
          958791.5625,  936912.5625,  933964.1250,  921607.1250,  920906.0625],
        [1354479.7500, 1342506.8750, 1308420.8750, 1275335.2500, 1251039.0000,
         1244824.2500, 1212024.8750, 1205945.2500, 1198274.6250, 1193709.8750],
        [ 833804.8750,  715804.3125,  641813.6250,  634040.4375,  632457.6250,
          631312.0625,  625601.5625,  625398.1250,  620559.6250,  614251.8125],
        [ 926486.5000,  900251.2500,  893025.6875,  841798.7500,  812124.1250,
          810272.8750,  807450.3125,  803623.1250,  769961.8750,  755877.8125],
        [1395975.6250, 1183613.1250, 1100332.8750, 1070760.5000, 1048321.0625,
         1021617.5625, 1017258.1875, 1007529.0625,  998108.8750,  994530.6250],
        [ 827758.1250,  808544.5000,  766647.7500,  743366.6875,  742828.8125,
          722804.0000,  716121.8125,  707719.3125,  698509.5625,  694947.4375],
        [ 798195.5000,  699792.4375,  677014.1875,  659195.9375,  643686.8125,
          641060.5625,  618559.0000,  616142.7500,  611588.1250,  606998.9375],
        [ 718532.5000,  668481.2500,  629113.5000,  612028.1250,  612013.5000,
          610069.4375,  602339.0000,  590599.1250,  588214.1250,  583050.2500],
        [ 860107.8125,  852188.0000,  838860.0625,  790615.7500,  782253.2500,
          773052.8125,  767570.2500,  744717.6875,  740556.8750,  739950.4375],
        [1010755.0000,  888390.3125,  795877.1250,  775624.8125,  753142.0625,
          729277.9375,  717259.1250,  710569.8750,  671376.6875,  664851.5625],
        [ 813958.6875,  773565.3125,  772796.2500,  755220.6875,  754469.1250,
          753170.8125,  748829.1250,  730270.3750,  724291.0000,  719279.8750],
        [ 906750.6250,  862845.3125,  860655.0625,  796585.6875,  789430.6250,
          781366.7500,  730698.8125,  723616.5000,  717620.3750,  715373.0000],
        [ 998410.6875,  962629.4375,  948725.3125,  938138.3750,  929521.1875,
          900915.1875,  895541.5625,  890163.6250,  888065.0625,  884814.5000],
        [1304873.1250, 1196492.0000, 1191119.5000, 1165348.3750, 1165100.6250,
         1163383.0000, 1155276.5000, 1124954.7500, 1122243.6250, 1115625.6250],
        [1097822.5000, 1062425.6250, 1042582.2500, 1023906.8125, 1019421.0000,
         1016503.7500,  908254.0000,  893402.1875,  886611.1875,  874738.8750],
        [1264885.6250, 1157033.0000, 1157033.0000, 1156113.1250, 1143368.8750,
         1134585.8750, 1129159.5000, 1126259.0000, 1125838.0000, 1094304.1250],
        [1306909.3750, 1302966.8750, 1293906.2500, 1225017.8750, 1212857.3750,
         1188950.6250, 1183204.5000, 1142100.2500, 1124761.5000, 1106403.2500],
        [1196748.7500, 1186386.3750, 1168445.5000, 1144832.0000, 1113591.0000,
         1094614.1250, 1052478.2500,  982615.0000,  954625.5625,  944784.1875],
        [1212697.7500, 1209816.8750, 1113712.1250, 1099056.5000, 1070040.7500,
         1055162.7500, 1048319.0625, 1030892.8125, 1009345.7500,  995613.3750],
        [1337864.8750, 1294577.7500, 1287665.2500, 1185432.8750, 1173202.1250,
         1171083.7500, 1070727.8750, 1065684.0000, 1051010.8750, 1010173.9375],
        [ 915755.5625,  915755.5625,  835515.5000,  820333.2500,  802146.1250,
          794750.8125,  787858.0000,  786378.3750,  782069.8125,  771279.5625],
        [ 899031.2500,  834508.8750,  833336.6875,  812689.7500,  785428.8125,
          778621.2500,  778469.0000,  770066.8125,  765998.7500,  761007.4375],
        [ 946653.8750,  931988.8750,  922191.8125,  906642.5000,  904274.7500,
          898914.6250,  883609.5000,  883262.3750,  873692.5000,  846312.9375],
        [ 874624.6250,  811395.6250,  787704.6875,  759404.5625,  749421.3750,
          709230.7500,  702344.3750,  695392.2500,  679630.2500,  679420.9375],
        [ 610538.6250,  609610.6250,  593547.6250,  592923.6250,  589885.3750,
          580410.0625,  576762.2500,  568887.0625,  543773.1250,  540091.2500],
        [ 724196.3750,  671690.5000,  623720.8125,  602129.8750,  576568.6250,
          575883.8750,  570034.0000,  569291.9375,  560260.1250,  552699.8750],
        [ 692313.3750,  674165.7500,  655241.0000,  650146.8125,  648131.7500,
          628234.5625,  624395.1250,  618262.9375,  616920.0625,  611322.8750],
        [1200548.5000, 1165454.0000, 1128000.2500, 1121876.6250, 1046132.8750,
         1045535.4375, 1035220.8125, 1027330.1875, 1005183.4375,  999087.0000],
        [1150860.3750, 1099010.3750, 1054899.1250, 1047662.4375, 1005025.3125,
          997987.0625,  995697.8750,  989377.7500,  967720.5000,  967608.8750],
        [1446383.6250, 1424592.6250, 1421491.5000, 1413312.2500, 1405214.7500,
         1392100.2500, 1360484.0000, 1348748.7500, 1345648.6250, 1336887.8750],
        [1344636.5000, 1285306.0000, 1284791.2500, 1270457.7500, 1255417.0000,
         1242108.6250, 1233560.7500, 1233200.7500, 1207452.8750, 1200495.8750],
        [ 799287.0625,  797313.0000,  792936.1250,  772270.2500,  772025.7500,
          752045.3750,  744445.7500,  731034.8125,  721759.7500,  718614.7500],
        [ 702242.6250,  672246.7500,  654770.0000,  650506.5625,  638745.1875,
          612538.4375,  606788.2500,  603285.8125,  602650.3750,  591967.1250],
        [ 806013.2500,  734824.6250,  715759.2500,  701565.8750,  699500.8125,
          691687.0625,  691559.1250,  679099.6250,  672854.1250,  669389.0000],
        [1035952.6250,  985074.2500,  942353.6250,  940507.7500,  901798.0000,
          898774.0000,  884912.4375,  882198.3750,  879773.6250,  878986.1250],
        [ 953994.8125,  793652.6250,  754517.3125,  707678.8750,  696536.5625,
          680541.4375,  675631.9375,  657742.1875,  654582.6875,  641696.6875],
        [ 816367.1250,  727732.1250,  697667.3750,  696989.6875,  696448.8125,
          686998.4375,  686991.9375,  671244.8125,  661678.1250,  657995.6875],
        [ 774679.3125,  745021.7500,  742122.1250,  732624.6875,  732623.3125,
          722385.0625,  722295.4375,  719970.9375,  718381.1250,  708829.1250],
        [ 574855.0625,  564162.8750,  557187.2500,  545641.6250,  532921.5625,
          524564.3750,  520133.7812,  519038.7188,  518305.1562,  513419.9688],
        [ 864772.1250,  858036.6875,  836998.1250,  805350.8750,  799524.9375,
          773644.2500,  768831.1250,  761183.8750,  757062.3750,  747112.8750],
        [1175692.0000, 1151762.8750, 1146396.5000, 1124907.5000, 1121521.5000,
         1117001.0000, 1116358.8750, 1113943.6250, 1099627.8750, 1098327.2500],
        [ 698631.4375,  663074.1875,  661059.3750,  622929.0625,  578956.1250,
          575973.4375,  573260.3125,  566272.1250,  563848.7500,  561048.7500],
        [ 731179.1250,  716029.6250,  681949.3125,  661695.1875,  627328.1250,
          622486.0625,  618895.9375,  618816.8750,  616539.5000,  610206.8125],
        [ 660404.0000,  631253.6875,  575736.7500,  555367.0625,  551090.3750,
          538427.5625,  520632.5312,  507874.1875,  503293.6562,  502689.7812]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[1101753.5000,       0.0000],
         [1019541.5625,       0.0000],
         [ 903382.6875,       0.0000],
         ...,
         [ 721019.4375,       0.0000],
         [ 696542.5625,       0.0000],
         [ 696420.3125,       0.0000]],

        [[1263283.5000,       0.0000],
         [      0.0000, 1115740.5000],
         [1085102.8750,       0.0000],
         ...,
         [1013775.6250,       0.0000],
         [ 998068.9375,       0.0000],
         [ 995321.9375,       0.0000]],

        [[1314844.1250,       0.0000],
         [1313479.2500,       0.0000],
         [1269306.0000,       0.0000],
         ...,
         [1195084.7500,       0.0000],
         [1187738.0000,       0.0000],
         [1183429.0000,       0.0000]],

        ...,

        [[ 698631.4375,       0.0000],
         [ 663074.1875,       0.0000],
         [ 661059.3750,       0.0000],
         ...,
         [ 566272.1250,       0.0000],
         [ 563848.7500,       0.0000],
         [ 561048.7500,       0.0000]],

        [[ 731179.1250,       0.0000],
         [ 716029.6250,       0.0000],
         [ 681949.3125,       0.0000],
         ...,
         [ 618816.8750,       0.0000],
         [ 616539.5000,       0.0000],
         [ 610206.8125,       0.0000]],

        [[ 660404.0000,       0.0000],
         [ 631253.6875,       0.0000],
         [ 575736.7500,       0.0000],
         ...,
         [ 507874.1875,       0.0000],
         [ 503293.6562,       0.0000],
         [ 502689.7812,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 8323925.5000,        0.0000],
        [ 9525144.0000,  1115740.5000],
        [11149846.0000,  1215360.7500],
        [11621700.0000,        0.0000],
        [ 8518573.0000,  2056318.0000],
        [14102643.0000,        0.0000],
        [13344023.0000,  1499638.5000],
        [12928906.0000,  1473656.8750],
        [10331882.0000,  1106237.6250],
        [ 8557616.0000,        0.0000],
        [10577286.0000,  1216493.6250],
        [13963786.0000,        0.0000],
        [11335248.0000,        0.0000],
        [10184518.0000,        0.0000],
        [ 8270452.0000,        0.0000],
        [ 8007175.0000,   801768.2500],
        [11374576.0000,        0.0000],
        [11399730.0000,        0.0000],
        [10932050.0000,  2739568.0000],
        [ 9940944.0000,   977895.8750],
        [ 8787982.0000,   996176.5625],
        [11335522.0000,  1251039.0000],
        [ 6575044.5000,        0.0000],
        [ 7550910.5000,   769961.8750],
        [10838048.0000,        0.0000],
        [ 6730738.5000,   698509.5625],
        [ 5895220.0000,   677014.1875],
        [ 6214441.0000,        0.0000],
        [ 7889872.5000,        0.0000],
        [ 7717125.0000,        0.0000],
        [ 7545851.5000,        0.0000],
        [ 7884943.0000,        0.0000],
        [ 8348860.0000,   888065.0625],
        [11704417.0000,        0.0000],
        [ 7912845.0000,  1912823.2500],
        [11488580.0000,        0.0000],
        [12087078.0000,        0.0000],
        [ 9652734.0000,  1186386.3750],
        [ 8721600.0000,  2123058.0000],
        [11647423.0000,        0.0000],
        [ 6645812.0000,  1566030.3750],
        [ 8019158.5000,        0.0000],
        [ 8997544.0000,        0.0000],
        [ 7448569.0000,        0.0000],
        [ 5806430.0000,        0.0000],
        [ 6026476.0000,        0.0000],
        [ 6419134.0000,        0.0000],
        [ 7546364.5000,  3228005.0000],
        [ 9286472.0000,   989377.7500],
        [12489649.0000,  1405214.7500],
        [ 8759555.0000,  3797872.2500],
        [ 7601732.5000,        0.0000],
        [ 5685235.0000,   650506.5625],
        [ 7062253.0000,        0.0000],
        [ 8345418.5000,   884912.4375],
        [ 7216575.0000,        0.0000],
        [ 7000113.5000,        0.0000],
        [ 5821958.0000,  1496974.7500],
        [ 5370230.5000,        0.0000],
        [ 7972518.0000,        0.0000],
        [11265539.0000,        0.0000],
        [ 6065053.0000,        0.0000],
        [ 6505126.5000,        0.0000],
        [ 5546769.0000,        0.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 1/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<31:14, 64.64s/it]  7%|▋         | 2/30 [01:05<12:39, 27.11s/it] 10%|█         | 3/30 [01:06<06:46, 15.07s/it] 13%|█▎        | 4/30 [01:06<04:04,  9.42s/it] 17%|█▋        | 5/30 [01:07<02:37,  6.29s/it] 20%|██        | 6/30 [01:08<01:45,  4.41s/it] 23%|██▎       | 7/30 [01:09<01:13,  3.21s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.43s/it] 30%|███       | 9/30 [01:10<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:11<00:30,  1.55s/it] 37%|███▋      | 11/30 [01:12<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:14,  1.07it/s] 50%|█████     | 15/30 [01:15<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:18<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:21<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:24<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.89s/it]
Epoch loss is 7.611055739720663
Epoch 2/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:47, 59.59s/it]  7%|▋         | 2/30 [01:02<12:09, 26.06s/it] 10%|█         | 3/30 [01:02<06:31, 14.50s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.07s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.07s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:07<00:38,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 6.7713282108306885
Epoch 3/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:49, 61.71s/it]  7%|▋         | 2/30 [01:02<12:10, 26.07s/it] 10%|█         | 3/30 [01:03<06:31, 14.51s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.08s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.07s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 6.643307097752889
Epoch 4/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:15, 60.55s/it]  7%|▋         | 2/30 [01:01<11:50, 25.37s/it] 10%|█         | 3/30 [01:02<06:21, 14.13s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 6.630094782511393
Epoch 5/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:55, 63.99s/it]  7%|▋         | 2/30 [01:04<12:30, 26.79s/it] 10%|█         | 3/30 [01:05<06:42, 14.90s/it] 13%|█▎        | 4/30 [01:06<04:02,  9.31s/it] 17%|█▋        | 5/30 [01:06<02:35,  6.22s/it] 20%|██        | 6/30 [01:07<01:44,  4.36s/it] 23%|██▎       | 7/30 [01:08<01:13,  3.18s/it] 27%|██▋       | 8/30 [01:09<00:52,  2.41s/it] 30%|███       | 9/30 [01:09<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.33it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.86s/it]
Epoch loss is 6.633786948521932
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0861, -0.0246,  0.0690,  ..., -0.0053,  0.0013,  0.0755],
        [ 0.0757, -0.0344,  0.0587,  ..., -0.0035,  0.0072,  0.0233],
        [ 0.0202, -0.0130,  0.0872,  ...,  0.0446,  0.0291,  0.0869],
        ...,
        [ 0.0165,  0.0092,  0.0564,  ...,  0.0362,  0.0214,  0.0982],
        [ 0.0310, -0.0020,  0.0661,  ...,  0.0298,  0.0167,  0.1085],
        [ 0.0461, -0.0059,  0.0609,  ...,  0.0149,  0.0344,  0.0499]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9825, 0.9806, 0.9730, 0.9690, 0.9629, 0.9614, 0.9606, 0.9586, 0.9585,
         0.9580],
        [0.9901, 0.9856, 0.9852, 0.9841, 0.9831, 0.9820, 0.9813, 0.9804, 0.9788,
         0.9788],
        [0.9917, 0.9911, 0.9908, 0.9893, 0.9887, 0.9883, 0.9877, 0.9870, 0.9870,
         0.9868],
        [0.9926, 0.9920, 0.9890, 0.9884, 0.9883, 0.9881, 0.9865, 0.9843, 0.9841,
         0.9841],
        [0.9880, 0.9861, 0.9854, 0.9849, 0.9817, 0.9812, 0.9804, 0.9797, 0.9789,
         0.9787],
        [0.9974, 0.9955, 0.9952, 0.9952, 0.9949, 0.9946, 0.9944, 0.9936, 0.9931,
         0.9930],
        [0.9977, 0.9974, 0.9972, 0.9972, 0.9970, 0.9969, 0.9964, 0.9961, 0.9960,
         0.9959],
        [0.9962, 0.9953, 0.9952, 0.9950, 0.9946, 0.9945, 0.9944, 0.9943, 0.9942,
         0.9942],
        [0.9906, 0.9881, 0.9880, 0.9860, 0.9848, 0.9826, 0.9813, 0.9810, 0.9801,
         0.9791],
        [0.9834, 0.9788, 0.9752, 0.9717, 0.9694, 0.9655, 0.9630, 0.9628, 0.9615,
         0.9610],
        [0.9902, 0.9890, 0.9890, 0.9888, 0.9881, 0.9875, 0.9871, 0.9833, 0.9828,
         0.9821],
        [0.9958, 0.9947, 0.9935, 0.9935, 0.9930, 0.9924, 0.9923, 0.9923, 0.9918,
         0.9907],
        [0.9944, 0.9914, 0.9907, 0.9883, 0.9814, 0.9796, 0.9787, 0.9757, 0.9754,
         0.9749],
        [0.9861, 0.9831, 0.9828, 0.9826, 0.9824, 0.9822, 0.9811, 0.9807, 0.9779,
         0.9776],
        [0.9793, 0.9731, 0.9730, 0.9724, 0.9708, 0.9692, 0.9679, 0.9678, 0.9672,
         0.9665],
        [0.9786, 0.9766, 0.9760, 0.9753, 0.9725, 0.9714, 0.9704, 0.9702, 0.9694,
         0.9688],
        [0.9902, 0.9868, 0.9862, 0.9849, 0.9843, 0.9842, 0.9840, 0.9838, 0.9807,
         0.9798],
        [0.9945, 0.9898, 0.9857, 0.9845, 0.9826, 0.9820, 0.9818, 0.9814, 0.9813,
         0.9806],
        [0.9946, 0.9931, 0.9929, 0.9927, 0.9927, 0.9923, 0.9921, 0.9912, 0.9912,
         0.9907],
        [0.9884, 0.9875, 0.9851, 0.9849, 0.9842, 0.9818, 0.9803, 0.9796, 0.9780,
         0.9758],
        [0.9846, 0.9799, 0.9779, 0.9762, 0.9760, 0.9758, 0.9757, 0.9740, 0.9739,
         0.9732],
        [0.9914, 0.9912, 0.9907, 0.9898, 0.9874, 0.9866, 0.9857, 0.9857, 0.9853,
         0.9849],
        [0.9695, 0.9622, 0.9585, 0.9572, 0.9570, 0.9568, 0.9567, 0.9564, 0.9553,
         0.9553],
        [0.9784, 0.9778, 0.9759, 0.9753, 0.9710, 0.9704, 0.9696, 0.9691, 0.9691,
         0.9684],
        [0.9922, 0.9835, 0.9816, 0.9813, 0.9793, 0.9782, 0.9781, 0.9768, 0.9761,
         0.9751],
        [0.9703, 0.9686, 0.9685, 0.9675, 0.9670, 0.9652, 0.9648, 0.9636, 0.9635,
         0.9624],
        [0.9661, 0.9618, 0.9591, 0.9591, 0.9553, 0.9547, 0.9540, 0.9532, 0.9527,
         0.9526],
        [0.9651, 0.9626, 0.9623, 0.9613, 0.9600, 0.9599, 0.9590, 0.9588, 0.9580,
         0.9552],
        [0.9763, 0.9733, 0.9722, 0.9687, 0.9683, 0.9681, 0.9673, 0.9671, 0.9669,
         0.9668],
        [0.9761, 0.9674, 0.9649, 0.9647, 0.9622, 0.9621, 0.9620, 0.9607, 0.9605,
         0.9604],
        [0.9722, 0.9710, 0.9675, 0.9668, 0.9668, 0.9668, 0.9666, 0.9654, 0.9652,
         0.9645],
        [0.9754, 0.9745, 0.9744, 0.9685, 0.9674, 0.9669, 0.9666, 0.9665, 0.9634,
         0.9631],
        [0.9808, 0.9802, 0.9788, 0.9783, 0.9781, 0.9765, 0.9764, 0.9761, 0.9761,
         0.9744],
        [0.9916, 0.9890, 0.9882, 0.9880, 0.9869, 0.9865, 0.9864, 0.9861, 0.9858,
         0.9857],
        [0.9844, 0.9838, 0.9818, 0.9783, 0.9776, 0.9776, 0.9771, 0.9770, 0.9762,
         0.9748],
        [0.9892, 0.9863, 0.9861, 0.9854, 0.9844, 0.9820, 0.9815, 0.9815, 0.9807,
         0.9804],
        [0.9910, 0.9893, 0.9885, 0.9882, 0.9860, 0.9857, 0.9847, 0.9841, 0.9830,
         0.9827],
        [0.9874, 0.9872, 0.9850, 0.9848, 0.9847, 0.9829, 0.9804, 0.9780, 0.9775,
         0.9754],
        [0.9870, 0.9867, 0.9845, 0.9837, 0.9826, 0.9825, 0.9824, 0.9814, 0.9807,
         0.9806],
        [0.9925, 0.9897, 0.9880, 0.9877, 0.9856, 0.9852, 0.9819, 0.9815, 0.9807,
         0.9770],
        [0.9741, 0.9741, 0.9725, 0.9712, 0.9687, 0.9681, 0.9677, 0.9670, 0.9668,
         0.9664],
        [0.9724, 0.9709, 0.9702, 0.9701, 0.9698, 0.9686, 0.9681, 0.9668, 0.9667,
         0.9649],
        [0.9790, 0.9770, 0.9769, 0.9753, 0.9746, 0.9735, 0.9734, 0.9731, 0.9714,
         0.9714],
        [0.9738, 0.9727, 0.9725, 0.9689, 0.9683, 0.9667, 0.9659, 0.9652, 0.9634,
         0.9631],
        [0.9548, 0.9531, 0.9510, 0.9502, 0.9495, 0.9489, 0.9483, 0.9481, 0.9474,
         0.9471],
        [0.9687, 0.9663, 0.9590, 0.9571, 0.9570, 0.9564, 0.9563, 0.9549, 0.9547,
         0.9528],
        [0.9614, 0.9609, 0.9609, 0.9601, 0.9584, 0.9581, 0.9570, 0.9568, 0.9565,
         0.9564],
        [0.9872, 0.9867, 0.9853, 0.9836, 0.9809, 0.9801, 0.9800, 0.9797, 0.9796,
         0.9786],
        [0.9884, 0.9855, 0.9838, 0.9826, 0.9826, 0.9807, 0.9803, 0.9796, 0.9795,
         0.9792],
        [0.9956, 0.9953, 0.9950, 0.9939, 0.9933, 0.9926, 0.9923, 0.9923, 0.9921,
         0.9921],
        [0.9927, 0.9914, 0.9907, 0.9903, 0.9897, 0.9889, 0.9881, 0.9874, 0.9873,
         0.9866],
        [0.9678, 0.9673, 0.9660, 0.9655, 0.9648, 0.9645, 0.9640, 0.9639, 0.9638,
         0.9630],
        [0.9653, 0.9643, 0.9633, 0.9629, 0.9625, 0.9621, 0.9600, 0.9599, 0.9599,
         0.9593],
        [0.9711, 0.9691, 0.9681, 0.9658, 0.9636, 0.9635, 0.9634, 0.9624, 0.9621,
         0.9615],
        [0.9810, 0.9778, 0.9774, 0.9752, 0.9744, 0.9741, 0.9737, 0.9731, 0.9729,
         0.9727],
        [0.9759, 0.9695, 0.9682, 0.9643, 0.9634, 0.9633, 0.9614, 0.9605, 0.9599,
         0.9597],
        [0.9664, 0.9653, 0.9642, 0.9626, 0.9619, 0.9619, 0.9616, 0.9614, 0.9601,
         0.9598],
        [0.9688, 0.9684, 0.9676, 0.9664, 0.9662, 0.9660, 0.9658, 0.9657, 0.9655,
         0.9650],
        [0.9571, 0.9566, 0.9520, 0.9520, 0.9508, 0.9491, 0.9486, 0.9485, 0.9484,
         0.9481],
        [0.9749, 0.9737, 0.9696, 0.9671, 0.9671, 0.9670, 0.9668, 0.9667, 0.9662,
         0.9657],
        [0.9872, 0.9857, 0.9856, 0.9851, 0.9850, 0.9844, 0.9843, 0.9842, 0.9841,
         0.9839],
        [0.9622, 0.9596, 0.9590, 0.9581, 0.9538, 0.9527, 0.9526, 0.9521, 0.9520,
         0.9519],
        [0.9678, 0.9631, 0.9603, 0.9590, 0.9585, 0.9585, 0.9577, 0.9568, 0.9566,
         0.9560],
        [0.9584, 0.9575, 0.9558, 0.9531, 0.9513, 0.9509, 0.9483, 0.9483, 0.9474,
         0.9469]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 1, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[1246254.3750, 1213594.5000, 1088050.8750, 1027898.6250,  942337.5000,
          921852.3750,  912033.1250,  885576.8125,  884996.0000,  878333.3125],
        [1389741.7500, 1303618.1250, 1295798.1250, 1275037.2500, 1256252.8750,
         1236823.7500, 1225964.5000, 1208708.6250, 1182753.2500, 1181486.1250],
        [1421101.1250, 1408693.8750, 1404014.5000, 1374114.2500, 1362308.1250,
         1354558.5000, 1342646.5000, 1329303.1250, 1328796.1250, 1324700.3750],
        [1439864.3750, 1427164.0000, 1367914.0000, 1354990.1250, 1353371.8750,
         1350863.8750, 1319125.6250, 1278350.2500, 1275444.7500, 1275444.7500],
        [1348239.6250, 1312731.6250, 1299429.0000, 1289142.2500, 1232500.0000,
         1222983.2500, 1209440.7500, 1197811.7500, 1183394.1250, 1180706.6250],
        [1542123.0000, 1499764.3750, 1494604.2500, 1493519.8750, 1487890.1250,
         1481388.5000, 1477614.1250, 1460414.6250, 1449404.8750, 1447758.1250],
        [1549185.2500, 1541711.2500, 1538527.0000, 1537136.6250, 1533875.5000,
         1531054.8750, 1519995.1250, 1512551.0000, 1511977.0000, 1509611.2500],
        [1515752.2500, 1496031.7500, 1494025.6250, 1490996.6250, 1482030.0000,
         1479524.8750, 1477757.8750, 1474383.6250, 1472301.3750, 1472106.1250],
        [1399489.3750, 1349405.0000, 1347630.2500, 1310278.8750, 1287449.1250,
         1248018.2500, 1224859.0000, 1220354.8750, 1203991.7500, 1187801.5000],
        [1262329.7500, 1181729.5000, 1122555.1250, 1068182.1250, 1033554.6250,
          977391.5000,  943473.2500,  940011.8750,  923216.9375,  916363.6250],
        [1392091.0000, 1367818.7500, 1367346.6250, 1363189.2500, 1350485.1250,
         1338219.6250, 1330784.6250, 1259846.2500, 1251555.7500, 1239435.5000],
        [1506463.1250, 1483925.2500, 1458474.5000, 1457764.0000, 1447014.2500,
         1434977.2500, 1434348.0000, 1433754.3750, 1423696.1250, 1402102.3750],
        [1476967.5000, 1415826.8750, 1401720.0000, 1353768.2500, 1226567.8750,
         1195643.3750, 1181227.0000, 1130390.8750, 1126060.2500, 1117384.5000],
        [1311673.0000, 1257090.6250, 1252180.1250, 1248777.8750, 1244705.5000,
         1241845.7500, 1221664.8750, 1214279.7500, 1167724.6250, 1161810.7500],
        [1190103.2500, 1089774.7500, 1087604.8750, 1078567.0000, 1054363.0000,
         1030461.3125, 1011392.3125, 1009731.8125, 1001459.3750,  992144.3125],
        [1178813.1250, 1145053.7500, 1136207.8750, 1123857.7500, 1080198.5000,
         1063814.6250, 1048690.1250, 1044828.7500, 1033066.8750, 1024671.7500],
        [1390363.5000, 1325452.2500, 1313151.1250, 1289504.8750, 1278222.2500,
         1276837.0000, 1273831.6250, 1269508.1250, 1214051.7500, 1198936.3750],
        [1478923.8750, 1383278.5000, 1304221.1250, 1282174.2500, 1248031.2500,
         1236583.1250, 1234220.8750, 1227287.5000, 1224498.1250, 1212593.7500],
        [1482331.1250, 1450429.5000, 1445375.7500, 1442129.0000, 1441251.8750,
         1434286.3750, 1428930.3750, 1411780.5000, 1410516.8750, 1401586.3750],
        [1355127.0000, 1339279.2500, 1293865.6250, 1290431.2500, 1276312.2500,
         1233160.7500, 1207956.1250, 1195903.3750, 1168008.6250, 1132577.1250],
        [1284379.6250, 1200538.1250, 1167241.5000, 1139091.6250, 1136271.7500,
         1131953.0000, 1130399.5000, 1104005.5000, 1102264.3750, 1091880.3750],
        [1415456.8750, 1410749.5000, 1401515.5000, 1383457.8750, 1337473.2500,
         1321206.7500, 1304441.3750, 1304319.5000, 1297884.6250, 1290028.8750],
        [1034406.6250,  932108.0000,  885190.0625,  868767.1875,  865256.3750,
          863326.7500,  861984.1875,  858016.2500,  845033.8125,  845033.8125],
        [1174716.8750, 1166114.3750, 1133745.3750, 1124332.5000, 1057928.5000,
         1048485.0625, 1036869.8750, 1029487.8750, 1029066.7500, 1018261.8125],
        [1431214.7500, 1263802.7500, 1230411.8750, 1225419.7500, 1190418.7500,
         1171949.6250, 1169768.7500, 1149309.3750, 1137745.3750, 1121916.2500],
        [1047168.9375, 1022434.3750, 1020878.3750, 1006613.8125,  999121.2500,
          973952.5000,  967361.6250,  951192.2500,  950588.2500,  935069.3125],
        [ 985378.6250,  927021.2500,  892578.6875,  891799.2500,  845632.8125,
          838343.3750,  829844.8125,  820040.7500,  814535.6875,  812782.0000],
        [ 971670.1875,  937346.8750,  933843.9375,  920615.3125,  904213.5625,
          902381.2500,  891334.1875,  888552.1250,  878666.8125,  844155.0625],
        [1140213.2500, 1093280.7500, 1075912.3750, 1023468.5000, 1017535.6875,
         1015311.1250, 1002914.1250, 1000890.3750,  996648.8125,  995388.3125],
        [1137155.2500, 1003970.6250,  969372.9375,  966702.1875,  932833.6250,
          931774.6875,  930291.0000,  913208.1250,  910846.6250,  908980.9375],
        [1076083.6250, 1057223.5000, 1005568.8750,  996526.2500,  996418.8750,
          996112.0000,  992378.0000,  976604.1875,  972922.0625,  964277.8750],
        [1125375.3750, 1111544.2500, 1110585.3750, 1020537.6875, 1004517.4375,
          997613.0625,  993461.3125,  991443.3750,  949009.4375,  945087.8750],
        [1217084.2500, 1206223.6250, 1181965.0000, 1174350.5000, 1170272.1250,
         1143293.5000, 1142794.2500, 1137812.6250, 1137319.0000, 1109890.7500],
        [1419776.2500, 1366672.6250, 1351532.6250, 1349038.2500, 1327340.8750,
         1319782.5000, 1317836.8750, 1311301.5000, 1305897.7500, 1305333.7500],
        [1280452.6250, 1269462.1250, 1234665.8750, 1173067.8750, 1162832.7500,
         1162468.0000, 1153428.1250, 1152902.5000, 1138652.7500, 1116638.8750],
        [1372119.7500, 1316142.5000, 1311895.6250, 1298576.6250, 1280688.2500,
         1237397.1250, 1227851.7500, 1227851.7500, 1214221.8750, 1210092.6250],
        [1407747.2500, 1373613.6250, 1358399.3750, 1352115.3750, 1310020.2500,
         1304101.7500, 1285546.2500, 1274656.7500, 1255036.2500, 1249947.8750],
        [1336644.3750, 1333353.2500, 1291147.6250, 1288834.8750, 1285764.5000,
         1254085.1250, 1209911.5000, 1168743.0000, 1160943.5000, 1125515.8750],
        [1329981.5000, 1324052.5000, 1282874.0000, 1267038.2500, 1248034.8750,
         1246536.1250, 1245051.0000, 1226168.0000, 1214751.2500, 1212341.6250],
        [1437001.2500, 1382290.8750, 1348850.3750, 1343034.5000, 1302042.7500,
         1295208.8750, 1235385.5000, 1227916.2500, 1215109.2500, 1152031.0000],
        [1104738.5000, 1104738.5000, 1080984.8750, 1061268.1250, 1023200.0625,
         1014405.1875, 1008568.2500,  998308.8125,  996131.0000,  990761.0000],
        [1078934.1250, 1055799.8750, 1045326.0625, 1043674.5000, 1039106.0000,
         1021263.0000, 1014995.5000,  995817.5000,  994162.6875,  969572.6875],
        [1184780.7500, 1152224.3750, 1151121.6250, 1124821.6250, 1112661.1250,
         1096593.0000, 1095028.6250, 1089972.2500, 1063907.0000, 1063772.0000],
        [1101041.3750, 1083194.2500, 1080574.6250, 1026853.1875, 1017146.6875,
          994319.1250,  983473.7500,  974004.5000,  949033.9375,  944872.5000],
        [ 839196.0625,  819018.3750,  794930.4375,  785840.8750,  778110.5000,
          771083.9375,  764946.1250,  762075.8125,  754571.3125,  751474.0000],
        [1023930.2500,  989194.7500,  891466.8125,  867210.1250,  866002.6875,
          858744.8125,  856669.6250,  840641.9375,  838392.9375,  815949.1250],
        [ 922081.0000,  915701.3750,  915200.2500,  905013.3125,  883063.6250,
          879647.8125,  866399.1875,  863603.4375,  860114.3750,  858534.3125],
        [1332800.1250, 1322621.2500, 1297564.0000, 1266470.5000, 1218322.2500,
         1204124.8750, 1202147.8750, 1197503.3750, 1195236.3750, 1178940.1250],
        [1356879.2500, 1301654.1250, 1270478.2500, 1248742.1250, 1247418.5000,
         1215067.5000, 1207911.1250, 1195140.6250, 1193766.8750, 1188782.8750],
        [1503212.5000, 1496900.8750, 1489415.0000, 1466880.2500, 1454796.0000,
         1439571.7500, 1434274.1250, 1433969.1250, 1430505.1250, 1429419.6250],
        [1441107.5000, 1415065.5000, 1401272.3750, 1392886.3750, 1381037.7500,
         1364940.2500, 1349199.1250, 1336720.8750, 1335188.1250, 1321942.8750],
        [1009949.4375, 1002970.5625,  984746.4375,  977236.7500,  968412.0000,
          963697.7500,  956205.5000,  955035.2500,  954033.0625,  943881.8125],
        [ 974844.6250,  961507.3750,  947609.5000,  942086.8125,  936304.2500,
          930900.7500,  903929.0000,  903056.2500,  902795.3125,  895118.9375],
        [1058677.3750, 1029020.6875, 1014927.7500,  981718.6250,  951829.2500,
          949867.8750,  949153.3750,  934912.3125,  931665.4375,  923837.0000],
        [1219428.7500, 1164792.8750, 1158525.7500, 1122499.5000, 1110581.1250,
         1104889.1250, 1099182.3750, 1089918.2500, 1087232.5000, 1083179.7500],
        [1134071.8750, 1035553.5000, 1016709.3125,  961417.5000,  948122.8750,
          947067.4375,  922638.6875,  910132.0625,  902733.3125,  899394.0000],
        [ 989977.1250,  975053.7500,  959901.2500,  938028.3125,  928392.5625,
          927995.1250,  924058.1250,  922362.4375,  904801.8750,  901182.3750],
        [1024113.8750, 1018777.6250, 1007546.3750,  990882.8750,  987606.3750,
          984339.8125,  981628.7500,  981015.7500,  978093.6250,  970240.5000],
        [ 867646.0625,  861300.5000,  805814.1875,  805679.6875,  791870.6250,
          772938.5625,  767541.0000,  766412.3750,  765240.1875,  762977.5000],
        [1118575.5000, 1099684.5000, 1037092.3750, 1000804.4375,  999832.3125,
          998126.0625,  996359.0000,  994482.1875,  987999.2500,  981012.8750],
        [1333175.2500, 1304608.1250, 1302978.0000, 1293478.1250, 1292097.3750,
         1280759.1250, 1278829.5000, 1276664.0000, 1275789.0000, 1272392.8750],
        [ 932425.4375,  898029.5000,  891204.1250,  880120.1250,  827683.9375,
          814114.0000,  813211.5625,  807076.1875,  806186.1875,  805287.9375],
        [1010467.8125,  944241.9375,  908160.3750,  891370.8125,  884421.3750,
          884004.0000,  874104.2500,  863549.9375,  860693.6875,  853347.6875],
        [ 883633.9375,  871557.9375,  850536.5000,  818631.1250,  798461.2500,
          793174.4375,  765096.4375,  765005.1875,  755297.0625,  749818.8750]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[1246254.3750,       0.0000],
         [1213594.5000,       0.0000],
         [1088050.8750,       0.0000],
         ...,
         [ 885576.8125,       0.0000],
         [ 884996.0000,       0.0000],
         [ 878333.3125,       0.0000]],

        [[1389741.7500,       0.0000],
         [      0.0000, 1303618.1250],
         [1295798.1250,       0.0000],
         ...,
         [1208708.6250,       0.0000],
         [1182753.2500,       0.0000],
         [1181486.1250,       0.0000]],

        [[1421101.1250,       0.0000],
         [1408693.8750,       0.0000],
         [1404014.5000,       0.0000],
         ...,
         [1329303.1250,       0.0000],
         [1328796.1250,       0.0000],
         [1324700.3750,       0.0000]],

        ...,

        [[ 932425.4375,       0.0000],
         [ 898029.5000,       0.0000],
         [ 891204.1250,       0.0000],
         ...,
         [ 807076.1875,       0.0000],
         [ 806186.1875,       0.0000],
         [ 805287.9375,       0.0000]],

        [[1010467.8125,       0.0000],
         [ 944241.9375,       0.0000],
         [ 908160.3750,       0.0000],
         ...,
         [ 863549.9375,       0.0000],
         [ 860693.6875,       0.0000],
         [ 853347.6875,       0.0000]],

        [[ 883633.9375,       0.0000],
         [ 871557.9375,       0.0000],
         [ 850536.5000,       0.0000],
         ...,
         [ 765005.1875,       0.0000],
         [ 755297.0625,       0.0000],
         [ 749818.8750,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[10000928.0000,        0.0000],
        [11252566.0000,  1303618.1250],
        [12287928.0000,  1362308.1250],
        [13442534.0000,        0.0000],
        [11176950.0000,  1299429.0000],
        [14834482.0000,        0.0000],
        [13747098.0000,  1538527.0000],
        [13339158.0000,  1515752.2500],
        [11531260.0000,  1248018.2500],
        [10368808.0000,        0.0000],
        [11892954.0000,  1367818.7500],
        [14482518.0000,        0.0000],
        [11508172.0000,  1117384.5000],
        [12321752.0000,        0.0000],
        [10545602.0000,        0.0000],
        [10879204.0000,        0.0000],
        [12829859.0000,        0.0000],
        [12831812.0000,        0.0000],
        [11474312.0000,  2874306.0000],
        [11259460.0000,  1233160.7500],
        [10356072.0000,  1131953.0000],
        [10767134.0000,  2699400.0000],
        [ 8859122.0000,        0.0000],
        [10819010.0000,        0.0000],
        [12091957.0000,        0.0000],
        [ 9874381.0000,        0.0000],
        [ 7766158.0000,   891799.2500],
        [ 9072780.0000,        0.0000],
        [10361563.0000,        0.0000],
        [ 9605136.0000,        0.0000],
        [10034116.0000,        0.0000],
        [10249175.0000,        0.0000],
        [10450733.0000,  1170272.1250],
        [13374513.0000,        0.0000],
        [ 9518601.0000,  2325970.5000],
        [12696838.0000,        0.0000],
        [13171184.0000,        0.0000],
        [11169180.0000,  1285764.5000],
        [10115040.0000,  2481789.5000],
        [12938871.0000,        0.0000],
        [ 9392343.0000,   990761.0000],
        [10258652.0000,        0.0000],
        [10070976.0000,  1063907.0000],
        [10154514.0000,        0.0000],
        [ 7821247.0000,        0.0000],
        [ 8848203.0000,        0.0000],
        [ 8869359.0000,        0.0000],
        [ 8695726.0000,  3720005.5000],
        [11210774.0000,  1215067.5000],
        [11708868.0000,  2870077.0000],
        [ 9594688.0000,  4144672.7500],
        [ 8738932.0000,   977236.7500],
        [ 8356066.0000,   942086.8125],
        [ 9725610.0000,        0.0000],
        [10081704.0000,  1158525.7500],
        [ 9677840.0000,        0.0000],
        [ 9371753.0000,        0.0000],
        [ 8933363.0000,   990882.8750],
        [ 7967421.0000,        0.0000],
        [ 9214136.0000,   999832.3125],
        [12910771.0000,        0.0000],
        [ 8475339.0000,        0.0000],
        [ 8089940.0000,   884421.3750],
        [ 8051213.0000,        0.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 6/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:20, 62.77s/it]  7%|▋         | 2/30 [01:03<12:16, 26.29s/it] 10%|█         | 3/30 [01:04<06:34, 14.62s/it] 13%|█▎        | 4/30 [01:05<03:57,  9.15s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.12s/it] 20%|██        | 6/30 [01:06<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:15<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 6.6676923910776775
Epoch 7/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:16, 60.58s/it]  7%|▋         | 2/30 [01:01<11:50, 25.38s/it] 10%|█         | 3/30 [01:02<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 6.660325638453165
Epoch 8/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:05, 60.18s/it]  7%|▋         | 2/30 [01:00<11:46, 25.22s/it] 10%|█         | 3/30 [01:01<06:19, 14.05s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.80s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.90s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.30it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 6.794751977920532
Epoch 9/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:27, 60.94s/it]  7%|▋         | 2/30 [01:01<11:55, 25.54s/it] 10%|█         | 3/30 [01:02<06:23, 14.22s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.90s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.96s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 6.7342804908752445
Epoch 10/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:21, 60.73s/it]  7%|▋         | 2/30 [01:01<11:52, 25.45s/it] 10%|█         | 3/30 [01:02<06:22, 14.17s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 6.7351737976074215
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0699, -0.0203,  0.0652,  ..., -0.0022,  0.0037,  0.0780],
        [ 0.0769, -0.0248,  0.0563,  ..., -0.0089,  0.0069,  0.0164],
        [ 0.0012, -0.0089,  0.0961,  ...,  0.0636,  0.0409,  0.0772],
        ...,
        [ 0.0250, -0.0048,  0.0551,  ...,  0.0283,  0.0074,  0.1001],
        [ 0.0287, -0.0104,  0.0653,  ...,  0.0203,  0.0113,  0.1094],
        [ 0.0511, -0.0159,  0.0617,  ..., -0.0013,  0.0276,  0.0542]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9840, 0.9837, 0.9738, 0.9694, 0.9614, 0.9612, 0.9610, 0.9609, 0.9579,
         0.9551],
        [0.9922, 0.9905, 0.9903, 0.9889, 0.9876, 0.9874, 0.9859, 0.9859, 0.9857,
         0.9854],
        [0.9949, 0.9946, 0.9944, 0.9922, 0.9919, 0.9917, 0.9915, 0.9912, 0.9906,
         0.9882],
        [0.9959, 0.9954, 0.9940, 0.9932, 0.9930, 0.9930, 0.9920, 0.9907, 0.9904,
         0.9904],
        [0.9925, 0.9922, 0.9915, 0.9914, 0.9910, 0.9885, 0.9883, 0.9881, 0.9877,
         0.9877],
        [0.9983, 0.9978, 0.9972, 0.9972, 0.9971, 0.9969, 0.9968, 0.9960, 0.9959,
         0.9957],
        [0.9988, 0.9986, 0.9986, 0.9986, 0.9984, 0.9981, 0.9980, 0.9980, 0.9979,
         0.9979],
        [0.9975, 0.9970, 0.9967, 0.9966, 0.9965, 0.9958, 0.9958, 0.9958, 0.9958,
         0.9956],
        [0.9930, 0.9905, 0.9897, 0.9895, 0.9890, 0.9874, 0.9863, 0.9861, 0.9846,
         0.9837],
        [0.9845, 0.9817, 0.9786, 0.9756, 0.9743, 0.9727, 0.9724, 0.9682, 0.9678,
         0.9678],
        [0.9941, 0.9938, 0.9921, 0.9920, 0.9918, 0.9917, 0.9906, 0.9905, 0.9878,
         0.9874],
        [0.9965, 0.9961, 0.9955, 0.9954, 0.9952, 0.9947, 0.9941, 0.9941, 0.9934,
         0.9930],
        [0.9961, 0.9938, 0.9925, 0.9906, 0.9872, 0.9855, 0.9826, 0.9806, 0.9798,
         0.9797],
        [0.9921, 0.9898, 0.9895, 0.9886, 0.9881, 0.9880, 0.9872, 0.9864, 0.9850,
         0.9848],
        [0.9846, 0.9832, 0.9823, 0.9799, 0.9794, 0.9793, 0.9783, 0.9781, 0.9771,
         0.9769],
        [0.9879, 0.9828, 0.9824, 0.9819, 0.9819, 0.9818, 0.9796, 0.9791, 0.9790,
         0.9790],
        [0.9928, 0.9908, 0.9888, 0.9884, 0.9872, 0.9872, 0.9869, 0.9868, 0.9856,
         0.9852],
        [0.9968, 0.9933, 0.9911, 0.9886, 0.9884, 0.9873, 0.9872, 0.9866, 0.9865,
         0.9859],
        [0.9968, 0.9949, 0.9941, 0.9939, 0.9938, 0.9935, 0.9935, 0.9933, 0.9933,
         0.9933],
        [0.9910, 0.9901, 0.9901, 0.9900, 0.9857, 0.9857, 0.9850, 0.9828, 0.9826,
         0.9812],
        [0.9882, 0.9843, 0.9825, 0.9802, 0.9801, 0.9799, 0.9792, 0.9788, 0.9784,
         0.9782],
        [0.9947, 0.9938, 0.9934, 0.9925, 0.9910, 0.9903, 0.9900, 0.9892, 0.9891,
         0.9888],
        [0.9787, 0.9746, 0.9745, 0.9724, 0.9722, 0.9721, 0.9719, 0.9709, 0.9705,
         0.9702],
        [0.9875, 0.9840, 0.9837, 0.9831, 0.9830, 0.9825, 0.9816, 0.9815, 0.9806,
         0.9799],
        [0.9920, 0.9836, 0.9826, 0.9821, 0.9821, 0.9806, 0.9805, 0.9804, 0.9788,
         0.9786],
        [0.9814, 0.9809, 0.9794, 0.9778, 0.9774, 0.9761, 0.9753, 0.9743, 0.9741,
         0.9737],
        [0.9754, 0.9748, 0.9733, 0.9725, 0.9715, 0.9704, 0.9703, 0.9700, 0.9694,
         0.9691],
        [0.9793, 0.9763, 0.9758, 0.9758, 0.9755, 0.9753, 0.9740, 0.9725, 0.9724,
         0.9723],
        [0.9864, 0.9856, 0.9836, 0.9833, 0.9815, 0.9805, 0.9805, 0.9803, 0.9802,
         0.9798],
        [0.9781, 0.9773, 0.9770, 0.9756, 0.9749, 0.9745, 0.9741, 0.9733, 0.9726,
         0.9725],
        [0.9833, 0.9823, 0.9823, 0.9809, 0.9802, 0.9799, 0.9791, 0.9781, 0.9773,
         0.9764],
        [0.9831, 0.9825, 0.9818, 0.9795, 0.9790, 0.9785, 0.9782, 0.9767, 0.9760,
         0.9757],
        [0.9892, 0.9871, 0.9870, 0.9859, 0.9856, 0.9852, 0.9844, 0.9839, 0.9836,
         0.9835],
        [0.9938, 0.9937, 0.9920, 0.9916, 0.9909, 0.9908, 0.9906, 0.9906, 0.9904,
         0.9900],
        [0.9916, 0.9894, 0.9877, 0.9876, 0.9872, 0.9868, 0.9861, 0.9837, 0.9827,
         0.9822],
        [0.9911, 0.9905, 0.9896, 0.9879, 0.9875, 0.9857, 0.9851, 0.9847, 0.9845,
         0.9839],
        [0.9920, 0.9918, 0.9905, 0.9901, 0.9900, 0.9898, 0.9895, 0.9893, 0.9892,
         0.9890],
        [0.9911, 0.9907, 0.9903, 0.9899, 0.9860, 0.9851, 0.9836, 0.9825, 0.9809,
         0.9805],
        [0.9900, 0.9900, 0.9899, 0.9894, 0.9886, 0.9885, 0.9881, 0.9875, 0.9867,
         0.9866],
        [0.9954, 0.9912, 0.9911, 0.9899, 0.9883, 0.9860, 0.9860, 0.9857, 0.9856,
         0.9854],
        [0.9831, 0.9828, 0.9819, 0.9797, 0.9796, 0.9796, 0.9779, 0.9770, 0.9769,
         0.9769],
        [0.9832, 0.9810, 0.9798, 0.9794, 0.9785, 0.9785, 0.9780, 0.9764, 0.9760,
         0.9758],
        [0.9888, 0.9873, 0.9871, 0.9839, 0.9839, 0.9824, 0.9821, 0.9819, 0.9818,
         0.9816],
        [0.9837, 0.9828, 0.9813, 0.9794, 0.9783, 0.9783, 0.9783, 0.9782, 0.9780,
         0.9775],
        [0.9706, 0.9686, 0.9681, 0.9673, 0.9667, 0.9666, 0.9660, 0.9659, 0.9658,
         0.9658],
        [0.9829, 0.9792, 0.9760, 0.9733, 0.9726, 0.9717, 0.9713, 0.9708, 0.9708,
         0.9702],
        [0.9753, 0.9750, 0.9747, 0.9742, 0.9729, 0.9725, 0.9722, 0.9721, 0.9720,
         0.9720],
        [0.9912, 0.9910, 0.9902, 0.9884, 0.9877, 0.9868, 0.9867, 0.9863, 0.9857,
         0.9844],
        [0.9938, 0.9914, 0.9901, 0.9895, 0.9892, 0.9879, 0.9868, 0.9868, 0.9867,
         0.9858],
        [0.9973, 0.9973, 0.9970, 0.9961, 0.9956, 0.9949, 0.9945, 0.9940, 0.9940,
         0.9934],
        [0.9954, 0.9952, 0.9933, 0.9929, 0.9920, 0.9919, 0.9918, 0.9918, 0.9904,
         0.9904],
        [0.9812, 0.9773, 0.9772, 0.9771, 0.9771, 0.9755, 0.9748, 0.9744, 0.9742,
         0.9738],
        [0.9809, 0.9783, 0.9778, 0.9778, 0.9766, 0.9760, 0.9758, 0.9757, 0.9753,
         0.9748],
        [0.9822, 0.9813, 0.9785, 0.9782, 0.9769, 0.9762, 0.9760, 0.9758, 0.9755,
         0.9755],
        [0.9856, 0.9855, 0.9853, 0.9837, 0.9823, 0.9822, 0.9821, 0.9819, 0.9815,
         0.9815],
        [0.9811, 0.9801, 0.9777, 0.9770, 0.9768, 0.9766, 0.9762, 0.9760, 0.9758,
         0.9753],
        [0.9769, 0.9766, 0.9761, 0.9752, 0.9746, 0.9746, 0.9744, 0.9739, 0.9738,
         0.9736],
        [0.9829, 0.9816, 0.9808, 0.9806, 0.9793, 0.9786, 0.9784, 0.9784, 0.9784,
         0.9783],
        [0.9743, 0.9726, 0.9709, 0.9708, 0.9689, 0.9688, 0.9686, 0.9686, 0.9675,
         0.9675],
        [0.9840, 0.9821, 0.9782, 0.9766, 0.9764, 0.9761, 0.9759, 0.9758, 0.9754,
         0.9754],
        [0.9921, 0.9921, 0.9920, 0.9918, 0.9916, 0.9910, 0.9908, 0.9907, 0.9907,
         0.9906],
        [0.9764, 0.9755, 0.9753, 0.9723, 0.9716, 0.9698, 0.9698, 0.9695, 0.9693,
         0.9692],
        [0.9796, 0.9775, 0.9756, 0.9747, 0.9737, 0.9735, 0.9729, 0.9724, 0.9718,
         0.9713],
        [0.9751, 0.9730, 0.9707, 0.9706, 0.9693, 0.9688, 0.9685, 0.9680, 0.9678,
         0.9678]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[1273089.6250, 1267484.2500, 1101376.3750, 1032887.6250,  921977.2500,
          919678.1875,  916281.5000,  915788.6875,  877324.5625,  842124.7500],
        [1431045.5000, 1397483.5000, 1392348.6250, 1365896.0000, 1340132.7500,
         1336068.2500, 1308195.0000, 1307542.7500, 1304615.6250, 1299508.3750],
        [1487058.8750, 1480463.3750, 1478044.0000, 1431130.2500, 1426217.0000,
         1422161.3750, 1417594.0000, 1411857.2500, 1398954.3750, 1352486.7500],
        [1509244.1250, 1498552.0000, 1468572.6250, 1452922.8750, 1448939.1250,
         1448762.2500, 1427076.8750, 1402002.1250, 1395561.6250, 1395561.6250],
        [1438371.0000, 1431250.2500, 1416908.7500, 1414679.6250, 1407807.6250,
         1357832.0000, 1354840.1250, 1350156.7500, 1342576.0000, 1342506.8750],
        [1562941.6250, 1550648.5000, 1538167.6250, 1537689.3750, 1534910.1250,
         1531056.2500, 1528389.5000, 1511280.7500, 1510295.1250, 1503989.6250],
        [1572753.1250, 1569317.6250, 1569160.5000, 1567939.8750, 1564174.7500,
         1557682.8750, 1555656.5000, 1554711.7500, 1554077.2500, 1553130.5000],
        [1543149.8750, 1532780.1250, 1526941.3750, 1524270.1250, 1523081.5000,
         1508103.1250, 1507364.1250, 1507201.7500, 1506780.5000, 1502376.8750],
        [1448866.0000, 1396467.0000, 1381357.7500, 1377859.2500, 1368300.2500,
         1337299.7500, 1315247.8750, 1311400.2500, 1284830.5000, 1267635.3750],
        [1281773.2500, 1232213.2500, 1178196.1250, 1129213.2500, 1107803.2500,
         1083428.7500, 1079339.7500, 1016304.0625, 1010244.2500, 1009948.5000],
        [1471780.5000, 1465118.6250, 1429166.1250, 1426693.1250, 1422450.2500,
         1420983.2500, 1399414.6250, 1396612.1250, 1345064.8750, 1336404.7500],
        [1522981.2500, 1514082.1250, 1499845.8750, 1498526.2500, 1493418.7500,
         1483372.0000, 1471007.2500, 1470066.3750, 1457339.8750, 1447026.6250],
        [1513542.2500, 1464122.7500, 1437987.0000, 1399869.7500, 1332457.0000,
         1301820.5000, 1248596.7500, 1212405.2500, 1199112.5000, 1197262.5000],
        [1429752.3750, 1383963.3750, 1377503.2500, 1360472.3750, 1350561.1250,
         1348204.7500, 1332552.3750, 1317948.6250, 1290998.7500, 1287082.0000],
        [1283808.8750, 1258388.3750, 1241945.1250, 1201353.7500, 1192845.1250,
         1189871.6250, 1173963.1250, 1169697.5000, 1153732.8750, 1151044.7500],
        [1345436.8750, 1251948.5000, 1244401.6250, 1236365.0000, 1236000.7500,
         1233873.6250, 1195182.8750, 1187631.5000, 1186147.5000, 1186102.3750],
        [1443163.6250, 1403335.7500, 1364647.3750, 1355540.7500, 1333087.5000,
         1332997.2500, 1326959.8750, 1325716.5000, 1302768.0000, 1295743.7500],
        [1528602.2500, 1454317.3750, 1409597.0000, 1360530.6250, 1356583.0000,
         1335362.6250, 1332239.8750, 1320851.6250, 1319008.7500, 1308497.0000],
        [1528402.6250, 1488279.0000, 1470881.0000, 1465721.0000, 1464794.5000,
         1458317.2500, 1457562.3750, 1454929.1250, 1454241.2500, 1453400.8750],
        [1406414.6250, 1389133.5000, 1388585.2500, 1386985.1250, 1304871.8750,
         1303835.6250, 1291410.0000, 1251714.5000, 1248022.8750, 1223710.0000],
        [1352168.2500, 1278006.5000, 1246437.3750, 1206185.6250, 1203816.1250,
         1201530.1250, 1189221.6250, 1181826.5000, 1175707.6250, 1172796.0000],
        [1483875.6250, 1464040.3750, 1456613.2500, 1438586.3750, 1407741.7500,
         1393080.3750, 1386625.3750, 1372218.0000, 1369893.1250, 1363760.1250],
        [1181275.3750, 1112882.8750, 1112250.5000, 1079195.6250, 1076532.2500,
         1074710.3750, 1071324.3750, 1055494.8750, 1050475.7500, 1045749.8125],
        [1338434.0000, 1273413.8750, 1268053.6250, 1257704.6250, 1255106.8750,
         1245540.3750, 1231147.8750, 1229363.3750, 1213229.8750, 1200605.7500],
        [1427349.0000, 1266787.0000, 1248736.1250, 1238827.0000, 1238573.0000,
         1212968.5000, 1211523.3750, 1209235.5000, 1182858.1250, 1179203.2500],
        [1226977.5000, 1218448.8750, 1192569.8750, 1165439.5000, 1159174.5000,
         1138183.8750, 1123957.3750, 1107984.0000, 1105470.8750, 1099582.7500],
        [1126912.1250, 1116556.8750, 1092744.0000, 1080671.5000, 1065765.3750,
         1048992.1250, 1047510.6250, 1042242.1875, 1033568.5000, 1029521.3125],
        [1190887.7500, 1140255.6250, 1132162.5000, 1132038.2500, 1127194.8750,
         1124188.8750, 1103433.8750, 1080929.1250, 1079586.7500, 1077025.1250],
        [1317330.5000, 1302257.5000, 1266433.0000, 1261018.2500, 1228753.7500,
         1210732.1250, 1210587.8750, 1208375.5000, 1206165.0000, 1198460.8750],
        [1170953.1250, 1157194.1250, 1151533.2500, 1129841.2500, 1118681.1250,
         1112155.0000, 1105091.5000, 1092160.5000, 1081763.3750, 1080690.0000],
        [1259801.7500, 1242919.1250, 1242032.7500, 1217854.0000, 1206553.8750,
         1200854.2500, 1187358.6250, 1170916.2500, 1157145.5000, 1143074.3750],
        [1257834.1250, 1246586.0000, 1234554.0000, 1194277.0000, 1185306.2500,
         1177265.0000, 1171694.8750, 1146838.2500, 1135015.5000, 1131162.0000],
        [1371955.0000, 1331858.7500, 1329383.0000, 1308056.5000, 1303327.2500,
         1296037.8750, 1280623.5000, 1271571.6250, 1266782.1250, 1264472.0000],
        [1463912.0000, 1463021.5000, 1427017.0000, 1420223.2500, 1405028.5000,
         1403924.8750, 1399721.6250, 1399420.0000, 1394630.3750, 1386321.2500],
        [1419069.7500, 1375794.0000, 1341730.0000, 1340030.5000, 1332734.1250,
         1325101.0000, 1312032.0000, 1268150.5000, 1249505.6250, 1241565.0000],
        [1408581.1250, 1396821.2500, 1378621.7500, 1347105.8750, 1339161.7500,
         1305083.5000, 1293211.7500, 1286496.7500, 1281923.6250, 1271075.7500],
        [1427079.6250, 1424376.5000, 1397282.2500, 1388419.6250, 1387219.2500,
         1384003.0000, 1378074.8750, 1373457.7500, 1371816.2500, 1366861.7500],
        [1408918.3750, 1401919.2500, 1393253.1250, 1384808.2500, 1309455.7500,
         1292975.0000, 1266642.0000, 1246835.7500, 1217329.2500, 1210692.8750],
        [1388239.7500, 1386601.6250, 1385883.7500, 1374571.6250, 1359982.0000,
         1357886.5000, 1350176.1250, 1339396.7500, 1322584.6250, 1320582.0000],
        [1499361.1250, 1410559.8750, 1410249.2500, 1384463.7500, 1353473.8750,
         1311105.1250, 1311082.6250, 1304228.7500, 1303477.7500, 1298667.1250],
        [1257099.0000, 1251684.6250, 1235202.8750, 1197589.1250, 1196107.6250,
         1195494.0000, 1166770.7500, 1151403.7500, 1150987.6250, 1150717.6250],
        [1259392.1250, 1219270.7500, 1199834.2500, 1192125.1250, 1177278.3750,
         1176340.2500, 1169432.0000, 1142554.6250, 1135604.3750, 1132469.1250],
        [1362952.6250, 1334428.2500, 1331137.5000, 1271893.0000, 1270637.0000,
         1243861.8750, 1238519.8750, 1235005.0000, 1233306.5000, 1231071.5000],
        [1268318.6250, 1251992.6250, 1225057.5000, 1191949.0000, 1174487.1250,
         1174027.0000, 1173523.2500, 1172796.0000, 1169072.8750, 1160731.0000],
        [1050947.7500, 1021826.1250, 1014327.8125, 1003589.5625,  994580.8750,
          993611.9375,  984614.9375,  983073.3750,  981673.6875,  981358.1875],
        [1252868.1250, 1188244.3750, 1135393.3750, 1092839.7500, 1082020.2500,
         1067586.2500, 1062729.7500, 1054743.1250, 1054358.0000, 1045197.4375],
        [1124540.6250, 1120131.8750, 1114451.6250, 1106400.1250, 1086114.3750,
         1080177.8750, 1075184.0000, 1073615.3750, 1072340.3750, 1072225.7500],
        [1411586.6250, 1407292.1250, 1390525.2500, 1356850.7500, 1343284.3750,
         1325011.1250, 1322755.0000, 1316136.2500, 1303738.7500, 1280359.7500],
        [1463811.5000, 1416136.1250, 1389405.1250, 1378110.3750, 1371562.5000,
         1346698.8750, 1326094.5000, 1325090.7500, 1324259.5000, 1306273.8750],
        [1539585.2500, 1539150.6250, 1533328.5000, 1512793.3750, 1502107.5000,
         1486809.2500, 1478598.1250, 1468603.3750, 1468138.5000, 1457023.1250],
        [1498227.6250, 1493727.8750, 1453485.6250, 1446804.3750, 1428209.7500,
         1426272.7500, 1423755.8750, 1422450.2500, 1395858.5000, 1395572.2500],
        [1223169.8750, 1156308.2500, 1155891.5000, 1154580.5000, 1153749.3750,
         1127271.2500, 1116405.7500, 1109753.2500, 1107012.2500, 1101013.1250],
        [1217609.0000, 1173869.0000, 1165610.7500, 1164610.7500, 1144786.1250,
         1135629.3750, 1132254.2500, 1131118.8750, 1124464.5000, 1116017.1250],
        [1240902.1250, 1224464.1250, 1177793.8750, 1171720.5000, 1150527.7500,
         1138349.8750, 1135779.8750, 1133149.7500, 1128226.2500, 1127351.7500],
        [1303143.3750, 1301371.1250, 1297533.1250, 1267915.8750, 1242067.1250,
         1240123.7500, 1239482.7500, 1235853.3750, 1228975.3750, 1228975.3750],
        [1222276.7500, 1203468.2500, 1164483.0000, 1151717.8750, 1148683.7500,
         1146191.0000, 1139416.5000, 1136008.5000, 1132302.8750, 1125029.7500],
        [1149841.1250, 1145072.2500, 1137063.1250, 1123548.0000, 1113522.0000,
         1113377.5000, 1110076.0000, 1101822.8750, 1100245.7500, 1097092.0000],
        [1253810.1250, 1230580.8750, 1216377.6250, 1212492.0000, 1191022.8750,
         1179514.7500, 1176239.2500, 1175285.0000, 1175051.8750, 1174173.6250],
        [1108738.6250, 1082611.7500, 1056405.1250, 1053875.5000, 1026266.8125,
         1024438.1875, 1022330.0625, 1021269.8125, 1006029.3125, 1005650.3750],
        [1273427.2500, 1239260.6250, 1171926.1250, 1144824.3750, 1142863.0000,
         1137698.7500, 1134622.6250, 1132028.6250, 1126054.8750, 1125679.0000],
        [1429178.5000, 1428920.7500, 1427790.2500, 1422653.7500, 1419437.8750,
         1408107.0000, 1403789.5000, 1402142.5000, 1401482.1250, 1399597.5000],
        [1142580.7500, 1127345.3750, 1124269.2500, 1076645.1250, 1066408.0000,
         1039163.5625, 1038977.2500, 1035208.0000, 1031643.1875, 1030036.8750],
        [1196457.7500, 1160059.2500, 1129246.6250, 1114267.7500, 1099780.0000,
         1095213.3750, 1086695.6250, 1079020.6250, 1069723.5000, 1062197.6250],
        [1121635.8750, 1088705.8750, 1052862.8750, 1051648.5000, 1031830.1875,
         1025353.0625, 1020607.8125, 1012886.5000, 1010415.6875, 1009661.5000]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[1273089.6250,       0.0000],
         [1267484.2500,       0.0000],
         [1101376.3750,       0.0000],
         ...,
         [ 915788.6875,       0.0000],
         [ 877324.5625,       0.0000],
         [ 842124.7500,       0.0000]],

        [[1431045.5000,       0.0000],
         [1397483.5000,       0.0000],
         [1392348.6250,       0.0000],
         ...,
         [1307542.7500,       0.0000],
         [1304615.6250,       0.0000],
         [1299508.3750,       0.0000]],

        [[1487058.8750,       0.0000],
         [1480463.3750,       0.0000],
         [1478044.0000,       0.0000],
         ...,
         [1411857.2500,       0.0000],
         [1398954.3750,       0.0000],
         [1352486.7500,       0.0000]],

        ...,

        [[1142580.7500,       0.0000],
         [1127345.3750,       0.0000],
         [1124269.2500,       0.0000],
         ...,
         [1035208.0000,       0.0000],
         [1031643.1875,       0.0000],
         [1030036.8750,       0.0000]],

        [[1196457.7500,       0.0000],
         [1160059.2500,       0.0000],
         [1129246.6250,       0.0000],
         ...,
         [1079020.6250,       0.0000],
         [1069723.5000,       0.0000],
         [1062197.6250,       0.0000]],

        [[1121635.8750,       0.0000],
         [1088705.8750,       0.0000],
         [1052862.8750,       0.0000],
         ...,
         [1012886.5000,       0.0000],
         [1010415.6875,       0.0000],
         [1009661.5000,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[10068013.0000,        0.0000],
        [12142703.0000,  1340132.7500],
        [12874837.0000,  1431130.2500],
        [13020119.0000,  1427076.8750],
        [11060726.0000,  2796203.0000],
        [15309369.0000,        0.0000],
        [14050665.0000,  1567939.8750],
        [13638900.0000,  1543149.8750],
        [12204434.0000,  1284830.5000],
        [11128464.0000,        0.0000],
        [11251957.0000,  2861730.7500],
        [14857666.0000,        0.0000],
        [12094771.0000,  1212405.2500],
        [13479039.0000,        0.0000],
        [12016651.0000,        0.0000],
        [12303090.0000,        0.0000],
        [13483960.0000,        0.0000],
        [13725590.0000,        0.0000],
        [11773418.0000,  2923111.7500],
        [11806098.0000,  1388585.2500],
        [12207696.0000,        0.0000],
        [11259478.0000,  2876956.0000],
        [10859892.0000,        0.0000],
        [12512600.0000,        0.0000],
        [12416060.0000,        0.0000],
        [11537790.0000,        0.0000],
        [ 9618719.0000,  1065765.3750],
        [11187704.0000,        0.0000],
        [12410115.0000,        0.0000],
        [11200063.0000,        0.0000],
        [12028510.0000,        0.0000],
        [11880533.0000,        0.0000],
        [11692209.0000,  1331858.7500],
        [14163221.0000,        0.0000],
        [10644175.0000,  2561537.5000],
        [12037006.0000,  1271075.7500],
        [13898592.0000,        0.0000],
        [10622525.0000,  2510304.2500],
        [13585904.0000,        0.0000],
        [13586670.0000,        0.0000],
        [11953057.0000,        0.0000],
        [10671832.0000,  1132469.1250],
        [11514293.0000,  1238519.8750],
        [11961955.0000,        0.0000],
        [ 9006014.0000,  1003589.5625],
        [11035981.0000,        0.0000],
        [10925182.0000,        0.0000],
        [ 9466149.0000,  3991390.7500],
        [10950790.0000,  2696653.2500],
        [13499329.0000,  1486809.2500],
        [11443832.0000,  2940532.2500],
        [10248847.0000,  1156308.2500],
        [10341359.0000,  1164610.7500],
        [11628266.0000,        0.0000],
        [11287908.0000,  1297533.1250],
        [11569578.0000,        0.0000],
        [10046588.0000,  1145072.2500],
        [11984548.0000,        0.0000],
        [10407615.0000,        0.0000],
        [10502331.0000,  1126054.8750],
        [14143100.0000,        0.0000],
        [10712277.0000,        0.0000],
        [10005966.0000,  1086695.6250],
        [10425608.0000,        0.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 11/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:12, 56.28s/it]  7%|▋         | 2/30 [00:57<11:01, 23.62s/it] 10%|█         | 3/30 [01:00<06:34, 14.62s/it] 13%|█▎        | 4/30 [01:01<03:57,  9.14s/it] 17%|█▋        | 5/30 [01:02<02:32,  6.12s/it] 20%|██        | 6/30 [01:03<01:43,  4.29s/it] 23%|██▎       | 7/30 [01:03<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:04<00:52,  2.38s/it] 30%|███       | 9/30 [01:05<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.29s/it] 40%|████      | 12/30 [01:07<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.07it/s] 50%|█████     | 15/30 [01:09<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.32it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.32it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 6.840142854054769
Epoch 12/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:36, 61.26s/it]  7%|▋         | 2/30 [01:05<12:48, 27.44s/it] 10%|█         | 3/30 [01:05<06:51, 15.25s/it] 13%|█▎        | 4/30 [01:06<04:07,  9.53s/it] 17%|█▋        | 5/30 [01:07<02:39,  6.36s/it] 20%|██        | 6/30 [01:08<01:46,  4.45s/it] 23%|██▎       | 7/30 [01:08<01:14,  3.24s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.45s/it] 30%|███       | 9/30 [01:10<00:40,  1.92s/it] 33%|███▎      | 10/30 [01:11<00:31,  1.56s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.31s/it] 40%|████      | 12/30 [01:12<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:15,  1.06it/s] 50%|█████     | 15/30 [01:14<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:26<00:00,  1.33it/s]100%|██████████| 30/30 [01:26<00:00,  2.87s/it]
Epoch loss is 6.7799209912618
Epoch 13/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:39, 61.36s/it]  7%|▋         | 2/30 [01:02<11:59, 25.71s/it] 10%|█         | 3/30 [01:02<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:04<02:29,  6.00s/it] 20%|██        | 6/30 [01:05<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 6.704457139968872
Epoch 14/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:51, 59.72s/it]  7%|▋         | 2/30 [01:00<11:40, 25.03s/it] 10%|█         | 3/30 [01:01<06:16, 13.94s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.73s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.85s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 6.830576546986898
Epoch 15/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:13, 58.40s/it]  7%|▋         | 2/30 [01:00<11:43, 25.12s/it] 10%|█         | 3/30 [01:00<06:17, 13.99s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.76s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 6.700455300013224
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0469, -0.0171,  0.0670,  ...,  0.0052,  0.0115,  0.0707],
        [ 0.0662, -0.0142,  0.0559,  ..., -0.0043,  0.0170,  0.0067],
        [-0.0180, -0.0069,  0.0976,  ...,  0.0802,  0.0525,  0.0576],
        ...,
        [ 0.0232, -0.0167,  0.0478,  ...,  0.0220, -0.0054,  0.0950],
        [ 0.0214, -0.0169,  0.0629,  ...,  0.0146,  0.0091,  0.1055],
        [ 0.0490, -0.0225,  0.0580,  ..., -0.0105,  0.0283,  0.0504]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9789, 0.9786, 0.9670, 0.9660, 0.9645, 0.9614, 0.9597, 0.9521, 0.9512,
         0.9497],
        [0.9919, 0.9916, 0.9908, 0.9902, 0.9894, 0.9888, 0.9888, 0.9885, 0.9882,
         0.9880],
        [0.9969, 0.9966, 0.9957, 0.9945, 0.9942, 0.9931, 0.9925, 0.9922, 0.9913,
         0.9904],
        [0.9968, 0.9963, 0.9962, 0.9948, 0.9947, 0.9946, 0.9935, 0.9934, 0.9922,
         0.9921],
        [0.9955, 0.9950, 0.9944, 0.9929, 0.9925, 0.9922, 0.9922, 0.9922, 0.9921,
         0.9917],
        [0.9985, 0.9985, 0.9982, 0.9981, 0.9979, 0.9979, 0.9977, 0.9976, 0.9973,
         0.9971],
        [0.9993, 0.9993, 0.9992, 0.9991, 0.9991, 0.9990, 0.9989, 0.9989, 0.9988,
         0.9986],
        [0.9979, 0.9978, 0.9975, 0.9974, 0.9970, 0.9970, 0.9969, 0.9966, 0.9965,
         0.9965],
        [0.9931, 0.9911, 0.9911, 0.9910, 0.9909, 0.9902, 0.9891, 0.9888, 0.9885,
         0.9884],
        [0.9805, 0.9794, 0.9791, 0.9781, 0.9758, 0.9754, 0.9721, 0.9719, 0.9717,
         0.9713],
        [0.9957, 0.9950, 0.9942, 0.9926, 0.9926, 0.9917, 0.9915, 0.9903, 0.9901,
         0.9894],
        [0.9969, 0.9968, 0.9964, 0.9962, 0.9962, 0.9958, 0.9946, 0.9946, 0.9942,
         0.9936],
        [0.9968, 0.9948, 0.9928, 0.9918, 0.9887, 0.9878, 0.9869, 0.9865, 0.9862,
         0.9832],
        [0.9947, 0.9929, 0.9925, 0.9918, 0.9903, 0.9898, 0.9896, 0.9888, 0.9886,
         0.9884],
        [0.9896, 0.9872, 0.9845, 0.9841, 0.9820, 0.9819, 0.9816, 0.9807, 0.9802,
         0.9797],
        [0.9914, 0.9890, 0.9876, 0.9873, 0.9867, 0.9865, 0.9837, 0.9836, 0.9834,
         0.9830],
        [0.9944, 0.9937, 0.9919, 0.9894, 0.9889, 0.9881, 0.9881, 0.9864, 0.9858,
         0.9856],
        [0.9977, 0.9951, 0.9935, 0.9926, 0.9907, 0.9897, 0.9891, 0.9881, 0.9877,
         0.9876],
        [0.9978, 0.9967, 0.9956, 0.9953, 0.9941, 0.9938, 0.9935, 0.9935, 0.9935,
         0.9934],
        [0.9933, 0.9919, 0.9908, 0.9904, 0.9884, 0.9839, 0.9836, 0.9818, 0.9816,
         0.9815],
        [0.9909, 0.9865, 0.9853, 0.9846, 0.9838, 0.9819, 0.9814, 0.9811, 0.9810,
         0.9804],
        [0.9962, 0.9950, 0.9944, 0.9938, 0.9933, 0.9927, 0.9919, 0.9916, 0.9909,
         0.9904],
        [0.9837, 0.9830, 0.9816, 0.9803, 0.9801, 0.9794, 0.9786, 0.9770, 0.9766,
         0.9766],
        [0.9913, 0.9910, 0.9899, 0.9895, 0.9880, 0.9876, 0.9875, 0.9865, 0.9857,
         0.9832],
        [0.9916, 0.9896, 0.9835, 0.9832, 0.9824, 0.9822, 0.9819, 0.9807, 0.9804,
         0.9794],
        [0.9876, 0.9875, 0.9837, 0.9835, 0.9820, 0.9817, 0.9811, 0.9810, 0.9810,
         0.9800],
        [0.9849, 0.9832, 0.9812, 0.9799, 0.9793, 0.9792, 0.9781, 0.9777, 0.9776,
         0.9774],
        [0.9861, 0.9850, 0.9841, 0.9823, 0.9821, 0.9814, 0.9811, 0.9810, 0.9806,
         0.9800],
        [0.9913, 0.9913, 0.9903, 0.9883, 0.9878, 0.9874, 0.9865, 0.9859, 0.9858,
         0.9855],
        [0.9847, 0.9831, 0.9822, 0.9801, 0.9797, 0.9789, 0.9771, 0.9766, 0.9761,
         0.9760],
        [0.9894, 0.9888, 0.9873, 0.9872, 0.9866, 0.9848, 0.9844, 0.9843, 0.9838,
         0.9832],
        [0.9874, 0.9874, 0.9869, 0.9867, 0.9852, 0.9852, 0.9843, 0.9841, 0.9839,
         0.9837],
        [0.9932, 0.9925, 0.9918, 0.9911, 0.9911, 0.9886, 0.9880, 0.9876, 0.9872,
         0.9867],
        [0.9953, 0.9932, 0.9932, 0.9931, 0.9927, 0.9925, 0.9922, 0.9920, 0.9917,
         0.9915],
        [0.9953, 0.9906, 0.9906, 0.9897, 0.9896, 0.9893, 0.9889, 0.9878, 0.9871,
         0.9865],
        [0.9924, 0.9923, 0.9900, 0.9898, 0.9884, 0.9883, 0.9882, 0.9880, 0.9863,
         0.9862],
        [0.9932, 0.9928, 0.9926, 0.9923, 0.9918, 0.9914, 0.9913, 0.9910, 0.9909,
         0.9909],
        [0.9918, 0.9912, 0.9908, 0.9908, 0.9884, 0.9844, 0.9843, 0.9829, 0.9824,
         0.9824],
        [0.9950, 0.9911, 0.9904, 0.9904, 0.9901, 0.9893, 0.9887, 0.9885, 0.9884,
         0.9870],
        [0.9967, 0.9926, 0.9916, 0.9904, 0.9904, 0.9899, 0.9891, 0.9888, 0.9879,
         0.9877],
        [0.9899, 0.9865, 0.9856, 0.9854, 0.9851, 0.9845, 0.9836, 0.9833, 0.9831,
         0.9829],
        [0.9866, 0.9849, 0.9845, 0.9845, 0.9834, 0.9828, 0.9827, 0.9824, 0.9819,
         0.9817],
        [0.9937, 0.9917, 0.9916, 0.9911, 0.9887, 0.9871, 0.9867, 0.9864, 0.9862,
         0.9856],
        [0.9865, 0.9862, 0.9855, 0.9850, 0.9846, 0.9841, 0.9831, 0.9824, 0.9816,
         0.9812],
        [0.9787, 0.9787, 0.9783, 0.9782, 0.9763, 0.9762, 0.9760, 0.9760, 0.9760,
         0.9758],
        [0.9893, 0.9855, 0.9834, 0.9813, 0.9813, 0.9811, 0.9806, 0.9803, 0.9797,
         0.9791],
        [0.9840, 0.9838, 0.9835, 0.9818, 0.9816, 0.9808, 0.9807, 0.9802, 0.9796,
         0.9796],
        [0.9923, 0.9919, 0.9916, 0.9899, 0.9895, 0.9891, 0.9886, 0.9879, 0.9866,
         0.9831],
        [0.9950, 0.9930, 0.9920, 0.9916, 0.9913, 0.9907, 0.9904, 0.9902, 0.9900,
         0.9893],
        [0.9980, 0.9979, 0.9978, 0.9971, 0.9970, 0.9958, 0.9956, 0.9951, 0.9946,
         0.9941],
        [0.9964, 0.9961, 0.9943, 0.9938, 0.9930, 0.9930, 0.9917, 0.9917, 0.9905,
         0.9899],
        [0.9874, 0.9835, 0.9809, 0.9809, 0.9799, 0.9795, 0.9788, 0.9788, 0.9786,
         0.9785],
        [0.9876, 0.9861, 0.9857, 0.9852, 0.9845, 0.9838, 0.9837, 0.9831, 0.9827,
         0.9821],
        [0.9858, 0.9858, 0.9849, 0.9839, 0.9837, 0.9835, 0.9831, 0.9830, 0.9830,
         0.9829],
        [0.9898, 0.9879, 0.9864, 0.9853, 0.9828, 0.9826, 0.9824, 0.9820, 0.9819,
         0.9819],
        [0.9854, 0.9851, 0.9850, 0.9841, 0.9838, 0.9838, 0.9834, 0.9828, 0.9825,
         0.9824],
        [0.9849, 0.9839, 0.9836, 0.9832, 0.9828, 0.9827, 0.9824, 0.9817, 0.9814,
         0.9811],
        [0.9888, 0.9888, 0.9888, 0.9873, 0.9872, 0.9866, 0.9866, 0.9863, 0.9860,
         0.9859],
        [0.9807, 0.9802, 0.9801, 0.9795, 0.9790, 0.9785, 0.9784, 0.9779, 0.9760,
         0.9759],
        [0.9884, 0.9843, 0.9840, 0.9833, 0.9826, 0.9814, 0.9796, 0.9792, 0.9781,
         0.9779],
        [0.9956, 0.9950, 0.9945, 0.9943, 0.9940, 0.9938, 0.9936, 0.9936, 0.9934,
         0.9930],
        [0.9843, 0.9839, 0.9837, 0.9810, 0.9809, 0.9804, 0.9802, 0.9797, 0.9789,
         0.9789],
        [0.9855, 0.9831, 0.9830, 0.9827, 0.9825, 0.9814, 0.9809, 0.9804, 0.9803,
         0.9801],
        [0.9855, 0.9819, 0.9819, 0.9818, 0.9811, 0.9810, 0.9801, 0.9793, 0.9789,
         0.9782]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[1184504.0000, 1179241.5000,  999280.4375,  985250.8125,  964334.8125,
          921449.8750,  900067.5625,  807528.9375,  797331.2500,  780309.3125],
        [1425219.0000, 1419593.5000, 1403211.3750, 1392041.8750, 1374717.1250,
         1363780.8750, 1363423.2500, 1358683.1250, 1352578.2500, 1347658.5000],
        [1530806.6250, 1524485.2500, 1504519.0000, 1480330.7500, 1472783.0000,
         1449246.0000, 1438619.2500, 1430911.7500, 1413765.1250, 1396079.3750],
        [1529770.5000, 1518634.5000, 1516024.1250, 1485491.2500, 1483074.8750,
         1482442.7500, 1457666.6250, 1456563.2500, 1432594.0000, 1429448.2500],
        [1501295.5000, 1489817.0000, 1477339.3750, 1445577.0000, 1438358.7500,
         1432577.6250, 1432083.1250, 1432083.1250, 1429216.6250, 1420984.5000],
        [1566502.0000, 1566254.0000, 1558755.7500, 1557152.6250, 1554026.8750,
         1552859.3750, 1548163.1250, 1546799.6250, 1539318.0000, 1535356.6250],
        [1584438.5000, 1583675.6250, 1581536.8750, 1579847.0000, 1578831.8750,
         1577969.3750, 1575730.2500, 1574956.5000, 1573567.7500, 1569681.2500],
        [1553197.0000, 1551456.2500, 1543335.3750, 1541978.8750, 1534201.7500,
         1534162.2500, 1532055.3750, 1524207.6250, 1522719.8750, 1522692.2500],
        [1450776.7500, 1408597.2500, 1408425.2500, 1408121.8750, 1405509.6250,
         1392205.1250, 1368920.1250, 1364147.7500, 1358456.3750, 1356377.2500],
        [1211477.1250, 1192313.8750, 1187503.5000, 1170487.5000, 1132833.2500,
         1126771.3750, 1073634.7500, 1070463.3750, 1068511.2500, 1061905.0000],
        [1505885.6250, 1489216.1250, 1473949.2500, 1439448.3750, 1439105.1250,
         1420595.7500, 1417139.8750, 1392510.5000, 1389672.7500, 1374807.5000],
        [1531878.5000, 1528991.6250, 1519334.2500, 1515905.6250, 1515492.1250,
         1506966.0000, 1482069.6250, 1481818.0000, 1472760.6250, 1460512.1250],
        [1528038.3750, 1486789.5000, 1443615.1250, 1422484.1250, 1361139.3750,
         1343890.3750, 1326284.2500, 1319420.1250, 1314518.1250, 1258554.0000],
        [1483845.8750, 1446014.1250, 1437498.8750, 1424190.3750, 1393024.5000,
         1383946.2500, 1378479.6250, 1364421.0000, 1358995.3750, 1356752.5000],
        [1380292.3750, 1333338.0000, 1282598.6250, 1274681.0000, 1238218.7500,
         1235949.8750, 1230605.5000, 1215491.7500, 1205788.8750, 1198009.5000],
        [1414998.0000, 1367920.5000, 1340669.7500, 1333919.2500, 1323999.3750,
         1319586.1250, 1268781.8750, 1265321.1250, 1261851.8750, 1255055.3750],
        [1478258.2500, 1461855.5000, 1425683.8750, 1374845.6250, 1365229.2500,
         1350845.8750, 1350500.5000, 1317669.7500, 1306531.7500, 1303101.0000],
        [1549207.3750, 1491355.1250, 1458520.3750, 1439400.2500, 1400832.7500,
         1380878.2500, 1368634.3750, 1350568.8750, 1342629.8750, 1340944.6250],
        [1551296.3750, 1525683.7500, 1502366.8750, 1495590.8750, 1470346.7500,
         1465585.3750, 1459409.5000, 1458893.1250, 1458443.8750, 1456098.0000],
        [1454654.5000, 1424645.5000, 1403005.3750, 1394259.3750, 1355629.8750,
         1270896.3750, 1265953.6250, 1234158.5000, 1230550.3750, 1229153.5000],
        [1404540.8750, 1320185.3750, 1298066.5000, 1283785.6250, 1270171.7500,
         1236133.8750, 1227540.3750, 1221752.2500, 1219919.6250, 1209783.3750],
        [1514776.8750, 1491020.7500, 1477318.2500, 1464128.2500, 1454160.7500,
         1441596.8750, 1425285.6250, 1419187.5000, 1404712.2500, 1394309.7500],
        [1268311.3750, 1256002.5000, 1230869.6250, 1207352.6250, 1205018.7500,
         1192613.0000, 1177999.3750, 1152562.8750, 1145512.3750, 1145367.1250],
        [1412634.5000, 1406927.1250, 1386109.8750, 1377942.1250, 1348231.8750,
         1339639.5000, 1337880.2500, 1319155.8750, 1304152.8750, 1258306.8750],
        [1420181.1250, 1380038.3750, 1264049.8750, 1258758.1250, 1244477.6250,
         1240969.6250, 1236521.7500, 1214804.5000, 1209242.5000, 1192088.8750],
        [1341091.6250, 1339215.3750, 1267998.0000, 1263546.1250, 1238039.1250,
         1232300.2500, 1221860.6250, 1219818.5000, 1219037.0000, 1202577.8750],
        [1290697.1250, 1259250.3750, 1223258.6250, 1200375.6250, 1191478.5000,
         1188218.3750, 1170485.2500, 1163132.1250, 1161788.6250, 1159454.2500],
        [1311198.8750, 1291407.5000, 1274880.3750, 1243002.1250, 1239673.1250,
         1227319.1250, 1222297.7500, 1220761.1250, 1213431.2500, 1202166.2500],
        [1413298.7500, 1412278.8750, 1393600.0000, 1354047.1250, 1344275.0000,
         1337229.6250, 1320550.5000, 1309037.3750, 1307178.6250, 1301486.5000],
        [1286299.2500, 1256649.5000, 1241830.2500, 1205146.2500, 1196725.8750,
         1184446.3750, 1153611.8750, 1145012.1250, 1137986.2500, 1135736.5000],
        [1376289.8750, 1363887.6250, 1334732.3750, 1332171.1250, 1321128.7500,
         1287288.2500, 1280137.5000, 1278202.7500, 1270072.5000, 1258562.5000],
        [1337163.2500, 1337010.2500, 1327477.6250, 1324159.7500, 1295254.6250,
         1295148.2500, 1279024.6250, 1276005.5000, 1272317.6250, 1268092.3750],
        [1452077.8750, 1438506.8750, 1423351.3750, 1410229.0000, 1408374.3750,
         1360356.8750, 1348590.6250, 1341236.1250, 1331960.3750, 1323105.7500],
        [1495941.8750, 1452807.8750, 1452640.2500, 1449323.3750, 1441822.3750,
         1438653.6250, 1431823.7500, 1427885.5000, 1422328.1250, 1417437.2500],
        [1495797.7500, 1398923.6250, 1398395.3750, 1380563.6250, 1378448.1250,
         1373971.3750, 1366079.7500, 1343630.2500, 1330821.3750, 1320456.0000],
        [1436445.1250, 1432848.1250, 1386441.6250, 1384298.6250, 1356647.6250,
         1354076.7500, 1352635.1250, 1347995.2500, 1315300.6250, 1314657.2500],
        [1451493.6250, 1443843.7500, 1439072.1250, 1434538.1250, 1423616.0000,
         1415081.7500, 1412689.6250, 1406339.6250, 1405779.0000, 1404684.1250],
        [1422507.2500, 1410282.8750, 1403084.2500, 1402678.7500, 1355503.1250,
         1280671.1250, 1278833.1250, 1252982.8750, 1245349.0000, 1244872.8750],
        [1489979.0000, 1410207.5000, 1395713.3750, 1394796.5000, 1389639.7500,
         1373981.7500, 1361365.2500, 1357596.3750, 1356382.5000, 1328507.1250],
        [1526315.3750, 1439225.8750, 1419803.3750, 1395906.3750, 1395754.6250,
         1384837.3750, 1369146.1250, 1364581.0000, 1345714.0000, 1342492.8750],
        [1384471.6250, 1319272.8750, 1303544.7500, 1298443.0000, 1293630.0000,
         1282738.1250, 1266631.1250, 1260170.6250, 1256709.3750, 1253257.6250],
        [1321802.8750, 1288938.1250, 1283176.1250, 1282524.1250, 1263296.7500,
         1251444.7500, 1249477.1250, 1244154.8750, 1235991.2500, 1231321.6250],
        [1461661.7500, 1422010.7500, 1419091.3750, 1410137.6250, 1360852.6250,
         1330948.3750, 1323307.6250, 1317064.1250, 1314183.3750, 1303145.7500],
        [1320493.8750, 1314704.8750, 1300482.7500, 1292338.8750, 1283419.6250,
         1275245.2500, 1257769.3750, 1243946.0000, 1230563.2500, 1222724.3750],
        [1180382.3750, 1179671.1250, 1173719.1250, 1172009.8750, 1141059.5000,
         1138783.1250, 1136503.7500, 1135517.8750, 1135080.3750, 1132604.1250],
        [1373695.0000, 1300544.7500, 1262627.1250, 1226004.2500, 1224771.3750,
         1221839.6250, 1212272.2500, 1208491.8750, 1197689.5000, 1187692.7500],
        [1272800.6250, 1269618.3750, 1264015.0000, 1234135.0000, 1230492.8750,
         1216084.2500, 1215011.8750, 1206583.7500, 1196121.2500, 1195044.8750],
        [1432748.3750, 1425410.7500, 1418690.8750, 1384643.2500, 1377621.5000,
         1369142.1250, 1359677.2500, 1345832.1250, 1322053.7500, 1256828.1250],
        [1490523.2500, 1447189.3750, 1427497.3750, 1419765.5000, 1413037.3750,
         1400997.0000, 1396170.0000, 1390311.8750, 1386513.0000, 1374275.2500],
        [1556104.5000, 1553293.5000, 1551238.6250, 1535085.7500, 1532457.2500,
         1506138.3750, 1503430.3750, 1491120.3750, 1481531.1250, 1470460.2500],
        [1519311.0000, 1513312.8750, 1474148.8750, 1464192.5000, 1448745.7500,
         1448103.5000, 1422244.1250, 1421951.1250, 1397447.5000, 1385939.2500],
        [1336520.7500, 1265041.2500, 1218833.5000, 1218010.8750, 1200977.8750,
         1194026.5000, 1182876.1250, 1182092.5000, 1178597.2500, 1177558.0000],
        [1341472.8750, 1312288.5000, 1304017.2500, 1294916.1250, 1282286.8750,
         1269781.8750, 1267625.7500, 1257751.3750, 1249500.8750, 1238645.0000],
        [1307233.3750, 1306605.2500, 1289200.0000, 1271384.8750, 1268099.6250,
         1264245.2500, 1257209.2500, 1255964.1250, 1255334.2500, 1254184.3750],
        [1382770.7500, 1346259.6250, 1316857.0000, 1296413.7500, 1251884.0000,
         1247680.2500, 1244571.5000, 1237283.7500, 1235304.1250, 1235304.1250],
        [1299829.3750, 1294216.1250, 1291543.0000, 1275973.8750, 1269932.0000,
         1269615.8750, 1262275.5000, 1252011.7500, 1245598.5000, 1244908.5000],
        [1289631.5000, 1272351.6250, 1265445.5000, 1258808.5000, 1252432.1250,
         1249804.8750, 1243764.5000, 1231510.7500, 1227391.6250, 1222197.3750],
        [1364184.1250, 1364052.8750, 1363713.2500, 1334712.0000, 1333184.1250,
         1320854.1250, 1320836.3750, 1315306.8750, 1309748.0000, 1308974.8750],
        [1214094.5000, 1206075.2500, 1204059.5000, 1193309.2500, 1184718.6250,
         1177155.0000, 1174716.8750, 1166738.3750, 1136343.2500, 1134168.1250],
        [1356606.2500, 1279469.8750, 1273326.3750, 1260827.0000, 1247777.8750,
         1226378.3750, 1196190.7500, 1188889.3750, 1171151.8750, 1166566.0000],
        [1501762.3750, 1490510.5000, 1480198.0000, 1475117.8750, 1469463.6250,
         1465301.6250, 1461221.3750, 1459974.6250, 1457205.1250, 1448584.0000],
        [1279300.3750, 1271891.7500, 1267099.8750, 1220727.3750, 1219021.8750,
         1210339.6250, 1205315.1250, 1197404.1250, 1184663.2500, 1184565.0000],
        [1301563.5000, 1256317.6250, 1255047.0000, 1249778.6250, 1245605.6250,
         1227093.2500, 1217884.2500, 1209369.2500, 1207320.3750, 1203868.8750],
        [1301579.6250, 1236323.6250, 1235991.2500, 1233445.3750, 1221629.8750,
         1219152.0000, 1204929.0000, 1190952.5000, 1184133.5000, 1171560.7500]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[1184504.0000,       0.0000],
         [1179241.5000,       0.0000],
         [ 999280.4375,       0.0000],
         ...,
         [ 807528.9375,       0.0000],
         [ 797331.2500,       0.0000],
         [ 780309.3125,       0.0000]],

        [[1425219.0000,       0.0000],
         [1419593.5000,       0.0000],
         [1403211.3750,       0.0000],
         ...,
         [1358683.1250,       0.0000],
         [1352578.2500,       0.0000],
         [1347658.5000,       0.0000]],

        [[1530806.6250,       0.0000],
         [1524485.2500,       0.0000],
         [1504519.0000,       0.0000],
         ...,
         [1430911.7500,       0.0000],
         [1413765.1250,       0.0000],
         [1396079.3750,       0.0000]],

        ...,

        [[1279300.3750,       0.0000],
         [1271891.7500,       0.0000],
         [1267099.8750,       0.0000],
         ...,
         [1197404.1250,       0.0000],
         [1184663.2500,       0.0000],
         [1184565.0000,       0.0000]],

        [[1301563.5000,       0.0000],
         [1256317.6250,       0.0000],
         [1255047.0000,       0.0000],
         ...,
         [      0.0000, 1209369.2500],
         [1207320.3750,       0.0000],
         [1203868.8750,       0.0000]],

        [[1301579.6250,       0.0000],
         [1236323.6250,       0.0000],
         [1235991.2500,       0.0000],
         ...,
         [1190952.5000,       0.0000],
         [1184133.5000,       0.0000],
         [1171560.7500,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 9519299.0000,        0.0000],
        [13800907.0000,        0.0000],
        [13202926.0000,  1438619.2500],
        [13306219.0000,  1485491.2500],
        [11559678.0000,  2939654.2500],
        [15525188.0000,        0.0000],
        [14198698.0000,  1581536.8750],
        [13808550.0000,  1551456.2500],
        [13921538.0000,        0.0000],
        [11295901.0000,        0.0000],
        [11362496.0000,  2979835.0000],
        [15015728.0000,        0.0000],
        [13804734.0000,        0.0000],
        [12668174.0000,  1358995.3750],
        [12594975.0000,        0.0000],
        [13152104.0000,        0.0000],
        [13734522.0000,        0.0000],
        [14122972.0000,        0.0000],
        [13387616.0000,  1456098.0000],
        [10577702.0000,  2685205.0000],
        [12691880.0000,        0.0000],
        [11552532.0000,  2933964.5000],
        [10829047.0000,  1152562.8750],
        [13490981.0000,        0.0000],
        [12661132.0000,        0.0000],
        [11326446.0000,  1219037.0000],
        [12008139.0000,        0.0000],
        [12446137.0000,        0.0000],
        [13492982.0000,        0.0000],
        [11943444.0000,        0.0000],
        [13102474.0000,        0.0000],
        [13011654.0000,        0.0000],
        [11019186.0000,  2818603.5000],
        [13008336.0000,  1422328.1250],
        [11123002.0000,  2664086.2500],
        [12324699.0000,  1356647.6250],
        [14237138.0000,        0.0000],
        [12043783.0000,  1252982.8750],
        [13858169.0000,        0.0000],
        [13983778.0000,        0.0000],
        [12918870.0000,        0.0000],
        [11400683.0000,  1251444.7500],
        [13662404.0000,        0.0000],
        [12741688.0000,        0.0000],
        [10388828.0000,  1136503.7500],
        [11189624.0000,  1226004.2500],
        [12299908.0000,        0.0000],
        [10937846.0000,  2754802.0000],
        [11332246.0000,  2814034.5000],
        [13674722.0000,  1506138.3750],
        [10130534.0000,  4364862.0000],
        [10936524.0000,  1218010.8750],
        [11568786.0000,  1249500.8750],
        [12729460.0000,        0.0000],
        [11497916.0000,  1296413.7500],
        [12705904.0000,        0.0000],
        [ 9973902.0000,  2539436.5000],
        [13335567.0000,        0.0000],
        [10585303.0000,  1206075.2500],
        [12367184.0000,        0.0000],
        [13249364.0000,  1459974.6250],
        [12240328.0000,        0.0000],
        [11164480.0000,  1209369.2500],
        [12199697.0000,        0.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 16/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:31, 61.07s/it]  7%|▋         | 2/30 [01:01<11:56, 25.59s/it] 10%|█         | 3/30 [01:02<06:24, 14.24s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.92s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.97s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:07<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 6.598713747660319
Epoch 17/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:40, 61.38s/it]  7%|▋         | 2/30 [01:02<12:00, 25.71s/it] 10%|█         | 3/30 [01:02<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:04<02:29,  6.00s/it] 20%|██        | 6/30 [01:05<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 6.538432137171427
Epoch 18/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:29, 56.89s/it]  7%|▋         | 2/30 [00:57<11:08, 23.87s/it] 10%|█         | 3/30 [00:58<05:59, 13.31s/it] 13%|█▎        | 4/30 [01:00<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:01<02:28,  5.93s/it] 20%|██        | 6/30 [01:01<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:02<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.32s/it] 30%|███       | 9/30 [01:04<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:05<00:24,  1.27s/it] 40%|████      | 12/30 [01:06<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.08it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 6.441383504867554
Epoch 19/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:38, 61.32s/it]  7%|▋         | 2/30 [01:04<12:32, 26.87s/it] 10%|█         | 3/30 [01:05<06:57, 15.46s/it] 13%|█▎        | 4/30 [01:06<04:10,  9.65s/it] 17%|█▋        | 5/30 [01:07<02:41,  6.44s/it] 20%|██        | 6/30 [01:08<01:48,  4.51s/it] 23%|██▎       | 7/30 [01:08<01:15,  3.28s/it] 27%|██▋       | 8/30 [01:09<00:54,  2.47s/it] 30%|███       | 9/30 [01:10<00:40,  1.93s/it] 33%|███▎      | 10/30 [01:11<00:31,  1.57s/it] 37%|███▋      | 11/30 [01:11<00:25,  1.32s/it] 40%|████      | 12/30 [01:12<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:15,  1.06it/s] 50%|█████     | 15/30 [01:14<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.88s/it]
Epoch loss is 6.366446590423584
Epoch 20/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:13, 60.48s/it]  7%|▋         | 2/30 [01:01<11:49, 25.34s/it] 10%|█         | 3/30 [01:01<06:21, 14.11s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 6.299035978317261
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0237, -0.0136,  0.0718,  ...,  0.0119,  0.0186,  0.0584],
        [ 0.0502, -0.0033,  0.0583,  ...,  0.0025,  0.0275, -0.0023],
        [-0.0294, -0.0023,  0.0886,  ...,  0.0887,  0.0600,  0.0378],
        ...,
        [ 0.0169, -0.0228,  0.0400,  ...,  0.0155, -0.0161,  0.0909],
        [ 0.0128, -0.0203,  0.0613,  ...,  0.0096,  0.0090,  0.1004],
        [ 0.0425, -0.0238,  0.0555,  ..., -0.0141,  0.0327,  0.0438]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9665, 0.9663, 0.9630, 0.9576, 0.9532, 0.9500, 0.9487, 0.9476, 0.9431,
         0.9408],
        [0.9914, 0.9914, 0.9908, 0.9901, 0.9893, 0.9892, 0.9891, 0.9889, 0.9885,
         0.9883],
        [0.9978, 0.9972, 0.9960, 0.9954, 0.9952, 0.9935, 0.9926, 0.9922, 0.9920,
         0.9913],
        [0.9970, 0.9967, 0.9960, 0.9953, 0.9953, 0.9951, 0.9951, 0.9948, 0.9947,
         0.9931],
        [0.9966, 0.9954, 0.9945, 0.9937, 0.9934, 0.9933, 0.9932, 0.9932, 0.9932,
         0.9926],
        [0.9989, 0.9989, 0.9986, 0.9983, 0.9982, 0.9982, 0.9979, 0.9975, 0.9974,
         0.9974],
        [0.9996, 0.9995, 0.9995, 0.9994, 0.9993, 0.9993, 0.9992, 0.9992, 0.9992,
         0.9990],
        [0.9983, 0.9977, 0.9977, 0.9976, 0.9976, 0.9975, 0.9971, 0.9968, 0.9966,
         0.9962],
        [0.9935, 0.9934, 0.9924, 0.9924, 0.9918, 0.9917, 0.9915, 0.9912, 0.9910,
         0.9900],
        [0.9859, 0.9836, 0.9820, 0.9742, 0.9729, 0.9719, 0.9718, 0.9710, 0.9697,
         0.9679],
        [0.9963, 0.9948, 0.9947, 0.9924, 0.9922, 0.9921, 0.9907, 0.9894, 0.9891,
         0.9885],
        [0.9968, 0.9967, 0.9964, 0.9962, 0.9956, 0.9954, 0.9946, 0.9945, 0.9939,
         0.9938],
        [0.9966, 0.9950, 0.9927, 0.9924, 0.9921, 0.9893, 0.9889, 0.9869, 0.9856,
         0.9854],
        [0.9951, 0.9947, 0.9930, 0.9921, 0.9914, 0.9912, 0.9910, 0.9908, 0.9893,
         0.9890],
        [0.9913, 0.9903, 0.9893, 0.9870, 0.9845, 0.9844, 0.9817, 0.9816, 0.9791,
         0.9787],
        [0.9921, 0.9919, 0.9889, 0.9874, 0.9869, 0.9869, 0.9857, 0.9849, 0.9846,
         0.9838],
        [0.9954, 0.9952, 0.9923, 0.9914, 0.9901, 0.9900, 0.9892, 0.9890, 0.9885,
         0.9864],
        [0.9979, 0.9955, 0.9941, 0.9939, 0.9916, 0.9899, 0.9890, 0.9888, 0.9882,
         0.9881],
        [0.9983, 0.9979, 0.9953, 0.9951, 0.9940, 0.9932, 0.9930, 0.9928, 0.9928,
         0.9926],
        [0.9937, 0.9912, 0.9884, 0.9874, 0.9872, 0.9834, 0.9834, 0.9829, 0.9806,
         0.9802],
        [0.9924, 0.9896, 0.9878, 0.9853, 0.9851, 0.9850, 0.9829, 0.9817, 0.9813,
         0.9810],
        [0.9964, 0.9957, 0.9946, 0.9944, 0.9938, 0.9935, 0.9911, 0.9910, 0.9909,
         0.9906],
        [0.9894, 0.9872, 0.9855, 0.9854, 0.9833, 0.9819, 0.9818, 0.9818, 0.9816,
         0.9816],
        [0.9941, 0.9918, 0.9917, 0.9910, 0.9901, 0.9893, 0.9893, 0.9891, 0.9885,
         0.9837],
        [0.9929, 0.9917, 0.9873, 0.9844, 0.9831, 0.9819, 0.9819, 0.9809, 0.9804,
         0.9782],
        [0.9902, 0.9899, 0.9873, 0.9871, 0.9859, 0.9859, 0.9858, 0.9858, 0.9857,
         0.9855],
        [0.9891, 0.9885, 0.9869, 0.9847, 0.9842, 0.9831, 0.9829, 0.9829, 0.9828,
         0.9824],
        [0.9896, 0.9869, 0.9862, 0.9862, 0.9854, 0.9849, 0.9848, 0.9841, 0.9829,
         0.9829],
        [0.9938, 0.9933, 0.9932, 0.9915, 0.9904, 0.9894, 0.9894, 0.9890, 0.9886,
         0.9883],
        [0.9877, 0.9858, 0.9827, 0.9821, 0.9818, 0.9792, 0.9773, 0.9772, 0.9769,
         0.9761],
        [0.9920, 0.9917, 0.9916, 0.9886, 0.9886, 0.9878, 0.9875, 0.9875, 0.9873,
         0.9866],
        [0.9903, 0.9900, 0.9899, 0.9898, 0.9888, 0.9882, 0.9879, 0.9870, 0.9866,
         0.9856],
        [0.9953, 0.9952, 0.9951, 0.9933, 0.9923, 0.9908, 0.9896, 0.9888, 0.9884,
         0.9884],
        [0.9949, 0.9933, 0.9933, 0.9932, 0.9930, 0.9929, 0.9928, 0.9917, 0.9915,
         0.9915],
        [0.9966, 0.9916, 0.9905, 0.9904, 0.9901, 0.9897, 0.9889, 0.9887, 0.9887,
         0.9875],
        [0.9938, 0.9936, 0.9920, 0.9913, 0.9909, 0.9901, 0.9897, 0.9897, 0.9894,
         0.9884],
        [0.9949, 0.9934, 0.9929, 0.9919, 0.9916, 0.9911, 0.9908, 0.9908, 0.9905,
         0.9903],
        [0.9913, 0.9903, 0.9896, 0.9895, 0.9878, 0.9869, 0.9868, 0.9861, 0.9834,
         0.9822],
        [0.9970, 0.9922, 0.9908, 0.9899, 0.9896, 0.9894, 0.9887, 0.9882, 0.9871,
         0.9867],
        [0.9971, 0.9944, 0.9925, 0.9920, 0.9914, 0.9913, 0.9899, 0.9892, 0.9891,
         0.9890],
        [0.9914, 0.9871, 0.9857, 0.9857, 0.9854, 0.9853, 0.9851, 0.9850, 0.9849,
         0.9843],
        [0.9889, 0.9886, 0.9871, 0.9865, 0.9861, 0.9856, 0.9855, 0.9851, 0.9850,
         0.9849],
        [0.9964, 0.9940, 0.9929, 0.9906, 0.9878, 0.9871, 0.9868, 0.9865, 0.9859,
         0.9854],
        [0.9864, 0.9864, 0.9861, 0.9860, 0.9848, 0.9847, 0.9834, 0.9830, 0.9826,
         0.9826],
        [0.9829, 0.9821, 0.9820, 0.9820, 0.9816, 0.9816, 0.9811, 0.9808, 0.9804,
         0.9794],
        [0.9912, 0.9896, 0.9863, 0.9856, 0.9848, 0.9848, 0.9840, 0.9837, 0.9831,
         0.9829],
        [0.9901, 0.9884, 0.9866, 0.9858, 0.9858, 0.9857, 0.9857, 0.9857, 0.9853,
         0.9853],
        [0.9922, 0.9913, 0.9907, 0.9900, 0.9886, 0.9884, 0.9880, 0.9872, 0.9839,
         0.9809],
        [0.9940, 0.9919, 0.9916, 0.9915, 0.9912, 0.9911, 0.9909, 0.9906, 0.9906,
         0.9905],
        [0.9981, 0.9980, 0.9979, 0.9974, 0.9974, 0.9960, 0.9959, 0.9957, 0.9947,
         0.9936],
        [0.9967, 0.9959, 0.9941, 0.9941, 0.9930, 0.9925, 0.9919, 0.9916, 0.9908,
         0.9906],
        [0.9893, 0.9870, 0.9859, 0.9810, 0.9791, 0.9769, 0.9766, 0.9749, 0.9748,
         0.9747],
        [0.9892, 0.9881, 0.9871, 0.9870, 0.9865, 0.9859, 0.9853, 0.9844, 0.9841,
         0.9823],
        [0.9893, 0.9892, 0.9891, 0.9859, 0.9856, 0.9849, 0.9844, 0.9844, 0.9842,
         0.9839],
        [0.9908, 0.9877, 0.9866, 0.9816, 0.9812, 0.9807, 0.9803, 0.9794, 0.9783,
         0.9775],
        [0.9898, 0.9882, 0.9879, 0.9874, 0.9871, 0.9869, 0.9866, 0.9853, 0.9853,
         0.9838],
        [0.9895, 0.9887, 0.9882, 0.9869, 0.9867, 0.9865, 0.9862, 0.9858, 0.9855,
         0.9846],
        [0.9917, 0.9914, 0.9910, 0.9908, 0.9904, 0.9902, 0.9897, 0.9894, 0.9894,
         0.9893],
        [0.9885, 0.9806, 0.9799, 0.9789, 0.9783, 0.9782, 0.9780, 0.9776, 0.9766,
         0.9759],
        [0.9890, 0.9872, 0.9871, 0.9849, 0.9805, 0.9790, 0.9777, 0.9759, 0.9758,
         0.9748],
        [0.9964, 0.9957, 0.9953, 0.9952, 0.9946, 0.9944, 0.9943, 0.9943, 0.9938,
         0.9937],
        [0.9865, 0.9863, 0.9860, 0.9858, 0.9849, 0.9840, 0.9839, 0.9838, 0.9836,
         0.9835],
        [0.9883, 0.9869, 0.9867, 0.9867, 0.9859, 0.9852, 0.9846, 0.9839, 0.9838,
         0.9835],
        [0.9908, 0.9882, 0.9873, 0.9860, 0.9853, 0.9847, 0.9846, 0.9846, 0.9845,
         0.9840]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 991983.5000,  989272.0625,  943860.2500,  872847.1875,  819544.2500,
          783761.6875,  768937.4375,  757060.8750,  710383.5625,  686648.6875],
        [1415532.5000, 1415174.8750, 1402327.1250, 1388425.0000, 1373362.1250,
         1371682.7500, 1369769.0000, 1364771.0000, 1357378.8750, 1353399.0000],
        [1550199.0000, 1536497.6250, 1510363.0000, 1499072.2500, 1495241.5000,
         1458446.7500, 1439257.3750, 1431311.7500, 1426913.5000, 1413112.7500],
        [1533486.5000, 1527235.5000, 1511956.8750, 1496088.7500, 1495538.1250,
         1491928.3750, 1491778.8750, 1486188.3750, 1484555.1250, 1449826.5000],
        [1523940.2500, 1498564.7500, 1479960.8750, 1462519.2500, 1456548.0000,
         1454015.1250, 1452093.1250, 1451634.8750, 1451634.8750, 1440217.2500],
        [1575928.6250, 1574327.3750, 1567878.5000, 1561063.1250, 1560784.7500,
         1560478.1250, 1554080.1250, 1543439.8750, 1542155.3750, 1542081.8750],
        [1590386.5000, 1588403.8750, 1588135.7500, 1587561.8750, 1584437.0000,
         1583701.2500, 1582757.6250, 1582605.2500, 1581794.8750, 1577412.7500],
        [1560890.3750, 1548262.0000, 1547842.7500, 1547332.1250, 1546135.8750,
         1544473.5000, 1536027.3750, 1528608.1250, 1523505.7500, 1515813.0000],
        [1458573.2500, 1456750.7500, 1435804.1250, 1434653.0000, 1422808.5000,
         1420437.2500, 1416592.6250, 1410530.2500, 1407612.8750, 1387034.1250],
        [1308553.1250, 1265930.7500, 1238193.8750, 1107744.1250, 1085970.3750,
         1071699.3750, 1069978.5000, 1058050.6250, 1038272.0000, 1011508.1250],
        [1518352.2500, 1485958.7500, 1484570.6250, 1435053.8750, 1432348.1250,
         1429909.1250, 1401363.2500, 1375051.5000, 1369711.5000, 1357059.2500],
        [1528201.5000, 1525563.0000, 1520195.1250, 1516077.6250, 1501824.0000,
         1497726.2500, 1481154.0000, 1479687.1250, 1466298.3750, 1465634.2500],
        [1523385.1250, 1489549.8750, 1441239.5000, 1435282.5000, 1428533.7500,
         1373371.3750, 1365674.6250, 1326308.3750, 1301931.0000, 1299660.7500],
        [1491141.7500, 1483138.6250, 1447832.7500, 1429125.1250, 1416312.8750,
         1411694.3750, 1408226.5000, 1403198.0000, 1373518.0000, 1368283.2500],
        [1413529.2500, 1393169.3750, 1373398.8750, 1328849.3750, 1282301.5000,
         1281064.5000, 1232701.0000, 1230990.5000, 1188077.8750, 1180035.7500],
        [1430483.3750, 1426256.3750, 1366412.0000, 1336389.5000, 1327624.5000,
         1327549.7500, 1305191.7500, 1289594.7500, 1283504.2500, 1269967.0000],
        [1499245.2500, 1494468.7500, 1434126.3750, 1415397.5000, 1388658.1250,
         1387223.2500, 1370825.0000, 1366749.5000, 1357953.7500, 1318338.3750],
        [1552794.2500, 1501579.0000, 1471692.0000, 1466697.0000, 1418620.5000,
         1385195.3750, 1366873.5000, 1362753.8750, 1351857.5000, 1349514.3750],
        [1561265.6250, 1553272.6250, 1497180.6250, 1491483.1250, 1469502.7500,
         1451756.7500, 1448296.8750, 1444736.2500, 1444093.0000, 1440229.6250],
        [1462875.0000, 1411522.0000, 1355830.2500, 1337267.8750, 1333545.2500,
         1262689.7500, 1261978.2500, 1254048.0000, 1212712.8750, 1206446.7500],
        [1436082.1250, 1378922.8750, 1343818.6250, 1297945.2500, 1293668.2500,
         1291429.6250, 1252966.1250, 1232744.5000, 1225042.3750, 1219671.8750],
        [1519356.0000, 1504368.3750, 1481895.7500, 1476999.8750, 1465370.2500,
         1459434.5000, 1409085.0000, 1406496.5000, 1404410.8750, 1399909.8750],
        [1376362.1250, 1333143.5000, 1301692.6250, 1299326.1250, 1260941.2500,
         1235939.2500, 1233403.0000, 1233363.0000, 1230335.6250, 1229784.2500],
        [1471011.5000, 1423610.6250, 1421296.2500, 1407790.1250, 1388520.2500,
         1373258.7500, 1373038.7500, 1369935.0000, 1358622.1250, 1267537.3750],
        [1445614.2500, 1421452.1250, 1334059.2500, 1281338.1250, 1256436.2500,
         1236564.1250, 1236263.5000, 1218968.3750, 1210354.6250, 1171472.5000],
        [1390905.8750, 1385379.0000, 1334867.3750, 1331239.0000, 1307624.8750,
         1307454.1250, 1307098.8750, 1305588.8750, 1304059.5000, 1301662.7500],
        [1369472.5000, 1357995.2500, 1326946.0000, 1286871.0000, 1277826.1250,
         1257554.6250, 1254306.3750, 1252982.8750, 1251503.2500, 1245391.8750],
        [1378709.7500, 1326336.2500, 1314142.1250, 1313727.2500, 1299663.2500,
         1288935.7500, 1287952.6250, 1275656.3750, 1253836.2500, 1253268.5000],
        [1464666.0000, 1453337.2500, 1452454.6250, 1417395.2500, 1395114.6250,
         1374793.1250, 1374662.0000, 1367129.0000, 1359478.8750, 1353039.0000],
        [1342676.0000, 1306601.6250, 1249549.7500, 1239738.1250, 1233910.1250,
         1188591.2500, 1156414.1250, 1154848.0000, 1150428.0000, 1137989.6250],
        [1427346.3750, 1420430.5000, 1418577.2500, 1360166.1250, 1359305.1250,
         1345123.8750, 1339362.3750, 1339120.8750, 1334998.3750, 1320884.3750],
        [1392470.7500, 1386735.2500, 1385153.1250, 1382367.2500, 1362868.2500,
         1352618.3750, 1346201.8750, 1329395.6250, 1321218.1250, 1303298.7500],
        [1495531.0000, 1494675.5000, 1493115.5000, 1454538.0000, 1432615.8750,
         1404113.6250, 1379057.0000, 1363856.3750, 1356779.7500, 1355421.7500],
        [1487324.1250, 1455140.1250, 1453905.6250, 1451918.6250, 1448306.3750,
         1445787.8750, 1444686.6250, 1421469.7500, 1417759.0000, 1417033.1250],
        [1525378.1250, 1419688.2500, 1396597.5000, 1394715.3750, 1389153.3750,
         1381518.6250, 1366346.8750, 1362416.0000, 1361206.8750, 1339539.8750],
        [1463747.2500, 1460377.1250, 1427949.5000, 1412421.6250, 1405150.3750,
         1388390.5000, 1381240.5000, 1380717.6250, 1375102.6250, 1355327.3750],
        [1487399.3750, 1455462.1250, 1446903.7500, 1426117.6250, 1420024.1250,
         1409930.5000, 1403932.8750, 1402463.5000, 1397174.3750, 1393084.3750],
        [1413767.8750, 1393222.5000, 1380025.2500, 1377789.7500, 1344513.3750,
         1327216.8750, 1324489.3750, 1311974.3750, 1263360.6250, 1240880.7500],
        [1532433.8750, 1432386.3750, 1402708.2500, 1385511.0000, 1380329.2500,
         1376181.0000, 1362391.2500, 1352962.8750, 1330286.0000, 1323933.7500],
        [1535574.8750, 1477834.0000, 1438081.6250, 1426585.6250, 1414794.2500,
         1412571.1250, 1385167.5000, 1371093.0000, 1369573.1250, 1366720.8750],
        [1416257.6250, 1331082.8750, 1304319.5000, 1304132.8750, 1299957.1250,
         1296764.8750, 1292991.0000, 1291038.1250, 1289453.2500, 1278912.3750],
        [1365505.3750, 1360411.3750, 1330579.0000, 1320087.1250, 1312007.0000,
         1302142.0000, 1300606.7500, 1293766.8750, 1291467.8750, 1290661.3750],
        [1519915.3750, 1469912.0000, 1444938.8750, 1398694.1250, 1343577.7500,
         1331538.7500, 1325888.5000, 1319481.7500, 1308434.6250, 1298962.0000],
        [1317355.6250, 1317123.1250, 1312886.8750, 1310567.6250, 1287796.7500,
         1285854.0000, 1262657.1250, 1255278.0000, 1248992.1250, 1247429.2500],
        [1253189.5000, 1239153.1250, 1238257.6250, 1236714.0000, 1231225.3750,
         1229553.2500, 1222279.0000, 1216635.2500, 1209281.6250, 1192351.5000],
        [1411574.5000, 1379125.3750, 1315981.8750, 1302481.0000, 1288377.7500,
         1287268.7500, 1273758.7500, 1267674.0000, 1257791.0000, 1252693.7500],
        [1389968.3750, 1355044.3750, 1322140.8750, 1306154.2500, 1305657.3750,
         1305102.1250, 1304519.7500, 1303768.5000, 1297480.0000, 1296751.3750],
        [1431896.0000, 1414256.0000, 1401876.5000, 1386589.6250, 1360211.5000,
         1355738.5000, 1348541.7500, 1333123.1250, 1272328.5000, 1217455.7500],
        [1469851.7500, 1425068.1250, 1419779.1250, 1417588.6250, 1411998.6250,
         1408883.5000, 1406036.5000, 1400036.7500, 1399696.2500, 1396509.5000],
        [1557885.0000, 1554751.6250, 1553434.1250, 1542695.2500, 1541971.5000,
         1512056.2500, 1508724.6250, 1505795.1250, 1483923.7500, 1460288.0000],
        [1526377.8750, 1510272.2500, 1471947.5000, 1470941.3750, 1448632.3750,
         1438329.8750, 1426287.7500, 1418593.3750, 1403761.5000, 1399629.6250],
        [1373231.2500, 1328522.5000, 1307524.0000, 1219281.1250, 1186802.7500,
         1151060.1250, 1145718.8750, 1118104.1250, 1117231.1250, 1114452.6250],
        [1371015.8750, 1350042.2500, 1331448.5000, 1328143.6250, 1319360.8750,
         1307915.6250, 1297929.1250, 1281173.1250, 1274885.2500, 1242459.2500],
        [1374208.5000, 1372194.3750, 1369605.7500, 1308811.3750, 1303452.8750,
         1290571.5000, 1280357.3750, 1280357.3750, 1277081.7500, 1271748.7500],
        [1403466.8750, 1342494.1250, 1321030.3750, 1230982.2500, 1222654.3750,
         1215106.8750, 1208349.0000, 1192373.0000, 1173113.7500, 1160800.7500],
        [1383042.5000, 1352653.1250, 1345398.3750, 1337600.7500, 1330381.1250,
         1327938.5000, 1322341.3750, 1296998.7500, 1296855.2500, 1270239.6250],
        [1376813.7500, 1361704.1250, 1352747.2500, 1327223.1250, 1323499.5000,
         1318940.7500, 1314770.1250, 1305785.6250, 1300861.1250, 1283900.7500],
        [1422275.2500, 1414574.3750, 1408015.7500, 1402339.1250, 1395616.2500,
         1391984.7500, 1381572.5000, 1375033.0000, 1374500.8750, 1373193.2500],
        [1357456.6250, 1212928.0000, 1201330.7500, 1184535.6250, 1173861.2500,
         1171441.2500, 1168891.2500, 1162101.1250, 1144912.8750, 1133434.0000],
        [1367668.7500, 1333413.1250, 1331341.8750, 1290566.6250, 1210724.1250,
         1185069.0000, 1164489.6250, 1134545.7500, 1132580.3750, 1117293.0000],
        [1520512.6250, 1504283.7500, 1496354.1250, 1493678.1250, 1480974.6250,
         1476646.3750, 1476205.6250, 1474253.0000, 1464759.6250, 1461599.1250],
        [1320516.5000, 1316550.5000, 1310880.0000, 1306768.5000, 1290311.8750,
         1273948.2500, 1271780.2500, 1268997.3750, 1266353.2500, 1264420.1250],
        [1353800.3750, 1326304.5000, 1323183.8750, 1322977.1250, 1308110.1250,
         1295124.8750, 1284481.3750, 1270632.1250, 1269507.0000, 1263964.3750],
        [1403674.3750, 1352205.6250, 1334480.3750, 1310412.6250, 1296368.0000,
         1286039.1250, 1285189.5000, 1284325.7500, 1281626.6250, 1273786.7500]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 991983.5000,       0.0000],
         [ 989272.0625,       0.0000],
         [ 943860.2500,       0.0000],
         ...,
         [ 757060.8750,       0.0000],
         [ 710383.5625,       0.0000],
         [ 686648.6875,       0.0000]],

        [[1415532.5000,       0.0000],
         [1415174.8750,       0.0000],
         [1402327.1250,       0.0000],
         ...,
         [1364771.0000,       0.0000],
         [1357378.8750,       0.0000],
         [1353399.0000,       0.0000]],

        [[1550199.0000,       0.0000],
         [1536497.6250,       0.0000],
         [1510363.0000,       0.0000],
         ...,
         [1431311.7500,       0.0000],
         [1426913.5000,       0.0000],
         [      0.0000, 1413112.7500]],

        ...,

        [[1320516.5000,       0.0000],
         [1316550.5000,       0.0000],
         [1310880.0000,       0.0000],
         ...,
         [1268997.3750,       0.0000],
         [1266353.2500,       0.0000],
         [1264420.1250,       0.0000]],

        [[1353800.3750,       0.0000],
         [1326304.5000,       0.0000],
         [1323183.8750,       0.0000],
         ...,
         [      0.0000, 1270632.1250],
         [1269507.0000,       0.0000],
         [1263964.3750,       0.0000]],

        [[1403674.3750,       0.0000],
         [1352205.6250,       0.0000],
         [1334480.3750,       0.0000],
         ...,
         [      0.0000, 1284325.7500],
         [1281626.6250,       0.0000],
         [1273786.7500,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 8324300.0000,        0.0000],
        [13811822.0000,        0.0000],
        [13347303.0000,  1413112.7500],
        [13473045.0000,  1495538.1250],
        [11690640.0000,  2980488.2500],
        [15582217.0000,        0.0000],
        [14259635.0000,  1587561.8750],
        [13852754.0000,  1546135.8750],
        [14250796.0000,        0.0000],
        [11255901.0000,        0.0000],
        [11285068.0000,  3004311.0000],
        [14982362.0000,        0.0000],
        [13984937.0000,        0.0000],
        [12820777.0000,  1411694.3750],
        [12904118.0000,        0.0000],
        [13362973.0000,        0.0000],
        [14032987.0000,        0.0000],
        [14227577.0000,        0.0000],
        [13332314.0000,  1469502.7500],
        [10429594.0000,  2669321.7500],
        [12972292.0000,        0.0000],
        [10197064.0000,  4330263.5000],
        [11498351.0000,  1235939.2500],
        [13854621.0000,        0.0000],
        [12812524.0000,        0.0000],
        [11970291.0000,  1305588.8750],
        [11626544.0000,  1254306.3750],
        [12992228.0000,        0.0000],
        [14012070.0000,        0.0000],
        [12160746.0000,        0.0000],
        [13665315.0000,        0.0000],
        [13562327.0000,        0.0000],
        [11301558.0000,  2928147.0000],
        [12998644.0000,  1444686.6250],
        [13936562.0000,        0.0000],
        [12622475.0000,  1427949.5000],
        [14242493.0000,        0.0000],
        [13377240.0000,        0.0000],
        [13879124.0000,        0.0000],
        [12831274.0000,  1366720.8750],
        [13104909.0000,        0.0000],
        [11876574.0000,  1290661.3750],
        [13761344.0000,        0.0000],
        [11583284.0000,  1262657.1250],
        [11029487.0000,  1239153.1250],
        [11762968.0000,  1273758.7500],
        [13186587.0000,        0.0000],
        [10817793.0000,  2704224.5000],
        [11346940.0000,  2808508.0000],
        [13715730.0000,  1505795.1250],
        [10143514.0000,  4371259.0000],
        [ 9728194.0000,  2333733.7500],
        [13104373.0000,        0.0000],
        [13128390.0000,        0.0000],
        [11247717.0000,  1222654.3750],
        [13263450.0000,        0.0000],
        [10570492.0000,  2695754.5000],
        [13939105.0000,        0.0000],
        [10553436.0000,  1357456.6250],
        [11150400.0000,  1117293.0000],
        [13375014.0000,  1474253.0000],
        [12890527.0000,        0.0000],
        [11747454.0000,  1270632.1250],
        [11823783.0000,  1284325.7500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 21/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:21, 60.73s/it]  7%|▋         | 2/30 [01:01<11:54, 25.53s/it] 10%|█         | 3/30 [01:02<06:23, 14.21s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.90s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.96s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 6.23941216468811
Epoch 22/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<31:06, 64.35s/it]  7%|▋         | 2/30 [01:05<12:34, 26.94s/it] 10%|█         | 3/30 [01:05<06:44, 14.98s/it] 13%|█▎        | 4/30 [01:06<04:03,  9.36s/it] 17%|█▋        | 5/30 [01:07<02:36,  6.26s/it] 20%|██        | 6/30 [01:08<01:45,  4.39s/it] 23%|██▎       | 7/30 [01:08<01:13,  3.20s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.42s/it] 30%|███       | 9/30 [01:10<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:11<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:26<00:00,  1.33it/s]100%|██████████| 30/30 [01:26<00:00,  2.88s/it]
Epoch loss is 6.261373901367188
Epoch 23/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:59, 57.92s/it]  7%|▋         | 2/30 [01:00<11:46, 25.22s/it] 10%|█         | 3/30 [01:01<06:23, 14.20s/it] 13%|█▎        | 4/30 [01:02<03:51,  8.89s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.95s/it] 20%|██        | 6/30 [01:03<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 6.160233402252198
Epoch 24/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:29, 58.95s/it]  7%|▋         | 2/30 [00:59<11:32, 24.72s/it] 10%|█         | 3/30 [01:00<06:11, 13.77s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.84s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 6.1389545122782385
Epoch 25/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:12, 60.44s/it]  7%|▋         | 2/30 [01:01<11:49, 25.33s/it] 10%|█         | 3/30 [01:01<06:20, 14.11s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.83s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 6.072414541244507
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0057, -0.0104,  0.0730,  ...,  0.0235,  0.0238,  0.0396],
        [ 0.0364,  0.0058,  0.0599,  ...,  0.0164,  0.0334, -0.0124],
        [-0.0321, -0.0002,  0.0769,  ...,  0.0960,  0.0617,  0.0208],
        ...,
        [ 0.0112, -0.0262,  0.0321,  ...,  0.0141, -0.0223,  0.0903],
        [ 0.0060, -0.0214,  0.0614,  ...,  0.0116,  0.0129,  0.0957],
        [ 0.0358, -0.0200,  0.0540,  ..., -0.0087,  0.0392,  0.0361]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9580, 0.9529, 0.9512, 0.9487, 0.9456, 0.9408, 0.9377, 0.9375, 0.9372,
         0.9362],
        [0.9926, 0.9907, 0.9906, 0.9904, 0.9903, 0.9902, 0.9892, 0.9884, 0.9883,
         0.9881],
        [0.9982, 0.9972, 0.9961, 0.9960, 0.9953, 0.9945, 0.9929, 0.9927, 0.9913,
         0.9913],
        [0.9969, 0.9962, 0.9957, 0.9957, 0.9955, 0.9954, 0.9952, 0.9951, 0.9951,
         0.9935],
        [0.9970, 0.9949, 0.9945, 0.9944, 0.9943, 0.9939, 0.9938, 0.9934, 0.9932,
         0.9928],
        [0.9993, 0.9992, 0.9984, 0.9983, 0.9982, 0.9981, 0.9980, 0.9980, 0.9978,
         0.9976],
        [0.9997, 0.9996, 0.9996, 0.9996, 0.9994, 0.9994, 0.9994, 0.9993, 0.9992,
         0.9991],
        [0.9984, 0.9980, 0.9977, 0.9977, 0.9975, 0.9972, 0.9971, 0.9969, 0.9962,
         0.9955],
        [0.9955, 0.9949, 0.9938, 0.9932, 0.9932, 0.9929, 0.9921, 0.9921, 0.9916,
         0.9912],
        [0.9886, 0.9855, 0.9825, 0.9786, 0.9762, 0.9726, 0.9712, 0.9701, 0.9677,
         0.9675],
        [0.9963, 0.9940, 0.9937, 0.9930, 0.9912, 0.9912, 0.9912, 0.9861, 0.9858,
         0.9855],
        [0.9965, 0.9961, 0.9960, 0.9958, 0.9947, 0.9943, 0.9941, 0.9941, 0.9941,
         0.9940],
        [0.9958, 0.9948, 0.9931, 0.9928, 0.9927, 0.9908, 0.9904, 0.9872, 0.9870,
         0.9859],
        [0.9959, 0.9944, 0.9939, 0.9939, 0.9934, 0.9931, 0.9930, 0.9914, 0.9908,
         0.9907],
        [0.9929, 0.9916, 0.9910, 0.9896, 0.9869, 0.9849, 0.9836, 0.9829, 0.9804,
         0.9780],
        [0.9947, 0.9913, 0.9892, 0.9884, 0.9881, 0.9874, 0.9854, 0.9854, 0.9849,
         0.9840],
        [0.9957, 0.9955, 0.9931, 0.9924, 0.9922, 0.9918, 0.9905, 0.9898, 0.9892,
         0.9891],
        [0.9978, 0.9955, 0.9943, 0.9940, 0.9914, 0.9909, 0.9906, 0.9905, 0.9896,
         0.9896],
        [0.9986, 0.9984, 0.9950, 0.9948, 0.9945, 0.9927, 0.9925, 0.9922, 0.9920,
         0.9918],
        [0.9929, 0.9900, 0.9861, 0.9860, 0.9858, 0.9828, 0.9824, 0.9821, 0.9817,
         0.9786],
        [0.9936, 0.9922, 0.9910, 0.9882, 0.9871, 0.9857, 0.9839, 0.9830, 0.9829,
         0.9826],
        [0.9963, 0.9962, 0.9955, 0.9942, 0.9937, 0.9932, 0.9918, 0.9907, 0.9902,
         0.9902],
        [0.9923, 0.9877, 0.9875, 0.9863, 0.9860, 0.9857, 0.9822, 0.9820, 0.9819,
         0.9819],
        [0.9949, 0.9920, 0.9915, 0.9915, 0.9912, 0.9910, 0.9907, 0.9880, 0.9880,
         0.9862],
        [0.9943, 0.9915, 0.9888, 0.9859, 0.9833, 0.9828, 0.9812, 0.9799, 0.9796,
         0.9777],
        [0.9906, 0.9901, 0.9898, 0.9884, 0.9878, 0.9878, 0.9868, 0.9864, 0.9863,
         0.9859],
        [0.9904, 0.9896, 0.9887, 0.9875, 0.9863, 0.9863, 0.9860, 0.9856, 0.9841,
         0.9838],
        [0.9902, 0.9890, 0.9857, 0.9845, 0.9838, 0.9837, 0.9835, 0.9825, 0.9822,
         0.9803],
        [0.9951, 0.9942, 0.9938, 0.9928, 0.9918, 0.9915, 0.9910, 0.9898, 0.9897,
         0.9890],
        [0.9900, 0.9865, 0.9848, 0.9824, 0.9822, 0.9816, 0.9792, 0.9775, 0.9772,
         0.9759],
        [0.9936, 0.9925, 0.9919, 0.9911, 0.9909, 0.9908, 0.9904, 0.9902, 0.9892,
         0.9884],
        [0.9928, 0.9925, 0.9919, 0.9913, 0.9901, 0.9900, 0.9889, 0.9878, 0.9870,
         0.9869],
        [0.9969, 0.9965, 0.9963, 0.9932, 0.9923, 0.9917, 0.9917, 0.9903, 0.9891,
         0.9891],
        [0.9940, 0.9938, 0.9937, 0.9935, 0.9934, 0.9933, 0.9922, 0.9918, 0.9915,
         0.9911],
        [0.9968, 0.9934, 0.9922, 0.9909, 0.9904, 0.9903, 0.9900, 0.9900, 0.9896,
         0.9890],
        [0.9950, 0.9943, 0.9940, 0.9924, 0.9918, 0.9912, 0.9906, 0.9903, 0.9899,
         0.9898],
        [0.9958, 0.9938, 0.9935, 0.9916, 0.9913, 0.9899, 0.9898, 0.9897, 0.9897,
         0.9894],
        [0.9913, 0.9908, 0.9907, 0.9894, 0.9887, 0.9884, 0.9883, 0.9877, 0.9869,
         0.9843],
        [0.9977, 0.9938, 0.9910, 0.9900, 0.9883, 0.9882, 0.9877, 0.9875, 0.9869,
         0.9867],
        [0.9972, 0.9956, 0.9937, 0.9928, 0.9925, 0.9919, 0.9915, 0.9911, 0.9911,
         0.9907],
        [0.9909, 0.9883, 0.9876, 0.9864, 0.9860, 0.9852, 0.9850, 0.9845, 0.9827,
         0.9824],
        [0.9923, 0.9909, 0.9894, 0.9889, 0.9888, 0.9858, 0.9858, 0.9855, 0.9853,
         0.9850],
        [0.9975, 0.9951, 0.9926, 0.9881, 0.9855, 0.9851, 0.9846, 0.9835, 0.9834,
         0.9832],
        [0.9864, 0.9852, 0.9847, 0.9843, 0.9843, 0.9837, 0.9822, 0.9816, 0.9816,
         0.9808],
        [0.9862, 0.9842, 0.9833, 0.9829, 0.9823, 0.9822, 0.9821, 0.9819, 0.9815,
         0.9806],
        [0.9915, 0.9915, 0.9884, 0.9869, 0.9866, 0.9865, 0.9855, 0.9847, 0.9826,
         0.9818],
        [0.9923, 0.9900, 0.9892, 0.9892, 0.9890, 0.9890, 0.9886, 0.9886, 0.9886,
         0.9885],
        [0.9912, 0.9904, 0.9890, 0.9889, 0.9874, 0.9871, 0.9863, 0.9861, 0.9795,
         0.9794],
        [0.9930, 0.9922, 0.9915, 0.9915, 0.9913, 0.9912, 0.9907, 0.9905, 0.9898,
         0.9896],
        [0.9980, 0.9979, 0.9978, 0.9977, 0.9976, 0.9961, 0.9960, 0.9957, 0.9945,
         0.9938],
        [0.9967, 0.9956, 0.9935, 0.9932, 0.9929, 0.9916, 0.9910, 0.9903, 0.9898,
         0.9883],
        [0.9896, 0.9879, 0.9876, 0.9771, 0.9761, 0.9718, 0.9711, 0.9693, 0.9688,
         0.9685],
        [0.9915, 0.9869, 0.9851, 0.9848, 0.9847, 0.9839, 0.9828, 0.9813, 0.9810,
         0.9805],
        [0.9922, 0.9913, 0.9895, 0.9883, 0.9871, 0.9864, 0.9864, 0.9829, 0.9824,
         0.9823],
        [0.9911, 0.9873, 0.9862, 0.9824, 0.9793, 0.9787, 0.9776, 0.9776, 0.9771,
         0.9763],
        [0.9922, 0.9903, 0.9893, 0.9878, 0.9875, 0.9873, 0.9871, 0.9863, 0.9863,
         0.9851],
        [0.9926, 0.9900, 0.9887, 0.9880, 0.9877, 0.9873, 0.9867, 0.9855, 0.9853,
         0.9853],
        [0.9927, 0.9926, 0.9925, 0.9925, 0.9922, 0.9914, 0.9913, 0.9904, 0.9902,
         0.9895],
        [0.9925, 0.9763, 0.9755, 0.9754, 0.9752, 0.9750, 0.9738, 0.9719, 0.9718,
         0.9715],
        [0.9879, 0.9872, 0.9867, 0.9843, 0.9806, 0.9728, 0.9719, 0.9719, 0.9711,
         0.9697],
        [0.9961, 0.9958, 0.9956, 0.9949, 0.9944, 0.9940, 0.9940, 0.9937, 0.9933,
         0.9932],
        [0.9869, 0.9860, 0.9842, 0.9841, 0.9841, 0.9839, 0.9839, 0.9838, 0.9837,
         0.9837],
        [0.9890, 0.9884, 0.9858, 0.9851, 0.9848, 0.9846, 0.9845, 0.9840, 0.9838,
         0.9831],
        [0.9928, 0.9910, 0.9892, 0.9884, 0.9874, 0.9870, 0.9865, 0.9858, 0.9841,
         0.9841]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 878763.1250,  816471.4375,  796951.9375,  769337.9375,  735537.6875,
          687136.6875,  657329.6250,  654848.6875,  652508.4375,  643306.3750],
        [1440273.5000, 1400549.5000, 1398680.8750, 1396175.2500, 1393120.1250,
         1390611.5000, 1372270.2500, 1355920.7500, 1354720.0000, 1350742.7500],
        [1559865.1250, 1537171.8750, 1513338.7500, 1510789.3750, 1495797.7500,
         1478634.7500, 1445712.0000, 1442796.2500, 1414176.3750, 1413344.6250],
        [1531311.8750, 1515928.6250, 1505562.5000, 1504348.3750, 1500502.5000,
         1498823.5000, 1493237.8750, 1493163.8750, 1493025.7500, 1458798.6250],
        [1533280.1250, 1488499.0000, 1480401.2500, 1476906.8750, 1474940.6250,
         1467094.3750, 1465504.3750, 1456020.2500, 1451342.8750, 1444183.8750],
        [1583609.1250, 1583127.5000, 1564110.6250, 1562600.2500, 1559085.7500,
         1557834.3750, 1554259.5000, 1554164.6250, 1551176.5000, 1547279.1250],
        [1593242.0000, 1592265.2500, 1590289.3750, 1590251.5000, 1587248.3750,
         1587079.0000, 1586517.5000, 1585289.3750, 1581816.0000, 1580749.8750],
        [1563761.6250, 1554953.3750, 1549516.1250, 1549080.2500, 1543983.1250,
         1536720.3750, 1534784.1250, 1530684.0000, 1516754.5000, 1501400.1250],
        [1500106.2500, 1487665.8750, 1465287.7500, 1451729.0000, 1451196.0000,
         1445688.6250, 1429860.0000, 1429228.8750, 1418395.8750, 1411954.2500],
        [1360150.6250, 1300888.3750, 1246726.3750, 1178582.6250, 1139069.8750,
         1081404.5000, 1061042.6250, 1044531.8750, 1008525.0000, 1005334.9375],
        [1518411.5000, 1467815.1250, 1461621.2500, 1448661.5000, 1412199.3750,
         1411146.5000, 1410663.5000, 1311771.7500, 1306226.5000, 1301703.7500],
        [1522290.0000, 1514313.2500, 1511266.2500, 1508129.0000, 1482618.1250,
         1475293.7500, 1471280.8750, 1470617.3750, 1470475.7500, 1468966.2500],
        [1507632.8750, 1486118.8750, 1450702.0000, 1443542.1250, 1441069.1250,
         1403175.1250, 1394518.6250, 1333135.7500, 1328930.3750, 1307906.8750],
        [1510105.1250, 1477580.3750, 1466975.5000, 1466790.7500, 1456930.0000,
         1450079.6250, 1448472.1250, 1416001.0000, 1403266.1250, 1401889.8750],
        [1445833.3750, 1419986.1250, 1407935.1250, 1379054.3750, 1327927.0000,
         1290674.8750, 1265627.7500, 1252780.8750, 1209250.5000, 1169517.8750],
        [1483367.6250, 1413363.5000, 1371465.6250, 1355145.1250, 1349894.0000,
         1336134.5000, 1298965.6250, 1298679.5000, 1289779.1250, 1273893.6250],
        [1505607.0000, 1500667.2500, 1451018.8750, 1434735.1250, 1431198.3750,
         1424263.7500, 1398075.3750, 1383815.5000, 1371574.2500, 1370368.7500],
        [1550222.6250, 1501189.6250, 1475600.5000, 1467987.2500, 1414755.2500,
         1405527.0000, 1400076.7500, 1396351.1250, 1380022.6250, 1378394.2500],
        [1567539.1250, 1564913.2500, 1490271.6250, 1485710.8750, 1478873.1250,
         1442167.5000, 1438460.2500, 1432255.2500, 1428431.7500, 1423329.6250],
        [1445490.1250, 1386399.2500, 1311853.1250, 1310057.7500, 1305957.5000,
         1252561.1250, 1243861.8750, 1239779.5000, 1231380.3750, 1178889.5000],
        [1461357.8750, 1432129.5000, 1407051.8750, 1351885.8750, 1331538.7500,
         1304692.7500, 1271038.1250, 1256061.2500, 1252910.0000, 1248727.7500],
        [1517741.2500, 1516212.1250, 1501302.7500, 1473347.7500, 1461721.6250,
         1452528.0000, 1423087.8750, 1401947.2500, 1392002.0000, 1391411.3750],
        [1432673.1250, 1341919.5000, 1338494.0000, 1315334.5000, 1309423.2500,
         1304421.5000, 1241317.5000, 1237733.3750, 1236565.3750, 1235735.5000],
        [1488195.2500, 1427703.0000, 1417527.7500, 1416946.5000, 1412043.1250,
         1406755.3750, 1401840.3750, 1348455.6250, 1347360.2500, 1313118.5000],
        [1476180.3750, 1417857.6250, 1364506.8750, 1307512.7500, 1260865.5000,
         1252032.1250, 1223381.1250, 1201600.0000, 1195876.0000, 1164464.0000],
        [1398238.1250, 1389866.3750, 1382769.3750, 1356299.7500, 1345003.2500,
         1343994.2500, 1325347.3750, 1316809.2500, 1314993.2500, 1308981.2500],
        [1394889.6250, 1379493.7500, 1361821.1250, 1338814.5000, 1316597.0000,
         1316220.3750, 1310953.8750, 1302607.7500, 1275684.2500, 1270594.6250],
        [1391522.8750, 1367568.3750, 1304268.5000, 1281637.6250, 1269410.0000,
         1268535.1250, 1264666.1250, 1245734.0000, 1241535.3750, 1207199.5000],
        [1492813.6250, 1472767.5000, 1463821.1250, 1444580.6250, 1422607.6250,
         1417768.5000, 1406259.1250, 1384255.0000, 1382064.1250, 1368415.1250],
        [1386732.5000, 1318879.1250, 1288478.5000, 1244509.6250, 1240555.3750,
         1229617.7500, 1189092.3750, 1160260.6250, 1155944.3750, 1134243.8750],
        [1460041.5000, 1437434.5000, 1424520.5000, 1408613.3750, 1404781.8750,
         1403670.5000, 1394841.8750, 1391577.2500, 1371290.5000, 1355107.6250],
        [1444345.0000, 1437134.2500, 1425490.7500, 1413193.6250, 1389352.1250,
         1386778.8750, 1366286.8750, 1344148.0000, 1328495.8750, 1326463.8750],
        [1532014.3750, 1522030.2500, 1516977.1250, 1452633.2500, 1433080.5000,
         1422382.5000, 1422089.5000, 1394234.0000, 1368733.5000, 1368706.1250],
        [1468555.7500, 1464052.8750, 1462958.7500, 1459015.6250, 1455718.8750,
         1454213.5000, 1432137.7500, 1423605.1250, 1418248.5000, 1410259.8750],
        [1529376.6250, 1455298.2500, 1432114.5000, 1405237.5000, 1395323.5000,
         1392549.1250, 1387540.8750, 1387289.5000, 1378680.8750, 1366767.8750],
        [1490402.3750, 1475624.3750, 1468862.5000, 1436090.2500, 1423727.3750,
         1411504.5000, 1400142.2500, 1392441.5000, 1384502.0000, 1383996.3750],
        [1507315.2500, 1464078.1250, 1457601.2500, 1418383.7500, 1413293.2500,
         1385706.6250, 1382462.1250, 1381957.3750, 1381946.8750, 1374559.7500],
        [1414040.1250, 1403058.7500, 1401406.0000, 1375290.2500, 1362013.2500,
         1355805.7500, 1353231.1250, 1343225.3750, 1326288.1250, 1279305.2500],
        [1548234.0000, 1464754.0000, 1406835.8750, 1386625.3750, 1354180.0000,
         1352618.3750, 1341791.3750, 1339428.7500, 1328071.5000, 1323177.7500],
        [1537579.3750, 1502752.3750, 1463225.2500, 1442895.3750, 1437799.2500,
         1426083.7500, 1416945.2500, 1409308.1250, 1408507.2500, 1400501.3750],
        [1405965.3750, 1354736.8750, 1340201.7500, 1318675.3750, 1310720.1250,
         1295312.6250, 1291594.7500, 1282072.8750, 1249366.2500, 1245438.1250],
        [1433282.7500, 1405134.3750, 1374940.0000, 1365440.2500, 1363336.2500,
         1306580.3750, 1305895.2500, 1301103.1250, 1297487.3750, 1291134.1250],
        [1543725.5000, 1491458.8750, 1439783.2500, 1350159.3750, 1301073.2500,
         1293865.6250, 1284187.3750, 1264836.2500, 1263275.0000, 1259228.7500],
        [1317338.0000, 1295899.5000, 1285632.0000, 1279618.7500, 1279078.2500,
         1267139.7500, 1240587.3750, 1230748.6250, 1230700.6250, 1215710.7500],
        [1314711.1250, 1276989.2500, 1260376.2500, 1253622.2500, 1242862.2500,
         1241328.2500, 1239264.1250, 1236290.6250, 1227889.3750, 1213350.2500],
        [1418128.1250, 1417680.6250, 1355168.3750, 1326581.6250, 1320932.1250,
         1320090.8750, 1301659.1250, 1286716.3750, 1247637.3750, 1233501.8750],
        [1433702.3750, 1386928.2500, 1371642.2500, 1371061.6250, 1368160.6250,
         1367340.1250, 1360744.8750, 1359830.2500, 1359587.7500, 1357201.5000],
        [1412009.3750, 1395105.2500, 1366658.2500, 1365786.6250, 1336682.6250,
         1331301.1250, 1316335.8750, 1312731.6250, 1194297.5000, 1192013.7500],
        [1447386.8750, 1431250.2500, 1417857.6250, 1417779.2500, 1413924.2500,
         1411045.6250, 1402102.3750, 1396871.8750, 1383029.2500, 1379879.1250],
        [1555577.7500, 1552982.3750, 1549934.3750, 1548267.8750, 1546317.1250,
         1514654.1250, 1512501.8750, 1504807.5000, 1479184.7500, 1465519.7500],
        [1526393.8750, 1502941.6250, 1458944.6250, 1452762.1250, 1446761.6250,
         1418433.7500, 1406838.6250, 1392978.1250, 1384057.1250, 1353563.0000],
        [1379905.5000, 1346158.2500, 1340472.7500, 1153004.7500, 1138077.3750,
         1070221.5000, 1059188.3750, 1032193.3750, 1025095.9375, 1021066.3125],
        [1416660.2500, 1327928.3750, 1292883.7500, 1287057.6250, 1285649.2500,
         1271021.2500, 1250956.7500, 1224323.0000, 1219142.7500, 1211321.1250],
        [1431882.3750, 1413066.8750, 1378011.7500, 1353565.5000, 1330159.1250,
         1317223.7500, 1317223.7500, 1253712.0000, 1244673.5000, 1241959.3750],
        [1409207.2500, 1334537.6250, 1313455.5000, 1245146.0000, 1191166.0000,
         1180587.2500, 1162822.7500, 1162218.5000, 1154466.0000, 1140671.1250],
        [1432080.3750, 1393590.6250, 1373729.0000, 1344501.7500, 1337750.1250,
         1334265.2500, 1330755.5000, 1316502.8750, 1315382.1250, 1294348.1250],
        [1440427.3750, 1386490.6250, 1361293.8750, 1347901.3750, 1341939.8750,
         1334103.7500, 1324230.5000, 1301160.1250, 1297706.3750, 1296936.8750],
        [1441221.6250, 1440169.1250, 1438376.6250, 1438172.1250, 1430566.6250,
         1415874.0000, 1412714.0000, 1395877.1250, 1391333.1250, 1377583.5000],
        [1436815.0000, 1140141.5000, 1128281.1250, 1125432.2500, 1122841.0000,
         1119190.0000, 1100693.8750, 1071828.1250, 1068946.3750, 1065069.3750],
        [1346937.6250, 1332476.1250, 1322456.1250, 1279261.2500, 1212265.3750,
         1085059.3750, 1070906.5000, 1070804.3750, 1058957.1250, 1038680.0000],
        [1514668.5000, 1507779.6250, 1503591.0000, 1488259.1250, 1477009.8750,
         1469373.8750, 1469128.7500, 1463193.1250, 1454246.7500, 1451443.7500],
        [1327328.1250, 1310945.0000, 1277095.1250, 1276012.8750, 1275917.8750,
         1271147.2500, 1270885.5000, 1269873.8750, 1268404.3750, 1267836.0000],
        [1368323.7500, 1356176.8750, 1306834.6250, 1293726.1250, 1288338.3750,
         1283516.3750, 1282599.8750, 1273288.7500, 1270336.5000, 1256872.5000],
        [1443758.2500, 1408142.0000, 1371718.1250, 1355835.3750, 1335784.1250,
         1329522.3750, 1320510.2500, 1306462.0000, 1275521.3750, 1274525.5000]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 878763.1250,       0.0000],
         [ 816471.4375,       0.0000],
         [ 796951.9375,       0.0000],
         ...,
         [ 654848.6875,       0.0000],
         [ 652508.4375,       0.0000],
         [ 643306.3750,       0.0000]],

        [[1440273.5000,       0.0000],
         [1400549.5000,       0.0000],
         [1398680.8750,       0.0000],
         ...,
         [1355920.7500,       0.0000],
         [1354720.0000,       0.0000],
         [1350742.7500,       0.0000]],

        [[1559865.1250,       0.0000],
         [1537171.8750,       0.0000],
         [1513338.7500,       0.0000],
         ...,
         [1442796.2500,       0.0000],
         [1414176.3750,       0.0000],
         [1413344.6250,       0.0000]],

        ...,

        [[1327328.1250,       0.0000],
         [1310945.0000,       0.0000],
         [1277095.1250,       0.0000],
         ...,
         [1269873.8750,       0.0000],
         [1268404.3750,       0.0000],
         [1267836.0000,       0.0000]],

        [[1368323.7500,       0.0000],
         [1356176.8750,       0.0000],
         [1306834.6250,       0.0000],
         ...,
         [1273288.7500,       0.0000],
         [1270336.5000,       0.0000],
         [1256872.5000,       0.0000]],

        [[1443758.2500,       0.0000],
         [1408142.0000,       0.0000],
         [1371718.1250,       0.0000],
         ...,
         [1306462.0000,       0.0000],
         [1275521.3750,       0.0000],
         [1274525.5000,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 7292192.0000,        0.0000],
        [13853064.0000,        0.0000],
        [14811626.0000,        0.0000],
        [13501538.0000,  1493163.8750],
        [11753551.0000,  2984623.0000],
        [14069969.0000,  1547279.1250],
        [14284459.0000,  1590289.3750],
        [13846853.0000,  1534784.1250],
        [14491113.0000,        0.0000],
        [11426256.0000,        0.0000],
        [11070188.0000,  2980032.7500],
        [14895250.0000,        0.0000],
        [12763596.0000,  1333135.7500],
        [13031115.0000,  1466975.5000],
        [13168588.0000,        0.0000],
        [13470688.0000,        0.0000],
        [14271324.0000,        0.0000],
        [12990104.0000,  1380022.6250],
        [11827781.0000,  2924171.0000],
        [11460740.0000,  1445490.1250],
        [12064483.0000,  1252910.0000],
        [11590473.0000,  2940829.0000],
        [11755884.0000,  1237733.3750],
        [13979946.0000,        0.0000],
        [12864277.0000,        0.0000],
        [12165493.0000,  1316809.2500],
        [11928862.0000,  1338814.5000],
        [12842078.0000,        0.0000],
        [14255352.0000,        0.0000],
        [12348314.0000,        0.0000],
        [14051878.0000,        0.0000],
        [13861690.0000,        0.0000],
        [11467786.0000,  2965095.0000],
        [13016630.0000,  1432137.7500],
        [14130179.0000,        0.0000],
        [12791670.0000,  1475624.3750],
        [11403400.0000,  2763904.2500],
        [12334358.0000,  1279305.2500],
        [13845717.0000,        0.0000],
        [13045096.0000,  1400501.3750],
        [13094084.0000,        0.0000],
        [12153200.0000,  1291134.1250],
        [13491594.0000,        0.0000],
        [11401866.0000,  1240587.3750],
        [11191973.0000,  1314711.1250],
        [13228096.0000,        0.0000],
        [13736200.0000,        0.0000],
        [10618899.0000,  2604023.0000],
        [11307323.0000,  2793803.5000],
        [13724940.0000,  1504807.5000],
        [10014789.0000,  4328885.0000],
        [ 9368118.0000,  2197265.7500],
        [12786944.0000,        0.0000],
        [13281478.0000,        0.0000],
        [ 9969236.0000,  2325041.2500],
        [13472906.0000,        0.0000],
        [11991764.0000,  1440427.3750],
        [14181887.0000,        0.0000],
        [ 8870596.0000,  2508643.0000],
        [ 9688042.0000,  2129761.5000],
        [13329320.0000,  1469373.8750],
        [12815447.0000,        0.0000],
        [11691675.0000,  1288338.3750],
        [12085996.0000,  1335784.1250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 26/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:14, 62.57s/it]  7%|▋         | 2/30 [01:03<12:13, 26.20s/it] 10%|█         | 3/30 [01:04<06:33, 14.58s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.12s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.10s/it] 20%|██        | 6/30 [01:06<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:07<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 6.098975308736166
Epoch 27/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:09, 60.34s/it]  7%|▋         | 2/30 [01:01<11:48, 25.29s/it] 10%|█         | 3/30 [01:01<06:20, 14.08s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.82s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.91s/it] 20%|██        | 6/30 [01:04<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 6.018531274795532
Epoch 28/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:56, 61.95s/it]  7%|▋         | 2/30 [01:02<12:06, 25.95s/it] 10%|█         | 3/30 [01:03<06:29, 14.44s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.04s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.05s/it] 20%|██        | 6/30 [01:05<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 5.974621438980103
Epoch 29/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:42, 59.41s/it]  7%|▋         | 2/30 [01:00<11:37, 24.90s/it] 10%|█         | 3/30 [01:01<06:24, 14.23s/it] 13%|█▎        | 4/30 [01:02<03:51,  8.91s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.97s/it] 20%|██        | 6/30 [01:03<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 6.023030932744344
Epoch 30/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:23, 56.66s/it]  7%|▋         | 2/30 [01:00<11:52, 25.46s/it] 10%|█         | 3/30 [01:01<06:22, 14.18s/it] 13%|█▎        | 4/30 [01:01<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.94s/it] 20%|██        | 6/30 [01:03<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 5.932587544123332
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0130, -0.0089,  0.0669,  ...,  0.0276,  0.0249,  0.0231],
        [ 0.0208,  0.0127,  0.0571,  ...,  0.0200,  0.0354, -0.0208],
        [-0.0346,  0.0015,  0.0656,  ...,  0.0954,  0.0612,  0.0088],
        ...,
        [ 0.0041, -0.0279,  0.0228,  ...,  0.0085, -0.0250,  0.0886],
        [-0.0070, -0.0208,  0.0591,  ...,  0.0089,  0.0189,  0.0892],
        [ 0.0232, -0.0139,  0.0503,  ..., -0.0075,  0.0455,  0.0286]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9469, 0.9357, 0.9349, 0.9347, 0.9311, 0.9304, 0.9262, 0.9237, 0.9230,
         0.9225],
        [0.9927, 0.9911, 0.9909, 0.9901, 0.9897, 0.9887, 0.9885, 0.9884, 0.9884,
         0.9869],
        [0.9982, 0.9963, 0.9961, 0.9961, 0.9946, 0.9942, 0.9930, 0.9930, 0.9925,
         0.9917],
        [0.9965, 0.9960, 0.9958, 0.9956, 0.9951, 0.9950, 0.9949, 0.9948, 0.9947,
         0.9941],
        [0.9969, 0.9948, 0.9948, 0.9947, 0.9947, 0.9946, 0.9943, 0.9935, 0.9934,
         0.9929],
        [0.9994, 0.9994, 0.9983, 0.9982, 0.9982, 0.9981, 0.9981, 0.9980, 0.9979,
         0.9977],
        [0.9998, 0.9997, 0.9996, 0.9996, 0.9995, 0.9995, 0.9995, 0.9993, 0.9992,
         0.9992],
        [0.9983, 0.9980, 0.9977, 0.9975, 0.9975, 0.9972, 0.9969, 0.9965, 0.9958,
         0.9948],
        [0.9958, 0.9948, 0.9944, 0.9941, 0.9932, 0.9931, 0.9914, 0.9911, 0.9910,
         0.9909],
        [0.9885, 0.9848, 0.9811, 0.9765, 0.9741, 0.9737, 0.9694, 0.9686, 0.9683,
         0.9652],
        [0.9962, 0.9934, 0.9928, 0.9914, 0.9913, 0.9901, 0.9894, 0.9840, 0.9839,
         0.9839],
        [0.9963, 0.9961, 0.9948, 0.9947, 0.9944, 0.9940, 0.9939, 0.9938, 0.9929,
         0.9922],
        [0.9949, 0.9939, 0.9929, 0.9923, 0.9920, 0.9910, 0.9905, 0.9890, 0.9872,
         0.9840],
        [0.9962, 0.9952, 0.9948, 0.9942, 0.9939, 0.9926, 0.9921, 0.9911, 0.9909,
         0.9905],
        [0.9936, 0.9903, 0.9902, 0.9898, 0.9892, 0.9868, 0.9848, 0.9820, 0.9815,
         0.9800],
        [0.9959, 0.9915, 0.9900, 0.9895, 0.9886, 0.9871, 0.9858, 0.9840, 0.9839,
         0.9838],
        [0.9951, 0.9946, 0.9936, 0.9933, 0.9924, 0.9924, 0.9918, 0.9913, 0.9907,
         0.9904],
        [0.9973, 0.9949, 0.9937, 0.9930, 0.9922, 0.9918, 0.9913, 0.9910, 0.9904,
         0.9900],
        [0.9986, 0.9984, 0.9949, 0.9947, 0.9938, 0.9937, 0.9934, 0.9918, 0.9917,
         0.9915],
        [0.9908, 0.9880, 0.9868, 0.9839, 0.9832, 0.9817, 0.9805, 0.9804, 0.9802,
         0.9774],
        [0.9937, 0.9933, 0.9922, 0.9889, 0.9888, 0.9856, 0.9850, 0.9834, 0.9831,
         0.9824],
        [0.9966, 0.9959, 0.9959, 0.9935, 0.9930, 0.9921, 0.9919, 0.9908, 0.9897,
         0.9896],
        [0.9926, 0.9878, 0.9873, 0.9862, 0.9859, 0.9854, 0.9836, 0.9822, 0.9815,
         0.9806],
        [0.9940, 0.9925, 0.9916, 0.9914, 0.9905, 0.9903, 0.9897, 0.9883, 0.9870,
         0.9862],
        [0.9944, 0.9904, 0.9884, 0.9858, 0.9822, 0.9820, 0.9812, 0.9781, 0.9765,
         0.9755],
        [0.9905, 0.9893, 0.9886, 0.9881, 0.9868, 0.9863, 0.9860, 0.9854, 0.9852,
         0.9849],
        [0.9895, 0.9891, 0.9872, 0.9871, 0.9864, 0.9860, 0.9857, 0.9856, 0.9840,
         0.9835],
        [0.9898, 0.9882, 0.9816, 0.9811, 0.9795, 0.9790, 0.9786, 0.9782, 0.9780,
         0.9775],
        [0.9953, 0.9936, 0.9933, 0.9932, 0.9922, 0.9918, 0.9916, 0.9904, 0.9893,
         0.9891],
        [0.9903, 0.9862, 0.9842, 0.9838, 0.9832, 0.9806, 0.9768, 0.9754, 0.9742,
         0.9741],
        [0.9937, 0.9927, 0.9922, 0.9921, 0.9916, 0.9914, 0.9907, 0.9906, 0.9892,
         0.9886],
        [0.9943, 0.9937, 0.9935, 0.9917, 0.9906, 0.9887, 0.9887, 0.9886, 0.9876,
         0.9863],
        [0.9969, 0.9966, 0.9958, 0.9917, 0.9917, 0.9911, 0.9906, 0.9899, 0.9885,
         0.9882],
        [0.9944, 0.9939, 0.9925, 0.9923, 0.9922, 0.9919, 0.9914, 0.9898, 0.9898,
         0.9897],
        [0.9958, 0.9930, 0.9929, 0.9921, 0.9916, 0.9903, 0.9899, 0.9896, 0.9896,
         0.9869],
        [0.9952, 0.9944, 0.9935, 0.9922, 0.9919, 0.9915, 0.9914, 0.9914, 0.9911,
         0.9907],
        [0.9959, 0.9930, 0.9928, 0.9922, 0.9922, 0.9913, 0.9907, 0.9906, 0.9904,
         0.9903],
        [0.9925, 0.9908, 0.9906, 0.9896, 0.9884, 0.9880, 0.9878, 0.9869, 0.9847,
         0.9842],
        [0.9976, 0.9939, 0.9907, 0.9890, 0.9887, 0.9880, 0.9870, 0.9868, 0.9862,
         0.9860],
        [0.9966, 0.9957, 0.9943, 0.9924, 0.9921, 0.9919, 0.9916, 0.9916, 0.9915,
         0.9914],
        [0.9887, 0.9885, 0.9875, 0.9858, 0.9855, 0.9845, 0.9842, 0.9839, 0.9801,
         0.9767],
        [0.9923, 0.9919, 0.9898, 0.9889, 0.9881, 0.9879, 0.9853, 0.9843, 0.9840,
         0.9838],
        [0.9975, 0.9950, 0.9908, 0.9841, 0.9836, 0.9822, 0.9815, 0.9804, 0.9794,
         0.9792],
        [0.9844, 0.9837, 0.9799, 0.9796, 0.9794, 0.9791, 0.9785, 0.9777, 0.9771,
         0.9768],
        [0.9871, 0.9841, 0.9833, 0.9822, 0.9821, 0.9807, 0.9802, 0.9789, 0.9777,
         0.9776],
        [0.9914, 0.9908, 0.9878, 0.9860, 0.9856, 0.9854, 0.9849, 0.9844, 0.9826,
         0.9791],
        [0.9910, 0.9908, 0.9908, 0.9903, 0.9901, 0.9900, 0.9898, 0.9897, 0.9887,
         0.9887],
        [0.9899, 0.9893, 0.9872, 0.9870, 0.9862, 0.9861, 0.9858, 0.9839, 0.9794,
         0.9791],
        [0.9924, 0.9924, 0.9913, 0.9912, 0.9909, 0.9908, 0.9903, 0.9898, 0.9895,
         0.9891],
        [0.9978, 0.9978, 0.9978, 0.9976, 0.9974, 0.9960, 0.9956, 0.9956, 0.9942,
         0.9942],
        [0.9965, 0.9951, 0.9924, 0.9922, 0.9920, 0.9911, 0.9892, 0.9890, 0.9872,
         0.9858],
        [0.9914, 0.9869, 0.9848, 0.9748, 0.9743, 0.9688, 0.9678, 0.9662, 0.9658,
         0.9633],
        [0.9916, 0.9851, 0.9844, 0.9780, 0.9778, 0.9770, 0.9770, 0.9765, 0.9751,
         0.9744],
        [0.9933, 0.9905, 0.9888, 0.9877, 0.9873, 0.9850, 0.9850, 0.9823, 0.9796,
         0.9775],
        [0.9904, 0.9875, 0.9849, 0.9817, 0.9792, 0.9780, 0.9778, 0.9768, 0.9763,
         0.9759],
        [0.9930, 0.9904, 0.9886, 0.9872, 0.9867, 0.9866, 0.9864, 0.9860, 0.9848,
         0.9847],
        [0.9935, 0.9893, 0.9865, 0.9853, 0.9852, 0.9849, 0.9843, 0.9818, 0.9816,
         0.9816],
        [0.9933, 0.9923, 0.9921, 0.9919, 0.9914, 0.9897, 0.9891, 0.9886, 0.9885,
         0.9880],
        [0.9938, 0.9751, 0.9735, 0.9725, 0.9703, 0.9680, 0.9674, 0.9657, 0.9647,
         0.9637],
        [0.9854, 0.9843, 0.9829, 0.9828, 0.9826, 0.9688, 0.9685, 0.9678, 0.9652,
         0.9641],
        [0.9952, 0.9949, 0.9948, 0.9939, 0.9930, 0.9927, 0.9925, 0.9924, 0.9922,
         0.9920],
        [0.9863, 0.9821, 0.9817, 0.9812, 0.9810, 0.9807, 0.9804, 0.9802, 0.9793,
         0.9791],
        [0.9878, 0.9856, 0.9833, 0.9825, 0.9816, 0.9812, 0.9811, 0.9806, 0.9792,
         0.9789],
        [0.9913, 0.9909, 0.9898, 0.9882, 0.9874, 0.9849, 0.9845, 0.9841, 0.9839,
         0.9838]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 749442.8125,  639038.8750,  631256.6250,  629875.9375,  597997.0000,
          592372.0000,  557721.0625,  537913.3125,  533014.0625,  529076.5000],
        [1441598.2500, 1409242.1250, 1404418.8750, 1388327.0000, 1380636.1250,
         1361474.3750, 1357433.2500, 1356073.3750, 1355377.8750, 1327961.2500],
        [1560734.1250, 1516941.1250, 1514184.7500, 1513270.8750, 1481278.2500,
         1473284.6250, 1449044.2500, 1448325.7500, 1436698.5000, 1422269.8750],
        [1521815.3750, 1510432.1250, 1506384.0000, 1501887.0000, 1491229.8750,
         1490214.7500, 1487985.2500, 1486283.2500, 1482959.0000, 1471300.5000],
        [1530276.7500, 1486249.2500, 1485291.3750, 1483188.0000, 1482917.8750,
         1481446.3750, 1474309.1250, 1457458.0000, 1455649.5000, 1446067.8750],
        [1586541.7500, 1586375.2500, 1562431.8750, 1559595.8750, 1559274.6250,
         1558078.0000, 1557773.5000, 1554398.8750, 1551971.1250, 1549491.0000],
        [1594771.3750, 1594319.7500, 1591686.8750, 1590500.2500, 1588075.1250,
         1587946.3750, 1587937.3750, 1584103.1250, 1582083.0000, 1581929.1250],
        [1562430.3750, 1556273.7500, 1549634.3750, 1544855.0000, 1543132.3750,
         1536711.6250, 1531772.0000, 1523264.5000, 1507929.1250, 1486155.7500],
        [1506168.5000, 1485586.1250, 1477264.7500, 1471302.0000, 1451767.7500,
         1449410.3750, 1414449.0000, 1408898.2500, 1406947.2500, 1405696.0000],
        [1358040.5000, 1288569.3750, 1222296.5000, 1144360.5000, 1105796.7500,
         1099141.3750, 1033857.2500, 1021186.1250, 1017967.6250,  972733.7500],
        [1516287.2500, 1456166.0000, 1443531.1250, 1415717.5000, 1413161.2500,
         1389236.8750, 1375957.8750, 1272613.7500, 1271651.6250, 1271569.1250],
        [1518567.8750, 1512640.3750, 1485343.8750, 1483837.3750, 1477083.0000,
         1468020.8750, 1466771.2500, 1465290.5000, 1445316.5000, 1430748.0000],
        [1486959.6250, 1467684.8750, 1444930.5000, 1433620.3750, 1427553.2500,
         1407249.2500, 1397574.1250, 1367881.3750, 1332051.8750, 1273033.7500],
        [1514956.0000, 1493451.5000, 1485343.8750, 1473026.0000, 1466915.1250,
         1439343.8750, 1428645.6250, 1409261.0000, 1404313.1250, 1396895.8750],
        [1461449.8750, 1393048.5000, 1391346.3750, 1383353.6250, 1370900.7500,
         1325882.1250, 1288511.6250, 1236821.2500, 1228746.7500, 1203003.5000],
        [1508455.6250, 1418129.5000, 1386476.0000, 1378132.6250, 1359853.6250,
         1330377.2500, 1306062.1250, 1273026.5000, 1272073.7500, 1269408.8750],
        [1491870.0000, 1481572.1250, 1460962.1250, 1455159.5000, 1436513.6250,
         1435303.0000, 1423039.1250, 1412874.2500, 1400847.5000, 1396176.6250],
        [1538891.0000, 1487673.0000, 1462010.2500, 1447901.8750, 1431665.2500,
         1422530.3750, 1413931.1250, 1406495.1250, 1395641.5000, 1386501.1250],
        [1569751.7500, 1564411.8750, 1487739.6250, 1482615.2500, 1465036.1250,
         1463595.1250, 1456036.8750, 1423302.3750, 1422040.6250, 1417852.2500],
        [1402895.5000, 1348039.0000, 1324363.1250, 1271827.5000, 1258452.0000,
         1232580.0000, 1210478.1250, 1209151.3750, 1206892.1250, 1158109.3750],
        [1462572.3750, 1454585.1250, 1431264.0000, 1365605.6250, 1363208.7500,
         1303096.0000, 1292348.7500, 1262500.6250, 1257461.1250, 1245270.7500],
        [1525291.0000, 1508429.6250, 1508350.6250, 1457640.1250, 1447371.7500,
         1430359.2500, 1424713.5000, 1403618.2500, 1381145.6250, 1379594.8750],
        [1440298.2500, 1343430.3750, 1334428.2500, 1313688.5000, 1307540.2500,
         1298860.2500, 1266587.6250, 1240951.8750, 1228526.5000, 1212477.0000],
        [1468587.8750, 1436941.1250, 1419861.6250, 1416107.6250, 1397267.5000,
         1393706.2500, 1381579.1250, 1354205.8750, 1328198.1250, 1313587.0000],
        [1477597.3750, 1394723.5000, 1356308.7500, 1305859.1250, 1241058.2500,
         1237899.8750, 1223940.0000, 1170603.5000, 1144606.1250, 1127884.1250],
        [1397159.7500, 1372958.8750, 1359117.1250, 1350155.3750, 1325308.1250,
         1316254.3750, 1311005.1250, 1299774.8750, 1296098.5000, 1290656.5000],
        [1377507.1250, 1369505.2500, 1333829.0000, 1331052.3750, 1317461.1250,
         1310118.8750, 1304665.3750, 1303416.7500, 1273021.6250, 1265070.2500],
        [1382616.5000, 1351215.6250, 1230698.2500, 1220952.1250, 1194344.2500,
         1186242.6250, 1178617.5000, 1172004.3750, 1169095.2500, 1160127.8750],
        [1495398.3750, 1459648.8750, 1455267.7500, 1451583.6250, 1431900.1250,
         1423583.3750, 1419571.8750, 1396139.3750, 1374437.8750, 1369668.5000],
        [1393223.8750, 1314447.8750, 1277430.1250, 1269637.6250, 1259441.3750,
         1213346.7500, 1149363.1250, 1125980.7500, 1106561.6250, 1106151.1250],
        [1462504.0000, 1441121.2500, 1432028.5000, 1429166.1250, 1419705.8750,
         1414423.2500, 1400774.0000, 1399195.8750, 1371682.7500, 1359360.8750],
        [1476032.5000, 1462353.3750, 1458601.1250, 1421387.1250, 1398850.2500,
         1362582.2500, 1361043.2500, 1359735.5000, 1339759.6250, 1316578.1250],
        [1529939.7500, 1524664.1250, 1507260.6250, 1420981.8750, 1420392.5000,
         1409459.8750, 1398902.2500, 1385278.5000, 1357038.5000, 1352159.1250],
        [1476285.8750, 1466860.6250, 1438062.5000, 1434394.3750, 1430612.8750,
         1425212.1250, 1414690.5000, 1384107.2500, 1383337.8750, 1381029.8750],
        [1507645.8750, 1448557.8750, 1445899.6250, 1430473.7500, 1418405.3750,
         1393622.5000, 1385768.7500, 1379014.7500, 1378841.2500, 1326480.2500],
        [1494782.3750, 1477829.8750, 1458131.0000, 1431836.0000, 1425970.8750,
         1417767.0000, 1415552.7500, 1414790.2500, 1409239.5000, 1400545.5000],
        [1509763.7500, 1447337.1250, 1443465.1250, 1432387.6250, 1431546.5000,
         1414211.5000, 1401654.6250, 1398718.2500, 1395837.2500, 1393864.5000],
        [1438473.8750, 1404074.7500, 1399195.8750, 1379324.0000, 1356891.0000,
         1348932.7500, 1344036.5000, 1326973.8750, 1286706.6250, 1277555.6250],
        [1546101.8750, 1465767.1250, 1401752.1250, 1367992.2500, 1360944.6250,
         1347760.0000, 1328404.6250, 1324975.8750, 1313101.0000, 1310378.8750],
        [1524856.1250, 1505344.3750, 1476072.0000, 1435400.1250, 1429027.0000,
         1424558.6250, 1420182.6250, 1419432.5000, 1416556.1250, 1415081.7500],
        [1361515.8750, 1357132.8750, 1338654.8750, 1306033.5000, 1300014.1250,
         1283347.5000, 1276742.0000, 1271411.6250, 1205093.3750, 1147295.6250],
        [1434301.3750, 1425796.7500, 1383349.7500, 1365719.0000, 1350644.8750,
         1345712.8750, 1297726.1250, 1279600.5000, 1273851.1250, 1270187.5000],
        [1544866.8750, 1489108.1250, 1402690.8750, 1274794.1250, 1266608.2500,
         1240418.1250, 1228866.2500, 1210185.0000, 1192326.5000, 1188934.7500],
        [1280762.7500, 1267913.3750, 1200653.8750, 1196364.2500, 1192078.6250,
         1186811.7500, 1176369.3750, 1163151.1250, 1153782.3750, 1148391.3750],
        [1331430.7500, 1276008.0000, 1261495.7500, 1240226.6250, 1239601.0000,
         1214460.5000, 1206109.7500, 1184449.7500, 1163703.6250, 1162581.0000],
        [1414331.5000, 1402791.2500, 1344694.2500, 1310641.3750, 1301847.7500,
         1299945.8750, 1289830.8750, 1281315.0000, 1248524.1250, 1187551.1250],
        [1406920.3750, 1403725.2500, 1402848.7500, 1392954.2500, 1390197.7500,
         1386642.6250, 1384259.0000, 1381743.7500, 1361450.8750, 1361450.8750],
        [1384775.3750, 1374363.2500, 1332984.5000, 1328329.8750, 1313623.2500,
         1311918.1250, 1306356.1250, 1270628.5000, 1192012.6250, 1187871.6250],
        [1436339.6250, 1436271.1250, 1413882.5000, 1411769.7500, 1406161.1250,
         1402850.1250, 1392367.1250, 1383971.2500, 1377884.3750, 1369206.1250],
        [1551420.6250, 1550465.1250, 1550373.5000, 1547200.8750, 1542782.1250,
         1510643.7500, 1503687.0000, 1502116.2500, 1473420.7500, 1472521.8750],
        [1523128.0000, 1491837.2500, 1435275.6250, 1430634.7500, 1427462.0000,
         1408969.3750, 1371468.2500, 1367359.7500, 1333668.6250, 1305770.7500],
        [1414936.0000, 1327637.1250, 1288468.6250, 1116673.0000, 1108519.7500,
         1024093.3750, 1010419.6250,  987024.5625,  981644.6250,  947689.9375],
        [1418908.6250, 1294103.7500, 1281499.5000, 1169521.2500, 1165947.5000,
         1151959.5000, 1151483.8750, 1144739.2500, 1120591.3750, 1110638.2500],
        [1453441.1250, 1397878.1250, 1363389.5000, 1343179.3750, 1334037.5000,
         1292373.3750, 1292373.3750, 1243487.0000, 1195162.2500, 1161042.0000],
        [1395987.6250, 1337996.2500, 1289400.3750, 1232528.2500, 1188776.0000,
         1168765.2500, 1165915.3750, 1148645.3750, 1141293.5000, 1134167.1250],
        [1448060.6250, 1395810.5000, 1360436.0000, 1332607.0000, 1323057.7500,
         1322080.3750, 1318612.5000, 1310463.8750, 1287448.0000, 1285688.5000],
        [1457583.1250, 1373391.0000, 1320219.3750, 1297309.2500, 1295649.8750,
         1290011.6250, 1278185.7500, 1234050.2500, 1230572.6250, 1230156.1250],
        [1454040.1250, 1433766.7500, 1430520.1250, 1425132.0000, 1414914.3750,
         1380991.6250, 1370018.6250, 1359844.5000, 1358179.1250, 1349128.3750],
        [1465475.1250, 1121458.3750, 1095987.7500, 1080660.1250, 1047161.0000,
         1013102.0000, 1004421.6875,  980116.1250,  966552.8750,  952688.3125],
        [1298687.0000, 1278768.3750, 1253470.5000, 1251108.2500, 1248228.8750,
         1024637.5625, 1020123.1250, 1010449.4375,  973920.0000,  958649.8125],
        [1493874.5000, 1487051.7500, 1486052.2500, 1467381.2500, 1448856.2500,
         1441273.7500, 1438217.5000, 1434906.1250, 1431610.6250, 1427841.8750],
        [1316766.6250, 1238856.5000, 1231706.8750, 1223336.7500, 1219168.3750,
         1214609.8750, 1208721.2500, 1205719.8750, 1190606.1250, 1187637.1250],
        [1343494.5000, 1303564.6250, 1260013.2500, 1246372.1250, 1230321.5000,
         1224143.1250, 1221623.0000, 1212832.0000, 1189628.8750, 1183699.8750],
        [1412295.0000, 1405832.7500, 1383662.5000, 1351380.5000, 1336645.7500,
         1289445.8750, 1281899.2500, 1274468.3750, 1270790.8750, 1268861.7500]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 749442.8125,       0.0000],
         [ 639038.8750,       0.0000],
         [ 631256.6250,       0.0000],
         ...,
         [ 537913.3125,       0.0000],
         [ 533014.0625,       0.0000],
         [ 529076.5000,       0.0000]],

        [[1441598.2500,       0.0000],
         [1409242.1250,       0.0000],
         [1404418.8750,       0.0000],
         ...,
         [1356073.3750,       0.0000],
         [1355377.8750,       0.0000],
         [1327961.2500,       0.0000]],

        [[1560734.1250,       0.0000],
         [1516941.1250,       0.0000],
         [1514184.7500,       0.0000],
         ...,
         [1448325.7500,       0.0000],
         [1436698.5000,       0.0000],
         [1422269.8750,       0.0000]],

        ...,

        [[1316766.6250,       0.0000],
         [1238856.5000,       0.0000],
         [1231706.8750,       0.0000],
         ...,
         [1205719.8750,       0.0000],
         [1190606.1250,       0.0000],
         [1187637.1250,       0.0000]],

        [[1343494.5000,       0.0000],
         [1303564.6250,       0.0000],
         [1260013.2500,       0.0000],
         ...,
         [1212832.0000,       0.0000],
         [1189628.8750,       0.0000],
         [1183699.8750,       0.0000]],

        [[1412295.0000,       0.0000],
         [1405832.7500,       0.0000],
         [1383662.5000,       0.0000],
         ...,
         [1274468.3750,       0.0000],
         [1270790.8750,       0.0000],
         [1268861.7500,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 5997708.0000,        0.0000],
        [13782542.0000,        0.0000],
        [14816032.0000,        0.0000],
        [13467532.0000,  1482959.0000],
        [13252578.0000,  1530276.7500],
        [15625933.0000,        0.0000],
        [14291664.0000,  1591686.8750],
        [13818895.0000,  1523264.5000],
        [14477490.0000,        0.0000],
        [11263950.0000,        0.0000],
        [10893888.0000,  2932004.7500],
        [14753619.0000,        0.0000],
        [12670658.0000,  1367881.3750],
        [13018701.0000,  1493451.5000],
        [13283064.0000,        0.0000],
        [13501995.0000,        0.0000],
        [14394317.0000,        0.0000],
        [12997599.0000,  1395641.5000],
        [10386564.0000,  4365817.0000],
        [11219893.0000,  1402895.5000],
        [12134817.0000,  1303096.0000],
        [11533372.0000,  2933143.0000],
        [12986788.0000,        0.0000],
        [13910042.0000,        0.0000],
        [12680480.0000,        0.0000],
        [13318488.0000,        0.0000],
        [11816143.0000,  1369505.2500],
        [12245914.0000,        0.0000],
        [14277201.0000,        0.0000],
        [12215584.0000,        0.0000],
        [14129962.0000,        0.0000],
        [13956922.0000,        0.0000],
        [11377236.0000,  2928842.0000],
        [12853564.0000,  1381029.8750],
        [14114710.0000,        0.0000],
        [12851663.0000,  1494782.3750],
        [11434743.0000,  2834042.2500],
        [12275458.0000,  1286706.6250],
        [13767178.0000,        0.0000],
        [14466510.0000,        0.0000],
        [12847241.0000,        0.0000],
        [13426890.0000,        0.0000],
        [13038799.0000,        0.0000],
        [11966280.0000,        0.0000],
        [10948636.0000,  1331430.7500],
        [13081472.0000,        0.0000],
        [13872194.0000,        0.0000],
        [10426075.0000,  2576788.0000],
        [12618933.0000,  1411769.7500],
        [13702515.0000,  1502116.2500],
        [ 9829808.0000,  4265766.0000],
        [ 9209662.0000,  1997444.2500],
        [10888802.0000,  1120591.3750],
        [13076364.0000,        0.0000],
        [11014698.0000,  1188776.0000],
        [13384265.0000,        0.0000],
        [11549546.0000,  1457583.1250],
        [13976536.0000,        0.0000],
        [ 8257726.0000,  2469896.7500],
        [ 9273282.0000,  2044760.7500],
        [13108210.0000,  1448856.2500],
        [12237128.0000,        0.0000],
        [11169321.0000,  1246372.1250],
        [11923902.0000,  1351380.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 31/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:56, 57.81s/it]  7%|▋         | 2/30 [00:58<11:18, 24.25s/it] 10%|█         | 3/30 [00:59<06:05, 13.52s/it] 13%|█▎        | 4/30 [01:00<03:40,  8.48s/it] 17%|█▋        | 5/30 [01:00<02:22,  5.69s/it] 20%|██        | 6/30 [01:01<01:36,  4.01s/it] 23%|██▎       | 7/30 [01:02<01:07,  2.95s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.25s/it] 30%|███       | 9/30 [01:03<00:37,  1.78s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.46s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.24s/it] 40%|████      | 12/30 [01:06<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  2.66s/it]
Epoch loss is 5.8792691389719645
Epoch 32/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:03, 58.05s/it]  7%|▋         | 2/30 [01:01<11:58, 25.67s/it] 10%|█         | 3/30 [01:01<06:25, 14.29s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.99s/it] 20%|██        | 6/30 [01:04<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 5.883327611287435
Epoch 33/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:07<32:26, 67.11s/it]  7%|▋         | 2/30 [01:08<13:09, 28.20s/it] 10%|█         | 3/30 [01:08<07:03, 15.67s/it] 13%|█▎        | 4/30 [01:09<04:14,  9.78s/it] 17%|█▋        | 5/30 [01:10<02:43,  6.52s/it] 20%|██        | 6/30 [01:11<01:49,  4.56s/it] 23%|██▎       | 7/30 [01:11<01:16,  3.31s/it] 27%|██▋       | 8/30 [01:12<00:54,  2.50s/it] 30%|███       | 9/30 [01:13<00:40,  1.95s/it] 33%|███▎      | 10/30 [01:14<00:31,  1.58s/it] 37%|███▋      | 11/30 [01:14<00:25,  1.33s/it] 40%|████      | 12/30 [01:15<00:20,  1.15s/it] 43%|████▎     | 13/30 [01:16<00:17,  1.03s/it] 47%|████▋     | 14/30 [01:17<00:15,  1.06it/s] 50%|█████     | 15/30 [01:17<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:18<00:11,  1.18it/s] 57%|█████▋    | 17/30 [01:19<00:10,  1.22it/s] 60%|██████    | 18/30 [01:20<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:20<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:21<00:07,  1.29it/s] 70%|███████   | 21/30 [01:22<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:23<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:23<00:05,  1.32it/s] 80%|████████  | 24/30 [01:24<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:25<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:26<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:26<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:27<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:28<00:00,  1.34it/s]100%|██████████| 30/30 [01:29<00:00,  1.34it/s]100%|██████████| 30/30 [01:29<00:00,  2.97s/it]
Epoch loss is 5.862976582845052
Epoch 34/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:47, 59.58s/it]  7%|▋         | 2/30 [01:00<11:39, 24.98s/it] 10%|█         | 3/30 [01:01<06:15, 13.92s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.72s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.85s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 5.830167150497436
Epoch 35/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:44, 59.47s/it]  7%|▋         | 2/30 [01:00<11:38, 24.93s/it] 10%|█         | 3/30 [01:00<06:14, 13.89s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.83s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 5.750657908121744
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-3.0551e-02, -6.1058e-03,  5.6570e-02,  ...,  2.6450e-02,
          2.2044e-02,  1.0139e-02],
        [ 4.8553e-03,  1.7004e-02,  5.0828e-02,  ...,  1.8106e-02,
          3.2880e-02, -2.8732e-02],
        [-3.9200e-02,  2.0939e-03,  5.4238e-02,  ...,  9.3458e-02,
          5.9424e-02, -1.1065e-03],
        ...,
        [-2.3437e-03, -2.7210e-02,  1.4169e-02,  ..., -2.8993e-06,
         -2.5238e-02,  8.5657e-02],
        [-2.3158e-02, -1.8696e-02,  5.5844e-02,  ...,  3.5225e-03,
          2.7238e-02,  7.9847e-02],
        [ 9.5236e-03, -6.2925e-03,  4.5204e-02,  ..., -9.1780e-03,
          5.0248e-02,  2.0192e-02]], device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9309, 0.9293, 0.9175, 0.9134, 0.9124, 0.9115, 0.9108, 0.9100, 0.9075,
         0.9046],
        [0.9919, 0.9914, 0.9912, 0.9894, 0.9891, 0.9890, 0.9884, 0.9882, 0.9874,
         0.9874],
        [0.9980, 0.9962, 0.9960, 0.9942, 0.9939, 0.9935, 0.9930, 0.9929, 0.9917,
         0.9912],
        [0.9959, 0.9958, 0.9955, 0.9948, 0.9948, 0.9947, 0.9945, 0.9943, 0.9942,
         0.9939],
        [0.9964, 0.9950, 0.9948, 0.9948, 0.9947, 0.9947, 0.9947, 0.9939, 0.9937,
         0.9936],
        [0.9994, 0.9994, 0.9982, 0.9982, 0.9982, 0.9980, 0.9979, 0.9978, 0.9977,
         0.9976],
        [0.9998, 0.9998, 0.9996, 0.9995, 0.9994, 0.9994, 0.9994, 0.9992, 0.9992,
         0.9991],
        [0.9981, 0.9979, 0.9977, 0.9974, 0.9971, 0.9971, 0.9969, 0.9959, 0.9952,
         0.9950],
        [0.9946, 0.9943, 0.9939, 0.9937, 0.9925, 0.9922, 0.9904, 0.9903, 0.9903,
         0.9902],
        [0.9864, 0.9823, 0.9786, 0.9745, 0.9703, 0.9673, 0.9673, 0.9659, 0.9655,
         0.9627],
        [0.9962, 0.9931, 0.9915, 0.9907, 0.9889, 0.9881, 0.9872, 0.9839, 0.9830,
         0.9830],
        [0.9962, 0.9960, 0.9937, 0.9936, 0.9935, 0.9933, 0.9930, 0.9930, 0.9925,
         0.9916],
        [0.9937, 0.9927, 0.9917, 0.9911, 0.9908, 0.9907, 0.9901, 0.9897, 0.9867,
         0.9845],
        [0.9959, 0.9958, 0.9949, 0.9944, 0.9929, 0.9920, 0.9911, 0.9909, 0.9904,
         0.9901],
        [0.9931, 0.9895, 0.9894, 0.9890, 0.9889, 0.9880, 0.9848, 0.9830, 0.9813,
         0.9794],
        [0.9964, 0.9929, 0.9915, 0.9886, 0.9876, 0.9869, 0.9854, 0.9846, 0.9842,
         0.9839],
        [0.9941, 0.9938, 0.9938, 0.9937, 0.9930, 0.9919, 0.9918, 0.9912, 0.9911,
         0.9905],
        [0.9964, 0.9941, 0.9927, 0.9924, 0.9923, 0.9914, 0.9910, 0.9907, 0.9902,
         0.9894],
        [0.9984, 0.9977, 0.9946, 0.9945, 0.9942, 0.9936, 0.9930, 0.9922, 0.9913,
         0.9912],
        [0.9874, 0.9862, 0.9852, 0.9823, 0.9816, 0.9814, 0.9797, 0.9784, 0.9776,
         0.9771],
        [0.9934, 0.9932, 0.9925, 0.9898, 0.9893, 0.9870, 0.9832, 0.9827, 0.9825,
         0.9825],
        [0.9968, 0.9959, 0.9953, 0.9924, 0.9919, 0.9916, 0.9911, 0.9908, 0.9889,
         0.9888],
        [0.9912, 0.9886, 0.9862, 0.9850, 0.9835, 0.9835, 0.9834, 0.9823, 0.9801,
         0.9801],
        [0.9930, 0.9919, 0.9911, 0.9903, 0.9901, 0.9887, 0.9887, 0.9880, 0.9843,
         0.9832],
        [0.9938, 0.9878, 0.9866, 0.9840, 0.9802, 0.9802, 0.9785, 0.9771, 0.9755,
         0.9745],
        [0.9890, 0.9888, 0.9856, 0.9852, 0.9852, 0.9833, 0.9830, 0.9824, 0.9821,
         0.9818],
        [0.9892, 0.9872, 0.9866, 0.9864, 0.9843, 0.9834, 0.9828, 0.9819, 0.9813,
         0.9810],
        [0.9900, 0.9860, 0.9777, 0.9754, 0.9748, 0.9743, 0.9741, 0.9732, 0.9730,
         0.9729],
        [0.9942, 0.9926, 0.9919, 0.9919, 0.9918, 0.9914, 0.9912, 0.9905, 0.9887,
         0.9886],
        [0.9894, 0.9856, 0.9844, 0.9839, 0.9828, 0.9772, 0.9748, 0.9724, 0.9720,
         0.9707],
        [0.9936, 0.9931, 0.9919, 0.9917, 0.9915, 0.9913, 0.9897, 0.9891, 0.9889,
         0.9882],
        [0.9951, 0.9944, 0.9937, 0.9914, 0.9906, 0.9889, 0.9888, 0.9879, 0.9865,
         0.9850],
        [0.9955, 0.9954, 0.9939, 0.9901, 0.9899, 0.9892, 0.9875, 0.9869, 0.9865,
         0.9857],
        [0.9943, 0.9923, 0.9907, 0.9906, 0.9905, 0.9899, 0.9897, 0.9895, 0.9886,
         0.9881],
        [0.9934, 0.9928, 0.9927, 0.9926, 0.9912, 0.9892, 0.9892, 0.9889, 0.9881,
         0.9867],
        [0.9951, 0.9932, 0.9929, 0.9926, 0.9925, 0.9918, 0.9917, 0.9912, 0.9908,
         0.9904],
        [0.9957, 0.9937, 0.9924, 0.9914, 0.9914, 0.9912, 0.9912, 0.9907, 0.9904,
         0.9902],
        [0.9921, 0.9910, 0.9900, 0.9877, 0.9877, 0.9874, 0.9862, 0.9846, 0.9834,
         0.9831],
        [0.9969, 0.9930, 0.9894, 0.9888, 0.9886, 0.9880, 0.9869, 0.9865, 0.9843,
         0.9832],
        [0.9955, 0.9949, 0.9942, 0.9917, 0.9916, 0.9914, 0.9913, 0.9911, 0.9905,
         0.9904],
        [0.9876, 0.9855, 0.9851, 0.9836, 0.9829, 0.9828, 0.9819, 0.9809, 0.9807,
         0.9719],
        [0.9912, 0.9902, 0.9896, 0.9888, 0.9869, 0.9863, 0.9847, 0.9837, 0.9835,
         0.9833],
        [0.9971, 0.9944, 0.9885, 0.9820, 0.9810, 0.9801, 0.9796, 0.9791, 0.9737,
         0.9718],
        [0.9824, 0.9811, 0.9755, 0.9748, 0.9745, 0.9744, 0.9739, 0.9737, 0.9727,
         0.9706],
        [0.9873, 0.9839, 0.9832, 0.9802, 0.9795, 0.9788, 0.9776, 0.9769, 0.9763,
         0.9756],
        [0.9900, 0.9891, 0.9846, 0.9843, 0.9841, 0.9828, 0.9828, 0.9815, 0.9815,
         0.9768],
        [0.9909, 0.9901, 0.9899, 0.9898, 0.9889, 0.9883, 0.9881, 0.9879, 0.9873,
         0.9864],
        [0.9882, 0.9879, 0.9863, 0.9849, 0.9849, 0.9848, 0.9843, 0.9811, 0.9806,
         0.9794],
        [0.9928, 0.9910, 0.9906, 0.9906, 0.9905, 0.9903, 0.9899, 0.9897, 0.9896,
         0.9877],
        [0.9978, 0.9976, 0.9976, 0.9976, 0.9969, 0.9954, 0.9954, 0.9949, 0.9943,
         0.9941],
        [0.9963, 0.9941, 0.9906, 0.9905, 0.9901, 0.9900, 0.9876, 0.9862, 0.9843,
         0.9840],
        [0.9920, 0.9859, 0.9822, 0.9774, 0.9724, 0.9704, 0.9662, 0.9629, 0.9629,
         0.9622],
        [0.9905, 0.9845, 0.9808, 0.9705, 0.9687, 0.9687, 0.9682, 0.9679, 0.9676,
         0.9674],
        [0.9935, 0.9887, 0.9883, 0.9867, 0.9847, 0.9814, 0.9814, 0.9808, 0.9765,
         0.9759],
        [0.9893, 0.9879, 0.9839, 0.9810, 0.9796, 0.9791, 0.9789, 0.9768, 0.9768,
         0.9737],
        [0.9925, 0.9894, 0.9872, 0.9869, 0.9856, 0.9851, 0.9846, 0.9838, 0.9838,
         0.9828],
        [0.9929, 0.9851, 0.9824, 0.9822, 0.9809, 0.9801, 0.9799, 0.9798, 0.9794,
         0.9761],
        [0.9920, 0.9894, 0.9884, 0.9881, 0.9880, 0.9873, 0.9873, 0.9871, 0.9830,
         0.9827],
        [0.9941, 0.9740, 0.9717, 0.9710, 0.9703, 0.9673, 0.9629, 0.9626, 0.9542,
         0.9528],
        [0.9846, 0.9819, 0.9818, 0.9817, 0.9772, 0.9669, 0.9660, 0.9630, 0.9613,
         0.9588],
        [0.9939, 0.9933, 0.9924, 0.9921, 0.9913, 0.9904, 0.9904, 0.9903, 0.9903,
         0.9897],
        [0.9833, 0.9807, 0.9791, 0.9782, 0.9734, 0.9722, 0.9719, 0.9716, 0.9715,
         0.9713],
        [0.9837, 0.9830, 0.9824, 0.9778, 0.9775, 0.9770, 0.9751, 0.9742, 0.9742,
         0.9718],
        [0.9902, 0.9889, 0.9874, 0.9864, 0.9832, 0.9831, 0.9831, 0.9814, 0.9812,
         0.9811]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 1, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 596409.7500,  582809.5625,  492778.8438,  464364.7500,  457743.5312,
          452173.8438,  447205.9688,  442605.4062,  426793.6875,  409520.7500],
        [1426100.0000, 1414784.7500, 1412125.2500, 1376124.6250, 1369407.2500,
         1367079.3750, 1356346.2500, 1352127.0000, 1337398.0000, 1336314.2500],
        [1555849.2500, 1515646.8750, 1511439.2500, 1472790.0000, 1467347.6250,
         1457811.1250, 1448102.0000, 1445430.8750, 1421137.7500, 1410373.0000],
        [1509257.1250, 1506417.0000, 1500535.5000, 1486643.3750, 1485087.5000,
         1483519.0000, 1478991.5000, 1476115.6250, 1472061.2500, 1466418.6250],
        [1520952.1250, 1490211.8750, 1486382.5000, 1484771.6250, 1484651.2500,
         1483329.5000, 1482628.0000, 1467806.6250, 1462676.8750, 1460845.1250],
        [1586858.0000, 1586673.3750, 1560775.8750, 1560030.2500, 1559484.3750,
         1555899.7500, 1553128.8750, 1550216.7500, 1548808.5000, 1545994.3750],
        [1595290.0000, 1595168.2500, 1592010.2500, 1589375.1250, 1587540.6250,
         1587322.6250, 1586826.1250, 1581823.5000, 1581197.7500, 1581004.6250],
        [1557391.7500, 1552806.0000, 1548406.7500, 1541483.3750, 1536173.8750,
         1535694.8750, 1531700.3750, 1510041.7500, 1493887.3750, 1490971.1250],
        [1482561.5000, 1474292.2500, 1467171.2500, 1463120.5000, 1437969.2500,
         1432580.3750, 1394480.0000, 1392975.3750, 1392709.7500, 1391712.6250],
        [1317128.2500, 1242301.7500, 1179526.0000, 1111363.0000, 1047712.3750,
         1003239.3750, 1002390.0625,  983667.0000,  977095.1250,  939075.5625],
        [1516411.6250, 1449761.6250, 1418214.6250, 1400802.0000, 1365514.5000,
         1351084.1250, 1332886.6250, 1271918.5000, 1255946.2500, 1255767.8750],
        [1515982.2500, 1511919.3750, 1463490.3750, 1460768.5000, 1458928.0000,
         1453658.7500, 1448947.5000, 1447577.3750, 1437817.0000, 1420307.2500],
        [1461763.5000, 1441350.8750, 1422348.5000, 1410171.2500, 1402587.8750,
         1400728.5000, 1388905.7500, 1381451.3750, 1322703.2500, 1282959.6250],
        [1510182.8750, 1506540.6250, 1488888.1250, 1476797.1250, 1446505.1250,
         1428261.3750, 1409789.3750, 1405968.1250, 1395264.8750, 1388284.6250],
        [1450876.3750, 1377615.0000, 1375075.1250, 1368214.1250, 1366600.8750,
         1348608.6250, 1287279.7500, 1255678.0000, 1225976.2500, 1191922.8750],
        [1520638.8750, 1446204.3750, 1417164.2500, 1359786.2500, 1340390.8750,
         1328125.8750, 1299564.1250, 1284853.7500, 1277175.6250, 1270680.6250],
        [1470083.1250, 1465201.0000, 1465079.5000, 1463532.2500, 1448620.0000,
         1425330.5000, 1424020.7500, 1410504.7500, 1408274.8750, 1396605.5000],
        [1520177.7500, 1471937.6250, 1442200.5000, 1436466.8750, 1432622.6250,
         1415401.6250, 1407506.8750, 1400811.3750, 1391891.8750, 1375324.2500],
        [1563913.6250, 1549544.1250, 1480488.8750, 1479108.6250, 1473804.5000,
         1460629.2500, 1447870.0000, 1431231.1250, 1412328.6250, 1411230.0000],
        [1337261.5000, 1313104.7500, 1295026.1250, 1242461.6250, 1230167.8750,
         1226852.2500, 1197879.2500, 1175725.6250, 1162016.8750, 1154561.7500],
        [1457177.3750, 1452471.2500, 1438166.7500, 1383925.1250, 1373822.0000,
         1329429.8750, 1258817.0000, 1249664.1250, 1246250.7500, 1245440.5000],
        [1529567.7500, 1509028.2500, 1495637.8750, 1435116.8750, 1426157.1250,
         1419110.3750, 1409859.2500, 1403529.8750, 1365575.6250, 1363888.8750],
        [1412133.3750, 1359495.6250, 1314425.3750, 1291139.1250, 1264891.6250,
         1264730.0000, 1262527.1250, 1242823.1250, 1204012.3750, 1203508.5000],
        [1447584.2500, 1424770.5000, 1409297.3750, 1393762.1250, 1389971.0000,
         1362438.1250, 1361365.2500, 1349070.5000, 1279145.3750, 1258497.7500],
        [1464050.1250, 1344677.6250, 1321349.1250, 1272611.3750, 1206123.5000,
         1205469.2500, 1176695.8750, 1154319.5000, 1127169.0000, 1111739.3750],
        [1366909.8750, 1364604.5000, 1301962.0000, 1295303.8750, 1295273.1250,
         1260239.1250, 1255215.7500, 1245051.0000, 1239948.6250, 1233644.2500],
        [1371994.2500, 1332586.6250, 1321324.0000, 1317583.0000, 1278679.5000,
         1261948.1250, 1251619.0000, 1236552.3750, 1224945.3750, 1219949.8750],
        [1386370.1250, 1310766.3750, 1164437.3750, 1126841.2500, 1116724.1250,
         1108902.6250, 1105394.0000, 1090551.3750, 1088827.3750, 1087275.0000],
        [1474047.7500, 1440665.0000, 1426151.7500, 1426027.8750, 1423777.6250,
         1416107.6250, 1411345.7500, 1396688.1250, 1361527.5000, 1360363.3750],
        [1375784.7500, 1302202.8750, 1280610.1250, 1272066.5000, 1252225.5000,
         1155983.0000, 1117102.2500, 1079115.3750, 1072743.2500, 1052693.1250],
        [1460969.1250, 1449179.6250, 1426155.7500, 1420830.1250, 1416761.5000,
         1413095.2500, 1380400.3750, 1368597.8750, 1365570.5000, 1351839.3750],
        [1491973.8750, 1477913.0000, 1462890.3750, 1414511.0000, 1400072.7500,
         1365157.6250, 1363914.8750, 1346521.6250, 1318833.7500, 1292268.6250],
        [1499834.3750, 1498920.6250, 1467287.3750, 1388715.0000, 1385081.7500,
         1371872.5000, 1337686.2500, 1327063.6250, 1320219.3750, 1305251.5000],
        [1476007.2500, 1432775.7500, 1400337.2500, 1399111.6250, 1397023.7500,
         1384492.7500, 1380666.3750, 1377724.0000, 1358950.0000, 1349527.2500],
        [1456710.5000, 1444480.0000, 1442437.1250, 1439624.0000, 1410442.8750,
         1371176.7500, 1370828.8750, 1365202.0000, 1351036.5000, 1322625.1250],
        [1492870.6250, 1452993.5000, 1445401.8750, 1439157.2500, 1438312.0000,
         1422590.0000, 1421064.5000, 1410290.8750, 1402645.3750, 1395127.8750],
        [1504289.5000, 1462276.6250, 1436532.7500, 1414917.1250, 1414461.0000,
         1410917.7500, 1410430.7500, 1401110.6250, 1395785.2500, 1391771.0000],
        [1428675.6250, 1407850.5000, 1388258.1250, 1342879.6250, 1341795.2500,
         1337026.7500, 1313826.2500, 1284651.6250, 1261592.0000, 1257591.8750],
        [1530707.3750, 1447824.3750, 1376349.0000, 1362944.8750, 1360669.5000,
         1347772.8750, 1327271.2500, 1319377.2500, 1278758.7500, 1258836.1250],
        [1501238.3750, 1487754.0000, 1472322.3750, 1420832.8750, 1419069.7500,
         1414396.2500, 1413986.3750, 1408669.8750, 1397796.6250, 1395689.5000],
        [1341073.7500, 1300000.5000, 1293372.1250, 1265327.2500, 1253100.0000,
         1251089.1250, 1235234.6250, 1217751.8750, 1213901.1250, 1070567.5000],
        [1411049.6250, 1391394.1250, 1379175.3750, 1362867.0000, 1326446.2500,
         1314993.2500, 1286912.7500, 1267926.6250, 1263384.7500, 1259893.1250],
        [1536261.7500, 1476576.0000, 1357826.8750, 1237652.0000, 1220628.5000,
         1203938.8750, 1196141.7500, 1186502.7500, 1099491.5000, 1070232.6250],
        [1244493.1250, 1222077.3750, 1127200.2500, 1116530.2500, 1111826.2500,
         1110586.3750, 1102874.2500, 1098903.5000, 1084040.6250, 1051764.8750],
        [1334344.2500, 1271905.1250, 1258368.0000, 1206746.0000, 1193704.2500,
         1182218.6250, 1161528.2500, 1151202.7500, 1139991.5000, 1128763.2500],
        [1387953.7500, 1369291.0000, 1284922.3750, 1279247.7500, 1275486.0000,
         1251844.6250, 1251351.6250, 1229063.2500, 1227981.7500, 1148864.6250],
        [1404858.2500, 1388364.0000, 1384494.0000, 1382948.7500, 1365371.2500,
         1354355.7500, 1350058.8750, 1346024.6250, 1335298.8750, 1317960.0000],
        [1352662.2500, 1346271.1250, 1315597.8750, 1289455.7500, 1289225.7500,
         1288156.5000, 1279635.8750, 1221140.7500, 1213198.7500, 1192693.7500],
        [1444656.2500, 1408113.6250, 1399257.2500, 1398471.5000, 1398128.6250,
         1392322.0000, 1385095.0000, 1382186.6250, 1379768.6250, 1343336.7500],
        [1550578.8750, 1546578.2500, 1546270.0000, 1545512.2500, 1532039.3750,
         1499362.5000, 1497900.5000, 1487216.3750, 1475441.3750, 1470290.6250],
        [1517802.0000, 1471828.2500, 1399321.1250, 1396448.2500, 1388618.2500,
         1387301.3750, 1340977.8750, 1313386.6250, 1279008.7500, 1273808.6250],
        [1427336.7500, 1308705.3750, 1241682.2500, 1158423.1250, 1079371.6250,
         1048708.0000,  987697.8125,  942546.0000,  942514.5000,  932308.0000],
        [1397606.1250, 1282903.2500, 1216297.6250, 1049885.8750, 1023179.6250,
         1022852.8125, 1016404.8125, 1011852.5000, 1007486.7500, 1004524.1875],
        [1458044.7500, 1361711.8750, 1353985.1250, 1322773.8750, 1285294.8750,
         1227299.2500, 1227299.2500, 1216570.1250, 1143936.0000, 1134912.6250],
        [1373097.7500, 1346002.8750, 1271400.6250, 1220029.0000, 1195670.7500,
         1187483.1250, 1184025.1250, 1149344.5000, 1148596.1250, 1099224.2500],
        [1438177.6250, 1376049.7500, 1332419.0000, 1326758.6250, 1302011.6250,
         1293935.8750, 1285103.7500, 1270323.2500, 1268844.7500, 1251221.6250],
        [1446674.7500, 1293348.7500, 1245248.1250, 1241254.7500, 1217650.8750,
         1204834.7500, 1200665.2500, 1198634.5000, 1192863.2500, 1137280.0000],
        [1427655.3750, 1375594.5000, 1355048.1250, 1350680.8750, 1348774.5000,
         1335128.3750, 1335051.8750, 1330813.7500, 1255092.5000, 1250560.6250],
        [1470523.3750, 1104543.6250, 1068619.1250, 1057351.6250, 1047527.5625,
         1002997.3125,  941677.2500,  938048.8750,  831839.9375,  815524.3750],
        [1284935.8750, 1235268.8750, 1233391.2500, 1231359.2500, 1155567.3750,
          997749.1875,  984957.6875,  943259.1250,  920560.9375,  888895.3750],
        [1466506.7500, 1454948.6250, 1435434.5000, 1429500.0000, 1412793.5000,
         1395022.6250, 1394945.6250, 1393549.3750, 1392992.6250, 1380472.8750],
        [1260378.6250, 1214159.3750, 1187004.1250, 1172224.5000, 1093922.1250,
         1075730.7500, 1070488.8750, 1067362.3750, 1064715.0000, 1061811.8750],
        [1268502.3750, 1255419.3750, 1244550.0000, 1164606.2500, 1160961.2500,
         1152500.1250, 1121299.0000, 1107201.2500, 1106302.0000, 1069718.3750],
        [1391311.8750, 1365909.1250, 1336053.1250, 1317618.2500, 1258006.8750,
         1257173.3750, 1256743.0000, 1227686.7500, 1223891.0000, 1222414.2500]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 596409.7500,       0.0000],
         [ 582809.5625,       0.0000],
         [ 492778.8438,       0.0000],
         ...,
         [ 442605.4062,       0.0000],
         [ 426793.6875,       0.0000],
         [ 409520.7500,       0.0000]],

        [[1426100.0000,       0.0000],
         [1414784.7500,       0.0000],
         [1412125.2500,       0.0000],
         ...,
         [1352127.0000,       0.0000],
         [1337398.0000,       0.0000],
         [1336314.2500,       0.0000]],

        [[1555849.2500,       0.0000],
         [1515646.8750,       0.0000],
         [1511439.2500,       0.0000],
         ...,
         [1445430.8750,       0.0000],
         [1421137.7500,       0.0000],
         [1410373.0000,       0.0000]],

        ...,

        [[1260378.6250,       0.0000],
         [1214159.3750,       0.0000],
         [1187004.1250,       0.0000],
         ...,
         [1067362.3750,       0.0000],
         [1064715.0000,       0.0000],
         [1061811.8750,       0.0000]],

        [[1268502.3750,       0.0000],
         [1255419.3750,       0.0000],
         [1244550.0000,       0.0000],
         ...,
         [1107201.2500,       0.0000],
         [1106302.0000,       0.0000],
         [1069718.3750,       0.0000]],

        [[1391311.8750,       0.0000],
         [1365909.1250,       0.0000],
         [      0.0000, 1336053.1250],
         ...,
         [1227686.7500,       0.0000],
         [1223891.0000,       0.0000],
         [1222414.2500,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 4772406.0000,        0.0000],
        [13747806.0000,        0.0000],
        [14705928.0000,        0.0000],
        [13398628.0000,  1466418.6250],
        [13303303.0000,  1520952.1250],
        [15607870.0000,        0.0000],
        [14285548.0000,  1592010.2500],
        [13788515.0000,  1510041.7500],
        [12935093.0000,  1394480.0000],
        [10803498.0000,        0.0000],
        [10750812.0000,  2867495.7500],
        [14619396.0000,        0.0000],
        [12533519.0000,  1381451.3750],
        [12946299.0000,  1510182.8750],
        [13247847.0000,        0.0000],
        [13544585.0000,        0.0000],
        [14377252.0000,        0.0000],
        [14294342.0000,        0.0000],
        [10337799.0000,  4372349.0000],
        [10997796.0000,  1337261.5000],
        [12105735.0000,  1329429.8750],
        [11442724.0000,  2914748.2500],
        [12819686.0000,        0.0000],
        [13675902.0000,        0.0000],
        [11272465.0000,  1111739.3750],
        [12858152.0000,        0.0000],
        [11445188.0000,  1371994.2500],
        [11586090.0000,        0.0000],
        [14136702.0000,        0.0000],
        [11960527.0000,        0.0000],
        [14053400.0000,        0.0000],
        [13934056.0000,        0.0000],
        [11065326.0000,  2836607.0000],
        [13956616.0000,        0.0000],
        [13974564.0000,        0.0000],
        [12827584.0000,  1492870.6250],
        [11369298.0000,  2873194.5000],
        [12102555.0000,  1261592.0000],
        [13610511.0000,        0.0000],
        [14331756.0000,        0.0000],
        [12441418.0000,        0.0000],
        [13264042.0000,        0.0000],
        [12585253.0000,        0.0000],
        [11270297.0000,        0.0000],
        [10694428.0000,  1334344.2500],
        [12706007.0000,        0.0000],
        [13629734.0000,        0.0000],
        [10222178.0000,  2565861.0000],
        [12533207.0000,  1398128.6250],
        [13651828.0000,  1499362.5000],
        [10863398.0000,  2905103.5000],
        [10081596.0000,   987697.8125],
        [10010141.0000,  1022852.8125],
        [12731828.0000,        0.0000],
        [10979204.0000,  1195670.7500],
        [13144846.0000,        0.0000],
        [10931780.0000,  1446674.7500],
        [13364400.0000,        0.0000],
        [ 7866452.5000,  2412200.5000],
        [ 8046041.0000,  2829903.5000],
        [12743374.0000,  1412793.5000],
        [11267798.0000,        0.0000],
        [10486453.0000,  1164606.2500],
        [11520755.0000,  1336053.1250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 36/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:18, 58.58s/it]  7%|▋         | 2/30 [01:01<12:04, 25.88s/it] 10%|█         | 3/30 [01:02<06:28, 14.40s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.03s/it] 20%|██        | 6/30 [01:04<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 5.70669535001119
Epoch 37/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:16, 60.57s/it]  7%|▋         | 2/30 [01:02<12:07, 25.99s/it] 10%|█         | 3/30 [01:03<06:30, 14.46s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.05s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.05s/it] 20%|██        | 6/30 [01:05<01:41,  4.25s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 5.666168340047201
Epoch 38/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:41, 59.38s/it]  7%|▋         | 2/30 [01:00<11:45, 25.19s/it] 10%|█         | 3/30 [01:01<06:18, 14.03s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.79s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 5.573561811447144
Epoch 39/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:18, 56.52s/it]  7%|▋         | 2/30 [00:58<11:27, 24.55s/it] 10%|█         | 3/30 [00:59<06:09, 13.68s/it] 13%|█▎        | 4/30 [01:00<03:42,  8.58s/it] 17%|█▋        | 5/30 [01:00<02:23,  5.75s/it] 20%|██        | 6/30 [01:01<01:37,  4.05s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.97s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.26s/it] 30%|███       | 9/30 [01:03<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.66s/it]
Epoch loss is 5.578097931543986
Epoch 40/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:29, 61.01s/it]  7%|▋         | 2/30 [01:01<11:55, 25.56s/it] 10%|█         | 3/30 [01:02<06:24, 14.23s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.91s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.97s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.29it/s] 70%|███████   | 21/30 [01:16<00:06,  1.30it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 5.562701845169068
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0466, -0.0009,  0.0452,  ...,  0.0204,  0.0150,  0.0014],
        [-0.0128,  0.0211,  0.0410,  ...,  0.0123,  0.0281, -0.0368],
        [-0.0458,  0.0015,  0.0407,  ...,  0.0899,  0.0578, -0.0089],
        ...,
        [-0.0085, -0.0255,  0.0072,  ..., -0.0087, -0.0245,  0.0795],
        [-0.0412, -0.0160,  0.0510,  ...,  0.0002,  0.0348,  0.0650],
        [-0.0050,  0.0014,  0.0374,  ..., -0.0122,  0.0525,  0.0106]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9246, 0.9072, 0.9061, 0.9004, 0.8983, 0.8963, 0.8960, 0.8955, 0.8946,
         0.8937],
        [0.9913, 0.9912, 0.9906, 0.9892, 0.9890, 0.9888, 0.9885, 0.9882, 0.9882,
         0.9882],
        [0.9974, 0.9963, 0.9960, 0.9938, 0.9926, 0.9922, 0.9907, 0.9907, 0.9904,
         0.9897],
        [0.9954, 0.9953, 0.9950, 0.9945, 0.9943, 0.9942, 0.9941, 0.9938, 0.9938,
         0.9937],
        [0.9958, 0.9949, 0.9948, 0.9948, 0.9946, 0.9944, 0.9943, 0.9942, 0.9940,
         0.9937],
        [0.9993, 0.9993, 0.9981, 0.9981, 0.9981, 0.9979, 0.9978, 0.9977, 0.9973,
         0.9972],
        [0.9998, 0.9998, 0.9996, 0.9994, 0.9994, 0.9994, 0.9993, 0.9991, 0.9991,
         0.9991],
        [0.9978, 0.9976, 0.9976, 0.9973, 0.9970, 0.9969, 0.9967, 0.9953, 0.9950,
         0.9944],
        [0.9937, 0.9934, 0.9927, 0.9925, 0.9913, 0.9910, 0.9900, 0.9900, 0.9899,
         0.9890],
        [0.9839, 0.9800, 0.9760, 0.9741, 0.9665, 0.9629, 0.9628, 0.9623, 0.9607,
         0.9588],
        [0.9962, 0.9923, 0.9903, 0.9894, 0.9886, 0.9854, 0.9845, 0.9837, 0.9836,
         0.9836],
        [0.9960, 0.9959, 0.9929, 0.9928, 0.9926, 0.9920, 0.9918, 0.9913, 0.9910,
         0.9908],
        [0.9924, 0.9912, 0.9901, 0.9899, 0.9896, 0.9893, 0.9891, 0.9888, 0.9858,
         0.9836],
        [0.9963, 0.9953, 0.9953, 0.9942, 0.9914, 0.9913, 0.9912, 0.9902, 0.9898,
         0.9897],
        [0.9915, 0.9894, 0.9893, 0.9879, 0.9878, 0.9846, 0.9841, 0.9836, 0.9808,
         0.9793],
        [0.9966, 0.9931, 0.9921, 0.9877, 0.9862, 0.9862, 0.9853, 0.9845, 0.9845,
         0.9805],
        [0.9940, 0.9937, 0.9937, 0.9926, 0.9913, 0.9912, 0.9910, 0.9906, 0.9905,
         0.9901],
        [0.9955, 0.9931, 0.9929, 0.9915, 0.9911, 0.9910, 0.9904, 0.9899, 0.9894,
         0.9889],
        [0.9978, 0.9965, 0.9951, 0.9939, 0.9937, 0.9932, 0.9926, 0.9920, 0.9907,
         0.9905],
        [0.9821, 0.9816, 0.9815, 0.9805, 0.9798, 0.9777, 0.9770, 0.9765, 0.9758,
         0.9747],
        [0.9929, 0.9920, 0.9920, 0.9902, 0.9898, 0.9870, 0.9830, 0.9813, 0.9811,
         0.9811],
        [0.9966, 0.9956, 0.9945, 0.9910, 0.9909, 0.9905, 0.9905, 0.9892, 0.9882,
         0.9874],
        [0.9890, 0.9882, 0.9853, 0.9843, 0.9834, 0.9825, 0.9824, 0.9796, 0.9790,
         0.9787],
        [0.9925, 0.9898, 0.9895, 0.9887, 0.9880, 0.9880, 0.9871, 0.9857, 0.9813,
         0.9801],
        [0.9926, 0.9846, 0.9840, 0.9808, 0.9777, 0.9765, 0.9751, 0.9742, 0.9732,
         0.9731],
        [0.9881, 0.9848, 0.9827, 0.9812, 0.9805, 0.9796, 0.9794, 0.9788, 0.9780,
         0.9774],
        [0.9889, 0.9871, 0.9866, 0.9816, 0.9814, 0.9806, 0.9801, 0.9800, 0.9794,
         0.9778],
        [0.9898, 0.9848, 0.9777, 0.9720, 0.9713, 0.9712, 0.9680, 0.9674, 0.9669,
         0.9667],
        [0.9924, 0.9919, 0.9904, 0.9904, 0.9903, 0.9902, 0.9895, 0.9895, 0.9891,
         0.9886],
        [0.9875, 0.9842, 0.9836, 0.9833, 0.9805, 0.9756, 0.9727, 0.9700, 0.9663,
         0.9651],
        [0.9934, 0.9917, 0.9913, 0.9904, 0.9902, 0.9901, 0.9893, 0.9890, 0.9888,
         0.9878],
        [0.9951, 0.9945, 0.9931, 0.9913, 0.9890, 0.9889, 0.9869, 0.9866, 0.9854,
         0.9839],
        [0.9935, 0.9932, 0.9912, 0.9877, 0.9873, 0.9863, 0.9856, 0.9848, 0.9844,
         0.9838],
        [0.9938, 0.9904, 0.9885, 0.9884, 0.9884, 0.9881, 0.9874, 0.9868, 0.9868,
         0.9863],
        [0.9930, 0.9927, 0.9912, 0.9896, 0.9880, 0.9872, 0.9871, 0.9865, 0.9864,
         0.9858],
        [0.9940, 0.9929, 0.9917, 0.9914, 0.9914, 0.9910, 0.9910, 0.9908, 0.9908,
         0.9899],
        [0.9955, 0.9943, 0.9926, 0.9916, 0.9910, 0.9903, 0.9897, 0.9894, 0.9893,
         0.9893],
        [0.9901, 0.9896, 0.9887, 0.9863, 0.9859, 0.9854, 0.9842, 0.9818, 0.9815,
         0.9806],
        [0.9956, 0.9916, 0.9884, 0.9879, 0.9874, 0.9871, 0.9868, 0.9859, 0.9818,
         0.9815],
        [0.9947, 0.9938, 0.9932, 0.9913, 0.9908, 0.9906, 0.9905, 0.9900, 0.9896,
         0.9890],
        [0.9855, 0.9817, 0.9812, 0.9808, 0.9798, 0.9798, 0.9794, 0.9790, 0.9762,
         0.9710],
        [0.9900, 0.9886, 0.9885, 0.9881, 0.9845, 0.9839, 0.9836, 0.9818, 0.9817,
         0.9816],
        [0.9967, 0.9933, 0.9865, 0.9812, 0.9792, 0.9784, 0.9765, 0.9758, 0.9712,
         0.9695],
        [0.9809, 0.9777, 0.9736, 0.9712, 0.9708, 0.9704, 0.9691, 0.9691, 0.9690,
         0.9648],
        [0.9874, 0.9831, 0.9816, 0.9809, 0.9789, 0.9788, 0.9780, 0.9775, 0.9761,
         0.9735],
        [0.9889, 0.9884, 0.9857, 0.9847, 0.9803, 0.9800, 0.9799, 0.9787, 0.9760,
         0.9750],
        [0.9902, 0.9890, 0.9886, 0.9883, 0.9881, 0.9851, 0.9849, 0.9844, 0.9840,
         0.9832],
        [0.9871, 0.9864, 0.9860, 0.9837, 0.9830, 0.9823, 0.9821, 0.9816, 0.9790,
         0.9789],
        [0.9924, 0.9911, 0.9896, 0.9892, 0.9892, 0.9889, 0.9887, 0.9887, 0.9882,
         0.9881],
        [0.9976, 0.9974, 0.9974, 0.9971, 0.9962, 0.9953, 0.9944, 0.9944, 0.9940,
         0.9938],
        [0.9960, 0.9930, 0.9891, 0.9887, 0.9885, 0.9883, 0.9863, 0.9839, 0.9826,
         0.9826],
        [0.9924, 0.9855, 0.9811, 0.9793, 0.9735, 0.9693, 0.9674, 0.9648, 0.9630,
         0.9585],
        [0.9893, 0.9849, 0.9767, 0.9639, 0.9629, 0.9626, 0.9611, 0.9599, 0.9594,
         0.9593],
        [0.9932, 0.9885, 0.9858, 0.9845, 0.9796, 0.9771, 0.9759, 0.9759, 0.9727,
         0.9726],
        [0.9879, 0.9878, 0.9824, 0.9804, 0.9800, 0.9787, 0.9787, 0.9772, 0.9758,
         0.9749],
        [0.9909, 0.9878, 0.9867, 0.9838, 0.9838, 0.9830, 0.9827, 0.9824, 0.9808,
         0.9789],
        [0.9920, 0.9799, 0.9785, 0.9784, 0.9775, 0.9771, 0.9764, 0.9763, 0.9757,
         0.9757],
        [0.9896, 0.9859, 0.9849, 0.9840, 0.9822, 0.9814, 0.9806, 0.9803, 0.9785,
         0.9773],
        [0.9936, 0.9746, 0.9712, 0.9709, 0.9706, 0.9658, 0.9610, 0.9603, 0.9504,
         0.9466],
        [0.9867, 0.9790, 0.9788, 0.9775, 0.9712, 0.9677, 0.9664, 0.9559, 0.9551,
         0.9544],
        [0.9918, 0.9911, 0.9895, 0.9889, 0.9887, 0.9876, 0.9875, 0.9874, 0.9868,
         0.9859],
        [0.9795, 0.9779, 0.9748, 0.9741, 0.9639, 0.9625, 0.9613, 0.9599, 0.9597,
         0.9589],
        [0.9845, 0.9772, 0.9770, 0.9765, 0.9740, 0.9721, 0.9676, 0.9653, 0.9642,
         0.9628],
        [0.9900, 0.9865, 0.9858, 0.9818, 0.9809, 0.9809, 0.9809, 0.9800, 0.9786,
         0.9784]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 545056.5625,  424840.0000,  418508.8438,  385527.4375,  374384.4062,
          363521.4688,  362239.6250,  359719.3125,  355196.6875,  350418.3125],
        [1413604.7500, 1410272.0000, 1399436.0000, 1371337.5000, 1367655.7500,
         1364140.0000, 1357606.7500, 1352631.2500, 1351996.7500, 1351980.0000],
        [1541592.2500, 1517179.7500, 1512227.8750, 1464513.7500, 1440209.0000,
         1432202.0000, 1402154.5000, 1401744.1250, 1395291.5000, 1382166.8750],
        [1498021.8750, 1496605.2500, 1490022.8750, 1479193.3750, 1475115.0000,
         1473862.1250, 1471157.3750, 1464512.3750, 1463825.3750, 1462445.3750],
        [1507319.5000, 1487695.7500, 1486785.1250, 1485114.5000, 1481093.2500,
         1476515.5000, 1475952.2500, 1473482.6250, 1468543.1250, 1462399.2500],
        [1585049.1250, 1584996.1250, 1558143.5000, 1557875.8750, 1557726.0000,
         1553462.2500, 1550982.7500, 1549033.0000, 1540159.5000, 1537891.8750],
        [1595299.1250, 1595279.2500, 1591758.2500, 1587374.1250, 1585921.6250,
         1585662.8750, 1584772.5000, 1580644.3750, 1580172.5000, 1579818.5000],
        [1550175.3750, 1546575.2500, 1546222.8750, 1539158.0000, 1534150.5000,
         1530462.1250, 1525635.7500, 1496024.5000, 1490912.7500, 1476998.5000],
        [1462996.3750, 1455893.8750, 1441139.1250, 1436789.0000, 1413856.8750,
         1407835.7500, 1387228.6250, 1386494.5000, 1385279.8750, 1367988.3750],
        [1271024.7500, 1201980.5000, 1136465.7500, 1105844.2500,  992115.0000,
          941835.2500,  940383.1250,  933872.3750,  913391.0625,  887926.1250],
        [1516045.7500, 1433475.3750, 1393261.1250, 1375406.8750, 1360290.6250,
         1299529.3750, 1282443.3750, 1268784.2500, 1265944.0000, 1265873.8750],
        [1512086.5000, 1509475.8750, 1445961.6250, 1443496.7500, 1439613.0000,
         1426591.1250, 1423857.7500, 1413863.6250, 1406299.2500, 1402724.3750],
        [1436608.1250, 1410966.2500, 1389001.1250, 1385715.8750, 1378905.7500,
         1373332.1250, 1370047.3750, 1363865.5000, 1305744.5000, 1265297.0000],
        [1518224.7500, 1496646.7500, 1496465.3750, 1473047.1250, 1415488.0000,
         1413945.8750, 1411615.0000, 1391569.3750, 1384202.3750, 1382160.2500],
        [1417978.0000, 1374486.3750, 1372982.3750, 1346466.3750, 1343671.1250,
         1284624.6250, 1275988.5000, 1266941.6250, 1216322.0000, 1190535.6250],
        [1523630.6250, 1450298.1250, 1429686.8750, 1342009.0000, 1314144.6250,
         1313947.8750, 1296397.7500, 1282725.8750, 1282546.1250, 1211894.3750],
        [1469020.8750, 1463477.7500, 1461784.5000, 1438815.6250, 1412414.7500,
         1410751.0000, 1406472.3750, 1398480.7500, 1396411.0000, 1389175.8750],
        [1500814.5000, 1450310.6250, 1445333.0000, 1416706.1250, 1409808.1250,
         1407068.0000, 1395555.0000, 1385381.6250, 1374967.5000, 1365795.7500],
        [1551596.7500, 1523237.0000, 1491786.1250, 1467413.3750, 1463627.1250,
         1452303.7500, 1440133.5000, 1428261.3750, 1401200.1250, 1397890.0000],
        [1239435.5000, 1230524.5000, 1229339.8750, 1210784.1250, 1199562.0000,
         1164315.2500, 1152305.6250, 1143190.0000, 1133218.8750, 1114741.7500],
        [1446342.3750, 1427596.8750, 1427127.2500, 1391577.2500, 1383844.6250,
         1329877.5000, 1255765.3750, 1225350.8750, 1222352.5000, 1221976.0000],
        [1525090.2500, 1503530.7500, 1479296.2500, 1406950.0000, 1404397.5000,
         1397950.0000, 1397142.3750, 1371777.0000, 1352458.3750, 1335752.3750],
        [1367012.8750, 1353017.0000, 1296314.7500, 1279169.7500, 1263187.1250,
         1246831.0000, 1244965.5000, 1195783.6250, 1185510.8750, 1180169.6250],
        [1437954.1250, 1383980.6250, 1377637.2500, 1362538.1250, 1349096.2500,
         1348523.7500, 1330831.5000, 1304986.3750, 1225779.7500, 1204535.0000],
        [1438914.3750, 1283613.1250, 1274112.2500, 1216976.3750, 1164530.7500,
         1144504.5000, 1120611.6250, 1106419.1250, 1091936.6250, 1089069.3750],
        [1349657.2500, 1288653.0000, 1249634.3750, 1222877.1250, 1210952.6250,
         1196399.5000, 1193073.7500, 1182717.1250, 1168426.5000, 1158018.7500],
        [1366610.1250, 1330719.8750, 1321968.0000, 1230483.5000, 1226386.6250,
         1213198.7500, 1203508.5000, 1203044.8750, 1192458.3750, 1164850.6250],
        [1383615.0000, 1287799.1250, 1164288.6250, 1073027.7500, 1061925.2500,
         1060896.8750, 1013626.6875, 1005152.7500,  996713.5000,  994513.5000],
        [1436650.6250, 1426124.5000, 1395676.1250, 1395456.5000, 1392685.7500,
         1391351.6250, 1378286.3750, 1376967.3750, 1369424.2500, 1359344.0000],
        [1337968.2500, 1277617.7500, 1265544.3750, 1260317.2500, 1211602.0000,
         1129513.7500, 1083173.5000, 1041900.3125,  988804.3125,  971841.6875],
        [1456996.7500, 1420847.7500, 1412607.5000, 1395809.1250, 1390875.5000,
         1388695.1250, 1374346.1250, 1368527.2500, 1363924.0000, 1344135.2500],
        [1492070.6250, 1479708.2500, 1450643.8750, 1412347.5000, 1367970.1250,
         1366604.8750, 1326241.2500, 1322417.0000, 1299001.5000, 1272348.0000],
        [1459122.8750, 1452900.7500, 1411266.2500, 1343366.3750, 1334410.3750,
         1314953.1250, 1302421.5000, 1288316.3750, 1280220.5000, 1268865.3750],
        [1464938.5000, 1394412.2500, 1357744.0000, 1356229.8750, 1355341.5000,
         1349211.8750, 1336986.0000, 1325842.8750, 1324748.3750, 1315625.5000],
        [1447527.6250, 1440988.0000, 1411965.0000, 1379024.0000, 1348240.7500,
         1332959.1250, 1330471.1250, 1320209.2500, 1317395.7500, 1307312.0000],
        [1469600.8750, 1446695.3750, 1422073.1250, 1416269.7500, 1414355.8750,
         1406860.1250, 1406684.3750, 1403472.3750, 1402967.7500, 1386183.8750],
        [1500947.7500, 1475534.2500, 1440199.3750, 1418663.7500, 1407500.1250,
         1392582.2500, 1380659.7500, 1375433.1250, 1372682.6250, 1372522.8750],
        [1389211.7500, 1379196.3750, 1360792.8750, 1315564.1250, 1308990.0000,
         1298414.5000, 1277537.3750, 1234451.5000, 1229402.0000, 1213346.7500],
        [1502556.0000, 1420248.8750, 1356717.6250, 1347030.1250, 1337228.3750,
         1330541.0000, 1325515.5000, 1308666.7500, 1233834.8750, 1228402.3750],
        [1484361.0000, 1464724.7500, 1452541.8750, 1413517.1250, 1403073.5000,
         1398583.5000, 1398078.0000, 1387376.6250, 1378758.3750, 1368231.0000],
        [1300438.2500, 1231883.1250, 1223588.7500, 1216266.2500, 1198682.6250,
         1198431.1250, 1192545.8750, 1185252.0000, 1139039.5000, 1057755.0000],
        [1387313.1250, 1359016.1250, 1357523.8750, 1350723.3750, 1283004.8750,
         1271477.0000, 1266933.1250, 1234451.5000, 1231722.1250, 1230169.0000],
        [1527301.1250, 1454267.5000, 1319703.2500, 1224054.3750, 1189489.3750,
         1174677.6250, 1144694.5000, 1132023.1250, 1061148.7500, 1035815.3125],
        [1218287.2500, 1164295.2500, 1096843.0000, 1059871.3750, 1054410.2500,
         1048912.1250, 1029073.6875, 1028769.4375, 1028106.4375,  967895.0000],
        [1337591.7500, 1257590.6250, 1230038.8750, 1218358.1250, 1184245.3750,
         1182555.8750, 1168580.2500, 1160031.6250, 1137008.8750, 1095939.6250],
        [1366280.3750, 1355056.0000, 1305261.5000, 1286931.1250, 1208168.1250,
         1202563.0000, 1200999.7500, 1180842.8750, 1135359.7500, 1119674.7500],
        [1390407.2500, 1367419.6250, 1360241.5000, 1354575.2500, 1350458.1250,
         1292811.0000, 1289869.0000, 1280561.2500, 1272832.2500, 1259722.5000],
        [1331642.7500, 1318215.2500, 1310685.1250, 1268008.8750, 1255566.6250,
         1242248.3750, 1239410.7500, 1230500.0000, 1184798.7500, 1183505.7500],
        [1434647.6250, 1408506.0000, 1378690.1250, 1372484.8750, 1372303.0000,
         1365230.6250, 1362538.1250, 1361889.8750, 1352375.7500, 1349829.7500],
        [1546914.6250, 1542270.1250, 1541589.2500, 1536437.6250, 1515298.3750,
         1495933.2500, 1477449.3750, 1476766.1250, 1469696.2500, 1464043.1250],
        [1511818.3750, 1448231.8750, 1369705.1250, 1361217.2500, 1357020.3750,
         1353464.7500, 1316615.8750, 1271752.3750, 1247831.3750, 1247636.2500],
        [1436495.7500, 1301724.8750, 1222493.5000, 1191243.2500, 1095957.3750,
         1032054.5000, 1004618.0625,  967392.9375,  943782.8125,  884109.3750],
        [1373860.0000, 1290465.6250, 1147422.5000,  954936.0000,  942270.0625,
          938439.0000,  917916.1250,  902179.0625,  896487.5625,  894965.3125],
        [1452263.5000, 1358824.3750, 1306311.2500, 1283309.5000, 1196231.8750,
         1154457.1250, 1134380.2500, 1134380.2500, 1083840.0000, 1082694.3750],
        [1345501.0000, 1343489.3750, 1244876.5000, 1209744.1250, 1202575.6250,
         1180523.1250, 1180129.1250, 1155003.3750, 1132520.0000, 1118475.1250],
        [1404315.7500, 1344181.3750, 1323630.7500, 1270466.2500, 1268824.2500,
         1254481.0000, 1249335.2500, 1244773.2500, 1215661.0000, 1183415.5000],
        [1427033.3750, 1200915.0000, 1176635.2500, 1175399.2500, 1159756.1250,
         1153382.0000, 1141568.8750, 1141116.1250, 1130721.8750, 1130312.2500],
        [1380189.7500, 1308707.8750, 1290465.6250, 1273149.1250, 1241641.8750,
         1226684.8750, 1212965.0000, 1208334.0000, 1177577.1250, 1156714.1250],
        [1460641.7500, 1112819.1250, 1059881.6250, 1056322.6250, 1051751.8750,
          982317.0625,  917144.3125,  907126.9375,  788283.3125,  746474.7500],
        [1322978.2500, 1185905.5000, 1181435.3750, 1161143.8750, 1059996.8750,
         1008839.5625,  989979.0000,  852162.0000,  842856.7500,  834620.3125],
        [1422910.2500, 1409576.8750, 1378109.0000, 1365388.1250, 1360926.5000,
         1340123.7500, 1339460.6250, 1335869.6250, 1324863.3750, 1307510.2500],
        [1193515.2500, 1166551.5000, 1117062.8750, 1105952.8750,  955436.1875,
          936749.9375,  920066.8125,  902371.7500,  899397.3750,  889074.3125],
        [1283163.8750, 1155255.6250, 1151676.1250, 1143566.3750, 1103674.8750,
         1073750.5000, 1007591.5000,  975176.5000,  959425.3750,  940193.8750],
        [1386899.1250, 1319748.5000, 1306700.0000, 1234184.3750, 1218953.2500,
         1218189.7500, 1218067.7500, 1203399.3750, 1178078.1250, 1175801.8750]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 545056.5625,       0.0000],
         [ 424840.0000,       0.0000],
         [ 418508.8438,       0.0000],
         ...,
         [ 359719.3125,       0.0000],
         [ 355196.6875,       0.0000],
         [ 350418.3125,       0.0000]],

        [[1413604.7500,       0.0000],
         [1410272.0000,       0.0000],
         [1399436.0000,       0.0000],
         ...,
         [1352631.2500,       0.0000],
         [1351996.7500,       0.0000],
         [1351980.0000,       0.0000]],

        [[1541592.2500,       0.0000],
         [1517179.7500,       0.0000],
         [1512227.8750,       0.0000],
         ...,
         [1401744.1250,       0.0000],
         [1395291.5000,       0.0000],
         [1382166.8750,       0.0000]],

        ...,

        [[1193515.2500,       0.0000],
         [1166551.5000,       0.0000],
         [1117062.8750,       0.0000],
         ...,
         [ 902371.7500,       0.0000],
         [ 899397.3750,       0.0000],
         [ 889074.3125,       0.0000]],

        [[1283163.8750,       0.0000],
         [1155255.6250,       0.0000],
         [1151676.1250,       0.0000],
         ...,
         [ 975176.5000,       0.0000],
         [ 959425.3750,       0.0000],
         [ 940193.8750,       0.0000]],

        [[1386899.1250,       0.0000],
         [1319748.5000,       0.0000],
         [      0.0000, 1306700.0000],
         ...,
         [1203399.3750,       0.0000],
         [1178078.1250,       0.0000],
         [1175801.8750,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 3939412.7500,        0.0000],
        [13740661.0000,        0.0000],
        [14489282.0000,        0.0000],
        [14774762.0000,        0.0000],
        [13297581.0000,  1507319.5000],
        [14035161.0000,  1540159.5000],
        [14274944.0000,  1591758.2500],
        [13740292.0000,  1496024.5000],
        [12759008.0000,  1386494.5000],
        [10324839.0000,        0.0000],
        [10662565.0000,  2798489.0000],
        [14423970.0000,        0.0000],
        [12300578.0000,  1378905.7500],
        [12865140.0000,  1518224.7500],
        [13089996.0000,        0.0000],
        [13447281.0000,        0.0000],
        [14246804.0000,        0.0000],
        [14151740.0000,        0.0000],
        [10257598.0000,  4359850.5000],
        [10586893.0000,  1230524.5000],
        [12001933.0000,  1329877.5000],
        [11290651.0000,  2883693.7500],
        [12611962.0000,        0.0000],
        [13325864.0000,        0.0000],
        [10841618.0000,  1089069.3750],
        [12220410.0000,        0.0000],
        [11086618.0000,  1366610.1250],
        [11041558.0000,        0.0000],
        [13921968.0000,        0.0000],
        [11568284.0000,        0.0000],
        [13916764.0000,        0.0000],
        [13789352.0000,        0.0000],
        [11996721.0000,  1459122.8750],
        [13581081.0000,        0.0000],
        [13636092.0000,        0.0000],
        [11319378.0000,  2855784.7500],
        [11253691.0000,  2883034.5000],
        [11793560.0000,  1213346.7500],
        [12156907.0000,  1233834.8750],
        [14149246.0000,        0.0000],
        [11943882.0000,        0.0000],
        [12972334.0000,        0.0000],
        [12263174.0000,        0.0000],
        [ 9728569.0000,   967895.0000],
        [10634349.0000,  1337591.7500],
        [12361137.0000,        0.0000],
        [13218898.0000,        0.0000],
        [ 8808730.0000,  3755852.2500],
        [12396606.0000,  1361889.8750],
        [13570464.0000,  1495933.2500],
        [10616454.0000,  2868838.7500],
        [10075255.0000,  1004618.0625],
        [ 9341024.0000,   917916.1250],
        [12186693.0000,        0.0000],
        [10932314.0000,  1180523.1250],
        [12759084.0000,        0.0000],
        [ 9268690.0000,  2568149.5000],
        [12476429.0000,        0.0000],
        [ 6968520.0000,  3114243.5000],
        [ 8615318.0000,  1824599.2500],
        [12223812.0000,  1360926.5000],
        [ 9149430.0000,   936749.9375],
        [ 9719724.0000,  1073750.5000],
        [11153322.0000,  1306700.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 41/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:09, 60.34s/it]  7%|▋         | 2/30 [01:01<11:48, 25.29s/it] 10%|█         | 3/30 [01:02<06:26, 14.33s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.97s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.00s/it] 20%|██        | 6/30 [01:04<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 5.504584185282389
Epoch 42/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:57, 59.90s/it]  7%|▋         | 2/30 [01:00<11:43, 25.11s/it] 10%|█         | 3/30 [01:01<06:17, 13.99s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.76s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 5.491292969385783
Epoch 43/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:28, 60.98s/it]  7%|▋         | 2/30 [01:03<12:18, 26.37s/it] 10%|█         | 3/30 [01:03<06:36, 14.67s/it] 13%|█▎        | 4/30 [01:04<03:58,  9.18s/it] 17%|█▋        | 5/30 [01:05<02:33,  6.14s/it] 20%|██        | 6/30 [01:06<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.38s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 5.40428155263265
Epoch 44/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:02, 58.02s/it]  7%|▋         | 2/30 [01:00<11:48, 25.30s/it] 10%|█         | 3/30 [01:01<06:20, 14.09s/it] 13%|█▎        | 4/30 [01:01<03:49,  8.82s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.91s/it] 20%|██        | 6/30 [01:03<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 5.366114012400309
Epoch 45/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:23, 56.66s/it]  7%|▋         | 2/30 [00:57<11:05, 23.77s/it] 10%|█         | 3/30 [00:58<06:05, 13.54s/it] 13%|█▎        | 4/30 [01:00<03:49,  8.83s/it] 17%|█▋        | 5/30 [01:01<02:27,  5.92s/it] 20%|██        | 6/30 [01:01<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:02<01:10,  3.04s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.31s/it] 30%|███       | 9/30 [01:04<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:05<00:24,  1.27s/it] 40%|████      | 12/30 [01:06<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.08it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 5.30243886311849
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0551,  0.0052,  0.0355,  ...,  0.0133,  0.0073, -0.0046],
        [-0.0285,  0.0253,  0.0312,  ...,  0.0053,  0.0209, -0.0429],
        [-0.0535,  0.0003,  0.0253,  ...,  0.0849,  0.0546, -0.0143],
        ...,
        [-0.0140, -0.0250,  0.0002,  ..., -0.0178, -0.0220,  0.0711],
        [-0.0589, -0.0140,  0.0452,  ..., -0.0032,  0.0402,  0.0468],
        [-0.0185,  0.0085,  0.0279,  ..., -0.0176,  0.0533,  0.0010]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9169, 0.9078, 0.8999, 0.8955, 0.8943, 0.8901, 0.8875, 0.8816, 0.8800,
         0.8786],
        [0.9911, 0.9908, 0.9906, 0.9896, 0.9895, 0.9893, 0.9889, 0.9877, 0.9877,
         0.9873],
        [0.9961, 0.9960, 0.9958, 0.9926, 0.9922, 0.9905, 0.9892, 0.9890, 0.9886,
         0.9878],
        [0.9948, 0.9945, 0.9942, 0.9939, 0.9939, 0.9938, 0.9938, 0.9932, 0.9932,
         0.9930],
        [0.9951, 0.9944, 0.9944, 0.9942, 0.9941, 0.9940, 0.9938, 0.9938, 0.9938,
         0.9935],
        [0.9991, 0.9991, 0.9979, 0.9978, 0.9978, 0.9977, 0.9975, 0.9975, 0.9969,
         0.9965],
        [0.9997, 0.9997, 0.9996, 0.9993, 0.9992, 0.9992, 0.9992, 0.9991, 0.9990,
         0.9989],
        [0.9973, 0.9973, 0.9971, 0.9970, 0.9968, 0.9966, 0.9960, 0.9949, 0.9945,
         0.9933],
        [0.9934, 0.9934, 0.9918, 0.9911, 0.9908, 0.9905, 0.9901, 0.9895, 0.9885,
         0.9885],
        [0.9806, 0.9786, 0.9758, 0.9745, 0.9665, 0.9611, 0.9603, 0.9600, 0.9568,
         0.9528],
        [0.9960, 0.9904, 0.9895, 0.9893, 0.9861, 0.9852, 0.9845, 0.9842, 0.9842,
         0.9837],
        [0.9959, 0.9959, 0.9920, 0.9918, 0.9914, 0.9913, 0.9904, 0.9902, 0.9897,
         0.9896],
        [0.9914, 0.9897, 0.9894, 0.9889, 0.9885, 0.9878, 0.9869, 0.9865, 0.9846,
         0.9809],
        [0.9965, 0.9962, 0.9953, 0.9942, 0.9911, 0.9908, 0.9906, 0.9901, 0.9901,
         0.9898],
        [0.9899, 0.9893, 0.9890, 0.9872, 0.9859, 0.9844, 0.9839, 0.9816, 0.9800,
         0.9790],
        [0.9966, 0.9927, 0.9920, 0.9863, 0.9861, 0.9855, 0.9854, 0.9836, 0.9834,
         0.9784],
        [0.9937, 0.9933, 0.9930, 0.9915, 0.9912, 0.9908, 0.9906, 0.9900, 0.9889,
         0.9886],
        [0.9954, 0.9929, 0.9921, 0.9907, 0.9900, 0.9899, 0.9894, 0.9887, 0.9887,
         0.9880],
        [0.9971, 0.9950, 0.9949, 0.9933, 0.9932, 0.9926, 0.9920, 0.9910, 0.9900,
         0.9900],
        [0.9788, 0.9788, 0.9781, 0.9778, 0.9765, 0.9758, 0.9756, 0.9755, 0.9751,
         0.9750],
        [0.9921, 0.9916, 0.9909, 0.9907, 0.9903, 0.9871, 0.9828, 0.9814, 0.9813,
         0.9804],
        [0.9962, 0.9950, 0.9936, 0.9897, 0.9895, 0.9890, 0.9890, 0.9875, 0.9872,
         0.9858],
        [0.9887, 0.9855, 0.9844, 0.9841, 0.9820, 0.9816, 0.9808, 0.9805, 0.9793,
         0.9773],
        [0.9916, 0.9893, 0.9892, 0.9876, 0.9861, 0.9853, 0.9832, 0.9828, 0.9824,
         0.9789],
        [0.9917, 0.9820, 0.9817, 0.9777, 0.9747, 0.9739, 0.9733, 0.9705, 0.9698,
         0.9689],
        [0.9868, 0.9791, 0.9783, 0.9779, 0.9768, 0.9764, 0.9762, 0.9750, 0.9743,
         0.9741],
        [0.9891, 0.9874, 0.9848, 0.9810, 0.9808, 0.9781, 0.9777, 0.9772, 0.9759,
         0.9744],
        [0.9895, 0.9837, 0.9785, 0.9709, 0.9691, 0.9691, 0.9679, 0.9664, 0.9629,
         0.9618],
        [0.9913, 0.9905, 0.9900, 0.9896, 0.9893, 0.9885, 0.9883, 0.9876, 0.9873,
         0.9872],
        [0.9862, 0.9840, 0.9829, 0.9828, 0.9791, 0.9772, 0.9720, 0.9707, 0.9660,
         0.9647],
        [0.9930, 0.9902, 0.9901, 0.9901, 0.9898, 0.9891, 0.9889, 0.9886, 0.9878,
         0.9861],
        [0.9948, 0.9938, 0.9922, 0.9902, 0.9879, 0.9860, 0.9857, 0.9852, 0.9844,
         0.9842],
        [0.9923, 0.9907, 0.9889, 0.9858, 0.9851, 0.9844, 0.9833, 0.9827, 0.9821,
         0.9815],
        [0.9935, 0.9898, 0.9876, 0.9871, 0.9861, 0.9858, 0.9856, 0.9852, 0.9849,
         0.9847],
        [0.9925, 0.9923, 0.9899, 0.9867, 0.9867, 0.9862, 0.9855, 0.9850, 0.9850,
         0.9843],
        [0.9916, 0.9910, 0.9908, 0.9906, 0.9904, 0.9903, 0.9901, 0.9900, 0.9893,
         0.9887],
        [0.9954, 0.9942, 0.9927, 0.9918, 0.9899, 0.9888, 0.9887, 0.9884, 0.9871,
         0.9870],
        [0.9887, 0.9882, 0.9873, 0.9850, 0.9842, 0.9837, 0.9832, 0.9812, 0.9793,
         0.9790],
        [0.9945, 0.9901, 0.9873, 0.9871, 0.9865, 0.9864, 0.9856, 0.9854, 0.9811,
         0.9801],
        [0.9942, 0.9939, 0.9924, 0.9905, 0.9901, 0.9899, 0.9893, 0.9891, 0.9890,
         0.9885],
        [0.9828, 0.9803, 0.9771, 0.9767, 0.9760, 0.9754, 0.9751, 0.9739, 0.9719,
         0.9708],
        [0.9880, 0.9877, 0.9864, 0.9864, 0.9844, 0.9816, 0.9805, 0.9796, 0.9793,
         0.9793],
        [0.9963, 0.9918, 0.9849, 0.9826, 0.9779, 0.9739, 0.9723, 0.9714, 0.9695,
         0.9685],
        [0.9797, 0.9754, 0.9737, 0.9720, 0.9710, 0.9667, 0.9657, 0.9645, 0.9642,
         0.9638],
        [0.9869, 0.9832, 0.9826, 0.9792, 0.9790, 0.9789, 0.9766, 0.9765, 0.9751,
         0.9744],
        [0.9884, 0.9878, 0.9877, 0.9857, 0.9793, 0.9781, 0.9778, 0.9729, 0.9724,
         0.9699],
        [0.9896, 0.9869, 0.9866, 0.9858, 0.9835, 0.9823, 0.9823, 0.9819, 0.9818,
         0.9808],
        [0.9856, 0.9841, 0.9841, 0.9826, 0.9821, 0.9816, 0.9789, 0.9779, 0.9777,
         0.9771],
        [0.9917, 0.9907, 0.9898, 0.9883, 0.9883, 0.9882, 0.9882, 0.9878, 0.9873,
         0.9871],
        [0.9972, 0.9971, 0.9969, 0.9966, 0.9951, 0.9950, 0.9943, 0.9941, 0.9929,
         0.9929],
        [0.9956, 0.9919, 0.9875, 0.9868, 0.9868, 0.9863, 0.9848, 0.9823, 0.9808,
         0.9800],
        [0.9923, 0.9849, 0.9812, 0.9792, 0.9750, 0.9702, 0.9644, 0.9644, 0.9636,
         0.9546],
        [0.9889, 0.9859, 0.9728, 0.9596, 0.9594, 0.9582, 0.9579, 0.9571, 0.9546,
         0.9531],
        [0.9926, 0.9882, 0.9852, 0.9819, 0.9736, 0.9727, 0.9726, 0.9689, 0.9689,
         0.9685],
        [0.9868, 0.9857, 0.9798, 0.9788, 0.9787, 0.9778, 0.9773, 0.9758, 0.9748,
         0.9733],
        [0.9889, 0.9866, 0.9855, 0.9827, 0.9824, 0.9818, 0.9815, 0.9800, 0.9786,
         0.9784],
        [0.9906, 0.9781, 0.9765, 0.9765, 0.9751, 0.9750, 0.9742, 0.9739, 0.9724,
         0.9712],
        [0.9860, 0.9821, 0.9819, 0.9776, 0.9764, 0.9736, 0.9730, 0.9725, 0.9724,
         0.9710],
        [0.9929, 0.9747, 0.9722, 0.9714, 0.9698, 0.9649, 0.9612, 0.9603, 0.9506,
         0.9473],
        [0.9871, 0.9734, 0.9733, 0.9722, 0.9687, 0.9656, 0.9643, 0.9514, 0.9510,
         0.9482],
        [0.9886, 0.9882, 0.9861, 0.9848, 0.9846, 0.9834, 0.9832, 0.9832, 0.9831,
         0.9824],
        [0.9781, 0.9723, 0.9687, 0.9681, 0.9591, 0.9541, 0.9539, 0.9535, 0.9513,
         0.9502],
        [0.9850, 0.9762, 0.9706, 0.9699, 0.9683, 0.9661, 0.9596, 0.9584, 0.9575,
         0.9538],
        [0.9894, 0.9849, 0.9834, 0.9813, 0.9806, 0.9792, 0.9789, 0.9788, 0.9777,
         0.9761]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 488359.5938,  428653.3125,  382992.6562,  359458.3438,  353705.6562,
          332951.3125,  320940.0000,  294728.8438,  288040.6875,  282545.1875],
        [1409808.1250, 1404176.5000, 1398792.8750, 1379848.8750, 1376426.5000,
         1373595.2500, 1365186.2500, 1342347.0000, 1341607.1250, 1335319.3750],
        [1514336.3750, 1511690.0000, 1507459.0000, 1440174.6250, 1430716.6250,
         1397906.0000, 1370777.8750, 1368121.5000, 1359074.3750, 1344736.6250],
        [1485236.2500, 1478647.5000, 1472318.1250, 1467217.3750, 1466052.2500,
         1465600.7500, 1463701.1250, 1453173.7500, 1451391.2500, 1447603.6250],
        [1492967.3750, 1478035.6250, 1476638.0000, 1473997.0000, 1471547.5000,
         1468247.6250, 1464446.7500, 1464297.2500, 1463871.5000, 1458692.8750],
        [1579931.5000, 1579813.8750, 1552573.6250, 1551476.8750, 1551102.5000,
         1548649.0000, 1544018.5000, 1543721.0000, 1532023.2500, 1522359.7500],
        [1594188.8750, 1594094.7500, 1590890.1250, 1584024.5000, 1582624.8750,
         1582267.0000, 1581336.3750, 1578997.6250, 1577990.5000, 1576083.3750],
        [1540340.1250, 1538880.6250, 1534883.7500, 1533373.7500, 1528863.2500,
         1524627.7500, 1512046.1250, 1488831.2500, 1479887.5000, 1453351.1250],
        [1457246.8750, 1455831.3750, 1422755.5000, 1408770.5000, 1403038.7500,
         1397356.8750, 1389224.8750, 1377509.7500, 1358837.2500, 1357099.2500],
        [1213383.7500, 1179535.1250, 1132462.7500, 1111189.2500,  992172.6875,
          917543.2500,  907484.2500,  904014.3750,  863535.9375,  815350.1250],
        [1511718.8750, 1396051.5000, 1378005.2500, 1372500.7500, 1311943.2500,
         1296217.2500, 1281614.3750, 1277405.7500, 1277112.1250, 1268265.3750],
        [1509753.7500, 1508445.5000, 1426787.0000, 1423123.3750, 1414840.2500,
         1412406.7500, 1395162.3750, 1390451.0000, 1381029.8750, 1378429.6250],
        [1414373.3750, 1380979.7500, 1375249.5000, 1366130.6250, 1357120.0000,
         1344687.7500, 1327967.6250, 1319061.5000, 1284460.5000, 1218077.0000],
        [1521709.5000, 1515983.6250, 1496712.3750, 1473169.2500, 1410134.8750,
         1402303.0000, 1399682.8750, 1389641.0000, 1388452.8750, 1383352.3750],
        [1386045.0000, 1373701.3750, 1367291.8750, 1332480.0000, 1308684.1250,
         1280891.0000, 1272278.7500, 1230014.1250, 1202493.0000, 1186368.2500],
        [1524293.3750, 1442522.3750, 1427161.2500, 1316713.7500, 1311881.8750,
         1301093.2500, 1298488.7500, 1265683.2500, 1261922.8750, 1174729.1250],
        [1461677.1250, 1454775.2500, 1447618.7500, 1417145.2500, 1411803.5000,
         1403074.8750, 1399481.3750, 1386394.0000, 1366307.8750, 1359631.8750],
        [1497846.1250, 1445888.6250, 1429348.7500, 1401524.8750, 1387834.6250,
         1386026.5000, 1376154.7500, 1362552.5000, 1361937.8750, 1348041.6250],
        [1535873.5000, 1491049.2500, 1488710.5000, 1453435.6250, 1451914.5000,
         1439426.3750, 1427670.3750, 1406980.7500, 1388098.0000, 1387090.8750],
        [1182646.1250, 1182518.6250, 1170332.3750, 1165374.0000, 1143951.3750,
         1132062.1250, 1128540.5000, 1127820.6250, 1120530.3750, 1118914.7500],
        [1428596.5000, 1420124.3750, 1405460.0000, 1400456.0000, 1392299.3750,
         1330244.0000, 1252466.7500, 1227188.0000, 1224707.1250, 1209235.5000],
        [1515262.3750, 1489771.3750, 1460254.5000, 1381935.0000, 1376989.7500,
         1367985.8750, 1367526.6250, 1338263.0000, 1333493.1250, 1305572.6250],
        [1362292.5000, 1301677.7500, 1280582.0000, 1275100.5000, 1237896.3750,
         1229546.2500, 1216410.1250, 1211333.8750, 1189894.3750, 1157773.6250],
        [1418623.2500, 1374458.8750, 1371366.2500, 1341133.8750, 1312595.2500,
         1297590.1250, 1259509.7500, 1251727.6250, 1245073.6250, 1184309.7500],
        [1420653.8750, 1237574.1250, 1231452.0000, 1163052.3750, 1114793.8750,
         1101734.6250, 1092711.6250, 1049444.3750, 1040004.2500, 1025880.2500],
        [1325829.0000, 1186506.2500, 1174481.6250, 1167181.3750, 1148652.0000,
         1142573.1250, 1139750.1250, 1119219.8750, 1108189.0000, 1104681.6250],
        [1369941.5000, 1336049.2500, 1288248.7500, 1219328.8750, 1215826.7500,
         1170565.6250, 1163891.2500, 1155762.5000, 1133758.3750, 1109955.3750],
        [1377474.3750, 1268600.3750, 1176482.6250, 1055266.3750, 1029843.3125,
         1029597.8750, 1011070.2500,  990393.5625,  941476.0000,  927257.3750],
        [1413335.1250, 1396713.3750, 1387669.1250, 1378510.0000, 1373096.3750,
         1357667.6250, 1354948.7500, 1340240.1250, 1334057.8750, 1332954.0000],
        [1313460.5000, 1274039.3750, 1254265.7500, 1251870.8750, 1187287.2500,
         1155753.7500, 1072999.1250, 1052267.6250,  984988.6875,  966509.5000],
        [1448137.8750, 1392118.8750, 1388940.1250, 1388468.6250, 1383031.8750,
         1369570.5000, 1365213.7500, 1359818.5000, 1343762.2500, 1312017.0000],
        [1485620.1250, 1464115.8750, 1431414.0000, 1392068.5000, 1345390.7500,
         1310152.7500, 1305046.1250, 1295362.0000, 1280114.3750, 1276221.0000],
        [1434501.2500, 1400660.3750, 1365260.5000, 1307118.8750, 1293058.8750,
         1281103.5000, 1260007.1250, 1250420.0000, 1238891.8750, 1228072.0000],
        [1457932.1250, 1383352.3750, 1341405.0000, 1330329.1250, 1311407.7500,
         1306928.1250, 1303293.6250, 1294816.0000, 1289122.5000, 1285488.6250],
        [1437847.2500, 1432708.7500, 1385903.6250, 1323611.8750, 1323026.2500,
         1313243.7500, 1300649.0000, 1291312.7500, 1290956.7500, 1279000.1250],
        [1419130.6250, 1406754.0000, 1403845.8750, 1398719.5000, 1396154.1250,
         1394187.5000, 1389670.1250, 1387216.6250, 1374059.1250, 1361149.7500],
        [1497716.2500, 1473152.5000, 1442001.1250, 1422750.1250, 1386195.7500,
         1363132.1250, 1361726.2500, 1356127.7500, 1331890.5000, 1329049.6250],
        [1361654.7500, 1352123.0000, 1334616.6250, 1291721.6250, 1276701.8750,
         1267461.2500, 1259276.8750, 1222925.0000, 1190946.7500, 1185947.3750],
        [1478568.5000, 1388394.5000, 1334330.2500, 1330349.3750, 1319260.2500,
         1317517.6250, 1303580.7500, 1298452.8750, 1221344.6250, 1205069.2500],
        [1473538.8750, 1467182.5000, 1435841.1250, 1396898.5000, 1389136.2500,
         1385754.2500, 1372589.6250, 1370086.5000, 1368560.0000, 1357536.7500],
        [1251357.6250, 1207825.8750, 1154444.0000, 1147619.5000, 1135474.5000,
         1126281.5000, 1121099.0000, 1102442.0000, 1071649.2500, 1055004.7500],
        [1348120.0000, 1342102.3750, 1318271.7500, 1317459.8750, 1279995.8750,
         1230856.6250, 1211175.6250, 1196050.5000, 1191191.0000, 1190833.1250],
        [1516987.3750, 1423936.5000, 1289047.5000, 1248167.0000, 1166826.3750,
         1101660.0000, 1077573.7500, 1063999.3750, 1034554.5625, 1020990.3750],
        [1198030.0000, 1125567.5000, 1098655.1250, 1072299.3750, 1058071.7500,
          994550.5000,  980872.5625,  964108.6250,  960292.2500,  953992.1250],
        [1326724.5000, 1258473.6250, 1247355.5000, 1188195.6250, 1185470.2500,
         1183891.8750, 1145847.8750, 1144035.3750, 1120766.5000, 1110737.8750],
        [1355916.8750, 1343966.0000, 1343080.6250, 1304562.1250, 1191400.0000,
         1170448.3750, 1165060.6250, 1086954.6250, 1078693.5000, 1041316.2500],
        [1380220.1250, 1327600.3750, 1321475.2500, 1305760.6250, 1263529.3750,
         1243099.3750, 1241978.2500, 1234941.5000, 1233766.6250, 1216917.1250],
        [1303285.0000, 1275296.2500, 1275000.7500, 1248987.3750, 1239531.2500,
         1230585.6250, 1184516.3750, 1166948.7500, 1163468.3750, 1153214.7500],
        [1421877.8750, 1400712.5000, 1383022.6250, 1353994.2500, 1353653.3750,
         1352391.3750, 1352226.2500, 1344772.5000, 1335459.3750, 1330888.6250],
        [1538571.1250, 1535967.3750, 1531019.8750, 1523710.5000, 1492380.8750,
         1489629.3750, 1475720.1250, 1470784.2500, 1445873.5000, 1445049.1250],
        [1503390.2500, 1424732.3750, 1338790.2500, 1325827.7500, 1325319.5000,
         1315454.8750, 1288414.6250, 1241951.1250, 1215965.8750, 1202637.6250],
        [1434231.6250, 1289397.8750, 1223484.8750, 1189251.1250, 1118948.8750,
         1044925.3750,  962230.2500,  962132.0625,  951881.8750,  836127.6875],
        [1364876.5000, 1308393.3750, 1084472.7500,  898244.5000,  895720.9375,
          880723.8750,  876435.6250,  867565.8125,  836409.1875,  819469.2500],
        [1440346.3750, 1352949.8750, 1294656.8750, 1235400.8750, 1097145.2500,
         1083557.8750, 1082629.3750, 1026758.2500, 1026758.2500, 1020166.0000],
        [1324739.6250, 1305230.3750, 1198614.0000, 1182950.6250, 1180042.5000,
         1164770.6250, 1157837.7500, 1132403.2500, 1116147.0000, 1092670.0000],
        [1366088.8750, 1320748.2500, 1300697.3750, 1249820.3750, 1245066.3750,
         1234538.6250, 1228656.5000, 1202482.7500, 1178388.1250, 1176165.1250],
        [1399664.2500, 1170301.1250, 1143563.0000, 1143446.2500, 1121210.2500,
         1119270.1250, 1106729.3750, 1102348.3750, 1078240.8750, 1060052.3750],
        [1310417.6250, 1238477.2500, 1236381.3750, 1161328.8750, 1142124.2500,
         1096848.1250, 1088924.0000, 1080705.5000, 1078905.3750, 1057695.5000],
        [1446323.0000, 1115120.2500, 1075205.5000, 1062833.0000, 1038799.8750,
          969294.3750,  919211.7500,  907795.0000,  789692.6250,  753575.3125],
        [1331512.0000, 1094298.8750, 1092703.2500, 1075670.2500, 1022625.5000,
          978642.3125,  960308.6875,  799395.3750,  794618.1875,  763826.4375],
        [1359174.2500, 1351423.0000, 1311714.2500, 1287838.3750, 1285189.5000,
         1263075.0000, 1258618.8750, 1258544.5000, 1257218.8750, 1244954.8750],
        [1170278.7500, 1077413.3750, 1023387.4375, 1014917.1250,  892714.8750,
          831022.4375,  828576.3750,  823199.2500,  797726.0000,  785753.9375],
        [1291044.2500, 1138657.2500, 1051102.0000, 1041166.3125, 1017175.7500,
          986009.3750,  898972.9375,  883332.3125,  871478.1875,  826745.1875],
        [1375132.8750, 1290628.1250, 1262907.6250, 1224986.2500, 1213549.3750,
         1188397.3750, 1183373.7500, 1182620.1250, 1163523.8750, 1137794.2500]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 488359.5938,       0.0000],
         [ 428653.3125,       0.0000],
         [ 382992.6562,       0.0000],
         ...,
         [ 294728.8438,       0.0000],
         [ 288040.6875,       0.0000],
         [ 282545.1875,       0.0000]],

        [[1409808.1250,       0.0000],
         [1404176.5000,       0.0000],
         [1398792.8750,       0.0000],
         ...,
         [1342347.0000,       0.0000],
         [1341607.1250,       0.0000],
         [1335319.3750,       0.0000]],

        [[1514336.3750,       0.0000],
         [1511690.0000,       0.0000],
         [1507459.0000,       0.0000],
         ...,
         [1368121.5000,       0.0000],
         [1359074.3750,       0.0000],
         [      0.0000, 1344736.6250]],

        ...,

        [[1170278.7500,       0.0000],
         [1077413.3750,       0.0000],
         [1023387.4375,       0.0000],
         ...,
         [ 823199.2500,       0.0000],
         [ 797726.0000,       0.0000],
         [ 785753.9375,       0.0000]],

        [[1291044.2500,       0.0000],
         [1138657.2500,       0.0000],
         [1051102.0000,       0.0000],
         ...,
         [ 883332.3125,       0.0000],
         [      0.0000,  871478.1875],
         [ 826745.1875,       0.0000]],

        [[1375132.8750,       0.0000],
         [      0.0000, 1290628.1250],
         [1262907.6250,       0.0000],
         ...,
         [1182620.1250,       0.0000],
         [1163523.8750,       0.0000],
         [1137794.2500,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 3532375.7500,        0.0000],
        [13727108.0000,        0.0000],
        [12900256.0000,  1344736.6250],
        [14650942.0000,        0.0000],
        [13219774.0000,  1492967.3750],
        [13973647.0000,  1532023.2500],
        [14251607.0000,  1590890.1250],
        [13655198.0000,  1479887.5000],
        [12638446.0000,  1389224.8750],
        [10036672.0000,        0.0000],
        [ 9294633.0000,  4076201.5000],
        [14240430.0000,        0.0000],
        [12021978.0000,  1366130.6250],
        [12859432.0000,  1521709.5000],
        [12940248.0000,        0.0000],
        [13324489.0000,        0.0000],
        [14107909.0000,        0.0000],
        [13997156.0000,        0.0000],
        [10151238.0000,  4319011.5000],
        [ 9200199.0000,  2272492.0000],
        [11960534.0000,  1330244.0000],
        [11094865.0000,  2842189.5000],
        [12462508.0000,        0.0000],
        [13056388.0000,        0.0000],
        [10427856.0000,  1049444.3750],
        [11617064.0000,        0.0000],
        [10793388.0000,  1369941.5000],
        [10807462.0000,        0.0000],
        [13669192.0000,        0.0000],
        [11513442.0000,        0.0000],
        [13751079.0000,        0.0000],
        [13585506.0000,        0.0000],
        [10396522.0000,  2662573.2500],
        [13304075.0000,        0.0000],
        [13378260.0000,        0.0000],
        [11124540.0000,  2806347.2500],
        [11104394.0000,  2859348.2500],
        [12743374.0000,        0.0000],
        [11975523.0000,  1221344.6250],
        [14017124.0000,        0.0000],
        [11373198.0000,        0.0000],
        [12626056.0000,        0.0000],
        [11943742.0000,        0.0000],
        [10406440.0000,        0.0000],
        [10584774.0000,  1326724.5000],
        [12081399.0000,        0.0000],
        [12769288.0000,        0.0000],
        [ 8525094.0000,  3715741.0000],
        [13628999.0000,        0.0000],
        [13459077.0000,  1489629.3750],
        [10353775.0000,  2828709.7500],
        [ 9967686.0000,  1044925.3750],
        [ 8955876.0000,   876435.6250],
        [11660369.0000,        0.0000],
        [10690635.0000,  1164770.6250],
        [12502653.0000,        0.0000],
        [ 8901598.0000,  2543227.2500],
        [11491808.0000,        0.0000],
        [ 7723733.0000,  2354118.0000],
        [ 8953293.0000,   960308.6875],
        [11589912.0000,  1287838.3750],
        [ 7585391.0000,  1659598.7500],
        [ 8148196.0000,  1857487.5000],
        [10932286.0000,  1290628.1250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 46/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:14, 60.49s/it]  7%|▋         | 2/30 [01:01<11:49, 25.35s/it] 10%|█         | 3/30 [01:01<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 5.27371997833252
Epoch 47/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:26, 63.00s/it]  7%|▋         | 2/30 [01:04<12:27, 26.71s/it] 10%|█         | 3/30 [01:05<06:41, 14.85s/it] 13%|█▎        | 4/30 [01:05<04:01,  9.29s/it] 17%|█▋        | 5/30 [01:06<02:35,  6.21s/it] 20%|██        | 6/30 [01:07<01:44,  4.35s/it] 23%|██▎       | 7/30 [01:08<01:13,  3.17s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.40s/it] 30%|███       | 9/30 [01:09<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  2.85s/it]
Epoch loss is 5.228825489679973
Epoch 48/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:50, 61.75s/it]  7%|▋         | 2/30 [01:02<12:04, 25.87s/it] 10%|█         | 3/30 [01:03<06:28, 14.40s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.03s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 5.218219629923502
Epoch 49/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:26, 58.85s/it]  7%|▋         | 2/30 [00:59<11:30, 24.68s/it] 10%|█         | 3/30 [01:00<06:16, 13.95s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.29it/s] 70%|███████   | 21/30 [01:14<00:06,  1.30it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 5.132414658864339
Epoch 50/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:05, 58.11s/it]  7%|▋         | 2/30 [01:00<11:43, 25.12s/it] 10%|█         | 3/30 [01:01<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:01<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.93s/it] 20%|██        | 6/30 [01:03<01:39,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 5.047891267140707
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0586,  0.0097,  0.0275,  ...,  0.0043, -0.0006, -0.0078],
        [-0.0423,  0.0264,  0.0257,  ..., -0.0003,  0.0122, -0.0462],
        [-0.0591, -0.0022,  0.0115,  ...,  0.0807,  0.0507, -0.0187],
        ...,
        [-0.0199, -0.0256, -0.0048,  ..., -0.0254, -0.0179,  0.0630],
        [-0.0736, -0.0127,  0.0405,  ..., -0.0076,  0.0428,  0.0300],
        [-0.0304,  0.0136,  0.0190,  ..., -0.0227,  0.0527, -0.0076]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9129, 0.9062, 0.8985, 0.8961, 0.8921, 0.8919, 0.8864, 0.8729, 0.8712,
         0.8691],
        [0.9923, 0.9907, 0.9903, 0.9903, 0.9896, 0.9887, 0.9882, 0.9879, 0.9870,
         0.9867],
        [0.9957, 0.9953, 0.9947, 0.9918, 0.9911, 0.9884, 0.9878, 0.9877, 0.9868,
         0.9865],
        [0.9940, 0.9938, 0.9937, 0.9934, 0.9933, 0.9923, 0.9920, 0.9917, 0.9917,
         0.9914],
        [0.9943, 0.9936, 0.9933, 0.9931, 0.9930, 0.9930, 0.9928, 0.9928, 0.9928,
         0.9928],
        [0.9986, 0.9986, 0.9975, 0.9974, 0.9972, 0.9972, 0.9969, 0.9969, 0.9964,
         0.9955],
        [0.9996, 0.9996, 0.9995, 0.9991, 0.9990, 0.9990, 0.9989, 0.9989, 0.9988,
         0.9986],
        [0.9970, 0.9967, 0.9967, 0.9964, 0.9964, 0.9963, 0.9954, 0.9945, 0.9937,
         0.9927],
        [0.9934, 0.9932, 0.9915, 0.9913, 0.9907, 0.9906, 0.9896, 0.9885, 0.9884,
         0.9877],
        [0.9785, 0.9774, 0.9770, 0.9726, 0.9684, 0.9616, 0.9587, 0.9583, 0.9504,
         0.9496],
        [0.9957, 0.9896, 0.9888, 0.9871, 0.9862, 0.9853, 0.9853, 0.9842, 0.9841,
         0.9816],
        [0.9958, 0.9958, 0.9909, 0.9908, 0.9906, 0.9899, 0.9894, 0.9894, 0.9892,
         0.9890],
        [0.9905, 0.9890, 0.9878, 0.9875, 0.9875, 0.9848, 0.9841, 0.9836, 0.9831,
         0.9777],
        [0.9968, 0.9967, 0.9954, 0.9939, 0.9913, 0.9909, 0.9906, 0.9905, 0.9905,
         0.9898],
        [0.9897, 0.9894, 0.9888, 0.9875, 0.9848, 0.9839, 0.9835, 0.9833, 0.9802,
         0.9774],
        [0.9962, 0.9916, 0.9912, 0.9873, 0.9858, 0.9844, 0.9837, 0.9817, 0.9810,
         0.9778],
        [0.9938, 0.9923, 0.9921, 0.9919, 0.9915, 0.9904, 0.9888, 0.9882, 0.9882,
         0.9878],
        [0.9959, 0.9919, 0.9910, 0.9909, 0.9889, 0.9889, 0.9884, 0.9883, 0.9866,
         0.9865],
        [0.9959, 0.9943, 0.9929, 0.9927, 0.9925, 0.9921, 0.9901, 0.9900, 0.9898,
         0.9892],
        [0.9797, 0.9773, 0.9769, 0.9766, 0.9762, 0.9761, 0.9756, 0.9746, 0.9739,
         0.9722],
        [0.9917, 0.9906, 0.9906, 0.9898, 0.9895, 0.9871, 0.9823, 0.9822, 0.9819,
         0.9810],
        [0.9959, 0.9942, 0.9925, 0.9888, 0.9880, 0.9875, 0.9871, 0.9870, 0.9851,
         0.9841],
        [0.9877, 0.9866, 0.9831, 0.9827, 0.9825, 0.9803, 0.9801, 0.9786, 0.9782,
         0.9765],
        [0.9903, 0.9893, 0.9885, 0.9858, 0.9852, 0.9830, 0.9825, 0.9803, 0.9785,
         0.9777],
        [0.9915, 0.9800, 0.9796, 0.9748, 0.9716, 0.9714, 0.9694, 0.9685, 0.9645,
         0.9643],
        [0.9854, 0.9753, 0.9750, 0.9750, 0.9747, 0.9730, 0.9721, 0.9719, 0.9716,
         0.9709],
        [0.9899, 0.9873, 0.9826, 0.9825, 0.9819, 0.9758, 0.9744, 0.9737, 0.9716,
         0.9711],
        [0.9895, 0.9826, 0.9800, 0.9703, 0.9690, 0.9689, 0.9679, 0.9655, 0.9604,
         0.9597],
        [0.9906, 0.9899, 0.9889, 0.9885, 0.9877, 0.9876, 0.9873, 0.9859, 0.9858,
         0.9850],
        [0.9857, 0.9842, 0.9832, 0.9828, 0.9796, 0.9779, 0.9742, 0.9735, 0.9711,
         0.9661],
        [0.9924, 0.9901, 0.9900, 0.9887, 0.9883, 0.9872, 0.9870, 0.9869, 0.9862,
         0.9843],
        [0.9942, 0.9925, 0.9911, 0.9878, 0.9858, 0.9855, 0.9847, 0.9845, 0.9828,
         0.9821],
        [0.9906, 0.9874, 0.9861, 0.9836, 0.9830, 0.9813, 0.9805, 0.9802, 0.9801,
         0.9799],
        [0.9931, 0.9895, 0.9877, 0.9866, 0.9860, 0.9847, 0.9841, 0.9841, 0.9835,
         0.9827],
        [0.9926, 0.9923, 0.9889, 0.9875, 0.9870, 0.9853, 0.9850, 0.9849, 0.9836,
         0.9836],
        [0.9901, 0.9901, 0.9900, 0.9897, 0.9897, 0.9888, 0.9883, 0.9880, 0.9877,
         0.9871],
        [0.9952, 0.9935, 0.9922, 0.9915, 0.9885, 0.9878, 0.9870, 0.9868, 0.9858,
         0.9858],
        [0.9884, 0.9867, 0.9857, 0.9846, 0.9835, 0.9821, 0.9811, 0.9807, 0.9788,
         0.9783],
        [0.9940, 0.9884, 0.9864, 0.9863, 0.9861, 0.9856, 0.9853, 0.9838, 0.9823,
         0.9811],
        [0.9943, 0.9940, 0.9923, 0.9909, 0.9900, 0.9899, 0.9897, 0.9879, 0.9878,
         0.9877],
        [0.9813, 0.9793, 0.9735, 0.9731, 0.9709, 0.9707, 0.9698, 0.9696, 0.9691,
         0.9685],
        [0.9873, 0.9846, 0.9845, 0.9842, 0.9830, 0.9818, 0.9778, 0.9774, 0.9766,
         0.9755],
        [0.9954, 0.9899, 0.9827, 0.9826, 0.9767, 0.9720, 0.9706, 0.9682, 0.9625,
         0.9610],
        [0.9787, 0.9737, 0.9737, 0.9732, 0.9716, 0.9653, 0.9650, 0.9648, 0.9633,
         0.9633],
        [0.9863, 0.9823, 0.9816, 0.9757, 0.9750, 0.9743, 0.9742, 0.9734, 0.9728,
         0.9727],
        [0.9890, 0.9857, 0.9848, 0.9847, 0.9783, 0.9771, 0.9740, 0.9689, 0.9684,
         0.9611],
        [0.9874, 0.9852, 0.9845, 0.9836, 0.9825, 0.9823, 0.9805, 0.9804, 0.9803,
         0.9798],
        [0.9831, 0.9814, 0.9809, 0.9800, 0.9789, 0.9788, 0.9752, 0.9743, 0.9741,
         0.9738],
        [0.9901, 0.9897, 0.9893, 0.9887, 0.9887, 0.9868, 0.9863, 0.9859, 0.9858,
         0.9851],
        [0.9968, 0.9967, 0.9961, 0.9957, 0.9944, 0.9943, 0.9937, 0.9937, 0.9927,
         0.9925],
        [0.9947, 0.9913, 0.9863, 0.9857, 0.9856, 0.9845, 0.9829, 0.9816, 0.9787,
         0.9777],
        [0.9912, 0.9839, 0.9812, 0.9785, 0.9751, 0.9724, 0.9643, 0.9628, 0.9577,
         0.9556],
        [0.9887, 0.9847, 0.9684, 0.9587, 0.9575, 0.9575, 0.9552, 0.9546, 0.9533,
         0.9517],
        [0.9920, 0.9865, 0.9836, 0.9799, 0.9732, 0.9715, 0.9667, 0.9663, 0.9651,
         0.9645],
        [0.9853, 0.9828, 0.9804, 0.9777, 0.9775, 0.9774, 0.9761, 0.9748, 0.9730,
         0.9717],
        [0.9883, 0.9867, 0.9851, 0.9841, 0.9827, 0.9818, 0.9818, 0.9801, 0.9785,
         0.9781],
        [0.9889, 0.9770, 0.9759, 0.9733, 0.9727, 0.9721, 0.9707, 0.9706, 0.9704,
         0.9700],
        [0.9811, 0.9778, 0.9762, 0.9717, 0.9695, 0.9691, 0.9651, 0.9646, 0.9638,
         0.9634],
        [0.9922, 0.9751, 0.9729, 0.9719, 0.9683, 0.9644, 0.9621, 0.9605, 0.9534,
         0.9524],
        [0.9865, 0.9676, 0.9675, 0.9656, 0.9626, 0.9617, 0.9592, 0.9468, 0.9463,
         0.9451],
        [0.9845, 0.9832, 0.9818, 0.9802, 0.9795, 0.9791, 0.9783, 0.9782, 0.9777,
         0.9777],
        [0.9757, 0.9667, 0.9643, 0.9617, 0.9571, 0.9524, 0.9520, 0.9484, 0.9475,
         0.9465],
        [0.9845, 0.9761, 0.9661, 0.9636, 0.9612, 0.9595, 0.9563, 0.9508, 0.9493,
         0.9454],
        [0.9884, 0.9847, 0.9806, 0.9790, 0.9788, 0.9778, 0.9770, 0.9768, 0.9766,
         0.9759]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 461331.8750,  419215.0625,  375124.2188,  362578.0000,  342389.0312,
          341404.0000,  315904.0000,  260587.1562,  254299.2812,  246618.2812],
        [1433330.6250, 1401129.3750, 1393863.1250, 1392364.5000, 1378811.1250,
         1362540.7500, 1351719.5000, 1346185.1250, 1329320.8750, 1323115.7500],
        [1504899.3750, 1496235.7500, 1483680.3750, 1424022.0000, 1409661.5000,
         1356865.0000, 1343482.8750, 1342894.8750, 1325267.7500, 1319206.2500],
        [1469606.5000, 1465517.0000, 1463253.1250, 1456621.5000, 1455279.0000,
         1434245.2500, 1426656.3750, 1421941.6250, 1420431.8750, 1415779.6250],
        [1474996.8750, 1461367.7500, 1453281.7500, 1449211.3750, 1448737.3750,
         1447472.3750, 1444701.7500, 1444335.3750, 1444097.1250, 1443852.0000],
        [1568292.7500, 1567922.0000, 1544027.2500, 1541270.2500, 1538484.5000,
         1538361.2500, 1530822.7500, 1530033.1250, 1520469.2500, 1501519.0000],
        [1591456.1250, 1591148.1250, 1589514.6250, 1579759.7500, 1578160.5000,
         1577503.0000, 1576349.5000, 1574597.5000, 1573758.3750, 1568409.3750],
        [1532569.7500, 1525931.1250, 1525846.7500, 1520844.7500, 1520546.1250,
         1516980.0000, 1498387.6250, 1479238.3750, 1463233.5000, 1442794.8750],
        [1457116.1250, 1452512.7500, 1417072.2500, 1413851.5000, 1401855.1250,
         1399397.2500, 1379663.2500, 1357016.5000, 1356361.8750, 1342707.8750],
        [1176902.3750, 1159359.1250, 1152674.8750, 1081600.5000, 1019150.8125,
          924859.5000,  887620.4375,  881470.0625,  788075.1250,  779408.7500],
        [1505094.5000, 1379708.1250, 1364202.3750, 1330388.7500, 1314218.5000,
         1297315.2500, 1297275.7500, 1276335.3750, 1275009.3750, 1231022.2500],
        [1507940.6250, 1507456.1250, 1404578.2500, 1402303.0000, 1398720.8750,
         1384408.2500, 1376170.5000, 1375191.7500, 1371153.1250, 1366808.1250],
        [1398144.7500, 1367513.5000, 1343482.8750, 1338872.0000, 1338420.0000,
         1288396.1250, 1274891.3750, 1265478.1250, 1256374.0000, 1162987.0000],
        [1527843.0000, 1526727.2500, 1499020.7500, 1467070.5000, 1413929.6250,
         1404502.0000, 1399326.6250, 1397302.2500, 1396368.3750, 1383341.7500],
        [1380757.1250, 1375940.8750, 1363489.6250, 1338215.7500, 1287218.2500,
         1271642.0000, 1264183.7500, 1261125.2500, 1206152.3750, 1159274.0000],
        [1516403.0000, 1419879.2500, 1410940.6250, 1333849.2500, 1306133.1250,
         1280906.8750, 1268099.6250, 1231887.7500, 1219495.1250, 1164624.0000],
        [1465250.0000, 1433396.2500, 1430034.6250, 1426199.3750, 1416671.0000,
         1394915.0000, 1362891.6250, 1352611.8750, 1352053.5000, 1343971.1250],
        [1508596.5000, 1426328.5000, 1406369.0000, 1404534.1250, 1366600.8750,
         1365188.8750, 1355845.8750, 1353769.5000, 1320611.0000, 1318705.5000],
        [1509714.8750, 1474793.0000, 1444965.0000, 1442570.6250, 1437090.5000,
         1429838.2500, 1388348.2500, 1387882.2500, 1382433.1250, 1371786.1250],
        [1196988.5000, 1157444.6250, 1151000.8750, 1145718.8750, 1138588.8750,
         1137555.5000, 1129246.6250, 1113825.7500, 1102615.3750, 1075441.3750],
        [1421526.7500, 1399473.3750, 1399424.0000, 1383641.2500, 1378115.6250,
         1331383.7500, 1242441.5000, 1241157.7500, 1235181.6250, 1220017.3750],
        [1508206.6250, 1472242.3750, 1438679.6250, 1364038.5000, 1348391.2500,
         1337835.5000, 1330705.8750, 1329164.8750, 1293131.6250, 1274377.1250],
        [1342344.2500, 1320678.8750, 1256518.8750, 1249215.0000, 1245484.5000,
         1207963.0000, 1204816.3750, 1179412.5000, 1171621.1250, 1143524.8750],
        [1394091.7500, 1373693.6250, 1357485.1250, 1307030.2500, 1295059.3750,
         1255413.2500, 1246898.7500, 1208221.1250, 1177303.1250, 1163370.7500],
        [1416757.5000, 1202178.8750, 1195291.1250, 1116414.2500, 1066994.8750,
         1063487.0000, 1033174.2500, 1020774.1875,  963281.5000,  960583.5000],
        [1299517.0000, 1124863.3750, 1120265.3750, 1118974.5000, 1114544.1250,
         1088459.8750, 1074337.3750, 1070765.6250, 1066650.0000, 1056519.0000],
        [1385396.1250, 1334607.6250, 1248679.0000, 1246552.7500, 1235478.6250,
         1133003.8750, 1110434.8750, 1098484.3750, 1067243.2500, 1058893.5000],
        [1376733.6250, 1247680.2500, 1202563.0000, 1046680.7500, 1027767.1875,
         1025871.4375, 1011378.8125,  976917.1875,  909235.0000,  900445.3125],
        [1400183.5000, 1385405.3750, 1365067.8750, 1358710.2500, 1341771.0000,
         1340229.8750, 1335627.5000, 1308536.8750, 1306702.5000, 1292040.7500],
        [1304344.3750, 1276998.8750, 1259666.0000, 1250861.2500, 1196317.3750,
         1166819.6250, 1107283.6250, 1095198.8750, 1059758.2500,  985385.2500],
        [1435960.2500, 1388938.7500, 1387169.0000, 1361788.5000, 1353517.7500,
         1332311.0000, 1329480.6250, 1327191.5000, 1314194.6250, 1279519.8750],
        [1473256.5000, 1437198.6250, 1409702.0000, 1344672.3750, 1306374.7500,
         1300472.8750, 1286995.0000, 1282267.2500, 1251148.7500, 1238452.5000],
        [1399924.5000, 1336581.8750, 1312645.2500, 1265321.1250, 1255369.0000,
         1224982.8750, 1211333.8750, 1206711.5000, 1204755.5000, 1201636.7500],
        [1450352.1250, 1378324.5000, 1343007.6250, 1321916.3750, 1310216.5000,
         1286636.6250, 1275286.5000, 1274559.5000, 1263636.6250, 1250614.3750],
        [1440504.2500, 1433507.0000, 1366508.5000, 1338103.5000, 1328960.8750,
         1296673.3750, 1292550.8750, 1290517.3750, 1266447.6250, 1265542.0000],
        [1389308.5000, 1388823.6250, 1387228.6250, 1381804.5000, 1381539.6250,
         1363078.7500, 1353697.1250, 1347454.1250, 1341882.2500, 1330897.5000],
        [1494695.3750, 1458039.2500, 1430723.5000, 1416533.2500, 1357010.0000,
         1343991.6250, 1328967.2500, 1325818.8750, 1307241.0000, 1307014.0000],
        [1355813.5000, 1324302.5000, 1304447.6250, 1284362.5000, 1264227.1250,
         1239661.3750, 1221942.1250, 1215418.7500, 1182592.0000, 1173235.6250],
        [1468468.8750, 1355169.6250, 1317779.0000, 1315545.2500, 1311904.3750,
         1303313.6250, 1297845.0000, 1269129.2500, 1242333.7500, 1221927.0000],
        [1474856.2500, 1469626.1250, 1432615.8750, 1404299.6250, 1386556.6250,
         1385833.5000, 1381799.1250, 1346520.2500, 1345273.8750, 1341823.3750],
        [1225159.2500, 1191414.7500, 1095600.0000, 1089103.6250, 1056526.0000,
         1053318.7500, 1038826.5625, 1036491.1875, 1029441.7500, 1020177.6250],
        [1335040.5000, 1283580.0000, 1282542.3750, 1276581.2500, 1255970.1250,
         1234106.7500, 1165895.3750, 1159339.2500, 1145452.2500, 1126997.1250],
        [1497839.0000, 1385803.1250, 1249368.6250, 1248268.2500, 1147818.6250,
         1072833.3750, 1051824.0000, 1015573.5625,  936602.5625,  916271.8750],
        [1180845.1250, 1099248.3750, 1099034.5000, 1090815.6250, 1066079.5000,
          974841.7500,  970034.2500,  968527.5000,  947318.5000,  947262.5625],
        [1316539.2500, 1242282.7500, 1229655.3750, 1130427.6250, 1119920.3750,
         1107991.3750, 1106923.6250, 1095070.3750, 1084284.5000, 1082928.7500],
        [1366954.2500, 1305208.0000, 1287723.0000, 1286242.7500, 1173041.0000,
         1153023.5000, 1103819.1250, 1026744.5000, 1019577.5000,  918551.8750],
        [1337131.3750, 1295768.5000, 1282601.1250, 1265672.3750, 1245697.1250,
         1242307.6250, 1211404.3750, 1208714.3750, 1207204.1250, 1198742.0000],
        [1256693.8750, 1226144.6250, 1218058.5000, 1202677.6250, 1184082.7500,
         1181485.0000, 1123161.1250, 1108913.2500, 1105674.5000, 1101049.8750],
        [1389446.2500, 1381862.5000, 1374148.2500, 1362417.2500, 1361849.6250,
         1325805.0000, 1316633.3750, 1307823.3750, 1307255.8750, 1292867.7500],
        [1529767.5000, 1525976.2500, 1514053.3750, 1505930.1250, 1476706.8750,
         1474597.3750, 1462485.8750, 1461894.5000, 1441504.7500, 1438679.6250],
        [1484419.2500, 1413739.6250, 1316476.3750, 1304703.8750, 1302790.3750,
         1282102.1250, 1253676.1250, 1229846.5000, 1181271.0000, 1164287.5000],
        [1411768.3750, 1271716.0000, 1223851.3750, 1177890.3750, 1121371.7500,
         1079512.6250,  960486.3750,  940523.9375,  875026.7500,  848747.4375],
        [1362131.5000, 1286282.0000, 1018607.6250,  886626.3750,  871618.5625,
          871509.7500,  844324.0625,  836488.9375,  821726.9375,  803132.8125],
        [1428431.7500, 1320141.2500, 1265551.7500, 1201548.5000, 1091785.6250,
         1064970.8750,  994588.4375,  988457.2500,  971964.0000,  963945.0000],
        [1297772.0000, 1251344.5000, 1209733.7500, 1163429.6250, 1159782.7500,
         1157945.8750, 1137728.0000, 1116134.2500, 1088825.2500, 1068500.0000],
        [1354687.7500, 1323134.7500, 1293700.2500, 1275725.7500, 1249422.2500,
         1233250.1250, 1233223.1250, 1203999.7500, 1177221.1250, 1171132.8750],
        [1366086.2500, 1151566.2500, 1134846.6250, 1093162.0000, 1084070.5000,
         1074764.7500, 1053347.8750, 1051254.5000, 1049132.2500, 1043001.8750],
        [1220833.3750, 1165332.8750, 1139627.2500, 1068620.2500, 1035471.5625,
         1029388.7500,  971761.0625,  964439.6875,  953951.1250,  948348.0625],
        [1432490.1250, 1121966.5000, 1086582.6250, 1071041.3750, 1018065.7500,
          962267.8750,  931237.3125,  910143.3125,  822141.6875,  811149.6250],
        [1320478.7500, 1007050.6250, 1006408.3750,  979650.7500,  937534.6875,
          926144.6875,  893105.7500,  747929.1250,  742841.5625,  730269.6875],
        [1281806.2500, 1258767.6250, 1233139.6250, 1206028.0000, 1193682.6250,
         1186817.5000, 1173781.7500, 1171931.6250, 1164179.8750, 1163567.1250],
        [1130998.0000,  994739.2500,  960742.0000,  926346.0625,  867552.5625,
          811284.2500,  806172.3750,  765835.9375,  755756.0000,  745330.9375],
        [1283039.1250, 1136964.5000,  986035.7500,  951204.9375,  919284.5000,
          896860.3750,  857056.9375,  792132.0000,  775230.6875,  733166.3125],
        [1355252.3750, 1285290.0000, 1212893.3750, 1185906.6250, 1182969.8750,
         1165575.1250, 1152595.7500, 1148671.6250, 1145674.1250, 1134864.0000]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 461331.8750,       0.0000],
         [ 419215.0625,       0.0000],
         [ 375124.2188,       0.0000],
         ...,
         [ 260587.1562,       0.0000],
         [ 254299.2812,       0.0000],
         [ 246618.2812,       0.0000]],

        [[1433330.6250,       0.0000],
         [1401129.3750,       0.0000],
         [1393863.1250,       0.0000],
         ...,
         [1346185.1250,       0.0000],
         [1329320.8750,       0.0000],
         [1323115.7500,       0.0000]],

        [[1504899.3750,       0.0000],
         [1496235.7500,       0.0000],
         [1483680.3750,       0.0000],
         ...,
         [1342894.8750,       0.0000],
         [1325267.7500,       0.0000],
         [1319206.2500,       0.0000]],

        ...,

        [[1130998.0000,       0.0000],
         [ 994739.2500,       0.0000],
         [ 960742.0000,       0.0000],
         ...,
         [ 765835.9375,       0.0000],
         [      0.0000,  755756.0000],
         [ 745330.9375,       0.0000]],

        [[1283039.1250,       0.0000],
         [1136964.5000,       0.0000],
         [ 986035.7500,       0.0000],
         ...,
         [ 792132.0000,       0.0000],
         [ 775230.6875,       0.0000],
         [      0.0000,  733166.3125]],

        [[1355252.3750,       0.0000],
         [      0.0000, 1285290.0000],
         [1212893.3750,       0.0000],
         ...,
         [1148671.6250,       0.0000],
         [1145674.1250,       0.0000],
         [1134864.0000,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 3379451.0000,        0.0000],
        [13712380.0000,        0.0000],
        [14006216.0000,        0.0000],
        [14429333.0000,        0.0000],
        [13037057.0000,  1474996.8750],
        [13860732.0000,  1520469.2500],
        [14211143.0000,  1589514.6250],
        [13563139.0000,  1463233.5000],
        [12578157.0000,  1399397.2500],
        [ 9851122.0000,        0.0000],
        [ 9137812.0000,  4132759.0000],
        [14094731.0000,        0.0000],
        [11691076.0000,  1343482.8750],
        [12887589.0000,  1527843.0000],
        [12907999.0000,        0.0000],
        [13152218.0000,        0.0000],
        [13977994.0000,        0.0000],
        [13826550.0000,        0.0000],
        [10014611.0000,  4254811.0000],
        [ 9077156.0000,  2271270.5000],
        [11920979.0000,  1331383.7500],
        [10894055.0000,  2802718.0000],
        [12321580.0000,        0.0000],
        [12778568.0000,        0.0000],
        [10018163.0000,  1020774.1875],
        [11134896.0000,        0.0000],
        [10533378.0000,  1385396.1250],
        [10725273.0000,        0.0000],
        [13434275.0000,        0.0000],
        [11702634.0000,        0.0000],
        [13510072.0000,        0.0000],
        [13330540.0000,        0.0000],
        [11219338.0000,  1399924.5000],
        [13154550.0000,        0.0000],
        [13319315.0000,        0.0000],
        [10930478.0000,  2735236.7500],
        [ 9647744.0000,  4122290.2500],
        [12566003.0000,        0.0000],
        [11861082.0000,  1242333.7500],
        [12627382.0000,  1341823.3750],
        [10836059.0000,        0.0000],
        [12265506.0000,        0.0000],
        [11522203.0000,        0.0000],
        [10344008.0000,        0.0000],
        [10199485.0000,  1316539.2500],
        [11640885.0000,        0.0000],
        [12495243.0000,        0.0000],
        [ 8116189.0000,  3591751.7500],
        [12127242.0000,  1292867.7500],
        [13354888.0000,  1476706.8750],
        [10144190.0000,  2789123.0000],
        [ 9831382.0000,  1079512.6250],
        [ 8730830.0000,   871618.5625],
        [11291385.0000,        0.0000],
        [10493250.0000,  1157945.8750],
        [12515498.0000,        0.0000],
        [ 7508815.5000,  3592417.2500],
        [10497774.0000,        0.0000],
        [ 7803359.0000,  2363727.5000],
        [ 8365269.0000,   926144.6875],
        [ 9655742.0000,  2377959.5000],
        [ 7202829.0000,  1561928.3750],
        [ 6821467.0000,  2509507.7500],
        [10684403.0000,  1285290.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 51/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:16, 60.58s/it]  7%|▋         | 2/30 [01:01<11:50, 25.39s/it] 10%|█         | 3/30 [01:02<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 5.069750150044759
Epoch 52/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:50, 59.68s/it]  7%|▋         | 2/30 [01:01<11:51, 25.43s/it] 10%|█         | 3/30 [01:01<06:22, 14.16s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 5.025061798095703
Epoch 53/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:20, 56.59s/it]  7%|▋         | 2/30 [01:00<11:57, 25.63s/it] 10%|█         | 3/30 [01:01<06:25, 14.27s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.93s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.98s/it] 20%|██        | 6/30 [01:03<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 5.020821285247803
Epoch 54/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:22, 62.85s/it]  7%|▋         | 2/30 [01:03<12:16, 26.32s/it] 10%|█         | 3/30 [01:04<06:35, 14.65s/it] 13%|█▎        | 4/30 [01:05<03:58,  9.16s/it] 17%|█▋        | 5/30 [01:05<02:33,  6.13s/it] 20%|██        | 6/30 [01:06<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.38s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Epoch loss is 4.928112522761027
Epoch 55/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:14, 62.58s/it]  7%|▋         | 2/30 [01:03<12:13, 26.21s/it] 10%|█         | 3/30 [01:04<06:33, 14.58s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.12s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.10s/it] 20%|██        | 6/30 [01:06<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:07<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 4.9257926146189375
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0596,  0.0132,  0.0197,  ..., -0.0043, -0.0067, -0.0119],
        [-0.0539,  0.0257,  0.0229,  ..., -0.0046,  0.0050, -0.0488],
        [-0.0654, -0.0056,  0.0011,  ...,  0.0763,  0.0478, -0.0226],
        ...,
        [-0.0239, -0.0261, -0.0081,  ..., -0.0311, -0.0149,  0.0539],
        [-0.0836, -0.0105,  0.0386,  ..., -0.0129,  0.0417,  0.0136],
        [-0.0396,  0.0170,  0.0126,  ..., -0.0254,  0.0500, -0.0157]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9131, 0.9030, 0.8975, 0.8929, 0.8889, 0.8880, 0.8753, 0.8652, 0.8648,
         0.8620],
        [0.9932, 0.9919, 0.9907, 0.9902, 0.9899, 0.9883, 0.9878, 0.9869, 0.9867,
         0.9866],
        [0.9952, 0.9939, 0.9930, 0.9909, 0.9886, 0.9883, 0.9858, 0.9856, 0.9847,
         0.9836],
        [0.9937, 0.9929, 0.9925, 0.9922, 0.9921, 0.9905, 0.9904, 0.9898, 0.9897,
         0.9895],
        [0.9935, 0.9927, 0.9920, 0.9920, 0.9917, 0.9917, 0.9916, 0.9915, 0.9912,
         0.9911],
        [0.9979, 0.9979, 0.9970, 0.9969, 0.9966, 0.9966, 0.9961, 0.9960, 0.9957,
         0.9947],
        [0.9995, 0.9995, 0.9994, 0.9989, 0.9988, 0.9988, 0.9988, 0.9987, 0.9986,
         0.9981],
        [0.9964, 0.9961, 0.9959, 0.9958, 0.9958, 0.9954, 0.9945, 0.9939, 0.9926,
         0.9921],
        [0.9943, 0.9929, 0.9915, 0.9910, 0.9908, 0.9901, 0.9890, 0.9888, 0.9887,
         0.9867],
        [0.9815, 0.9752, 0.9740, 0.9726, 0.9725, 0.9637, 0.9525, 0.9524, 0.9524,
         0.9404],
        [0.9953, 0.9897, 0.9884, 0.9882, 0.9863, 0.9861, 0.9839, 0.9835, 0.9802,
         0.9801],
        [0.9957, 0.9956, 0.9901, 0.9891, 0.9890, 0.9887, 0.9884, 0.9884, 0.9878,
         0.9869],
        [0.9892, 0.9889, 0.9865, 0.9861, 0.9838, 0.9811, 0.9800, 0.9799, 0.9796,
         0.9743],
        [0.9971, 0.9965, 0.9953, 0.9931, 0.9919, 0.9918, 0.9909, 0.9903, 0.9902,
         0.9901],
        [0.9900, 0.9893, 0.9891, 0.9878, 0.9854, 0.9841, 0.9832, 0.9821, 0.9808,
         0.9767],
        [0.9952, 0.9889, 0.9884, 0.9872, 0.9841, 0.9815, 0.9802, 0.9789, 0.9782,
         0.9756],
        [0.9935, 0.9920, 0.9913, 0.9913, 0.9913, 0.9897, 0.9885, 0.9878, 0.9874,
         0.9872],
        [0.9965, 0.9912, 0.9911, 0.9903, 0.9900, 0.9888, 0.9884, 0.9875, 0.9875,
         0.9868],
        [0.9937, 0.9921, 0.9920, 0.9918, 0.9908, 0.9901, 0.9891, 0.9889, 0.9879,
         0.9877],
        [0.9805, 0.9784, 0.9777, 0.9777, 0.9775, 0.9759, 0.9759, 0.9758, 0.9744,
         0.9740],
        [0.9918, 0.9899, 0.9898, 0.9888, 0.9887, 0.9867, 0.9836, 0.9821, 0.9816,
         0.9812],
        [0.9953, 0.9927, 0.9912, 0.9873, 0.9862, 0.9861, 0.9854, 0.9845, 0.9824,
         0.9819],
        [0.9878, 0.9868, 0.9846, 0.9829, 0.9825, 0.9794, 0.9770, 0.9770, 0.9764,
         0.9761],
        [0.9894, 0.9887, 0.9876, 0.9851, 0.9843, 0.9830, 0.9806, 0.9792, 0.9787,
         0.9778],
        [0.9920, 0.9787, 0.9778, 0.9731, 0.9698, 0.9686, 0.9674, 0.9671, 0.9651,
         0.9637],
        [0.9849, 0.9788, 0.9753, 0.9742, 0.9725, 0.9720, 0.9720, 0.9717, 0.9700,
         0.9693],
        [0.9899, 0.9866, 0.9836, 0.9811, 0.9810, 0.9735, 0.9713, 0.9702, 0.9691,
         0.9687],
        [0.9895, 0.9819, 0.9817, 0.9707, 0.9706, 0.9696, 0.9673, 0.9624, 0.9610,
         0.9599],
        [0.9900, 0.9885, 0.9880, 0.9870, 0.9863, 0.9857, 0.9854, 0.9847, 0.9842,
         0.9819],
        [0.9865, 0.9845, 0.9837, 0.9833, 0.9814, 0.9773, 0.9771, 0.9757, 0.9747,
         0.9689],
        [0.9916, 0.9897, 0.9894, 0.9868, 0.9860, 0.9859, 0.9848, 0.9840, 0.9834,
         0.9811],
        [0.9935, 0.9907, 0.9892, 0.9854, 0.9853, 0.9837, 0.9832, 0.9824, 0.9822,
         0.9797],
        [0.9887, 0.9833, 0.9826, 0.9814, 0.9800, 0.9800, 0.9798, 0.9797, 0.9789,
         0.9787],
        [0.9928, 0.9891, 0.9878, 0.9867, 0.9866, 0.9842, 0.9841, 0.9836, 0.9830,
         0.9825],
        [0.9933, 0.9924, 0.9887, 0.9883, 0.9881, 0.9860, 0.9847, 0.9844, 0.9843,
         0.9834],
        [0.9903, 0.9895, 0.9889, 0.9885, 0.9882, 0.9869, 0.9867, 0.9865, 0.9844,
         0.9843],
        [0.9947, 0.9919, 0.9904, 0.9899, 0.9861, 0.9854, 0.9850, 0.9846, 0.9842,
         0.9841],
        [0.9892, 0.9853, 0.9850, 0.9850, 0.9842, 0.9808, 0.9798, 0.9794, 0.9787,
         0.9778],
        [0.9936, 0.9867, 0.9858, 0.9847, 0.9843, 0.9839, 0.9831, 0.9824, 0.9817,
         0.9815],
        [0.9946, 0.9944, 0.9922, 0.9918, 0.9903, 0.9901, 0.9900, 0.9881, 0.9877,
         0.9872],
        [0.9799, 0.9770, 0.9707, 0.9687, 0.9668, 0.9662, 0.9650, 0.9637, 0.9630,
         0.9629],
        [0.9854, 0.9847, 0.9818, 0.9811, 0.9811, 0.9793, 0.9765, 0.9762, 0.9752,
         0.9718],
        [0.9942, 0.9870, 0.9829, 0.9807, 0.9757, 0.9714, 0.9689, 0.9686, 0.9616,
         0.9534],
        [0.9774, 0.9731, 0.9727, 0.9721, 0.9708, 0.9665, 0.9657, 0.9640, 0.9632,
         0.9622],
        [0.9842, 0.9818, 0.9767, 0.9736, 0.9701, 0.9694, 0.9677, 0.9675, 0.9667,
         0.9666],
        [0.9875, 0.9846, 0.9804, 0.9798, 0.9764, 0.9755, 0.9719, 0.9632, 0.9617,
         0.9577],
        [0.9838, 0.9831, 0.9830, 0.9818, 0.9814, 0.9790, 0.9788, 0.9777, 0.9773,
         0.9771],
        [0.9805, 0.9798, 0.9788, 0.9752, 0.9749, 0.9739, 0.9727, 0.9725, 0.9694,
         0.9692],
        [0.9900, 0.9884, 0.9883, 0.9880, 0.9876, 0.9860, 0.9841, 0.9839, 0.9833,
         0.9831],
        [0.9965, 0.9955, 0.9947, 0.9943, 0.9940, 0.9934, 0.9930, 0.9923, 0.9922,
         0.9916],
        [0.9937, 0.9906, 0.9851, 0.9844, 0.9837, 0.9831, 0.9817, 0.9812, 0.9777,
         0.9762],
        [0.9888, 0.9843, 0.9810, 0.9770, 0.9763, 0.9729, 0.9652, 0.9624, 0.9558,
         0.9541],
        [0.9892, 0.9824, 0.9648, 0.9643, 0.9605, 0.9591, 0.9570, 0.9509, 0.9500,
         0.9488],
        [0.9913, 0.9828, 0.9811, 0.9794, 0.9739, 0.9707, 0.9666, 0.9641, 0.9620,
         0.9618],
        [0.9837, 0.9824, 0.9788, 0.9778, 0.9768, 0.9768, 0.9765, 0.9737, 0.9716,
         0.9695],
        [0.9888, 0.9879, 0.9869, 0.9835, 0.9833, 0.9831, 0.9825, 0.9808, 0.9782,
         0.9777],
        [0.9867, 0.9770, 0.9742, 0.9712, 0.9694, 0.9692, 0.9691, 0.9689, 0.9676,
         0.9671],
        [0.9759, 0.9734, 0.9701, 0.9693, 0.9622, 0.9597, 0.9575, 0.9571, 0.9570,
         0.9555],
        [0.9913, 0.9744, 0.9728, 0.9711, 0.9661, 0.9646, 0.9643, 0.9591, 0.9583,
         0.9562],
        [0.9852, 0.9631, 0.9619, 0.9602, 0.9568, 0.9535, 0.9521, 0.9494, 0.9407,
         0.9401],
        [0.9800, 0.9764, 0.9763, 0.9755, 0.9753, 0.9752, 0.9747, 0.9745, 0.9735,
         0.9735],
        [0.9736, 0.9626, 0.9585, 0.9562, 0.9553, 0.9538, 0.9505, 0.9487, 0.9482,
         0.9442],
        [0.9828, 0.9759, 0.9642, 0.9607, 0.9591, 0.9568, 0.9544, 0.9475, 0.9459,
         0.9437],
        [0.9868, 0.9825, 0.9790, 0.9763, 0.9758, 0.9757, 0.9753, 0.9742, 0.9740,
         0.9738]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 462464.8438,  400039.5312,  370276.8438,  346333.8438,  327439.1562,
          323045.3125,  269595.7188,  233319.8750,  232020.6875,  222962.1562],
        [1451138.0000, 1426165.2500, 1400776.6250, 1390785.2500, 1386088.6250,
         1353770.7500, 1343918.6250, 1327116.8750, 1322808.0000, 1321824.3750],
        [1493863.2500, 1466402.0000, 1447487.6250, 1405225.3750, 1360641.0000,
         1353445.3750, 1306045.8750, 1302779.2500, 1285976.6250, 1265909.0000],
        [1463242.0000, 1445457.0000, 1438406.7500, 1430834.0000, 1429764.5000,
         1396340.5000, 1395222.3750, 1382615.1250, 1380770.3750, 1377195.8750],
        [1458243.5000, 1442291.3750, 1426950.3750, 1426736.7500, 1421120.1250,
         1420666.1250, 1418458.1250, 1418117.2500, 1411765.7500, 1409957.3750],
        [1553176.3750, 1552684.6250, 1532562.5000, 1531643.3750, 1524280.2500,
         1523848.6250, 1514066.2500, 1512245.2500, 1504926.6250, 1482824.5000],
        [1588597.7500, 1588019.1250, 1587610.2500, 1575643.0000, 1573752.2500,
         1573228.6250, 1572100.7500, 1569817.6250, 1568959.8750, 1558303.8750],
        [1519796.6250, 1513053.0000, 1509290.1250, 1506960.2500, 1506599.5000,
         1498647.7500, 1480364.6250, 1467747.8750, 1440527.6250, 1429024.3750],
        [1474776.0000, 1445907.8750, 1417441.2500, 1406856.0000, 1402982.5000,
         1388374.7500, 1367769.2500, 1363433.7500, 1361484.6250, 1323327.7500],
        [1229505.1250, 1123242.6250, 1103729.6250, 1082564.2500, 1079748.5000,
          952513.0625,  811420.4375,  811039.0625,  810241.9375,  682747.7500],
        [1497172.0000, 1380913.8750, 1355239.5000, 1351494.0000, 1314992.1250,
         1312253.5000, 1272278.7500, 1263400.3750, 1206120.1250, 1204019.2500],
        [1504592.2500, 1502500.1250, 1388744.1250, 1370462.8750, 1366840.7500,
         1362408.1250, 1356126.3750, 1355980.2500, 1345268.7500, 1327333.2500],
        [1372261.2500, 1365397.2500, 1320259.6250, 1312100.8750, 1270245.6250,
         1221411.0000, 1202228.1250, 1200339.0000, 1195002.7500, 1108248.1250],
        [1534722.6250, 1522149.2500, 1496962.2500, 1450594.1250, 1425757.3750,
         1423111.1250, 1405519.0000, 1394053.2500, 1391889.1250, 1389390.6250],
        [1386435.0000, 1373429.0000, 1368794.8750, 1343723.8750, 1298419.5000,
         1275117.5000, 1258381.2500, 1238511.5000, 1216627.0000, 1146746.5000],
        [1494151.0000, 1365078.2500, 1354981.0000, 1332529.5000, 1274518.1250,
         1229263.6250, 1205945.2500, 1184392.1250, 1171239.0000, 1128944.1250],
        [1459341.3750, 1427626.8750, 1413835.2500, 1413707.2500, 1413308.1250,
         1381064.1250, 1358479.6250, 1344208.2500, 1336495.2500, 1333481.7500],
        [1521383.0000, 1412033.6250, 1409238.2500, 1393657.0000, 1387849.1250,
         1364225.8750, 1356505.3750, 1339275.5000, 1338526.0000, 1325997.1250],
        [1463046.6250, 1429294.2500, 1427164.0000, 1422793.5000, 1403179.2500,
         1389729.8750, 1369194.3750, 1365527.5000, 1346158.2500, 1342240.7500],
        [1210594.7500, 1174853.5000, 1164328.6250, 1163009.1250, 1160484.1250,
         1134884.5000, 1134149.7500, 1132607.3750, 1109474.8750, 1103900.1250],
        [1424053.3750, 1385270.6250, 1383728.3750, 1364099.6250, 1361788.5000,
         1323281.1250, 1265618.1250, 1238419.5000, 1229662.3750, 1223187.3750],
        [1496576.8750, 1442664.1250, 1410979.6250, 1335339.6250, 1313921.5000,
         1312523.8750, 1298641.1250, 1282379.7500, 1243829.7500, 1235259.5000],
        [1344167.2500, 1325238.7500, 1283416.0000, 1253710.8750, 1245610.5000,
         1192333.2500, 1151895.7500, 1151425.7500, 1143112.6250, 1136993.6250],
        [1375067.2500, 1362552.5000, 1341142.8750, 1292723.5000, 1278610.0000,
         1255700.6250, 1213228.7500, 1188489.1250, 1180287.8750, 1165917.6250],
        [1427698.8750, 1180596.2500, 1164879.5000, 1089369.5000, 1039770.1875,
         1022385.6250, 1004347.8750,  999902.8750,  972295.9375,  953291.8125],
        [1289861.5000, 1182118.3750, 1124807.6250, 1107140.0000, 1079905.0000,
         1073439.2500, 1072083.6250, 1068210.6250, 1042964.1250, 1032370.5000],
        [1385167.5000, 1321942.8750, 1265648.2500, 1221461.0000, 1220488.7500,
         1095958.5000, 1061453.3750, 1046124.8750, 1029042.1875, 1023642.2500],
        [1378271.8750, 1235657.7500, 1231585.8750, 1053139.0000, 1051625.5000,
         1036688.9375, 1002363.3125,  934962.3125,  916226.4375,  902908.0625],
        [1387559.2500, 1357167.8750, 1347497.7500, 1328521.1250, 1316555.6250,
         1304563.2500, 1299026.3750, 1285281.3750, 1276557.0000, 1234970.8750],
        [1319875.6250, 1283243.3750, 1267897.6250, 1260982.1250, 1227508.8750,
         1156440.6250, 1153495.2500, 1131054.1250, 1114431.3750, 1025856.7500],
        [1419546.1250, 1381282.7500, 1376215.1250, 1325944.1250, 1309402.0000,
         1309279.6250, 1287843.2500, 1273075.1250, 1261884.3750, 1222413.0000],
        [1457923.7500, 1401293.6250, 1371648.7500, 1299452.5000, 1297988.6250,
         1267642.6250, 1259726.1250, 1244056.3750, 1241639.6250, 1197240.8750],
        [1360973.2500, 1261144.5000, 1247474.5000, 1226096.6250, 1203331.6250,
         1202494.2500, 1199612.2500, 1197934.0000, 1183881.7500, 1180903.6250],
        [1444287.1250, 1370249.8750, 1344944.2500, 1323213.0000, 1320953.5000,
         1276935.6250, 1274970.3750, 1266537.0000, 1256050.3750, 1247166.2500],
        [1454984.7500, 1435235.8750, 1361047.2500, 1353519.0000, 1349356.1250,
         1310641.3750, 1286245.2500, 1280967.8750, 1278330.7500, 1262905.2500],
        [1393185.3750, 1376970.0000, 1365282.6250, 1357202.8750, 1351535.1250,
         1327086.5000, 1323591.6250, 1320205.5000, 1281431.1250, 1278799.0000],
        [1482773.5000, 1426003.5000, 1395310.1250, 1385330.1250, 1312546.3750,
         1299729.0000, 1291942.1250, 1284090.6250, 1276345.1250, 1275921.6250],
        [1370895.5000, 1296896.0000, 1292172.5000, 1290906.3750, 1277864.0000,
         1216552.7500, 1198450.5000, 1191510.2500, 1179934.5000, 1164722.8750],
        [1460783.8750, 1323935.0000, 1305864.0000, 1286442.7500, 1278488.0000,
         1272135.6250, 1256844.8750, 1244954.8750, 1232816.2500, 1228833.5000],
        [1481986.2500, 1477225.3750, 1431206.6250, 1423016.1250, 1392549.1250,
         1389928.6250, 1387630.7500, 1350997.7500, 1342783.5000, 1333118.0000],
        [1200427.1250, 1152004.6250, 1053591.0000, 1023562.1875,  995748.2500,
          986869.1875,  970642.1875,  952142.5000,  942653.0000,  941698.7500],
        [1299627.3750, 1286882.0000, 1233201.8750, 1222004.0000, 1220897.3750,
         1189862.6250, 1144624.6250, 1138865.7500, 1123675.5000, 1070385.7500],
        [1473352.0000, 1329358.8750, 1254347.0000, 1214882.1250, 1131268.8750,
         1063428.2500, 1026369.5625, 1022598.1875,  924267.9375,  822166.0000],
        [1158214.2500, 1089863.1250, 1083446.2500, 1074743.2500, 1054539.0000,
          992133.9375,  980142.3125,  956769.1875,  945631.5000,  932815.8750],
        [1276586.2500, 1234490.3750, 1147860.2500, 1096986.2500, 1044579.6875,
         1034232.0000, 1008578.8125, 1005228.5000,  994271.6875,  992960.2500],
        [1338671.5000, 1283392.7500, 1209955.2500, 1199439.5000, 1141729.0000,
         1128441.3750, 1071641.1250,  945384.4375,  925948.6250,  873999.1875],
        [1269591.7500, 1256244.5000, 1255015.8750, 1233983.1250, 1226328.1250,
         1186098.8750, 1182124.0000, 1163765.7500, 1157650.0000, 1153204.8750],
        [1210455.1250, 1198742.0000, 1181489.5000, 1122698.6250, 1117900.3750,
         1102652.2500, 1083409.1250, 1080986.8750, 1033350.6250, 1030953.7500],
        [1386462.7500, 1355136.1250, 1353980.0000, 1347555.7500, 1340053.5000,
         1311110.1250, 1275719.6250, 1271857.8750, 1260608.1250, 1256533.3750],
        [1521422.2500, 1500704.3750, 1484557.8750, 1474811.2500, 1469303.7500,
         1456920.2500, 1447182.6250, 1434442.2500, 1432313.8750, 1418600.2500],
        [1463327.1250, 1398730.2500, 1293669.3750, 1281258.7500, 1267253.3750,
         1256912.0000, 1232133.3750, 1223120.8750, 1164065.5000, 1139020.0000],
        [1362960.5000, 1277926.0000, 1220322.3750, 1152168.2500, 1140317.6250,
         1086269.7500,  973574.5000,  935535.8125,  850728.8125,  830818.8125],
        [1372305.6250, 1244296.1250,  967655.0000,  960604.5625,  910299.5000,
          892762.5625,  866094.3125,  793800.9375,  783616.6875,  770520.8750],
        [1413635.7500, 1251579.5000, 1221952.6250, 1191795.5000, 1102740.6250,
         1053213.2500,  993211.1875,  958531.8750,  930387.6875,  927857.9375],
        [1268702.1250, 1244082.5000, 1182292.0000, 1165315.0000, 1149361.0000,
         1148154.7500, 1144181.5000, 1098977.8750, 1066018.5000, 1035527.8750],
        [1362946.2500, 1345608.8750, 1326374.1250, 1263567.8750, 1259995.2500,
         1256895.2500, 1246753.6250, 1216562.1250, 1172808.2500, 1163652.5000],
        [1323025.0000, 1152347.3750, 1107274.1250, 1060330.3750, 1033910.5625,
         1030407.2500, 1029619.4375, 1026736.6875, 1008060.5000, 1000543.0000],
        [1134930.0000, 1093767.7500, 1043921.4375, 1032241.5625,  932108.8750,
          900223.8125,  872520.1875,  866722.2500,  865892.0000,  847301.3750],
        [1414088.7500, 1110100.3750, 1084361.0000, 1058726.8750,  985756.5000,
          964615.3125,  961603.6875,  891948.1250,  881651.6250,  856330.6250],
        [1294913.6250,  944377.0000,  928375.7500,  906509.4375,  862786.0625,
          824168.5625,  806783.7500,  776508.5625,  686354.7500,  680563.5625],
        [1202261.5000, 1141700.6250, 1140772.3750, 1127605.5000, 1124993.2500,
         1123098.1250, 1114129.6250, 1112017.1250, 1095908.2500, 1095637.6250],
        [1097700.0000,  938235.0000,  885006.0625,  856269.3750,  844888.7500,
          827320.1250,  788625.4375,  769217.6250,  763163.0625,  720966.5625],
        [1252107.2500, 1133964.8750,  959603.7500,  912547.3125,  892568.4375,
          863380.3125,  834428.5625,  755891.5000,  738713.7500,  716037.1250],
        [1325260.1250, 1245996.6250, 1186386.3750, 1140904.0000, 1132682.0000,
         1131280.7500, 1124966.3750, 1107015.3750, 1103747.5000, 1100336.0000]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 462464.8438,       0.0000],
         [ 400039.5312,       0.0000],
         [ 370276.8438,       0.0000],
         ...,
         [ 233319.8750,       0.0000],
         [ 232020.6875,       0.0000],
         [ 222962.1562,       0.0000]],

        [[1451138.0000,       0.0000],
         [1426165.2500,       0.0000],
         [1400776.6250,       0.0000],
         ...,
         [1327116.8750,       0.0000],
         [1322808.0000,       0.0000],
         [1321824.3750,       0.0000]],

        [[1493863.2500,       0.0000],
         [1466402.0000,       0.0000],
         [1447487.6250,       0.0000],
         ...,
         [1302779.2500,       0.0000],
         [1285976.6250,       0.0000],
         [1265909.0000,       0.0000]],

        ...,

        [[1097700.0000,       0.0000],
         [ 938235.0000,       0.0000],
         [ 885006.0625,       0.0000],
         ...,
         [ 769217.6250,       0.0000],
         [      0.0000,  763163.0625],
         [ 720966.5625,       0.0000]],

        [[1252107.2500,       0.0000],
         [1133964.8750,       0.0000],
         [ 959603.7500,       0.0000],
         ...,
         [ 755891.5000,       0.0000],
         [ 738713.7500,       0.0000],
         [ 716037.1250,       0.0000]],

        [[1325260.1250,       0.0000],
         [      0.0000, 1245996.6250],
         [1186386.3750,       0.0000],
         ...,
         [1107015.3750,       0.0000],
         [1103747.5000,       0.0000],
         [1100336.0000,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 3187498.0000,        0.0000],
        [13724392.0000,        0.0000],
        [13687776.0000,        0.0000],
        [12744626.0000,  1395222.3750],
        [12796064.0000,  1458243.5000],
        [13727332.0000,  1504926.6250],
        [14168424.0000,  1587610.2500],
        [12002460.0000,  2869552.0000],
        [12563979.0000,  1388374.7500],
        [ 9686752.0000,        0.0000],
        [ 7792944.5000,  5364939.0000],
        [13880257.0000,        0.0000],
        [11247234.0000,  1320259.6250],
        [12899426.0000,  1534722.6250],
        [12906186.0000,        0.0000],
        [12741042.0000,        0.0000],
        [13881548.0000,        0.0000],
        [12522694.0000,  1325997.1250],
        [ 9786198.0000,  4172131.0000],
        [ 9164793.0000,  2323493.2500],
        [11875828.0000,  1323281.1250],
        [10625797.0000,  2746319.2500],
        [12227904.0000,        0.0000],
        [12653720.0000,        0.0000],
        [ 9832153.0000,  1022385.6250],
        [11072900.0000,        0.0000],
        [10285762.0000,  1385167.5000],
        [10743429.0000,        0.0000],
        [13137700.0000,        0.0000],
        [11940786.0000,        0.0000],
        [13166885.0000,        0.0000],
        [13038613.0000,        0.0000],
        [ 9721970.0000,  2541877.0000],
        [13125308.0000,        0.0000],
        [13373233.0000,        0.0000],
        [12018086.0000,  1357202.8750],
        [ 9415521.0000,  4014471.5000],
        [12479904.0000,        0.0000],
        [11634254.0000,  1256844.8750],
        [12667659.0000,  1342783.5000],
        [10219339.0000,        0.0000],
        [11930027.0000,        0.0000],
        [11262038.0000,        0.0000],
        [10268299.0000,        0.0000],
        [ 9559188.0000,  1276586.2500],
        [11118602.0000,        0.0000],
        [12084007.0000,        0.0000],
        [ 7749056.0000,  3413582.0000],
        [11898410.0000,  1260608.1250],
        [13183338.0000,  1456920.2500],
        [ 9988910.0000,  2730580.5000],
        [ 9744352.0000,  1086269.7500],
        [ 7885576.5000,  1676379.2500],
        [11044906.0000,        0.0000],
        [10337298.0000,  1165315.0000],
        [12615164.0000,        0.0000],
        [ 7189607.5000,  3582646.5000],
        [ 9589629.0000,        0.0000],
        [ 7830479.0000,  2378704.0000],
        [ 7804831.5000,   906509.4375],
        [ 7931782.5000,  3346341.2500],
        [ 7728229.0000,   763163.0625],
        [ 7332246.0000,  1726997.0000],
        [10352578.0000,  1245996.6250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 56/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:34, 61.20s/it]  7%|▋         | 2/30 [01:02<12:04, 25.88s/it] 10%|█         | 3/30 [01:03<06:28, 14.40s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.03s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 4.932127602895101
Epoch 57/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:12, 60.42s/it]  7%|▋         | 2/30 [01:02<12:06, 25.93s/it] 10%|█         | 3/30 [01:03<06:41, 14.86s/it] 13%|█▎        | 4/30 [01:04<04:01,  9.29s/it] 17%|█▋        | 5/30 [01:05<02:35,  6.21s/it] 20%|██        | 6/30 [01:06<01:44,  4.35s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.17s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.40s/it] 30%|███       | 9/30 [01:08<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 4.90165049235026
Epoch 58/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:35, 61.23s/it]  7%|▋         | 2/30 [01:01<11:58, 25.65s/it] 10%|█         | 3/30 [01:02<06:25, 14.28s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.94s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.98s/it] 20%|██        | 6/30 [01:04<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 4.831622791290283
Epoch 59/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:32, 59.05s/it]  7%|▋         | 2/30 [01:00<11:37, 24.92s/it] 10%|█         | 3/30 [01:00<06:14, 13.88s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.83s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 4.766360187530518
Epoch 60/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:47, 59.57s/it]  7%|▋         | 2/30 [01:00<11:39, 24.97s/it] 10%|█         | 3/30 [01:01<06:20, 14.08s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.82s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.91s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 4.733039172490438
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0551,  0.0138,  0.0134,  ..., -0.0124, -0.0115, -0.0140],
        [-0.0610,  0.0263,  0.0209,  ..., -0.0085,  0.0002, -0.0484],
        [-0.0702, -0.0077, -0.0083,  ...,  0.0701,  0.0441, -0.0237],
        ...,
        [-0.0238, -0.0264, -0.0098,  ..., -0.0353, -0.0119,  0.0447],
        [-0.0877, -0.0069,  0.0384,  ..., -0.0217,  0.0404, -0.0007],
        [-0.0428,  0.0193,  0.0100,  ..., -0.0284,  0.0487, -0.0232]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9102, 0.9039, 0.8896, 0.8880, 0.8783, 0.8768, 0.8657, 0.8650, 0.8634,
         0.8586],
        [0.9939, 0.9932, 0.9913, 0.9905, 0.9898, 0.9881, 0.9877, 0.9874, 0.9872,
         0.9859],
        [0.9944, 0.9915, 0.9909, 0.9897, 0.9879, 0.9860, 0.9835, 0.9829, 0.9815,
         0.9815],
        [0.9930, 0.9911, 0.9906, 0.9901, 0.9901, 0.9887, 0.9881, 0.9873, 0.9873,
         0.9872],
        [0.9924, 0.9911, 0.9900, 0.9898, 0.9897, 0.9895, 0.9894, 0.9892, 0.9885,
         0.9880],
        [0.9970, 0.9969, 0.9963, 0.9963, 0.9956, 0.9956, 0.9950, 0.9948, 0.9946,
         0.9942],
        [0.9993, 0.9993, 0.9992, 0.9987, 0.9986, 0.9985, 0.9985, 0.9983, 0.9982,
         0.9978],
        [0.9956, 0.9952, 0.9952, 0.9951, 0.9947, 0.9940, 0.9935, 0.9931, 0.9916,
         0.9911],
        [0.9939, 0.9928, 0.9910, 0.9908, 0.9902, 0.9897, 0.9887, 0.9884, 0.9878,
         0.9877],
        [0.9823, 0.9760, 0.9726, 0.9709, 0.9688, 0.9629, 0.9511, 0.9418, 0.9411,
         0.9407],
        [0.9945, 0.9890, 0.9888, 0.9865, 0.9862, 0.9861, 0.9842, 0.9822, 0.9800,
         0.9778],
        [0.9953, 0.9952, 0.9892, 0.9878, 0.9875, 0.9873, 0.9867, 0.9865, 0.9858,
         0.9853],
        [0.9881, 0.9869, 0.9846, 0.9837, 0.9782, 0.9777, 0.9731, 0.9730, 0.9725,
         0.9713],
        [0.9972, 0.9962, 0.9953, 0.9927, 0.9924, 0.9920, 0.9914, 0.9911, 0.9908,
         0.9904],
        [0.9899, 0.9897, 0.9890, 0.9869, 0.9867, 0.9833, 0.9815, 0.9809, 0.9793,
         0.9755],
        [0.9939, 0.9877, 0.9850, 0.9830, 0.9818, 0.9783, 0.9773, 0.9770, 0.9745,
         0.9700],
        [0.9933, 0.9923, 0.9917, 0.9910, 0.9907, 0.9907, 0.9889, 0.9886, 0.9868,
         0.9867],
        [0.9966, 0.9917, 0.9913, 0.9906, 0.9893, 0.9893, 0.9882, 0.9880, 0.9877,
         0.9857],
        [0.9915, 0.9907, 0.9901, 0.9888, 0.9883, 0.9882, 0.9879, 0.9876, 0.9871,
         0.9861],
        [0.9817, 0.9808, 0.9796, 0.9793, 0.9780, 0.9764, 0.9763, 0.9761, 0.9752,
         0.9744],
        [0.9915, 0.9897, 0.9892, 0.9880, 0.9869, 0.9849, 0.9848, 0.9832, 0.9813,
         0.9804],
        [0.9948, 0.9913, 0.9902, 0.9852, 0.9844, 0.9834, 0.9822, 0.9822, 0.9795,
         0.9794],
        [0.9884, 0.9860, 0.9856, 0.9819, 0.9814, 0.9793, 0.9772, 0.9772, 0.9752,
         0.9734],
        [0.9883, 0.9873, 0.9873, 0.9851, 0.9829, 0.9829, 0.9819, 0.9816, 0.9760,
         0.9759],
        [0.9925, 0.9774, 0.9772, 0.9725, 0.9708, 0.9689, 0.9687, 0.9668, 0.9645,
         0.9606],
        [0.9849, 0.9814, 0.9771, 0.9763, 0.9750, 0.9738, 0.9729, 0.9720, 0.9706,
         0.9697],
        [0.9887, 0.9859, 0.9833, 0.9797, 0.9784, 0.9716, 0.9694, 0.9672, 0.9670,
         0.9667],
        [0.9901, 0.9822, 0.9810, 0.9727, 0.9716, 0.9692, 0.9672, 0.9625, 0.9619,
         0.9600],
        [0.9880, 0.9875, 0.9863, 0.9856, 0.9851, 0.9838, 0.9831, 0.9830, 0.9822,
         0.9799],
        [0.9877, 0.9853, 0.9839, 0.9837, 0.9832, 0.9805, 0.9801, 0.9775, 0.9737,
         0.9717],
        [0.9917, 0.9896, 0.9882, 0.9857, 0.9849, 0.9845, 0.9831, 0.9804, 0.9802,
         0.9802],
        [0.9928, 0.9889, 0.9864, 0.9863, 0.9851, 0.9830, 0.9810, 0.9803, 0.9800,
         0.9796],
        [0.9855, 0.9789, 0.9782, 0.9781, 0.9778, 0.9773, 0.9771, 0.9769, 0.9769,
         0.9759],
        [0.9920, 0.9880, 0.9867, 0.9860, 0.9859, 0.9846, 0.9824, 0.9823, 0.9820,
         0.9820],
        [0.9937, 0.9918, 0.9902, 0.9883, 0.9878, 0.9861, 0.9855, 0.9849, 0.9838,
         0.9836],
        [0.9908, 0.9887, 0.9866, 0.9865, 0.9864, 0.9859, 0.9852, 0.9842, 0.9831,
         0.9825],
        [0.9932, 0.9895, 0.9877, 0.9869, 0.9829, 0.9822, 0.9820, 0.9820, 0.9811,
         0.9809],
        [0.9896, 0.9861, 0.9857, 0.9857, 0.9854, 0.9822, 0.9805, 0.9802, 0.9800,
         0.9786],
        [0.9933, 0.9865, 0.9833, 0.9828, 0.9821, 0.9821, 0.9821, 0.9802, 0.9800,
         0.9794],
        [0.9953, 0.9941, 0.9923, 0.9923, 0.9907, 0.9907, 0.9895, 0.9881, 0.9878,
         0.9876],
        [0.9800, 0.9745, 0.9709, 0.9627, 0.9617, 0.9613, 0.9608, 0.9605, 0.9590,
         0.9574],
        [0.9849, 0.9834, 0.9804, 0.9793, 0.9772, 0.9757, 0.9747, 0.9741, 0.9737,
         0.9726],
        [0.9928, 0.9840, 0.9819, 0.9784, 0.9737, 0.9688, 0.9668, 0.9664, 0.9637,
         0.9502],
        [0.9750, 0.9716, 0.9705, 0.9688, 0.9687, 0.9679, 0.9664, 0.9633, 0.9622,
         0.9614],
        [0.9807, 0.9791, 0.9731, 0.9666, 0.9609, 0.9603, 0.9594, 0.9590, 0.9581,
         0.9581],
        [0.9837, 0.9811, 0.9745, 0.9727, 0.9726, 0.9725, 0.9687, 0.9569, 0.9542,
         0.9531],
        [0.9835, 0.9827, 0.9778, 0.9778, 0.9769, 0.9765, 0.9758, 0.9755, 0.9755,
         0.9720],
        [0.9795, 0.9746, 0.9739, 0.9727, 0.9691, 0.9685, 0.9641, 0.9635, 0.9624,
         0.9622],
        [0.9891, 0.9867, 0.9860, 0.9840, 0.9840, 0.9834, 0.9811, 0.9808, 0.9807,
         0.9804],
        [0.9960, 0.9939, 0.9937, 0.9930, 0.9924, 0.9924, 0.9923, 0.9920, 0.9919,
         0.9905],
        [0.9925, 0.9892, 0.9830, 0.9821, 0.9805, 0.9802, 0.9800, 0.9793, 0.9752,
         0.9732],
        [0.9854, 0.9850, 0.9806, 0.9778, 0.9747, 0.9729, 0.9648, 0.9642, 0.9552,
         0.9537],
        [0.9895, 0.9802, 0.9673, 0.9637, 0.9602, 0.9599, 0.9547, 0.9543, 0.9497,
         0.9437],
        [0.9905, 0.9793, 0.9793, 0.9775, 0.9734, 0.9701, 0.9660, 0.9599, 0.9596,
         0.9592],
        [0.9857, 0.9787, 0.9770, 0.9766, 0.9756, 0.9750, 0.9727, 0.9701, 0.9694,
         0.9690],
        [0.9885, 0.9885, 0.9868, 0.9840, 0.9837, 0.9831, 0.9816, 0.9814, 0.9784,
         0.9779],
        [0.9844, 0.9765, 0.9758, 0.9694, 0.9681, 0.9642, 0.9636, 0.9636, 0.9625,
         0.9622],
        [0.9712, 0.9700, 0.9699, 0.9624, 0.9562, 0.9555, 0.9537, 0.9525, 0.9523,
         0.9512],
        [0.9903, 0.9722, 0.9710, 0.9680, 0.9676, 0.9638, 0.9625, 0.9594, 0.9592,
         0.9567],
        [0.9840, 0.9611, 0.9580, 0.9574, 0.9529, 0.9520, 0.9477, 0.9469, 0.9390,
         0.9374],
        [0.9743, 0.9739, 0.9738, 0.9736, 0.9710, 0.9709, 0.9704, 0.9702, 0.9695,
         0.9692],
        [0.9703, 0.9580, 0.9567, 0.9538, 0.9520, 0.9514, 0.9511, 0.9467, 0.9456,
         0.9434],
        [0.9802, 0.9745, 0.9631, 0.9604, 0.9579, 0.9568, 0.9528, 0.9512, 0.9503,
         0.9441],
        [0.9845, 0.9773, 0.9762, 0.9731, 0.9723, 0.9713, 0.9712, 0.9696, 0.9696,
         0.9684]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 443794.7812,  405705.0000,  330562.0000,  323201.8438,  281145.0625,
          275275.0312,  234999.6250,  232738.2969,  227469.4375,  212402.0781],
        [1467311.2500, 1453085.0000, 1412520.0000, 1397696.7500, 1382561.1250,
         1349336.7500, 1342787.3750, 1336807.5000, 1332176.3750, 1307779.6250],
        [1477728.3750, 1418300.0000, 1405701.3750, 1381896.7500, 1345521.6250,
         1309906.5000, 1264994.2500, 1253483.6250, 1229359.7500, 1229196.8750],
        [1447526.2500, 1409623.8750, 1399696.2500, 1389324.3750, 1389231.6250,
         1361670.3750, 1349365.1250, 1334592.3750, 1334141.8750, 1332158.5000],
        [1434737.7500, 1410120.1250, 1386596.3750, 1383326.0000, 1381825.6250,
         1376916.2500, 1375794.0000, 1372238.8750, 1357602.8750, 1348945.6250],
        [1532300.8750, 1530580.3750, 1517796.2500, 1517367.7500, 1503735.8750,
         1502762.3750, 1489814.1250, 1485120.1250, 1481960.7500, 1473422.2500],
        [1584160.5000, 1583879.5000, 1582834.6250, 1570758.0000, 1568198.6250,
         1567022.0000, 1566403.3750, 1561165.7500, 1560133.0000, 1551269.6250],
        [1502983.1250, 1494662.6250, 1493417.2500, 1493082.6250, 1482691.6250,
         1469289.8750, 1457976.6250, 1450342.3750, 1419084.6250, 1409766.5000],
        [1467137.6250, 1443696.3750, 1406896.2500, 1403056.1250, 1391911.7500,
         1382235.3750, 1362156.1250, 1356592.1250, 1344333.8750, 1342659.3750],
        [1242422.5000, 1135567.6250, 1082421.8750, 1055583.3750, 1024706.8750,
          941473.3750,  796136.0625,  697193.1875,  689717.5625,  685505.0000],
        [1480100.6250, 1366992.0000, 1363329.6250, 1319884.5000, 1313853.8750,
         1312721.6250, 1277530.0000, 1240356.6250, 1202518.2500, 1165838.6250],
        [1497013.5000, 1494396.1250, 1371289.1250, 1343513.6250, 1338499.1250,
         1335009.8750, 1324220.3750, 1319902.1250, 1307006.6250, 1297671.6250],
        [1350441.3750, 1326970.0000, 1285019.1250, 1267935.1250, 1172834.0000,
         1164499.6250, 1089863.1250, 1088378.8750, 1080144.8750, 1062707.3750],
        [1536636.8750, 1516116.6250, 1496050.2500, 1441069.1250, 1436268.3750,
         1428400.2500, 1414461.0000, 1409863.2500, 1402815.3750, 1394966.8750],
        [1385685.5000, 1380896.7500, 1367625.7500, 1326866.2500, 1323760.7500,
         1260147.8750, 1227923.2500, 1217584.6250, 1191016.0000, 1127461.5000],
        [1465749.0000, 1342124.1250, 1292278.5000, 1255117.6250, 1234687.1250,
         1173991.1250, 1157414.8750, 1151363.1250, 1111370.5000, 1042104.0625],
        [1455260.8750, 1433571.1250, 1421505.0000, 1407054.6250, 1402122.5000,
         1401097.2500, 1364936.3750, 1360460.6250, 1326098.3750, 1323190.2500],
        [1524776.1250, 1421773.5000, 1412771.8750, 1399729.6250, 1373934.7500,
         1373490.5000, 1351427.0000, 1348312.8750, 1341721.1250, 1304395.3750],
        [1416507.5000, 1401813.6250, 1388297.8750, 1364331.1250, 1354561.1250,
         1352087.0000, 1345454.8750, 1341300.2500, 1331777.5000, 1311649.1250],
        [1232846.8750, 1216290.6250, 1196253.5000, 1191040.0000, 1169521.2500,
         1142771.5000, 1140539.5000, 1136904.7500, 1123658.3750, 1109945.7500],
        [1417408.8750, 1381047.0000, 1372283.3750, 1348262.7500, 1326434.8750,
         1290426.3750, 1287514.2500, 1258635.7500, 1224659.2500, 1208840.0000],
        [1486572.5000, 1414090.2500, 1390931.1250, 1295105.0000, 1280342.6250,
         1262482.6250, 1240625.2500, 1240499.8750, 1194657.5000, 1191605.6250],
        [1356449.7500, 1310470.1250, 1302274.8750, 1235937.0000, 1226110.6250,
         1191219.3750, 1156137.3750, 1155005.5000, 1122796.0000, 1093662.5000],
        [1354397.0000, 1334652.2500, 1334023.5000, 1293382.0000, 1253888.8750,
         1253646.2500, 1235591.7500, 1230084.6250, 1135496.1250, 1133853.5000],
        [1438688.0000, 1158905.8750, 1155574.0000, 1080439.6250, 1054076.5000,
         1025757.9375, 1023702.7500,  996121.5000,  964287.0000,  911562.7500],
        [1288999.6250, 1226982.1250, 1154483.5000, 1141469.8750, 1120213.0000,
         1100166.0000, 1086945.3750, 1073037.0000, 1051856.2500, 1037889.8125],
        [1362296.5000, 1307662.3750, 1261294.7500, 1197213.3750, 1174673.1250,
         1066864.6250, 1034174.8125, 1001956.1875,  999210.8125,  994050.8125],
        [1388970.6250, 1240770.7500, 1220502.7500, 1082850.3750, 1066205.6250,
         1030969.5000, 1002153.0625,  937013.5000,  928669.7500,  903794.5625],
        [1348003.0000, 1339092.7500, 1316295.6250, 1302271.2500, 1292785.1250,
         1270576.3750, 1257156.5000, 1255794.1250, 1241542.5000, 1201533.6250],
        [1342714.3750, 1297023.5000, 1271429.7500, 1266998.5000, 1258570.8750,
         1211521.0000, 1203720.7500, 1160385.6250, 1098626.8750, 1068686.5000],
        [1421490.1250, 1378812.3750, 1352820.8750, 1304182.7500, 1289518.3750,
         1282916.7500, 1256328.3750, 1208768.5000, 1205357.6250, 1205309.5000],
        [1443096.2500, 1366612.6250, 1317826.8750, 1316541.7500, 1293052.7500,
         1255741.5000, 1220288.5000, 1206931.2500, 1202688.0000, 1195439.2500],
        [1300336.3750, 1183669.5000, 1171596.5000, 1170922.8750, 1165111.7500,
         1157506.5000, 1154470.3750, 1151277.5000, 1150443.2500, 1134369.3750],
        [1428125.2500, 1349017.7500, 1322853.5000, 1309350.7500, 1307642.3750,
         1283719.6250, 1244347.1250, 1242971.2500, 1237526.8750, 1237216.5000],
        [1462253.0000, 1424091.3750, 1391635.6250, 1353973.5000, 1343643.1250,
         1311330.2500, 1300908.2500, 1290574.0000, 1269299.8750, 1266174.6250],
        [1403587.3750, 1361578.1250, 1321244.5000, 1320143.7500, 1317928.5000,
         1308896.2500, 1296122.0000, 1276895.3750, 1256178.6250, 1246500.5000],
        [1451929.7500, 1376636.5000, 1342003.8750, 1327564.8750, 1252982.8750,
         1241351.8750, 1237715.7500, 1236745.8750, 1222460.8750, 1218639.5000],
        [1379747.5000, 1311341.5000, 1305312.5000, 1303926.5000, 1299772.3750,
         1240221.8750, 1211332.6250, 1205409.5000, 1202697.2500, 1179390.0000],
        [1453984.6250, 1319431.5000, 1260693.6250, 1251485.3750, 1239077.3750,
         1238656.8750, 1238486.7500, 1205701.5000, 1203305.2500, 1192838.1250],
        [1495660.7500, 1471282.2500, 1434519.0000, 1434193.3750, 1401809.5000,
         1401583.7500, 1377435.0000, 1350307.3750, 1344727.6250, 1341078.8750],
        [1203346.5000, 1111720.2500, 1055412.2500,  939880.1250,  926584.5625,
          920737.4375,  914250.2500,  910765.8125,  890795.4375,  870537.8750],
        [1290342.6250, 1263149.7500, 1209596.5000, 1190794.6250, 1154718.1250,
         1130442.6250, 1114538.7500, 1104725.7500, 1099826.1250, 1082679.8750],
        [1443886.3750, 1272786.1250, 1235191.1250, 1175752.5000, 1098865.7500,
         1024573.9375,  995373.1875,  990116.8125,  952226.0625,  785363.6250],
        [1120092.3750, 1066518.8750, 1049797.7500, 1025363.8125, 1022931.7500,
         1011428.0000,  989813.7500,  947698.0625,  933065.8750,  921616.8125],
        [1214390.8750, 1186754.1250, 1088971.7500,  993440.5000,  915120.8750,
          907989.8125,  895908.8750,  890575.5000,  880074.8125,  879809.6875],
        [1267553.1250, 1221761.6250, 1111060.0000, 1082960.8750, 1082630.3750,
         1080598.2500, 1023632.4375,  864989.8750,  831939.1250,  818370.3750],
        [1263934.1250, 1250027.7500, 1165516.2500, 1164658.3750, 1150251.3750,
         1143631.7500, 1132590.1250, 1127624.8750, 1127023.8750, 1072165.5000],
        [1194283.7500, 1113036.7500, 1102223.2500, 1083097.1250, 1029553.6875,
         1020093.0000,  958059.3750,  949789.9375,  935202.1250,  932300.0000],
        [1369096.3750, 1323147.3750, 1309786.6250, 1273713.7500, 1272463.2500,
         1263083.5000, 1221143.0000, 1215897.3750, 1215286.5000, 1208737.3750],
        [1511400.3750, 1466663.5000, 1463335.5000, 1447179.8750, 1435329.0000,
         1434884.2500, 1432819.3750, 1426946.2500, 1425881.1250, 1396384.3750],
        [1437509.8750, 1371419.8750, 1255331.8750, 1239748.8750, 1210719.5000,
         1206370.8750, 1202291.2500, 1190893.3750, 1122697.5000, 1090655.3750],
        [1299095.6250, 1291809.0000, 1213602.6250, 1165333.8750, 1114876.7500,
         1086931.8750,  968357.5000,  960161.2500,  844411.0625,  825652.2500],
        [1377703.0000, 1205189.8750, 1002755.3125,  952490.3125,  905674.6875,
          902222.0625,  837281.5000,  833246.0625,  780268.4375,  716192.1875],
        [1398023.3750, 1191262.6250, 1190810.5000, 1160372.3750, 1093700.0000,
         1044686.3125,  984042.2500,  902998.5000,  898697.7500,  892863.8750],
        [1303782.2500, 1180407.1250, 1152348.5000, 1146398.6250, 1128755.7500,
         1120131.8750, 1083998.1250, 1044698.1875, 1033033.3750, 1027382.1875],
        [1358655.7500, 1357968.0000, 1324504.6250, 1273965.2500, 1268221.8750,
         1256612.3750, 1230260.5000, 1226687.2500, 1176045.1250, 1167484.1250],
        [1279960.6250, 1143448.3750, 1132256.3750, 1033364.4375, 1014960.6250,
          960095.3125,  951392.6875,  951076.0625,  936298.0000,  931944.5000],
        [1060601.3750, 1043185.8750, 1040751.3750,  935755.2500,  856382.0625,
          847206.8750,  826321.8750,  811795.8125,  809792.3750,  796973.1875],
        [1393277.0000, 1075378.8750, 1057920.5000, 1013151.2500, 1007609.7500,
          954432.5625,  936433.8125,  896383.2500,  893472.8750,  861906.9375],
        [1273948.2500,  918572.0625,  878084.6250,  870238.1875,  816110.1875,
          806453.0625,  758329.1250,  749419.9375,  669553.1250,  654144.0000],
        [1109123.6250, 1101985.7500, 1101245.1250, 1097059.5000, 1057525.1250,
         1056586.5000, 1048556.0000, 1044784.8750, 1035247.4375, 1030889.8750],
        [1046351.3750,  877701.1875,  861674.3125,  826734.1250,  806215.3750,
          799413.6250,  795305.8750,  747739.4375,  736188.2500,  713436.0000],
        [1206629.7500, 1112066.0000,  944403.1250,  909496.0625,  877113.7500,
          863526.8750,  815287.1875,  796836.3750,  786347.6875,  719764.3125],
        [1281867.3750, 1157226.1250, 1139156.8750, 1089310.2500, 1076640.0000,
         1061854.3750, 1060000.8750, 1036545.5625, 1036051.3750, 1019443.3750]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 443794.7812,       0.0000],
         [ 405705.0000,       0.0000],
         [ 330562.0000,       0.0000],
         ...,
         [ 232738.2969,       0.0000],
         [ 227469.4375,       0.0000],
         [ 212402.0781,       0.0000]],

        [[1467311.2500,       0.0000],
         [1453085.0000,       0.0000],
         [1412520.0000,       0.0000],
         ...,
         [1336807.5000,       0.0000],
         [1332176.3750,       0.0000],
         [1307779.6250,       0.0000]],

        [[1477728.3750,       0.0000],
         [1418300.0000,       0.0000],
         [1405701.3750,       0.0000],
         ...,
         [1253483.6250,       0.0000],
         [1229359.7500,       0.0000],
         [1229196.8750,       0.0000]],

        ...,

        [[1046351.3750,       0.0000],
         [ 877701.1875,       0.0000],
         [ 861674.3125,       0.0000],
         ...,
         [ 747739.4375,       0.0000],
         [ 736188.2500,       0.0000],
         [ 713436.0000,       0.0000]],

        [[1206629.7500,       0.0000],
         [1112066.0000,       0.0000],
         [ 944403.1250,       0.0000],
         ...,
         [ 796836.3750,       0.0000],
         [      0.0000,  786347.6875],
         [ 719764.3125,       0.0000]],

        [[1281867.3750,       0.0000],
         [      0.0000, 1157226.1250],
         [1139156.8750,       0.0000],
         ...,
         [1036545.5625,       0.0000],
         [1036051.3750,       0.0000],
         [1019443.3750,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2732293.5000,   234999.6250],
        [13782062.0000,        0.0000],
        [13316088.0000,        0.0000],
        [12385661.0000,  1361670.3750],
        [12393366.0000,  1434737.7500],
        [13552900.0000,  1481960.7500],
        [14111664.0000,  1584160.5000],
        [11844446.0000,  2828851.0000],
        [12544083.0000,  1356592.1250],
        [ 9350728.0000,        0.0000],
        [ 7684455.5000,  5358670.0000],
        [13628522.0000,        0.0000],
        [10603774.0000,  1285019.1250],
        [12940012.0000,  1536636.8750],
        [12808968.0000,        0.0000],
        [12226201.0000,        0.0000],
        [13895297.0000,        0.0000],
        [12500906.0000,  1351427.0000],
        [ 9496180.0000,  4111599.5000],
        [ 9331827.0000,  2327944.7500],
        [11825086.0000,  1290426.3750],
        [10310876.0000,  2686036.0000],
        [12150064.0000,        0.0000],
        [12559016.0000,        0.0000],
        [ 9755039.0000,  1054076.5000],
        [11282043.0000,        0.0000],
        [10037101.0000,  1362296.5000],
        [10801901.0000,        0.0000],
        [12825050.0000,        0.0000],
        [12179678.0000,        0.0000],
        [12905505.0000,        0.0000],
        [12818219.0000,        0.0000],
        [ 9284898.0000,  2454806.7500],
        [12962771.0000,        0.0000],
        [13413884.0000,        0.0000],
        [11800179.0000,  1308896.2500],
        [ 9041666.0000,  3866365.5000],
        [12639152.0000,        0.0000],
        [11342968.0000,  1260693.6250],
        [14052598.0000,        0.0000],
        [ 9744030.0000,        0.0000],
        [11640816.0000,        0.0000],
        [10974135.0000,        0.0000],
        [10088327.0000,        0.0000],
        [ 8666282.0000,  1186754.1250],
        [10385497.0000,        0.0000],
        [11597424.0000,        0.0000],
        [ 7200018.5000,  3117620.5000],
        [11451212.0000,  1221143.0000],
        [13005494.0000,  1435329.0000],
        [ 9683757.0000,  2643880.7500],
        [ 9683300.0000,  1086931.8750],
        [ 7830533.5000,  1682490.5000],
        [ 9858760.0000,   898697.7500],
        [10068588.0000,  1152348.5000],
        [12640404.0000,        0.0000],
        [ 6779131.5000,  3555665.5000],
        [ 9028766.0000,        0.0000],
        [ 7689080.5000,  2400886.7500],
        [ 7476280.0000,   918572.0625],
        [ 7428113.0000,  3254891.0000],
        [ 8210759.5000,        0.0000],
        [ 7368009.5000,  1663461.5000],
        [ 9800871.0000,  1157226.1250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 61/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:22, 60.78s/it]  7%|▋         | 2/30 [01:01<11:53, 25.47s/it] 10%|█         | 3/30 [01:02<06:22, 14.18s/it] 13%|█▎        | 4/30 [01:03<03:50,  8.88s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.95s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 4.741131416956583
Epoch 62/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:40, 61.39s/it]  7%|▋         | 2/30 [01:02<12:00, 25.72s/it] 10%|█         | 3/30 [01:02<06:26, 14.32s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:04<02:29,  6.00s/it] 20%|██        | 6/30 [01:05<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 4.620216019948324
Epoch 63/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:00, 60.03s/it]  7%|▋         | 2/30 [01:00<11:44, 25.16s/it] 10%|█         | 3/30 [01:01<06:18, 14.01s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 4.672048807144165
Epoch 64/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:06, 62.28s/it]  7%|▋         | 2/30 [01:03<12:10, 26.09s/it] 10%|█         | 3/30 [01:04<06:43, 14.95s/it] 13%|█▎        | 4/30 [01:05<04:02,  9.35s/it] 17%|█▋        | 5/30 [01:06<02:36,  6.25s/it] 20%|██        | 6/30 [01:06<01:45,  4.38s/it] 23%|██▎       | 7/30 [01:07<01:13,  3.19s/it] 27%|██▋       | 8/30 [01:08<00:53,  2.41s/it] 30%|███       | 9/30 [01:09<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.30s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  2.84s/it]
Epoch loss is 4.5861876010894775
Epoch 65/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:01, 62.13s/it]  7%|▋         | 2/30 [01:02<12:08, 26.03s/it] 10%|█         | 3/30 [01:03<06:31, 14.48s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 4.604157225290934
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0475,  0.0146,  0.0080,  ..., -0.0196, -0.0163, -0.0129],
        [-0.0628,  0.0283,  0.0190,  ..., -0.0104, -0.0054, -0.0439],
        [-0.0737, -0.0086, -0.0151,  ...,  0.0636,  0.0387, -0.0229],
        ...,
        [-0.0217, -0.0267, -0.0106,  ..., -0.0407, -0.0101,  0.0359],
        [-0.0881, -0.0026,  0.0365,  ..., -0.0292,  0.0394, -0.0114],
        [-0.0428,  0.0217,  0.0092,  ..., -0.0308,  0.0484, -0.0293]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9040, 0.9022, 0.8841, 0.8839, 0.8720, 0.8697, 0.8643, 0.8612, 0.8560,
         0.8500],
        [0.9943, 0.9941, 0.9914, 0.9910, 0.9893, 0.9886, 0.9879, 0.9877, 0.9876,
         0.9870],
        [0.9931, 0.9926, 0.9910, 0.9894, 0.9889, 0.9828, 0.9816, 0.9814, 0.9811,
         0.9792],
        [0.9923, 0.9884, 0.9877, 0.9876, 0.9874, 0.9869, 0.9861, 0.9860, 0.9854,
         0.9852],
        [0.9908, 0.9892, 0.9871, 0.9869, 0.9868, 0.9867, 0.9865, 0.9858, 0.9856,
         0.9843],
        [0.9959, 0.9958, 0.9957, 0.9956, 0.9946, 0.9945, 0.9938, 0.9936, 0.9933,
         0.9932],
        [0.9990, 0.9990, 0.9989, 0.9985, 0.9983, 0.9982, 0.9982, 0.9978, 0.9977,
         0.9975],
        [0.9949, 0.9945, 0.9945, 0.9943, 0.9938, 0.9927, 0.9926, 0.9922, 0.9910,
         0.9903],
        [0.9928, 0.9921, 0.9907, 0.9906, 0.9901, 0.9899, 0.9895, 0.9879, 0.9874,
         0.9872],
        [0.9798, 0.9783, 0.9711, 0.9654, 0.9627, 0.9597, 0.9490, 0.9418, 0.9403,
         0.9383],
        [0.9930, 0.9881, 0.9877, 0.9862, 0.9857, 0.9856, 0.9847, 0.9807, 0.9787,
         0.9772],
        [0.9947, 0.9946, 0.9881, 0.9864, 0.9861, 0.9855, 0.9844, 0.9840, 0.9837,
         0.9816],
        [0.9868, 0.9857, 0.9810, 0.9807, 0.9743, 0.9705, 0.9701, 0.9657, 0.9620,
         0.9612],
        [0.9967, 0.9954, 0.9944, 0.9926, 0.9922, 0.9918, 0.9917, 0.9917, 0.9912,
         0.9908],
        [0.9898, 0.9896, 0.9885, 0.9873, 0.9849, 0.9810, 0.9804, 0.9800, 0.9754,
         0.9753],
        [0.9931, 0.9869, 0.9840, 0.9823, 0.9797, 0.9776, 0.9764, 0.9733, 0.9722,
         0.9693],
        [0.9930, 0.9915, 0.9915, 0.9913, 0.9912, 0.9903, 0.9900, 0.9886, 0.9870,
         0.9865],
        [0.9959, 0.9920, 0.9918, 0.9912, 0.9894, 0.9893, 0.9891, 0.9874, 0.9872,
         0.9843],
        [0.9909, 0.9890, 0.9872, 0.9871, 0.9861, 0.9858, 0.9851, 0.9842, 0.9840,
         0.9840],
        [0.9848, 0.9812, 0.9800, 0.9800, 0.9785, 0.9779, 0.9772, 0.9765, 0.9727,
         0.9725],
        [0.9911, 0.9899, 0.9889, 0.9880, 0.9865, 0.9863, 0.9849, 0.9839, 0.9822,
         0.9791],
        [0.9939, 0.9894, 0.9892, 0.9833, 0.9820, 0.9794, 0.9793, 0.9776, 0.9771,
         0.9764],
        [0.9889, 0.9868, 0.9835, 0.9807, 0.9798, 0.9779, 0.9764, 0.9759, 0.9743,
         0.9735],
        [0.9866, 0.9860, 0.9844, 0.9836, 0.9825, 0.9824, 0.9817, 0.9812, 0.9772,
         0.9759],
        [0.9932, 0.9779, 0.9771, 0.9726, 0.9718, 0.9707, 0.9685, 0.9671, 0.9667,
         0.9579],
        [0.9835, 0.9824, 0.9779, 0.9776, 0.9765, 0.9746, 0.9744, 0.9707, 0.9705,
         0.9695],
        [0.9873, 0.9853, 0.9821, 0.9771, 0.9768, 0.9703, 0.9680, 0.9675, 0.9657,
         0.9634],
        [0.9904, 0.9812, 0.9806, 0.9741, 0.9731, 0.9687, 0.9677, 0.9643, 0.9643,
         0.9618],
        [0.9866, 0.9860, 0.9849, 0.9841, 0.9831, 0.9827, 0.9818, 0.9802, 0.9796,
         0.9792],
        [0.9881, 0.9847, 0.9840, 0.9826, 0.9825, 0.9820, 0.9819, 0.9790, 0.9766,
         0.9733],
        [0.9908, 0.9886, 0.9863, 0.9847, 0.9844, 0.9835, 0.9822, 0.9805, 0.9802,
         0.9799],
        [0.9918, 0.9874, 0.9863, 0.9845, 0.9844, 0.9821, 0.9808, 0.9805, 0.9765,
         0.9755],
        [0.9813, 0.9773, 0.9751, 0.9747, 0.9746, 0.9743, 0.9743, 0.9739, 0.9734,
         0.9732],
        [0.9904, 0.9865, 0.9852, 0.9847, 0.9847, 0.9843, 0.9811, 0.9811, 0.9809,
         0.9809],
        [0.9934, 0.9915, 0.9904, 0.9877, 0.9871, 0.9858, 0.9856, 0.9841, 0.9831,
         0.9828],
        [0.9906, 0.9867, 0.9853, 0.9845, 0.9837, 0.9835, 0.9830, 0.9824, 0.9809,
         0.9805],
        [0.9914, 0.9875, 0.9822, 0.9803, 0.9800, 0.9785, 0.9785, 0.9776, 0.9767,
         0.9764],
        [0.9900, 0.9879, 0.9875, 0.9873, 0.9858, 0.9850, 0.9824, 0.9823, 0.9812,
         0.9803],
        [0.9930, 0.9869, 0.9834, 0.9819, 0.9811, 0.9810, 0.9808, 0.9792, 0.9781,
         0.9776],
        [0.9956, 0.9937, 0.9932, 0.9918, 0.9914, 0.9903, 0.9885, 0.9881, 0.9874,
         0.9874],
        [0.9803, 0.9728, 0.9719, 0.9609, 0.9584, 0.9573, 0.9569, 0.9529, 0.9528,
         0.9524],
        [0.9846, 0.9817, 0.9789, 0.9763, 0.9754, 0.9749, 0.9738, 0.9719, 0.9707,
         0.9706],
        [0.9915, 0.9810, 0.9802, 0.9775, 0.9703, 0.9660, 0.9657, 0.9648, 0.9645,
         0.9508],
        [0.9726, 0.9682, 0.9677, 0.9671, 0.9668, 0.9661, 0.9661, 0.9617, 0.9614,
         0.9607],
        [0.9790, 0.9716, 0.9704, 0.9580, 0.9576, 0.9565, 0.9539, 0.9535, 0.9524,
         0.9515],
        [0.9774, 0.9758, 0.9667, 0.9663, 0.9654, 0.9624, 0.9615, 0.9491, 0.9464,
         0.9458],
        [0.9842, 0.9819, 0.9768, 0.9750, 0.9745, 0.9726, 0.9695, 0.9690, 0.9683,
         0.9665],
        [0.9780, 0.9732, 0.9667, 0.9637, 0.9622, 0.9618, 0.9604, 0.9568, 0.9538,
         0.9530],
        [0.9860, 0.9845, 0.9818, 0.9798, 0.9785, 0.9785, 0.9774, 0.9772, 0.9770,
         0.9737],
        [0.9955, 0.9937, 0.9926, 0.9918, 0.9918, 0.9915, 0.9911, 0.9908, 0.9906,
         0.9895],
        [0.9910, 0.9870, 0.9793, 0.9785, 0.9775, 0.9763, 0.9756, 0.9751, 0.9716,
         0.9713],
        [0.9835, 0.9825, 0.9782, 0.9780, 0.9739, 0.9710, 0.9652, 0.9626, 0.9547,
         0.9529],
        [0.9894, 0.9766, 0.9669, 0.9637, 0.9566, 0.9538, 0.9531, 0.9516, 0.9489,
         0.9431],
        [0.9902, 0.9782, 0.9778, 0.9751, 0.9714, 0.9681, 0.9648, 0.9612, 0.9563,
         0.9550],
        [0.9859, 0.9748, 0.9744, 0.9737, 0.9726, 0.9723, 0.9692, 0.9674, 0.9672,
         0.9662],
        [0.9872, 0.9869, 0.9855, 0.9847, 0.9835, 0.9827, 0.9818, 0.9809, 0.9793,
         0.9788],
        [0.9821, 0.9765, 0.9756, 0.9668, 0.9648, 0.9628, 0.9597, 0.9575, 0.9566,
         0.9563],
        [0.9698, 0.9666, 0.9649, 0.9545, 0.9544, 0.9537, 0.9526, 0.9521, 0.9500,
         0.9494],
        [0.9889, 0.9697, 0.9681, 0.9678, 0.9631, 0.9631, 0.9594, 0.9589, 0.9578,
         0.9570],
        [0.9818, 0.9628, 0.9553, 0.9552, 0.9531, 0.9517, 0.9479, 0.9426, 0.9372,
         0.9358],
        [0.9739, 0.9737, 0.9717, 0.9670, 0.9669, 0.9660, 0.9659, 0.9653, 0.9652,
         0.9652],
        [0.9645, 0.9593, 0.9553, 0.9513, 0.9508, 0.9508, 0.9467, 0.9431, 0.9411,
         0.9386],
        [0.9785, 0.9740, 0.9627, 0.9618, 0.9582, 0.9560, 0.9553, 0.9532, 0.9466,
         0.9445],
        [0.9807, 0.9729, 0.9702, 0.9686, 0.9668, 0.9656, 0.9650, 0.9647, 0.9644,
         0.9629]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 406078.5625,  395888.6875,  305414.4688,  304920.8750,  257139.3906,
          248712.0938,  230138.7031,  220332.4219,  204484.7344,  187853.2656],
        [1474711.2500, 1471637.2500, 1415343.6250, 1406622.6250, 1373387.0000,
         1359895.1250, 1346665.3750, 1341644.2500, 1340968.8750, 1329773.5000],
        [1450832.1250, 1439669.3750, 1407951.2500, 1375788.6250, 1365782.7500,
         1251307.5000, 1229886.2500, 1227268.8750, 1222316.2500, 1188102.7500],
        [1433524.7500, 1356422.6250, 1342864.1250, 1339992.2500, 1336907.0000,
         1326715.6250, 1311266.5000, 1310807.6250, 1298191.6250, 1296088.6250],
        [1402983.8750, 1371057.6250, 1330034.7500, 1327763.6250, 1325954.2500,
         1323898.3750, 1320224.3750, 1307036.5000, 1302920.8750, 1278827.0000],
        [1510059.0000, 1506182.8750, 1504712.7500, 1501811.0000, 1481524.1250,
         1479912.8750, 1465339.5000, 1461271.5000, 1453875.1250, 1452771.8750],
        [1578552.0000, 1577638.3750, 1576412.6250, 1565729.8750, 1562148.8750,
         1560758.0000, 1560278.7500, 1549863.3750, 1548138.1250, 1544620.7500],
        [1488649.5000, 1479081.8750, 1478672.8750, 1474177.0000, 1463645.2500,
         1441173.5000, 1439599.2500, 1432087.1250, 1406594.3750, 1392558.3750],
        [1444316.1250, 1430359.2500, 1401665.2500, 1399345.2500, 1388430.2500,
         1385084.3750, 1377777.8750, 1346887.5000, 1336194.5000, 1332350.3750],
        [1199773.6250, 1173298.2500, 1059161.1250,  975829.5625,  939893.5625,
          900103.6250,  772851.5625,  696965.8125,  682525.1250,  662480.0000],
        [1448694.6250, 1349376.7500, 1342382.7500, 1314219.7500, 1305349.8750,
         1301944.6250, 1286934.7500, 1215215.8750, 1180603.1250, 1155187.2500],
        [1483935.0000, 1480488.8750, 1350772.3750, 1317358.1250, 1312301.1250,
         1300838.8750, 1280064.3750, 1272919.6250, 1267927.8750, 1230644.2500],
        [1326063.0000, 1304550.8750, 1219931.2500, 1213907.0000, 1109317.2500,
         1049469.5000, 1044165.3125,  980328.3125,  930557.1875,  919508.8750],
        [1527714.7500, 1497727.6250, 1477194.3750, 1440500.1250, 1432089.8750,
         1423389.3750, 1421502.2500, 1421484.7500, 1410737.5000, 1404062.6250],
        [1383931.7500, 1379585.7500, 1358238.7500, 1334328.8750, 1290086.6250,
         1219402.0000, 1209995.6250, 1202627.2500, 1126399.6250, 1124123.5000],
        [1449175.5000, 1327582.6250, 1272946.3750, 1243037.6250, 1197101.5000,
         1161551.5000, 1142279.0000, 1093009.7500, 1076032.3750, 1031916.8125],
        [1447732.0000, 1417191.1250, 1416402.1250, 1413209.7500, 1411033.5000,
         1393177.3750, 1386637.3750, 1359587.7500, 1329333.5000, 1318831.2500],
        [1508308.8750, 1427237.5000, 1422550.6250, 1411263.6250, 1375321.6250,
         1373226.0000, 1370051.2500, 1335776.5000, 1333580.8750, 1278263.6250],
        [1405870.2500, 1367861.8750, 1333354.6250, 1331914.6250, 1312913.1250,
         1307221.0000, 1293786.6250, 1277648.2500, 1273440.6250, 1272897.7500],
        [1287376.6250, 1223994.8750, 1203136.6250, 1202358.8750, 1177727.6250,
         1166651.6250, 1155349.2500, 1143736.3750, 1083586.8750, 1080994.0000],
        [1408732.8750, 1385651.2500, 1364846.6250, 1347371.8750, 1319023.7500,
         1315713.3750, 1288961.5000, 1270892.7500, 1240996.7500, 1186394.1250],
        [1466306.8750, 1374609.6250, 1370971.5000, 1260549.2500, 1237465.5000,
         1191865.8750, 1190281.3750, 1162655.3750, 1153191.6250, 1142803.1250],
        [1365648.6250, 1325779.6250, 1264593.7500, 1213890.7500, 1199631.7500,
         1166353.5000, 1141675.6250, 1133741.1250, 1109112.0000, 1095751.5000],
        [1321281.1250, 1309483.1250, 1280525.7500, 1265626.5000, 1245807.5000,
         1244863.3750, 1231830.1250, 1224088.2500, 1154850.2500, 1133550.7500],
        [1451431.3750, 1166963.2500, 1154442.7500, 1081455.0000, 1069764.2500,
         1052812.6250, 1020951.4375, 1000042.1250,  994967.0000,  877103.7500],
        [1265110.1250, 1245071.2500, 1167345.0000, 1162029.1250, 1144397.5000,
         1112527.3750, 1109951.1250, 1052429.1250, 1049842.8750, 1034815.0625],
        [1335697.6250, 1297988.6250, 1238723.0000, 1154187.5000, 1149031.1250,
         1046264.6250, 1013545.5625, 1006467.8125,  980939.0625,  949019.4375],
        [1394952.2500, 1223696.1250, 1212154.3750, 1105860.0000, 1089915.1250,
         1023029.3750, 1008458.5625,  960946.3125,  960512.0625,  927705.7500],
        [1321739.8750, 1309604.2500, 1290106.3750, 1274292.1250, 1256472.1250,
         1249149.5000, 1233384.2500, 1205864.7500, 1196432.6250, 1189238.6250],
        [1350097.5000, 1286594.8750, 1273300.8750, 1248879.0000, 1246833.3750,
         1237209.5000, 1235644.7500, 1185901.0000, 1145152.0000, 1092522.0000],
        [1404054.6250, 1358898.2500, 1315902.8750, 1286840.3750, 1281164.7500,
         1264234.3750, 1241185.0000, 1210762.1250, 1206732.2500, 1200364.2500],
        [1422811.1250, 1337292.1250, 1315072.2500, 1281795.2500, 1280571.0000,
         1238597.7500, 1216883.5000, 1210932.0000, 1144171.6250, 1128073.5000],
        [1225847.6250, 1156881.8750, 1120889.5000, 1114711.0000, 1113137.6250,
         1108108.7500, 1107857.2500, 1102853.1250, 1095142.5000, 1090787.5000],
        [1395869.1250, 1320356.6250, 1294706.1250, 1285654.1250, 1285548.6250,
         1279007.6250, 1221839.6250, 1221579.8750, 1218676.6250, 1217613.6250],
        [1456962.0000, 1416648.0000, 1395190.3750, 1342905.2500, 1330661.5000,
         1306455.7500, 1303456.5000, 1275402.1250, 1257917.0000, 1250804.0000],
        [1400167.6250, 1324222.8750, 1296551.0000, 1281893.0000, 1268079.1250,
         1264512.8750, 1256133.1250, 1245273.1250, 1217546.3750, 1211076.2500],
        [1415336.7500, 1339359.7500, 1241233.5000, 1207650.8750, 1202989.7500,
         1177025.7500, 1176737.3750, 1162734.1250, 1147767.2500, 1142809.6250],
        [1387024.7500, 1346474.0000, 1339509.2500, 1335544.7500, 1305890.2500,
         1291365.6250, 1244540.6250, 1242143.0000, 1223084.7500, 1207070.6250],
        [1447766.5000, 1326834.6250, 1262069.6250, 1234872.0000, 1222389.7500,
         1219496.2500, 1216620.1250, 1188964.2500, 1169843.6250, 1162437.0000],
        [1502440.0000, 1462087.0000, 1451871.6250, 1423128.7500, 1415862.0000,
         1393023.2500, 1357161.3750, 1349783.3750, 1337394.1250, 1335748.5000],
        [1207215.6250, 1085008.6250, 1071744.2500,  915773.8750,  883595.1875,
          869902.1250,  864364.0000,  816519.6875,  815613.8125,  811151.1875],
        [1283526.1250, 1232502.3750, 1183097.2500, 1139904.3750, 1125360.2500,
         1118095.6250, 1100002.3750, 1070502.1250, 1052478.2500, 1051700.7500],
        [1417077.7500, 1219357.8750, 1206117.7500, 1161091.8750, 1047227.8750,
          984040.4375,  980589.1875,  967443.6875,  963367.8750,  791994.5000],
        [1081973.8750, 1015394.4375, 1008404.7500,  999851.4375,  995787.1250,
          986244.5000,  985953.8750,  926141.1250,  921531.5625,  912730.9375],
        [1185366.1250, 1066310.3750, 1048861.0000,  878565.3750,  872803.1250,
          859694.5000,  828235.0625,  823781.1875,  811260.3125,  800387.8125],
        [1158353.5000, 1131850.5000,  994280.2500,  988853.3125,  975891.9375,
          935501.8750,  923567.3750,  773338.8750,  743684.3750,  737866.0625],
        [1276358.5000, 1236025.3750, 1148231.5000, 1119331.0000, 1111771.1250,
         1081678.8750, 1034723.3750, 1028442.8125, 1017313.5000,  991122.0000],
        [1169183.2500, 1090993.5000,  994023.3125,  952538.4375,  932231.6250,
          927917.2500,  909354.6875,  862962.9375,  827530.8125,  818155.0000],
        [1310221.5000, 1282115.6250, 1233392.5000, 1199620.3750, 1177215.5000,
         1176874.2500, 1158961.1250, 1156035.8750, 1152476.0000, 1099274.5000],
        [1499764.3750, 1461781.7500, 1439615.7500, 1423671.6250, 1422919.6250,
         1416552.0000, 1410016.5000, 1403558.0000, 1398379.3750, 1378183.8750],
        [1407872.0000, 1329389.2500, 1190658.3750, 1176824.8750, 1160116.7500,
         1141429.6250, 1129705.6250, 1121094.7500, 1065874.2500, 1062342.6250],
        [1264575.7500, 1247146.1250, 1171792.0000, 1168796.5000, 1102141.2500,
         1056964.5000,  973728.6250,  937709.8750,  837382.9375,  816059.6250],
        [1374791.8750, 1146180.0000,  997691.1250,  952727.4375,  860912.8750,
          826623.0000,  818993.4375,  801979.3750,  771617.2500,  710011.7500],
        [1391292.0000, 1172207.7500, 1164769.5000, 1122013.5000, 1062999.3750,
         1014197.2500,  967879.3125,  919259.0625,  857035.6875,  841112.6250],
        [1307845.7500, 1116407.8750, 1109410.2500, 1098860.5000, 1081250.7500,
         1076936.7500, 1030017.1875, 1004908.3750, 1000964.8125,  988105.6875],
        [1333249.0000, 1327201.6250, 1300009.1250, 1286857.5000, 1264941.1250,
         1250292.3750, 1233363.0000, 1217705.3750, 1190241.6250, 1181711.5000],
        [1238558.8750, 1144549.3750, 1130069.7500,  996296.2500,  968093.4375,
          940250.3750,  900307.9375,  871592.0000,  861094.3125,  856652.4375],
        [1040031.0000,  993251.0000,  969827.0000,  835378.4375,  834713.5000,
          826413.3125,  813579.2500,  807093.1250,  783163.2500,  776661.1250],
        [1365481.8750, 1037780.0000, 1014961.6250, 1010905.3750,  944874.2500,
          944740.9375,  896097.7500,  889859.7500,  876148.1250,  865754.8750],
        [1234588.1250,  940572.3125,  844710.6875,  843776.7500,  818368.0000,
          802407.0000,  759880.4375,  704492.4375,  652045.0000,  639922.5625],
        [1102355.7500, 1099523.1250, 1068109.7500,  998717.3125,  997337.2500,
          984076.1250,  982735.8750,  974552.7500,  973584.6875,  973210.6250],
        [ 963800.6875,  894192.3125,  844769.5000,  798599.8750,  792668.5000,
          792471.9375,  746934.8125,  710145.1875,  690380.2500,  665884.6250],
        [1176776.6250, 1104031.7500,  938772.0000,  927146.8125,  881367.5000,
          853425.8125,  844533.5000,  819731.8750,  746332.3750,  724675.8750],
        [1214264.7500, 1086114.3750, 1045965.2500, 1021775.4375,  995440.5625,
          979472.3125,  971255.1875,  967101.4375,  961834.8125,  941908.8750]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 406078.5625,       0.0000],
         [ 395888.6875,       0.0000],
         [ 305414.4688,       0.0000],
         ...,
         [ 220332.4219,       0.0000],
         [      0.0000,  204484.7344],
         [ 187853.2656,       0.0000]],

        [[1474711.2500,       0.0000],
         [1471637.2500,       0.0000],
         [1415343.6250,       0.0000],
         ...,
         [1341644.2500,       0.0000],
         [1340968.8750,       0.0000],
         [1329773.5000,       0.0000]],

        [[1450832.1250,       0.0000],
         [1439669.3750,       0.0000],
         [1407951.2500,       0.0000],
         ...,
         [1227268.8750,       0.0000],
         [1222316.2500,       0.0000],
         [1188102.7500,       0.0000]],

        ...,

        [[ 963800.6875,       0.0000],
         [ 894192.3125,       0.0000],
         [ 844769.5000,       0.0000],
         ...,
         [ 710145.1875,       0.0000],
         [ 690380.2500,       0.0000],
         [ 665884.6250,       0.0000]],

        [[1176776.6250,       0.0000],
         [1104031.7500,       0.0000],
         [ 938772.0000,       0.0000],
         ...,
         [ 819731.8750,       0.0000],
         [      0.0000,  746332.3750],
         [ 724675.8750,       0.0000]],

        [[1214264.7500,       0.0000],
         [1086114.3750,       0.0000],
         [      0.0000, 1045965.2500],
         ...,
         [ 967101.4375,       0.0000],
         [ 961834.8125,       0.0000],
         [ 941908.8750,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2307766.2500,   453196.8125],
        [13860649.0000,        0.0000],
        [13158906.0000,        0.0000],
        [12041515.0000,  1311266.5000],
        [11887717.0000,  1402983.8750],
        [13363586.0000,  1453875.1250],
        [14045589.0000,  1578552.0000],
        [13089645.0000,  1406594.3750],
        [13842411.0000,        0.0000],
        [ 9062882.0000,        0.0000],
        [ 7614009.0000,  5285900.0000],
        [13297251.0000,        0.0000],
        [ 9883892.0000,  1213907.0000],
        [12928689.0000,  1527714.7500],
        [12628720.0000,        0.0000],
        [11994634.0000,        0.0000],
        [12574304.0000,  1318831.2500],
        [12465529.0000,  1370051.2500],
        [ 9169911.0000,  4006997.5000],
        [ 9477267.0000,  2247645.5000],
        [11857692.0000,  1270892.7500],
        [ 9915541.0000,  2635159.0000],
        [12016178.0000,        0.0000],
        [12411907.0000,        0.0000],
        [ 9788478.0000,  1081455.0000],
        [11343518.0000,        0.0000],
        [ 9836167.0000,  1335697.6250],
        [10907230.0000,        0.0000],
        [12526285.0000,        0.0000],
        [12302135.0000,        0.0000],
        [12770138.0000,        0.0000],
        [12576200.0000,        0.0000],
        [ 8902260.0000,  2333956.5000],
        [12740852.0000,        0.0000],
        [13336402.0000,        0.0000],
        [11509322.0000,  1256133.1250],
        [ 8534813.0000,  3678831.0000],
        [12922648.0000,        0.0000],
        [11189224.0000,  1262069.6250],
        [14028500.0000,        0.0000],
        [ 9340888.0000,        0.0000],
        [11357170.0000,        0.0000],
        [10738308.0000,        0.0000],
        [ 9834014.0000,        0.0000],
        [ 7230389.0000,  1944875.7500],
        [ 9363188.0000,        0.0000],
        [11044998.0000,        0.0000],
        [ 6720174.0000,  2764716.7500],
        [10768972.0000,  1177215.5000],
        [12837890.0000,  1416552.0000],
        [ 8181856.0000,  3603452.0000],
        [ 9519333.0000,  1056964.5000],
        [ 7628998.0000,  1632530.1250],
        [ 9593508.0000,   919259.0625],
        [ 9715847.0000,  1098860.5000],
        [12585572.0000,        0.0000],
        [ 5537634.0000,  4369830.0000],
        [ 7853697.5000,   826413.3125],
        [ 7443342.5000,  2403262.0000],
        [ 7300191.0000,   940572.3125],
        [ 6986400.5000,  3167802.7500],
        [ 7899848.0000,        0.0000],
        [ 7389094.0000,  1627699.8750],
        [ 9139167.0000,  1045965.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 66/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:31, 56.93s/it]  7%|▋         | 2/30 [01:01<12:10, 26.07s/it] 10%|█         | 3/30 [01:02<06:31, 14.51s/it] 13%|█▎        | 4/30 [01:02<03:56,  9.08s/it] 17%|█▋        | 5/30 [01:03<02:31,  6.07s/it] 20%|██        | 6/30 [01:04<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.36s/it] 30%|███       | 9/30 [01:06<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 4.589981683095297
Epoch 67/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:15, 58.48s/it]  7%|▋         | 2/30 [01:01<11:57, 25.64s/it] 10%|█         | 3/30 [01:01<06:25, 14.27s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.94s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.98s/it] 20%|██        | 6/30 [01:04<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 4.552859528859456
Epoch 68/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:05, 60.18s/it]  7%|▋         | 2/30 [01:00<11:46, 25.22s/it] 10%|█         | 3/30 [01:01<06:19, 14.05s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.80s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 4.611074813206991
Epoch 69/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:19, 60.67s/it]  7%|▋         | 2/30 [01:01<11:51, 25.42s/it] 10%|█         | 3/30 [01:02<06:22, 14.16s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.86s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 4.564630158742269
Epoch 70/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:59, 57.91s/it]  7%|▋         | 2/30 [00:59<11:35, 24.85s/it] 10%|█         | 3/30 [01:00<06:13, 13.85s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.68s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.82s/it] 20%|██        | 6/30 [01:02<01:38,  4.09s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 4.477224715550741
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0393,  0.0175,  0.0016,  ..., -0.0283, -0.0204, -0.0103],
        [-0.0646,  0.0313,  0.0179,  ..., -0.0106, -0.0101, -0.0392],
        [-0.0750, -0.0078, -0.0206,  ...,  0.0568,  0.0320, -0.0222],
        ...,
        [-0.0177, -0.0265, -0.0118,  ..., -0.0463, -0.0088,  0.0268],
        [-0.0861,  0.0024,  0.0340,  ..., -0.0359,  0.0382, -0.0186],
        [-0.0439,  0.0247,  0.0101,  ..., -0.0335,  0.0483, -0.0357]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8954, 0.8925, 0.8772, 0.8770, 0.8731, 0.8685, 0.8545, 0.8477, 0.8446,
         0.8404],
        [0.9945, 0.9937, 0.9913, 0.9906, 0.9893, 0.9891, 0.9881, 0.9881, 0.9880,
         0.9878],
        [0.9931, 0.9926, 0.9923, 0.9903, 0.9868, 0.9840, 0.9827, 0.9808, 0.9798,
         0.9791],
        [0.9924, 0.9855, 0.9848, 0.9847, 0.9844, 0.9842, 0.9841, 0.9838, 0.9837,
         0.9835],
        [0.9890, 0.9876, 0.9843, 0.9840, 0.9829, 0.9829, 0.9827, 0.9825, 0.9813,
         0.9804],
        [0.9948, 0.9946, 0.9945, 0.9943, 0.9935, 0.9933, 0.9932, 0.9918, 0.9917,
         0.9914],
        [0.9987, 0.9986, 0.9985, 0.9982, 0.9979, 0.9979, 0.9978, 0.9971, 0.9971,
         0.9971],
        [0.9942, 0.9935, 0.9932, 0.9929, 0.9928, 0.9918, 0.9914, 0.9906, 0.9896,
         0.9894],
        [0.9926, 0.9918, 0.9912, 0.9911, 0.9901, 0.9899, 0.9896, 0.9882, 0.9880,
         0.9880],
        [0.9776, 0.9767, 0.9671, 0.9604, 0.9569, 0.9554, 0.9481, 0.9414, 0.9404,
         0.9393],
        [0.9915, 0.9875, 0.9863, 0.9858, 0.9856, 0.9851, 0.9831, 0.9786, 0.9774,
         0.9774],
        [0.9930, 0.9928, 0.9863, 0.9847, 0.9827, 0.9818, 0.9813, 0.9800, 0.9796,
         0.9783],
        [0.9852, 0.9850, 0.9788, 0.9763, 0.9692, 0.9673, 0.9629, 0.9608, 0.9555,
         0.9512],
        [0.9960, 0.9946, 0.9933, 0.9923, 0.9922, 0.9919, 0.9914, 0.9912, 0.9911,
         0.9910],
        [0.9886, 0.9878, 0.9872, 0.9863, 0.9812, 0.9799, 0.9782, 0.9778, 0.9768,
         0.9763],
        [0.9924, 0.9863, 0.9827, 0.9818, 0.9783, 0.9756, 0.9746, 0.9687, 0.9684,
         0.9680],
        [0.9929, 0.9916, 0.9912, 0.9911, 0.9911, 0.9902, 0.9901, 0.9876, 0.9870,
         0.9867],
        [0.9947, 0.9930, 0.9919, 0.9907, 0.9898, 0.9890, 0.9882, 0.9866, 0.9865,
         0.9830],
        [0.9904, 0.9861, 0.9853, 0.9840, 0.9832, 0.9823, 0.9819, 0.9812, 0.9811,
         0.9799],
        [0.9874, 0.9820, 0.9818, 0.9805, 0.9794, 0.9784, 0.9781, 0.9755, 0.9742,
         0.9740],
        [0.9908, 0.9902, 0.9885, 0.9882, 0.9881, 0.9858, 0.9852, 0.9840, 0.9829,
         0.9804],
        [0.9934, 0.9873, 0.9870, 0.9791, 0.9786, 0.9761, 0.9751, 0.9737, 0.9735,
         0.9732],
        [0.9883, 0.9869, 0.9816, 0.9784, 0.9782, 0.9760, 0.9748, 0.9742, 0.9740,
         0.9736],
        [0.9857, 0.9837, 0.9836, 0.9827, 0.9811, 0.9808, 0.9797, 0.9797, 0.9787,
         0.9776],
        [0.9928, 0.9773, 0.9773, 0.9731, 0.9721, 0.9718, 0.9695, 0.9684, 0.9674,
         0.9592],
        [0.9836, 0.9818, 0.9788, 0.9771, 0.9761, 0.9756, 0.9751, 0.9711, 0.9703,
         0.9701],
        [0.9864, 0.9849, 0.9818, 0.9779, 0.9738, 0.9705, 0.9688, 0.9648, 0.9642,
         0.9630],
        [0.9900, 0.9792, 0.9786, 0.9769, 0.9733, 0.9682, 0.9660, 0.9660, 0.9654,
         0.9638],
        [0.9857, 0.9844, 0.9835, 0.9816, 0.9814, 0.9806, 0.9802, 0.9792, 0.9787,
         0.9770],
        [0.9875, 0.9841, 0.9836, 0.9826, 0.9807, 0.9804, 0.9791, 0.9790, 0.9770,
         0.9733],
        [0.9890, 0.9859, 0.9849, 0.9840, 0.9839, 0.9823, 0.9817, 0.9814, 0.9811,
         0.9809],
        [0.9905, 0.9879, 0.9837, 0.9828, 0.9819, 0.9813, 0.9808, 0.9798, 0.9737,
         0.9725],
        [0.9780, 0.9767, 0.9747, 0.9742, 0.9723, 0.9722, 0.9718, 0.9717, 0.9710,
         0.9706],
        [0.9886, 0.9858, 0.9849, 0.9836, 0.9832, 0.9829, 0.9810, 0.9810, 0.9803,
         0.9802],
        [0.9933, 0.9902, 0.9901, 0.9872, 0.9869, 0.9862, 0.9858, 0.9840, 0.9830,
         0.9823],
        [0.9910, 0.9848, 0.9840, 0.9832, 0.9829, 0.9824, 0.9818, 0.9818, 0.9806,
         0.9792],
        [0.9917, 0.9850, 0.9848, 0.9817, 0.9810, 0.9761, 0.9752, 0.9748, 0.9745,
         0.9742],
        [0.9897, 0.9894, 0.9890, 0.9883, 0.9864, 0.9857, 0.9838, 0.9838, 0.9826,
         0.9826],
        [0.9915, 0.9867, 0.9819, 0.9804, 0.9802, 0.9797, 0.9785, 0.9763, 0.9757,
         0.9743],
        [0.9959, 0.9938, 0.9936, 0.9917, 0.9916, 0.9893, 0.9891, 0.9877, 0.9876,
         0.9874],
        [0.9783, 0.9709, 0.9702, 0.9622, 0.9529, 0.9521, 0.9513, 0.9509, 0.9479,
         0.9473],
        [0.9827, 0.9804, 0.9765, 0.9748, 0.9737, 0.9732, 0.9724, 0.9687, 0.9674,
         0.9654],
        [0.9901, 0.9791, 0.9784, 0.9767, 0.9681, 0.9649, 0.9641, 0.9638, 0.9633,
         0.9511],
        [0.9690, 0.9660, 0.9655, 0.9642, 0.9630, 0.9627, 0.9627, 0.9621, 0.9607,
         0.9603],
        [0.9751, 0.9671, 0.9628, 0.9581, 0.9546, 0.9534, 0.9508, 0.9498, 0.9486,
         0.9430],
        [0.9684, 0.9683, 0.9572, 0.9551, 0.9545, 0.9542, 0.9455, 0.9430, 0.9399,
         0.9391],
        [0.9842, 0.9818, 0.9758, 0.9752, 0.9725, 0.9688, 0.9667, 0.9608, 0.9601,
         0.9601],
        [0.9753, 0.9702, 0.9615, 0.9597, 0.9549, 0.9532, 0.9510, 0.9494, 0.9446,
         0.9444],
        [0.9821, 0.9809, 0.9764, 0.9758, 0.9748, 0.9721, 0.9711, 0.9695, 0.9691,
         0.9687],
        [0.9947, 0.9928, 0.9925, 0.9915, 0.9901, 0.9897, 0.9896, 0.9886, 0.9884,
         0.9876],
        [0.9898, 0.9848, 0.9754, 0.9750, 0.9744, 0.9735, 0.9707, 0.9697, 0.9695,
         0.9680],
        [0.9827, 0.9806, 0.9773, 0.9729, 0.9728, 0.9680, 0.9636, 0.9589, 0.9512,
         0.9511],
        [0.9890, 0.9738, 0.9646, 0.9605, 0.9532, 0.9492, 0.9484, 0.9476, 0.9442,
         0.9429],
        [0.9891, 0.9767, 0.9757, 0.9684, 0.9672, 0.9635, 0.9626, 0.9610, 0.9523,
         0.9469],
        [0.9849, 0.9715, 0.9710, 0.9701, 0.9697, 0.9693, 0.9675, 0.9642, 0.9637,
         0.9635],
        [0.9853, 0.9844, 0.9843, 0.9835, 0.9821, 0.9812, 0.9810, 0.9789, 0.9785,
         0.9774],
        [0.9788, 0.9755, 0.9745, 0.9624, 0.9586, 0.9585, 0.9583, 0.9564, 0.9556,
         0.9530],
        [0.9688, 0.9613, 0.9569, 0.9561, 0.9518, 0.9514, 0.9512, 0.9487, 0.9486,
         0.9456],
        [0.9871, 0.9716, 0.9647, 0.9627, 0.9619, 0.9579, 0.9571, 0.9562, 0.9549,
         0.9521],
        [0.9777, 0.9641, 0.9529, 0.9526, 0.9522, 0.9514, 0.9474, 0.9390, 0.9335,
         0.9332],
        [0.9734, 0.9720, 0.9677, 0.9646, 0.9636, 0.9635, 0.9631, 0.9627, 0.9616,
         0.9614],
        [0.9629, 0.9582, 0.9565, 0.9485, 0.9462, 0.9460, 0.9452, 0.9407, 0.9381,
         0.9364],
        [0.9767, 0.9746, 0.9637, 0.9623, 0.9598, 0.9562, 0.9526, 0.9489, 0.9454,
         0.9418],
        [0.9768, 0.9707, 0.9645, 0.9623, 0.9617, 0.9611, 0.9607, 0.9597, 0.9596,
         0.9577]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 358900.0000,  344476.3125,  276912.2500,  276276.2812,  261317.3281,
          244374.9062,  200214.7188,  181689.9375,  173906.9375,  163586.2188],
        [1480339.1250, 1462159.5000, 1413062.8750, 1398672.7500, 1374127.2500,
         1369279.2500, 1350875.3750, 1349518.2500, 1348854.3750, 1345180.3750],
        [1449989.7500, 1438873.2500, 1433676.5000, 1392404.3750, 1326106.0000,
         1273845.0000, 1250560.6250, 1217158.5000, 1198475.7500, 1188083.5000],
        [1435871.2500, 1300489.0000, 1288114.7500, 1285214.1250, 1279718.7500,
         1277253.5000, 1274380.8750, 1270299.0000, 1268365.7500, 1265102.7500],
        [1368169.7500, 1341030.2500, 1278178.3750, 1273622.7500, 1253490.8750,
         1252895.6250, 1249653.5000, 1246649.1250, 1225824.2500, 1210066.1250],
        [1486128.7500, 1481846.2500, 1479667.3750, 1475748.1250, 1457787.6250,
         1454660.0000, 1453105.7500, 1423286.2500, 1421122.8750, 1414739.0000],
        [1571155.0000, 1568587.5000, 1567476.3750, 1559851.7500, 1553746.7500,
         1552923.1250, 1551910.5000, 1535560.1250, 1534478.2500, 1534332.0000],
        [1472877.1250, 1458759.6250, 1452674.8750, 1446647.1250, 1444200.3750,
         1424228.3750, 1414535.2500, 1399593.5000, 1380135.8750, 1376017.0000],
        [1438800.5000, 1422697.1250, 1411878.8750, 1409700.6250, 1389185.2500,
         1384750.1250, 1380064.7500, 1352653.1250, 1347828.2500, 1347500.5000],
        [1161795.2500, 1146891.8750, 1000835.9375,  908539.0000,  864345.8125,
          846279.8125,  762968.8125,  693027.5000,  683197.8750,  672662.3125],
        [1418172.7500, 1337944.0000, 1315189.0000, 1306636.3750, 1302441.3750,
         1293331.5000, 1256414.6250, 1177956.7500, 1159185.5000, 1158340.1250],
        [1448085.5000, 1442870.5000, 1315187.6250, 1285868.7500, 1250168.3750,
         1234570.5000, 1225256.2500, 1202658.1250, 1195774.5000, 1174147.8750],
        [1294434.6250, 1292569.3750, 1182690.1250, 1141448.1250, 1031349.0625,
         1003549.3750,  942215.2500,  914440.4375,  847632.7500,  797446.1250],
        [1511081.8750, 1480702.0000, 1455210.8750, 1433221.2500, 1432329.0000,
         1425708.3750, 1414443.5000, 1411211.1250, 1409868.6250, 1406860.1250],
        [1360504.8750, 1344257.0000, 1332267.7500, 1315502.5000, 1222821.1250,
         1200379.1250, 1172557.7500, 1164880.6250, 1149505.6250, 1141455.7500],
        [1435259.2500, 1316168.8750, 1249411.5000, 1233724.2500, 1174389.7500,
         1130047.1250, 1113753.5000, 1023206.0000, 1018593.0625, 1013512.6250],
        [1446437.5000, 1419536.7500, 1411328.2500, 1409126.6250, 1408653.6250,
         1391351.6250, 1388429.0000, 1340991.8750, 1329403.2500, 1324157.2500],
        [1483311.1250, 1448296.8750, 1426377.5000, 1401697.3750, 1383459.2500,
         1367791.3750, 1351798.2500, 1321797.8750, 1318925.6250, 1255138.0000],
        [1395507.1250, 1311526.5000, 1298081.5000, 1273672.5000, 1259709.2500,
         1242205.7500, 1236186.8750, 1223341.3750, 1222000.5000, 1200746.6250],
        [1337306.1250, 1238085.1250, 1234376.2500, 1210624.7500, 1192502.7500,
         1175062.0000, 1169604.8750, 1128187.5000, 1107692.3750, 1103183.5000],
        [1404126.8750, 1391921.0000, 1357059.2500, 1351804.6250, 1350000.8750,
         1306602.7500, 1296172.6250, 1273860.7500, 1253206.3750, 1209101.7500],
        [1456284.1250, 1334078.2500, 1329866.1250, 1187648.5000, 1178665.7500,
         1136651.1250, 1121552.5000, 1099118.2500, 1095742.0000, 1090826.0000],
        [1354215.0000, 1327013.0000, 1230172.6250, 1175903.8750, 1172414.6250,
         1135016.5000, 1117109.7500, 1106596.3750, 1104370.8750, 1098022.5000],
        [1303835.6250, 1268099.6250, 1266105.7500, 1250167.1250, 1220831.0000,
         1216054.0000, 1197489.7500, 1196713.3750, 1179886.1250, 1161832.8750],
        [1444310.6250, 1156810.1250, 1156633.6250, 1089786.2500, 1073872.3750,
         1070334.7500, 1034868.4375, 1018643.5625, 1004551.8750,  893142.3750],
        [1265200.5000, 1234594.0000, 1182310.0000, 1154504.3750, 1137030.6250,
         1129555.7500, 1121772.7500, 1058968.2500, 1046546.0625, 1043276.5000],
        [1318457.7500, 1289305.7500, 1233617.1250, 1167811.5000, 1099913.1250,
         1049431.3750, 1024904.2500,  967254.5625,  959301.8750,  943737.8125],
        [1387715.5000, 1189340.7500, 1179171.7500, 1150356.7500, 1092598.1250,
         1016660.7500,  984629.0000,  984221.5625,  975979.4375,  953962.9375],
        [1303899.1250, 1280056.8750, 1263664.2500, 1229660.0000, 1226840.5000,
         1213338.6250, 1205791.2500, 1189767.3750, 1179862.3750, 1152788.1250],
        [1338584.7500, 1274553.5000, 1266611.7500, 1248321.7500, 1214477.8750,
         1210047.6250, 1187200.0000, 1185846.6250, 1152260.6250, 1093008.6250],
        [1367932.2500, 1309092.2500, 1290663.7500, 1273242.5000, 1271045.3750,
         1243391.0000, 1232598.7500, 1226538.7500, 1221258.3750, 1218260.6250],
        [1397914.0000, 1347126.5000, 1268392.3750, 1252118.0000, 1235532.8750,
         1225489.8750, 1216744.2500, 1199635.2500, 1098864.6250, 1080608.6250],
        [1169113.1250, 1147857.0000, 1115694.7500, 1107555.1250, 1077341.5000,
         1076561.0000, 1069640.8750, 1068360.3750, 1057405.0000, 1050779.3750],
        [1358957.7500, 1307168.6250, 1289844.3750, 1266137.1250, 1259473.7500,
         1253189.5000, 1219931.2500, 1219750.8750, 1207399.8750, 1205795.7500],
        [1454834.8750, 1390366.1250, 1389334.8750, 1332101.3750, 1327273.7500,
         1314684.8750, 1307393.1250, 1273463.6250, 1254613.7500, 1242614.5000],
        [1407387.3750, 1288338.3750, 1272857.7500, 1258837.3750, 1254217.8750,
         1244283.0000, 1234346.7500, 1233078.3750, 1212789.1250, 1189785.3750],
        [1421505.0000, 1292128.1250, 1287203.6250, 1232475.3750, 1219336.8750,
         1137336.5000, 1123522.2500, 1115841.6250, 1111370.5000, 1106428.6250],
        [1381870.3750, 1375072.5000, 1367229.2500, 1353493.1250, 1317073.0000,
         1304752.5000, 1268846.0000, 1268831.6250, 1248754.0000, 1248534.8750],
        [1418310.7500, 1322709.6250, 1236410.8750, 1209979.6250, 1206143.1250,
         1197219.1250, 1177409.8750, 1141038.8750, 1130562.3750, 1108645.6250],
        [1509563.7500, 1464329.3750, 1459832.6250, 1421026.6250, 1418872.2500,
         1373920.2500, 1369997.6250, 1342830.8750, 1341365.3750, 1337079.1250],
        [1174152.3750, 1056592.6250, 1045878.5000,  932055.6250,  817045.5000,
          806892.3125,  797599.0000,  793393.0000,  760223.3750,  753513.5000],
        [1249400.7500, 1209243.5000, 1144691.2500, 1116593.1250, 1099107.8750,
         1091568.0000, 1078428.1250, 1023516.3125, 1005118.2500,  976015.6875],
        [1388344.2500, 1187442.3750, 1174855.8750, 1147181.7500, 1015055.5000,
          968869.2500,  958126.0625,  953991.1875,  947864.3125,  796261.3125],
        [1027515.3750,  985307.2500,  977078.3125,  959075.9375,  943717.0625,
          939236.7500,  938965.4375,  930924.6875,  912475.9375,  907468.6875],
        [1121441.2500,  999623.5000,  940609.1250,  879866.7500,  836958.1875,
          822922.1875,  792992.8750,  781264.0000,  767617.1250,  709254.4375],
        [1019247.0000, 1017595.9375,  868273.5000,  842323.1250,  835772.9375,
          832174.8125,  735165.2500,  709326.8125,  677702.8125,  670850.6250],
        [1277601.8750, 1233165.3750, 1132634.3750, 1123626.2500, 1080193.3750,
         1024774.3125,  995095.0625,  913714.1875,  905084.0625,  904423.1875],
        [1124636.1250, 1045671.1250,  922884.1875,  899418.0000,  839824.5625,
          820413.0625,  794818.2500,  777120.4375,  725101.6875,  723097.7500],
        [1239125.8750, 1217938.8750, 1143009.0000, 1132518.8750, 1115981.0000,
         1074503.3750, 1059707.7500, 1035219.8125, 1029722.5625, 1023725.2500],
        [1484020.0000, 1444453.8750, 1436971.2500, 1418063.2500, 1389774.8750,
         1381543.6250, 1380200.2500, 1359214.3750, 1355713.8750, 1339757.1250],
        [1384244.5000, 1287693.5000, 1125608.1250, 1119183.6250, 1109587.1250,
         1095370.1250, 1052700.1250, 1038634.4375, 1035718.5000, 1012673.1250],
        [1250498.6250, 1213820.1250, 1157318.7500, 1086230.2500, 1085014.8750,
         1012510.8750,  951872.8125,  890157.6875,  796959.5000,  795851.3750],
        [1368492.1250, 1100517.5000,  965066.2500,  909628.7500,  820316.8125,
          774590.0000,  765462.0625,  757542.6875,  721049.7500,  707697.7500],
        [1368862.7500, 1147855.8750, 1131490.0000, 1018880.6250, 1000967.6875,
          949769.0625,  938359.3750,  917135.6250,  809176.3125,  749295.6250],
        [1290327.8750, 1065036.8750, 1056793.1250, 1044665.3125, 1038414.5625,
         1032327.1875, 1006526.3750,  959402.5000,  952646.5625,  949911.3750],
        [1296708.1250, 1280906.8750, 1279494.3750, 1263395.5000, 1239528.8750,
         1223989.0000, 1220219.8750, 1183755.2500, 1176374.8750, 1159097.1250],
        [1182396.8750, 1127191.6250, 1112392.7500,  935310.0625,  885242.4375,
          884340.4375,  882151.1875,  858604.7500,  848633.2500,  817542.7500],
        [1024413.8125,  921050.9375,  864051.6250,  854821.1250,  803281.3750,
          798718.6250,  797391.3125,  768679.3750,  768430.1875,  736105.3750],
        [1331228.8750, 1067374.5000,  966180.5000,  939125.7500,  928867.3125,
          876798.4375,  867300.2500,  856523.3750,  839634.8125,  807194.8125],
        [1163698.1250,  957972.5625,  816784.5000,  813412.3750,  808300.1875,
          799714.0625,  755003.1875,  669830.9375,  618814.5000,  616087.5625],
        [1094041.1250, 1073111.6250, 1009275.4375,  964975.1250,  950845.7500,
          949884.1250,  945275.3125,  939195.5625,  924075.8125,  921401.5000],
        [ 941678.0625,  880669.3125,  859687.9375,  767205.8125,  741643.1250,
          740311.1250,  731263.5000,  685968.6250,  660736.6250,  644993.2500],
        [1146567.1250, 1113042.1250,  952399.5000,  933663.1250,  901765.2500,
          855845.6250,  813032.4375,  771351.6875,  734023.3750,  696616.8750],
        [1149581.3750, 1052768.3750,  963944.0625,  933558.0625,  926285.1250,
          917905.5625,  913041.7500,  900438.4375,  899049.2500,  874974.0625]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 358900.0000,       0.0000],
         [ 344476.3125,       0.0000],
         [ 276912.2500,       0.0000],
         ...,
         [ 181689.9375,       0.0000],
         [ 173906.9375,       0.0000],
         [ 163586.2188,       0.0000]],

        [[1480339.1250,       0.0000],
         [1462159.5000,       0.0000],
         [1413062.8750,       0.0000],
         ...,
         [1349518.2500,       0.0000],
         [1348854.3750,       0.0000],
         [1345180.3750,       0.0000]],

        [[1449989.7500,       0.0000],
         [1438873.2500,       0.0000],
         [1433676.5000,       0.0000],
         ...,
         [1217158.5000,       0.0000],
         [1198475.7500,       0.0000],
         [1188083.5000,       0.0000]],

        ...,

        [[ 941678.0625,       0.0000],
         [ 880669.3125,       0.0000],
         [ 859687.9375,       0.0000],
         ...,
         [ 685968.6250,       0.0000],
         [ 660736.6250,       0.0000],
         [ 644993.2500,       0.0000]],

        [[1146567.1250,       0.0000],
         [1113042.1250,       0.0000],
         [ 952399.5000,       0.0000],
         ...,
         [ 771351.6875,       0.0000],
         [ 734023.3750,       0.0000],
         [      0.0000,  696616.8750]],

        [[1149581.3750,       0.0000],
         [1052768.3750,       0.0000],
         [      0.0000,  963944.0625],
         ...,
         [ 900438.4375,       0.0000],
         [      0.0000,  899049.2500],
         [ 874974.0625,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2037065.2500,   444589.6250],
        [13892069.0000,        0.0000],
        [13169173.0000,        0.0000],
        [12944809.0000,        0.0000],
        [11331411.0000,  1368169.7500],
        [13133354.0000,  1414739.0000],
        [13958866.0000,  1571155.0000],
        [14269668.0000,        0.0000],
        [13885060.0000,        0.0000],
        [ 8057346.5000,   683197.8750],
        [ 8698918.0000,  4026693.2500],
        [12774589.0000,        0.0000],
        [ 9306327.0000,  1141448.1250],
        [12869554.0000,  1511081.8750],
        [12404131.0000,        0.0000],
        [11708066.0000,        0.0000],
        [13869416.0000,        0.0000],
        [12406795.0000,  1351798.2500],
        [ 8869000.0000,  3793977.7500],
        [10788932.0000,  1107692.3750],
        [11919997.0000,  1273860.7500],
        [ 9508706.0000,  2521726.7500],
        [11820835.0000,        0.0000],
        [12261014.0000,        0.0000],
        [ 9853168.0000,  1089786.2500],
        [11373758.0000,        0.0000],
        [ 9735278.0000,  1318457.7500],
        [10914636.0000,        0.0000],
        [12245668.0000,        0.0000],
        [12170912.0000,        0.0000],
        [12654023.0000,        0.0000],
        [12322426.0000,        0.0000],
        [ 8694634.0000,  2245674.0000],
        [12587648.0000,        0.0000],
        [13286681.0000,        0.0000],
        [11351638.0000,  1244283.0000],
        [ 9644103.0000,  2403045.2500],
        [13134458.0000,        0.0000],
        [10912020.0000,  1236410.8750],
        [14038818.0000,        0.0000],
        [ 8937346.0000,        0.0000],
        [10993684.0000,        0.0000],
        [10537992.0000,        0.0000],
        [ 9521766.0000,        0.0000],
        [ 6050810.0000,  2601740.0000],
        [ 8208433.0000,        0.0000],
        [10590312.0000,        0.0000],
        [ 6187178.5000,  2485806.5000],
        [ 9955472.0000,  1115981.0000],
        [12599938.0000,  1389774.8750],
        [ 7785833.5000,  3475579.0000],
        [ 8431872.0000,  1808362.2500],
        [ 7295456.5000,  1594906.7500],
        [ 9114657.0000,   917135.6250],
        [ 9357637.0000,  1038414.5625],
        [12323470.0000,        0.0000],
        [ 5227484.5000,  4306322.0000],
        [ 7482123.0000,   854821.1250],
        [ 7081625.0000,  2398603.5000],
        [ 6442831.0000,  1576787.0000],
        [ 7689695.0000,  2082387.0000],
        [ 7654158.0000,        0.0000],
        [ 7319925.0000,  1598382.1250],
        [ 7668552.5000,  1862993.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 71/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:43, 63.56s/it]  7%|▋         | 2/30 [01:04<12:25, 26.61s/it] 10%|█         | 3/30 [01:05<06:39, 14.80s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.25s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.19s/it] 20%|██        | 6/30 [01:07<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:08<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.40s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.85s/it]
Epoch loss is 4.472768147786458
Epoch 72/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:41, 59.36s/it]  7%|▋         | 2/30 [01:00<11:36, 24.88s/it] 10%|█         | 3/30 [01:00<06:14, 13.86s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.69s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.82s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 4.439015277226766
Epoch 73/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:21, 60.73s/it]  7%|▋         | 2/30 [01:01<11:52, 25.45s/it] 10%|█         | 3/30 [01:02<06:22, 14.17s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 4.48897598584493
Epoch 74/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:40, 57.27s/it]  7%|▋         | 2/30 [00:58<11:12, 24.02s/it] 10%|█         | 3/30 [00:58<06:01, 13.40s/it] 13%|█▎        | 4/30 [00:59<03:38,  8.40s/it] 17%|█▋        | 5/30 [01:00<02:21,  5.64s/it] 20%|██        | 6/30 [01:01<01:35,  3.98s/it] 23%|██▎       | 7/30 [01:01<01:07,  2.92s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.23s/it] 30%|███       | 9/30 [01:03<00:37,  1.77s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.45s/it] 37%|███▋      | 11/30 [01:04<00:23,  1.24s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:07<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:10<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:13<00:05,  1.32it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:15<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:16<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  2.64s/it]
Epoch loss is 4.4227474451065065
Epoch 75/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<31:04, 64.29s/it]  7%|▋         | 2/30 [01:05<12:33, 26.91s/it] 10%|█         | 3/30 [01:06<06:54, 15.36s/it] 13%|█▎        | 4/30 [01:07<04:09,  9.59s/it] 17%|█▋        | 5/30 [01:08<02:40,  6.40s/it] 20%|██        | 6/30 [01:08<01:47,  4.48s/it] 23%|██▎       | 7/30 [01:09<01:14,  3.26s/it] 27%|██▋       | 8/30 [01:10<00:54,  2.46s/it] 30%|███       | 9/30 [01:11<00:40,  1.93s/it] 33%|███▎      | 10/30 [01:11<00:31,  1.56s/it] 37%|███▋      | 11/30 [01:12<00:24,  1.31s/it] 40%|████      | 12/30 [01:13<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:14<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:15,  1.06it/s] 50%|█████     | 15/30 [01:15<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:16<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:17<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:18<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:19<00:07,  1.30it/s] 70%|███████   | 21/30 [01:20<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:21<00:05,  1.32it/s] 80%|████████  | 24/30 [01:22<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:23<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:24<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:25<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:26<00:00,  1.33it/s]100%|██████████| 30/30 [01:26<00:00,  1.33it/s]100%|██████████| 30/30 [01:27<00:00,  2.90s/it]
Epoch loss is 4.384211842219035
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0331,  0.0188,  0.0006,  ..., -0.0315, -0.0274, -0.0075],
        [-0.0652,  0.0332,  0.0174,  ..., -0.0094, -0.0141, -0.0342],
        [-0.0771, -0.0053, -0.0226,  ...,  0.0527,  0.0254, -0.0219],
        ...,
        [-0.0144, -0.0258, -0.0144,  ..., -0.0502, -0.0082,  0.0207],
        [-0.0826,  0.0061,  0.0305,  ..., -0.0409,  0.0368, -0.0209],
        [-0.0454,  0.0282,  0.0102,  ..., -0.0364,  0.0478, -0.0409]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8968, 0.8884, 0.8836, 0.8737, 0.8715, 0.8655, 0.8588, 0.8574, 0.8568,
         0.8412],
        [0.9945, 0.9931, 0.9911, 0.9904, 0.9904, 0.9897, 0.9896, 0.9882, 0.9881,
         0.9875],
        [0.9929, 0.9928, 0.9912, 0.9900, 0.9856, 0.9841, 0.9822, 0.9811, 0.9801,
         0.9801],
        [0.9921, 0.9839, 0.9837, 0.9827, 0.9822, 0.9818, 0.9803, 0.9796, 0.9795,
         0.9795],
        [0.9870, 0.9866, 0.9822, 0.9810, 0.9786, 0.9784, 0.9781, 0.9781, 0.9773,
         0.9773],
        [0.9938, 0.9934, 0.9929, 0.9927, 0.9924, 0.9921, 0.9921, 0.9905, 0.9895,
         0.9894],
        [0.9982, 0.9982, 0.9981, 0.9979, 0.9976, 0.9975, 0.9975, 0.9967, 0.9967,
         0.9963],
        [0.9927, 0.9916, 0.9915, 0.9912, 0.9911, 0.9910, 0.9908, 0.9894, 0.9887,
         0.9885],
        [0.9916, 0.9913, 0.9913, 0.9909, 0.9895, 0.9892, 0.9884, 0.9883, 0.9880,
         0.9874],
        [0.9766, 0.9755, 0.9629, 0.9553, 0.9545, 0.9495, 0.9493, 0.9441, 0.9438,
         0.9422],
        [0.9908, 0.9872, 0.9862, 0.9857, 0.9854, 0.9824, 0.9813, 0.9777, 0.9770,
         0.9767],
        [0.9901, 0.9897, 0.9840, 0.9828, 0.9782, 0.9781, 0.9770, 0.9744, 0.9738,
         0.9737],
        [0.9850, 0.9843, 0.9766, 0.9738, 0.9646, 0.9639, 0.9582, 0.9554, 0.9509,
         0.9427],
        [0.9939, 0.9938, 0.9929, 0.9923, 0.9922, 0.9916, 0.9915, 0.9908, 0.9906,
         0.9905],
        [0.9864, 0.9861, 0.9828, 0.9825, 0.9813, 0.9804, 0.9801, 0.9780, 0.9759,
         0.9743],
        [0.9912, 0.9868, 0.9826, 0.9805, 0.9768, 0.9735, 0.9730, 0.9678, 0.9674,
         0.9668],
        [0.9934, 0.9922, 0.9913, 0.9910, 0.9906, 0.9904, 0.9890, 0.9884, 0.9882,
         0.9869],
        [0.9937, 0.9935, 0.9912, 0.9900, 0.9894, 0.9881, 0.9861, 0.9859, 0.9856,
         0.9829],
        [0.9893, 0.9870, 0.9828, 0.9816, 0.9813, 0.9802, 0.9787, 0.9781, 0.9773,
         0.9772],
        [0.9888, 0.9824, 0.9815, 0.9814, 0.9809, 0.9790, 0.9774, 0.9767, 0.9766,
         0.9758],
        [0.9908, 0.9907, 0.9897, 0.9874, 0.9873, 0.9853, 0.9839, 0.9839, 0.9826,
         0.9808],
        [0.9924, 0.9859, 0.9857, 0.9763, 0.9754, 0.9747, 0.9725, 0.9724, 0.9701,
         0.9701],
        [0.9883, 0.9867, 0.9796, 0.9773, 0.9758, 0.9729, 0.9729, 0.9722, 0.9720,
         0.9698],
        [0.9847, 0.9846, 0.9833, 0.9820, 0.9813, 0.9809, 0.9792, 0.9784, 0.9780,
         0.9767],
        [0.9924, 0.9777, 0.9753, 0.9733, 0.9729, 0.9725, 0.9724, 0.9680, 0.9647,
         0.9616],
        [0.9842, 0.9797, 0.9776, 0.9760, 0.9750, 0.9742, 0.9729, 0.9715, 0.9711,
         0.9695],
        [0.9842, 0.9837, 0.9807, 0.9778, 0.9702, 0.9701, 0.9689, 0.9631, 0.9621,
         0.9610],
        [0.9890, 0.9799, 0.9785, 0.9755, 0.9736, 0.9685, 0.9678, 0.9678, 0.9645,
         0.9634],
        [0.9845, 0.9837, 0.9831, 0.9807, 0.9793, 0.9790, 0.9790, 0.9789, 0.9783,
         0.9768],
        [0.9859, 0.9855, 0.9829, 0.9813, 0.9794, 0.9772, 0.9772, 0.9766, 0.9750,
         0.9720],
        [0.9869, 0.9842, 0.9839, 0.9830, 0.9819, 0.9819, 0.9818, 0.9818, 0.9811,
         0.9808],
        [0.9890, 0.9876, 0.9829, 0.9819, 0.9812, 0.9798, 0.9796, 0.9778, 0.9725,
         0.9723],
        [0.9754, 0.9740, 0.9739, 0.9710, 0.9701, 0.9693, 0.9692, 0.9677, 0.9677,
         0.9667],
        [0.9858, 0.9857, 0.9815, 0.9808, 0.9801, 0.9800, 0.9797, 0.9796, 0.9795,
         0.9788],
        [0.9930, 0.9897, 0.9880, 0.9864, 0.9863, 0.9862, 0.9859, 0.9842, 0.9841,
         0.9825],
        [0.9906, 0.9843, 0.9832, 0.9823, 0.9816, 0.9813, 0.9813, 0.9813, 0.9790,
         0.9779],
        [0.9907, 0.9884, 0.9829, 0.9812, 0.9798, 0.9752, 0.9736, 0.9729, 0.9721,
         0.9720],
        [0.9896, 0.9885, 0.9884, 0.9862, 0.9858, 0.9848, 0.9838, 0.9838, 0.9834,
         0.9833],
        [0.9905, 0.9858, 0.9811, 0.9809, 0.9784, 0.9772, 0.9760, 0.9743, 0.9738,
         0.9737],
        [0.9960, 0.9939, 0.9935, 0.9917, 0.9913, 0.9896, 0.9891, 0.9885, 0.9875,
         0.9873],
        [0.9751, 0.9698, 0.9673, 0.9614, 0.9492, 0.9474, 0.9472, 0.9459, 0.9449,
         0.9426],
        [0.9811, 0.9787, 0.9749, 0.9736, 0.9727, 0.9722, 0.9718, 0.9675, 0.9659,
         0.9640],
        [0.9881, 0.9783, 0.9753, 0.9748, 0.9655, 0.9642, 0.9630, 0.9607, 0.9600,
         0.9490],
        [0.9668, 0.9650, 0.9641, 0.9638, 0.9623, 0.9621, 0.9609, 0.9607, 0.9602,
         0.9595],
        [0.9726, 0.9639, 0.9576, 0.9544, 0.9540, 0.9489, 0.9482, 0.9454, 0.9427,
         0.9395],
        [0.9595, 0.9582, 0.9476, 0.9454, 0.9432, 0.9430, 0.9372, 0.9346, 0.9315,
         0.9315],
        [0.9837, 0.9814, 0.9761, 0.9757, 0.9708, 0.9665, 0.9653, 0.9612, 0.9563,
         0.9561],
        [0.9687, 0.9628, 0.9539, 0.9533, 0.9440, 0.9416, 0.9414, 0.9398, 0.9385,
         0.9371],
        [0.9786, 0.9755, 0.9708, 0.9696, 0.9680, 0.9649, 0.9638, 0.9636, 0.9635,
         0.9635],
        [0.9939, 0.9921, 0.9905, 0.9897, 0.9889, 0.9883, 0.9877, 0.9862, 0.9858,
         0.9853],
        [0.9882, 0.9824, 0.9726, 0.9723, 0.9710, 0.9710, 0.9677, 0.9673, 0.9662,
         0.9661],
        [0.9828, 0.9799, 0.9774, 0.9706, 0.9677, 0.9649, 0.9647, 0.9562, 0.9497,
         0.9467],
        [0.9891, 0.9720, 0.9571, 0.9527, 0.9522, 0.9517, 0.9455, 0.9439, 0.9439,
         0.9371],
        [0.9871, 0.9744, 0.9736, 0.9596, 0.9594, 0.9587, 0.9577, 0.9576, 0.9462,
         0.9433],
        [0.9835, 0.9709, 0.9669, 0.9667, 0.9663, 0.9656, 0.9642, 0.9629, 0.9625,
         0.9621],
        [0.9844, 0.9844, 0.9818, 0.9816, 0.9807, 0.9801, 0.9796, 0.9774, 0.9773,
         0.9760],
        [0.9751, 0.9737, 0.9733, 0.9612, 0.9568, 0.9541, 0.9535, 0.9521, 0.9497,
         0.9493],
        [0.9677, 0.9559, 0.9532, 0.9498, 0.9494, 0.9473, 0.9468, 0.9455, 0.9454,
         0.9388],
        [0.9853, 0.9743, 0.9608, 0.9596, 0.9570, 0.9553, 0.9539, 0.9497, 0.9496,
         0.9495],
        [0.9721, 0.9659, 0.9512, 0.9498, 0.9485, 0.9481, 0.9435, 0.9351, 0.9328,
         0.9284],
        [0.9717, 0.9684, 0.9623, 0.9620, 0.9616, 0.9607, 0.9596, 0.9594, 0.9577,
         0.9562],
        [0.9649, 0.9573, 0.9509, 0.9464, 0.9432, 0.9420, 0.9412, 0.9408, 0.9381,
         0.9362],
        [0.9744, 0.9742, 0.9648, 0.9619, 0.9615, 0.9554, 0.9487, 0.9450, 0.9428,
         0.9411],
        [0.9743, 0.9696, 0.9642, 0.9625, 0.9623, 0.9589, 0.9581, 0.9577, 0.9573,
         0.9557]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 366235.6562,  324906.8438,  303248.1875,  263463.8438,  255418.2812,
          234224.7969,  213025.0625,  208560.9844,  207037.0625,  165495.5938],
        [1479064.8750, 1450626.0000, 1409270.5000, 1396096.7500, 1395899.7500,
         1381452.6250, 1378733.5000, 1352128.2500, 1349849.0000, 1338495.3750],
        [1446919.0000, 1443474.7500, 1411763.0000, 1387141.2500, 1301859.0000,
         1274317.6250, 1240883.1250, 1220827.5000, 1204722.2500, 1204313.2500],
        [1429429.1250, 1271598.2500, 1267787.6250, 1250290.0000, 1241769.8750,
         1234260.8750, 1208051.7500, 1195848.6250, 1194410.2500, 1193316.1250],
        [1328962.1250, 1321421.0000, 1241763.8750, 1219441.6250, 1179543.0000,
         1176097.8750, 1170697.3750, 1169825.7500, 1157710.6250, 1157001.0000],
        [1465423.2500, 1457028.6250, 1445049.1250, 1442142.8750, 1435337.2500,
         1430293.7500, 1430061.7500, 1396770.6250, 1376947.7500, 1375908.1250],
        [1560103.2500, 1559027.8750, 1558455.6250, 1553748.2500, 1545689.2500,
         1545030.3750, 1544918.3750, 1527384.1250, 1527295.3750, 1517910.5000],
        [1442841.6250, 1418946.5000, 1417384.5000, 1411913.8750, 1410163.1250,
         1407149.8750, 1402432.7500, 1376333.2500, 1361961.2500, 1357756.8750],
        [1418627.2500, 1414075.3750, 1413736.8750, 1404791.3750, 1377532.2500,
         1371604.3750, 1355039.2500, 1354453.8750, 1348409.2500, 1336027.6250],
        [1145473.1250, 1128364.0000,  941310.8750,  844665.5625,  835898.0625,
          777774.4375,  776156.8125,  720281.3750,  716746.9375,  700410.0000],
        [1402836.6250, 1332961.6250, 1314758.8750, 1303837.0000, 1298639.8750,
         1244347.1250, 1224927.8750, 1164492.8750, 1152219.8750, 1147355.7500],
        [1388883.2500, 1380834.8750, 1273642.1250, 1250862.5000, 1172838.5000,
         1170210.7500, 1151414.7500, 1110105.6250, 1100621.5000, 1099293.3750],
        [1292384.5000, 1278358.8750, 1144847.2500, 1100572.1250,  965323.9375,
          955556.4375,  880562.6250,  845997.3750,  793888.8125,  706278.5000],
        [1466876.0000, 1463891.0000, 1446843.0000, 1432758.0000, 1430557.0000,
         1419211.8750, 1416669.6250, 1402299.0000, 1399528.0000, 1397726.1250],
        [1317503.7500, 1311397.7500, 1251687.0000, 1246315.0000, 1224750.3750,
         1209311.6250, 1203988.2500, 1169027.2500, 1133840.5000, 1109182.8750],
        [1412266.7500, 1325767.1250, 1247988.5000, 1210860.3750, 1148587.3750,
         1096739.3750, 1087730.3750, 1010839.8125, 1004960.1250,  995274.4375],
        [1457132.8750, 1431721.2500, 1413174.7500, 1407408.8750, 1400087.3750,
         1395819.8750, 1368253.2500, 1355536.7500, 1352043.1250, 1326466.3750],
        [1462326.7500, 1458142.1250, 1410440.2500, 1387716.8750, 1375625.8750,
         1349761.5000, 1312717.8750, 1308658.0000, 1302029.1250, 1254122.1250],
        [1374413.0000, 1329835.6250, 1252253.0000, 1230908.3750, 1224800.5000,
         1206258.1250, 1180334.0000, 1170326.7500, 1156684.3750, 1155751.5000],
        [1363327.1250, 1244228.3750, 1227846.0000, 1227546.2500, 1218383.7500,
         1185972.2500, 1158563.2500, 1147831.8750, 1144794.8750, 1132671.1250],
        [1402727.0000, 1402225.5000, 1381935.0000, 1336710.6250, 1335682.2500,
         1296748.7500, 1271743.7500, 1271582.5000, 1247804.0000, 1216723.3750],
        [1434855.5000, 1308686.6250, 1303761.1250, 1141063.8750, 1126828.3750,
         1115180.8750, 1079896.7500, 1078328.2500, 1044420.3125, 1043425.6875],
        [1353264.8750, 1322805.5000, 1196494.2500, 1157185.3750, 1133114.1250,
         1087204.6250, 1086140.1250, 1076012.8750, 1072602.2500, 1039460.8750],
        [1285971.7500, 1284281.6250, 1261144.5000, 1238231.6250, 1225508.6250,
         1218340.7500, 1188275.0000, 1175783.8750, 1168259.3750, 1147102.0000],
        [1434847.3750, 1163359.6250, 1124341.1250, 1093526.8750, 1086889.3750,
         1080949.7500, 1078618.3750, 1013810.4375,  966315.0625,  924239.7500],
        [1277360.7500, 1197448.6250, 1162678.6250, 1136203.5000, 1119099.3750,
         1106993.2500, 1087348.6250, 1064826.6250, 1058364.5000, 1034390.8125],
        [1276701.8750, 1268602.8750, 1213925.5000, 1165803.0000, 1045779.8125,
         1043285.4375, 1026409.6875,  944021.3125,  931908.0625,  916879.3125],
        [1368035.3750, 1200178.7500, 1177930.8750, 1127002.3750, 1096770.7500,
         1020092.0625, 1010393.5625, 1009613.3750,  963376.0625,  948273.9375],
        [1282959.6250, 1267144.6250, 1256662.6250, 1214657.3750, 1190089.5000,
         1184831.6250, 1184732.1250, 1183599.5000, 1172935.8750, 1148154.7500],
        [1307769.6250, 1300812.7500, 1253411.8750, 1224312.3750, 1192180.8750,
         1155906.8750, 1154672.8750, 1145899.2500, 1119408.8750, 1072399.7500],
        [1327176.2500, 1277532.3750, 1271343.7500, 1255551.0000, 1235564.6250,
         1235463.2500, 1233494.7500, 1233374.8750, 1221314.2500, 1217109.7500],
        [1367640.1250, 1340215.8750, 1253276.8750, 1235516.2500, 1223386.8750,
         1198664.2500, 1195808.7500, 1166109.8750, 1079852.3750, 1077084.7500],
        [1125864.8750, 1104572.0000, 1102588.1250, 1057278.0000, 1043710.3750,
         1032753.6250, 1030366.9375, 1008784.6875, 1008209.5625,  995110.3125],
        [1305937.5000, 1305205.5000, 1228711.6250, 1215892.7500, 1204672.8750,
         1201814.3750, 1197765.0000, 1195459.7500, 1194214.3750, 1182754.3750],
        [1449001.3750, 1381249.7500, 1347895.0000, 1317109.3750, 1314987.1250,
         1313610.7500, 1308442.0000, 1276082.2500, 1275421.5000, 1246178.2500],
        [1399899.1250, 1278322.1250, 1258993.3750, 1243304.3750, 1230383.7500,
         1225159.2500, 1224551.7500, 1224404.7500, 1184966.0000, 1167455.2500],
        [1402155.8750, 1355371.3750, 1253304.2500, 1223868.7500, 1198563.7500,
         1122548.7500, 1098181.6250, 1086618.8750, 1074512.6250, 1073264.2500],
        [1379676.5000, 1357684.3750, 1355133.5000, 1314785.1250, 1306389.7500,
         1288511.6250, 1269568.6250, 1269502.1250, 1262778.7500, 1260437.3750],
        [1396820.0000, 1306245.2500, 1221894.3750, 1217664.7500, 1174608.2500,
         1155103.6250, 1135924.0000, 1108100.2500, 1099965.6250, 1098517.8750],
        [1511708.7500, 1466842.5000, 1459239.6250, 1420843.6250, 1413700.5000,
         1380267.3750, 1369477.8750, 1358764.7500, 1339559.0000, 1335677.2500],
        [1120977.1250, 1039435.1250, 1002831.8125,  922512.0000,  774794.6250,
          754988.0625,  752986.9375,  739235.9375,  728349.3125,  704490.4375],
        [1221145.3750, 1180111.1250, 1118155.2500, 1097725.1250, 1084092.2500,
         1076423.3750, 1069614.3750, 1005377.1250,  982929.0000,  957108.6875],
        [1350535.3750, 1174551.0000, 1124250.0000, 1117298.2500,  977474.5000,
          959849.1250,  943817.8750,  912243.6250,  903425.7500,  772792.6250],
        [ 996020.7500,  969964.8125,  957576.1250,  953864.7500,  934447.9375,
          931593.4375,  914854.7500,  913296.0000,  905894.9375,  897914.7500],
        [1081964.6250,  956105.1875,  873889.1875,  834028.3750,  829014.2500,
          770910.4375,  763242.4375,  733914.8750,  706122.1875,  673927.9375],
        [ 897895.8750,  880170.5625,  756964.8750,  733345.3750,  710955.5625,
          708871.0625,  652437.5625,  628983.3750,  601546.7500,  601300.7500],
        [1267394.7500, 1226397.1250, 1138161.0000, 1130419.0000, 1054840.7500,
          991154.1875,  974622.3750,  919529.9375,  857677.5000,  854174.0625],
        [1023561.2500,  941019.1875,  828717.8125,  821106.5000,  719161.8750,
          694364.4375,  693135.1875,  677365.5000,  664614.4375,  651176.3125],
        [1178014.0000, 1127192.6250, 1054996.6250, 1036837.2500, 1012458.6875,
          968979.2500,  953876.5625,  951239.4375,  950355.3125,  950335.3750],
        [1465817.3750, 1430196.8750, 1396620.1250, 1381236.6250, 1365337.3750,
         1354513.2500, 1343383.0000, 1314718.7500, 1307096.3750, 1297687.8750],
        [1351501.6250, 1245056.8750, 1082661.3750, 1076980.8750, 1057895.2500,
         1056954.3750, 1008339.3125, 1002639.6250,  987897.5625,  986118.4375],
        [1251133.2500, 1200254.2500, 1157953.6250, 1051603.3750, 1009314.8750,
          969102.1875,  966421.9375,  856056.2500,  779596.0625,  747582.5625],
        [1370083.8750, 1072009.1250,  867268.0000,  814433.9375,  807910.2500,
          803177.1875,  735128.1250,  718494.8125,  717575.1875,  651480.6250],
        [1330888.6250, 1109914.0000, 1097606.8750,  898829.8125,  896058.4375,
          886819.1875,  874015.0625,  873310.1250,  742195.0625,  711413.3750],
        [1263746.2500, 1056728.6250,  998032.7500,  994348.5000,  988840.0625,
          979315.4375,  959661.5000,  942495.6875,  936244.4375,  931055.2500],
        [1280479.5000, 1280015.5000, 1233712.5000, 1229954.3750, 1214276.3750,
         1204381.0000, 1196243.2500, 1158839.6250, 1157519.7500, 1135331.6250],
        [1122042.5000, 1099363.6250, 1092981.6250,  919529.9375,  863861.3125,
          830343.5625,  823028.1250,  807215.5000,  779703.1250,  775253.6250],
        [1008222.0625,  851820.6875,  820037.6250,  780725.4375,  776755.1875,
          753310.1875,  748789.1875,  734226.4375,  733089.4375,  667731.3125],
        [1297614.8750, 1108995.6250,  913663.6875,  899147.8750,  865849.0625,
          845504.5625,  828259.5625,  780593.6875,  778439.3125,  777988.8125],
        [1074014.7500,  982619.6875,  796780.9375,  781464.3750,  766877.3750,
          762652.3750,  714171.8750,  633313.5000,  612891.3750,  575170.3750],
        [1068509.1250, 1018406.5625,  934231.4375,  930360.2500,  924506.8125,
          912210.5625,  898233.3750,  896194.3125,  874156.7500,  855532.2500],
        [ 969140.9375,  869413.6250,  793390.0000,  743620.5000,  711089.1875,
          698500.8750,  691127.9375,  687188.5000,  661227.6875,  643514.3125],
        [1110532.3750, 1106611.2500,  967202.9375,  927985.3750,  923260.0625,
          845925.6250,  769268.2500,  729240.3750,  706463.7500,  690272.3125],
        [1108168.8750, 1036323.1250,  960270.2500,  935968.6250,  934471.0625,
          890206.9375,  880019.4375,  874600.3750,  870028.2500,  849414.6875]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 366235.6562,       0.0000],
         [ 324906.8438,       0.0000],
         [ 303248.1875,       0.0000],
         ...,
         [      0.0000,  208560.9844],
         [ 207037.0625,       0.0000],
         [ 165495.5938,       0.0000]],

        [[1479064.8750,       0.0000],
         [1450626.0000,       0.0000],
         [1409270.5000,       0.0000],
         ...,
         [1352128.2500,       0.0000],
         [1349849.0000,       0.0000],
         [1338495.3750,       0.0000]],

        [[1446919.0000,       0.0000],
         [1443474.7500,       0.0000],
         [1411763.0000,       0.0000],
         ...,
         [1220827.5000,       0.0000],
         [1204722.2500,       0.0000],
         [1204313.2500,       0.0000]],

        ...,

        [[ 969140.9375,       0.0000],
         [ 869413.6250,       0.0000],
         [ 793390.0000,       0.0000],
         ...,
         [ 687188.5000,       0.0000],
         [ 661227.6875,       0.0000],
         [ 643514.3125,       0.0000]],

        [[1110532.3750,       0.0000],
         [1106611.2500,       0.0000],
         [ 967202.9375,       0.0000],
         ...,
         [ 729240.3750,       0.0000],
         [ 706463.7500,       0.0000],
         [ 690272.3125,       0.0000]],

        [[1108168.8750,       0.0000],
         [1036323.1250,       0.0000],
         [      0.0000,  960270.2500],
         ...,
         [ 874600.3750,       0.0000],
         [ 870028.2500,       0.0000],
         [ 849414.6875,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2077637.1250,   463979.2500],
        [13931617.0000,        0.0000],
        [13136221.0000,        0.0000],
        [12486762.0000,        0.0000],
        [10793502.0000,  1328962.1250],
        [14254962.0000,        0.0000],
        [13879460.0000,  1560103.2500],
        [14006884.0000,        0.0000],
        [12445888.0000,  1348409.2500],
        [ 7866800.5000,   720281.3750],
        [ 8624435.0000,  3961942.5000],
        [12098707.0000,        0.0000],
        [ 8863198.0000,  1100572.1250],
        [12809483.0000,  1466876.0000],
        [12177004.0000,        0.0000],
        [11541014.0000,        0.0000],
        [13907644.0000,        0.0000],
        [12312882.0000,  1308658.0000],
        [ 9804512.0000,  2477053.5000],
        [12051165.0000,        0.0000],
        [11892139.0000,  1271743.7500],
        [ 9231622.0000,  2444825.0000],
        [11524284.0000,        0.0000],
        [12192900.0000,        0.0000],
        [ 9873370.0000,  1093526.8750],
        [11244714.0000,        0.0000],
        [ 9556615.0000,  1276701.8750],
        [10921668.0000,        0.0000],
        [12085768.0000,        0.0000],
        [11926776.0000,        0.0000],
        [12507925.0000,        0.0000],
        [12137556.0000,        0.0000],
        [ 8362940.0000,  2146298.5000],
        [12232428.0000,        0.0000],
        [13229978.0000,        0.0000],
        [ 9991662.0000,  2445777.5000],
        [ 9591257.0000,  2297133.0000],
        [13064467.0000,        0.0000],
        [ 9584850.0000,  2329994.5000],
        [12716522.0000,  1339559.0000],
        [ 8540602.0000,        0.0000],
        [10792682.0000,        0.0000],
        [10236238.0000,        0.0000],
        [ 9375429.0000,        0.0000],
        [ 5756973.5000,  2466146.0000],
        [ 6570925.0000,   601546.7500],
        [10414370.0000,        0.0000],
        [ 5480819.0000,  2233403.5000],
        [ 9147447.0000,  1036837.2500],
        [12291270.0000,  1365337.3750],
        [ 7514007.0000,  3342039.0000],
        [ 8240320.0000,  1748698.2500],
        [ 6288469.5000,  2269091.7500],
        [ 8534232.0000,   886819.1875],
        [ 9061628.0000,   988840.0625],
        [12090754.0000,        0.0000],
        [ 4879405.5000,  4233917.5000],
        [ 7022887.0000,   851820.6875],
        [ 6689446.5000,  2406610.5000],
        [ 6084023.5000,  1615933.2500],
        [ 7363574.5000,  1948766.7500],
        [ 7468213.5000,        0.0000],
        [ 7848777.5000,   927985.3750],
        [ 7444730.5000,  1894741.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 76/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:34, 61.19s/it]  7%|▋         | 2/30 [01:01<11:57, 25.63s/it] 10%|█         | 3/30 [01:02<06:25, 14.27s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.93s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.98s/it] 20%|██        | 6/30 [01:04<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 4.375633653004964
Epoch 77/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:45, 59.49s/it]  7%|▋         | 2/30 [01:00<11:38, 24.94s/it] 10%|█         | 3/30 [01:00<06:15, 13.89s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 4.357491151491801
Epoch 78/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<31:02, 64.22s/it]  7%|▋         | 2/30 [01:04<12:32, 26.89s/it] 10%|█         | 3/30 [01:05<06:43, 14.95s/it] 13%|█▎        | 4/30 [01:06<04:02,  9.34s/it] 17%|█▋        | 5/30 [01:07<02:36,  6.24s/it] 20%|██        | 6/30 [01:07<01:45,  4.38s/it] 23%|██▎       | 7/30 [01:08<01:13,  3.19s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.41s/it] 30%|███       | 9/30 [01:10<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:26<00:00,  2.87s/it]
Epoch loss is 4.379638449350993
Epoch 79/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:13, 60.47s/it]  7%|▋         | 2/30 [01:01<11:49, 25.34s/it] 10%|█         | 3/30 [01:01<06:21, 14.11s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 4.347844839096069
Epoch 80/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:04, 60.17s/it]  7%|▋         | 2/30 [01:00<11:46, 25.22s/it] 10%|█         | 3/30 [01:01<06:19, 14.04s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.79s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 4.327135292689006
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0235,  0.0177, -0.0013,  ..., -0.0316, -0.0331, -0.0017],
        [-0.0646,  0.0346,  0.0182,  ..., -0.0085, -0.0176, -0.0293],
        [-0.0769, -0.0025, -0.0247,  ...,  0.0497,  0.0189, -0.0230],
        ...,
        [-0.0124, -0.0258, -0.0183,  ..., -0.0536, -0.0078,  0.0155],
        [-0.0781,  0.0084,  0.0267,  ..., -0.0455,  0.0348, -0.0210],
        [-0.0459,  0.0307,  0.0105,  ..., -0.0386,  0.0462, -0.0455]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8902, 0.8836, 0.8831, 0.8827, 0.8690, 0.8684, 0.8662, 0.8575, 0.8534,
         0.8486],
        [0.9944, 0.9922, 0.9911, 0.9907, 0.9906, 0.9904, 0.9889, 0.9886, 0.9885,
         0.9874],
        [0.9933, 0.9917, 0.9892, 0.9886, 0.9843, 0.9823, 0.9822, 0.9815, 0.9812,
         0.9808],
        [0.9920, 0.9828, 0.9811, 0.9799, 0.9798, 0.9797, 0.9787, 0.9772, 0.9744,
         0.9732],
        [0.9854, 0.9850, 0.9799, 0.9778, 0.9756, 0.9755, 0.9744, 0.9742, 0.9729,
         0.9726],
        [0.9928, 0.9924, 0.9911, 0.9908, 0.9907, 0.9907, 0.9906, 0.9899, 0.9890,
         0.9874],
        [0.9976, 0.9976, 0.9976, 0.9975, 0.9972, 0.9972, 0.9970, 0.9963, 0.9962,
         0.9953],
        [0.9914, 0.9913, 0.9908, 0.9899, 0.9893, 0.9889, 0.9883, 0.9873, 0.9872,
         0.9871],
        [0.9909, 0.9907, 0.9907, 0.9903, 0.9886, 0.9885, 0.9883, 0.9881, 0.9873,
         0.9864],
        [0.9781, 0.9755, 0.9586, 0.9558, 0.9525, 0.9492, 0.9462, 0.9460, 0.9453,
         0.9415],
        [0.9897, 0.9865, 0.9859, 0.9844, 0.9834, 0.9799, 0.9788, 0.9771, 0.9754,
         0.9753],
        [0.9872, 0.9866, 0.9819, 0.9810, 0.9756, 0.9736, 0.9723, 0.9687, 0.9683,
         0.9683],
        [0.9825, 0.9825, 0.9772, 0.9735, 0.9604, 0.9590, 0.9585, 0.9475, 0.9459,
         0.9361],
        [0.9929, 0.9923, 0.9920, 0.9919, 0.9914, 0.9913, 0.9913, 0.9901, 0.9900,
         0.9899],
        [0.9836, 0.9830, 0.9826, 0.9817, 0.9789, 0.9788, 0.9781, 0.9773, 0.9759,
         0.9723],
        [0.9907, 0.9865, 0.9821, 0.9797, 0.9754, 0.9720, 0.9703, 0.9697, 0.9693,
         0.9674],
        [0.9932, 0.9929, 0.9912, 0.9908, 0.9906, 0.9893, 0.9889, 0.9889, 0.9879,
         0.9879],
        [0.9929, 0.9917, 0.9895, 0.9894, 0.9890, 0.9875, 0.9854, 0.9852, 0.9834,
         0.9832],
        [0.9883, 0.9870, 0.9804, 0.9795, 0.9790, 0.9785, 0.9766, 0.9761, 0.9754,
         0.9745],
        [0.9869, 0.9817, 0.9806, 0.9805, 0.9801, 0.9797, 0.9785, 0.9772, 0.9763,
         0.9742],
        [0.9913, 0.9911, 0.9910, 0.9870, 0.9865, 0.9861, 0.9834, 0.9832, 0.9824,
         0.9808],
        [0.9903, 0.9849, 0.9837, 0.9748, 0.9741, 0.9730, 0.9717, 0.9713, 0.9683,
         0.9671],
        [0.9890, 0.9860, 0.9792, 0.9769, 0.9729, 0.9705, 0.9698, 0.9698, 0.9687,
         0.9677],
        [0.9848, 0.9839, 0.9828, 0.9828, 0.9826, 0.9804, 0.9783, 0.9778, 0.9768,
         0.9764],
        [0.9917, 0.9765, 0.9750, 0.9747, 0.9726, 0.9724, 0.9723, 0.9664, 0.9631,
         0.9625],
        [0.9840, 0.9775, 0.9759, 0.9750, 0.9721, 0.9716, 0.9711, 0.9707, 0.9706,
         0.9672],
        [0.9831, 0.9822, 0.9785, 0.9761, 0.9698, 0.9688, 0.9679, 0.9635, 0.9629,
         0.9591],
        [0.9872, 0.9811, 0.9767, 0.9738, 0.9716, 0.9708, 0.9698, 0.9670, 0.9631,
         0.9630],
        [0.9844, 0.9842, 0.9823, 0.9798, 0.9797, 0.9786, 0.9779, 0.9777, 0.9777,
         0.9765],
        [0.9853, 0.9841, 0.9835, 0.9804, 0.9778, 0.9764, 0.9746, 0.9741, 0.9729,
         0.9721],
        [0.9852, 0.9850, 0.9829, 0.9827, 0.9826, 0.9823, 0.9817, 0.9811, 0.9811,
         0.9805],
        [0.9871, 0.9862, 0.9828, 0.9818, 0.9795, 0.9774, 0.9755, 0.9750, 0.9734,
         0.9731],
        [0.9753, 0.9706, 0.9691, 0.9672, 0.9669, 0.9650, 0.9649, 0.9642, 0.9625,
         0.9619],
        [0.9847, 0.9832, 0.9795, 0.9794, 0.9787, 0.9786, 0.9786, 0.9785, 0.9779,
         0.9778],
        [0.9928, 0.9892, 0.9869, 0.9859, 0.9850, 0.9848, 0.9844, 0.9841, 0.9836,
         0.9824],
        [0.9894, 0.9857, 0.9826, 0.9825, 0.9821, 0.9812, 0.9784, 0.9783, 0.9774,
         0.9768],
        [0.9899, 0.9878, 0.9832, 0.9773, 0.9760, 0.9741, 0.9721, 0.9714, 0.9710,
         0.9706],
        [0.9886, 0.9867, 0.9855, 0.9850, 0.9833, 0.9832, 0.9829, 0.9826, 0.9825,
         0.9819],
        [0.9899, 0.9851, 0.9808, 0.9806, 0.9767, 0.9766, 0.9746, 0.9734, 0.9734,
         0.9732],
        [0.9959, 0.9939, 0.9929, 0.9916, 0.9901, 0.9894, 0.9892, 0.9884, 0.9880,
         0.9874],
        [0.9725, 0.9674, 0.9645, 0.9603, 0.9469, 0.9446, 0.9440, 0.9422, 0.9388,
         0.9385],
        [0.9781, 0.9744, 0.9708, 0.9700, 0.9696, 0.9694, 0.9690, 0.9679, 0.9648,
         0.9639],
        [0.9853, 0.9772, 0.9711, 0.9710, 0.9643, 0.9616, 0.9589, 0.9587, 0.9544,
         0.9459],
        [0.9653, 0.9650, 0.9641, 0.9628, 0.9618, 0.9605, 0.9600, 0.9596, 0.9583,
         0.9574],
        [0.9696, 0.9592, 0.9583, 0.9541, 0.9482, 0.9463, 0.9437, 0.9418, 0.9385,
         0.9377],
        [0.9481, 0.9448, 0.9367, 0.9354, 0.9322, 0.9320, 0.9317, 0.9311, 0.9292,
         0.9288],
        [0.9830, 0.9809, 0.9769, 0.9756, 0.9687, 0.9650, 0.9629, 0.9614, 0.9577,
         0.9568],
        [0.9536, 0.9472, 0.9401, 0.9401, 0.9400, 0.9366, 0.9298, 0.9272, 0.9230,
         0.9220],
        [0.9734, 0.9691, 0.9620, 0.9615, 0.9599, 0.9560, 0.9554, 0.9553, 0.9546,
         0.9545],
        [0.9929, 0.9910, 0.9890, 0.9878, 0.9872, 0.9865, 0.9862, 0.9857, 0.9841,
         0.9838],
        [0.9866, 0.9813, 0.9727, 0.9714, 0.9709, 0.9705, 0.9666, 0.9656, 0.9622,
         0.9619],
        [0.9826, 0.9816, 0.9785, 0.9681, 0.9659, 0.9653, 0.9614, 0.9527, 0.9509,
         0.9451],
        [0.9878, 0.9719, 0.9541, 0.9518, 0.9446, 0.9445, 0.9402, 0.9375, 0.9374,
         0.9330],
        [0.9853, 0.9751, 0.9691, 0.9563, 0.9543, 0.9530, 0.9514, 0.9476, 0.9398,
         0.9388],
        [0.9819, 0.9703, 0.9639, 0.9633, 0.9630, 0.9626, 0.9625, 0.9619, 0.9605,
         0.9600],
        [0.9849, 0.9844, 0.9807, 0.9797, 0.9793, 0.9783, 0.9768, 0.9759, 0.9757,
         0.9748],
        [0.9733, 0.9723, 0.9717, 0.9642, 0.9532, 0.9508, 0.9494, 0.9483, 0.9448,
         0.9429],
        [0.9658, 0.9547, 0.9464, 0.9455, 0.9438, 0.9434, 0.9403, 0.9397, 0.9355,
         0.9344],
        [0.9854, 0.9762, 0.9546, 0.9536, 0.9523, 0.9493, 0.9488, 0.9487, 0.9465,
         0.9459],
        [0.9670, 0.9639, 0.9492, 0.9455, 0.9427, 0.9423, 0.9371, 0.9343, 0.9246,
         0.9238],
        [0.9712, 0.9679, 0.9620, 0.9619, 0.9614, 0.9593, 0.9585, 0.9585, 0.9536,
         0.9519],
        [0.9636, 0.9571, 0.9457, 0.9448, 0.9417, 0.9412, 0.9379, 0.9349, 0.9330,
         0.9327],
        [0.9736, 0.9718, 0.9658, 0.9635, 0.9609, 0.9569, 0.9461, 0.9436, 0.9431,
         0.9394],
        [0.9710, 0.9697, 0.9663, 0.9636, 0.9611, 0.9587, 0.9570, 0.9553, 0.9547,
         0.9518]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 1],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 333326.8438,  303518.4062,  301277.6562,  299381.8438,  246237.0781,
          244204.1406,  236547.9375,  208882.8594,  197155.2344,  183924.0469],
        [1477710.0000, 1431106.8750, 1409395.3750, 1400725.7500, 1399219.7500,
         1396224.6250, 1365091.3750, 1360676.0000, 1357635.2500, 1336748.8750],
        [1454116.3750, 1422139.6250, 1371342.7500, 1359801.7500, 1279544.3750,
         1243044.7500, 1240128.3750, 1229344.5000, 1223647.0000, 1215752.6250],
        [1427617.2500, 1251868.3750, 1221958.5000, 1201491.1250, 1199777.0000,
         1197099.1250, 1180752.8750, 1155121.2500, 1109594.3750, 1090964.3750],
        [1298888.8750, 1290752.3750, 1201360.5000, 1164759.5000, 1129036.6250,
         1127209.8750, 1110747.3750, 1107456.7500, 1087318.6250, 1081356.0000],
        [1444903.0000, 1434980.0000, 1409597.0000, 1403017.2500, 1401946.0000,
         1401901.8750, 1400070.0000, 1385024.8750, 1368250.6250, 1336397.1250],
        [1546565.0000, 1546441.1250, 1545493.1250, 1545195.3750, 1537567.7500,
         1536733.6250, 1534144.7500, 1518046.6250, 1516547.5000, 1496977.8750],
        [1416271.1250, 1413238.1250, 1404126.8750, 1385763.5000, 1373400.2500,
         1364940.2500, 1353936.0000, 1335240.3750, 1331973.0000, 1331646.6250],
        [1404978.8750, 1401174.7500, 1400416.0000, 1394187.5000, 1360027.3750,
         1357082.5000, 1354279.5000, 1350889.6250, 1334289.5000, 1317203.6250],
        [1169666.2500, 1128485.5000,  885847.9375,  851237.6250,  812482.0625,
          774345.5000,  741723.0625,  740169.2500,  732271.8750,  693587.4375],
        [1380366.2500, 1319797.6250, 1307668.6250, 1281333.2500, 1263105.2500,
         1201487.7500, 1182390.1250, 1153512.8750, 1125593.2500, 1124913.8750],
        [1332825.6250, 1322064.0000, 1235848.6250, 1220062.7500, 1128654.5000,
         1097674.8750, 1077598.5000, 1023094.7500, 1017346.5000, 1017169.9375],
        [1247215.1250, 1246880.8750, 1156161.6250, 1095612.5000,  909213.3125,
          890552.5000,  884041.9375,  755876.3750,  738694.0625,  642596.8750],
        [1446932.8750, 1432696.5000, 1426595.1250, 1426164.0000, 1416194.1250,
         1413600.7500, 1413406.6250, 1389712.6250, 1387448.2500, 1385688.1250],
        [1266721.6250, 1256043.2500, 1248015.8750, 1232005.2500, 1184296.1250,
         1182231.1250, 1170850.3750, 1156965.7500, 1134066.6250, 1077623.1250],
        [1401385.8750, 1319900.8750, 1238675.7500, 1196616.3750, 1126541.5000,
         1072315.7500, 1046925.3125, 1038640.4375, 1031769.1250, 1004610.3125],
        [1451895.1250, 1445901.0000, 1411561.1250, 1402364.5000, 1398820.8750,
         1374215.1250, 1365481.8750, 1365420.7500, 1346754.0000, 1346149.2500],
        [1446219.6250, 1420883.0000, 1377484.8750, 1376133.7500, 1368379.8750,
         1337887.8750, 1299674.3750, 1295689.3750, 1263296.7500, 1258770.1250],
        [1354470.6250, 1328169.0000, 1209264.3750, 1194581.1250, 1184883.6250,
         1177580.5000, 1145694.8750, 1137317.8750, 1125878.7500, 1111633.3750],
        [1326908.0000, 1232525.8750, 1213102.7500, 1210515.1250, 1205080.7500,
         1197940.8750, 1176274.0000, 1156034.8750, 1141453.6250, 1106844.3750],
        [1413967.3750, 1409126.6250, 1407975.5000, 1328564.2500, 1320242.0000,
         1312621.5000, 1261997.5000, 1259395.7500, 1245071.2500, 1217194.6250],
        [1393751.5000, 1289956.2500, 1268674.2500, 1115828.7500, 1104625.7500,
         1088545.0000, 1067579.1250, 1062769.2500, 1016870.1875, 1000415.1250],
        [1367752.3750, 1310816.3750, 1188544.7500, 1150107.7500, 1086037.7500,
         1049410.3750, 1039724.5625, 1039385.5625, 1022769.9375, 1008880.8750],
        [1288006.6250, 1270878.2500, 1252321.1250, 1250816.0000, 1248080.1250,
         1209581.5000, 1173932.8750, 1165540.6250, 1148155.8750, 1142455.5000],
        [1422074.5000, 1143304.5000, 1120252.6250, 1114301.7500, 1081808.7500,
         1079355.1250, 1076606.1250,  990317.9375,  944016.8750,  937131.5000],
        [1272925.7500, 1160536.1250, 1133718.3750, 1118984.1250, 1073911.2500,
         1066257.5000, 1058812.7500, 1053422.2500, 1052216.3750, 1002331.7500],
        [1257546.2500, 1241470.2500, 1176954.0000, 1138230.5000, 1038830.6250,
         1024670.7500, 1011802.3125,  949442.1875,  941496.7500,  891870.7500],
        [1333587.2500, 1221282.7500, 1147191.6250, 1100574.2500, 1066640.8750,
         1055027.8750, 1040252.1875,  999240.3750,  944998.6250,  943292.3750],
        [1279710.3750, 1276800.5000, 1242161.8750, 1199661.6250, 1198300.8750,
         1178936.7500, 1166400.2500, 1163844.6250, 1163193.2500, 1143840.1250],
        [1297221.2500, 1274727.2500, 1263733.0000, 1210071.8750, 1165076.1250,
         1142568.7500, 1113899.0000, 1105096.7500, 1087332.0000, 1073946.1250],
        [1295709.2500, 1290789.5000, 1253276.8750, 1249162.5000, 1247807.5000,
         1243111.1250, 1231988.7500, 1221765.1250, 1221499.5000, 1210981.6250],
        [1331007.8750, 1314594.6250, 1252233.8750, 1233880.7500, 1194662.0000,
         1158654.0000, 1127602.2500, 1119847.7500, 1095148.7500, 1090057.5000],
        [1124033.3750, 1051444.0000, 1028935.2500, 1001979.1250,  997662.5625,
          971284.8125,  969387.7500,  960211.6250,  936160.5000,  928673.2500],
        [1286409.6250, 1259603.5000, 1193328.6250, 1192310.5000, 1179916.3750,
         1179016.6250, 1178761.3750, 1176716.0000, 1166719.5000, 1165757.5000],
        [1443856.1250, 1370960.8750, 1326825.7500, 1309219.7500, 1291300.3750,
         1287290.7500, 1279756.6250, 1275244.0000, 1265601.1250, 1244062.2500],
        [1375996.0000, 1304774.7500, 1247666.0000, 1247108.1250, 1238747.7500,
         1223539.6250, 1175501.2500, 1174423.3750, 1158206.6250, 1149038.7500],
        [1384454.3750, 1344763.5000, 1258527.6250, 1157817.7500, 1136143.8750,
         1106111.1250, 1074207.2500, 1063726.3750, 1057046.1250, 1051091.0000],
        [1359849.7500, 1323064.1250, 1300358.7500, 1291836.1250, 1261106.0000,
         1259521.8750, 1252641.1250, 1248196.7500, 1246726.3750, 1234970.8750],
        [1386193.1250, 1293522.6250, 1215968.1250, 1212186.7500, 1146484.0000,
         1145881.6250, 1113598.5000, 1093837.7500, 1093758.3750, 1091811.6250],
        [1509879.0000, 1467497.3750, 1445317.7500, 1418793.6250, 1388911.0000,
         1375799.1250, 1371289.1250, 1355195.5000, 1347778.0000, 1336537.3750],
        [1080866.2500, 1004105.5625,  964009.3750,  907640.0625,  749491.4375,
          725639.9375,  718565.4375,  700490.1875,  667345.5000,  664730.4375],
        [1169654.0000, 1109562.6250, 1055028.8750, 1042448.0000, 1036180.8125,
         1034165.9375, 1027487.9375, 1011590.0625,  967718.6250,  956186.3125],
        [1296768.6250, 1155426.3750, 1059076.2500, 1057452.5000,  961433.0625,
          924806.6250,  889272.6875,  887241.3750,  834478.6875,  739016.0000],
        [ 974743.2500,  970198.8750,  958481.5625,  941153.8125,  926935.4375,
          910360.3125,  903181.0625,  899159.0000,  882028.3750,  870296.3125],
        [1037099.2500,  893665.5000,  881647.4375,  830421.1875,  764012.9375,
          743314.1875,  715721.6875,  697021.0000,  664755.8125,  656711.8125],
        [ 761902.1250,  727397.0000,  648091.0000,  636074.2500,  607406.0000,
          605365.8125,  603325.5000,  597892.6875,  581966.9375,  578360.6250],
        [1254763.3750, 1218129.3750, 1149720.6250, 1128557.6250, 1023891.2500,
          970848.6250,  941299.2500,  921393.5625,  874666.3125,  863427.1875],
        [ 824847.9375,  752959.6875,  680267.6250,  680085.3750,  679402.7500,
          646791.8125,  587359.3125,  565272.3125,  532755.3750,  524865.0625],
        [1095168.5000, 1028810.6875,  929633.0000,  923099.8750,  901972.5625,
          852997.8125,  846425.1250,  844785.6250,  836386.8750,  835517.9375],
        [1446975.5000, 1406330.2500, 1368215.3750, 1344645.5000, 1333556.7500,
         1318698.0000, 1314430.3750, 1304599.3750, 1275872.8750, 1269256.3750],
        [1321063.2500, 1225798.5000, 1083180.7500, 1063746.7500, 1055332.7500,
         1049334.2500,  992460.3750,  978772.0000,  933177.0625,  928834.5000],
        [1248711.0000, 1229841.7500, 1176479.3750, 1014015.4375,  983062.0625,
          975317.8750,  922514.6250,  814433.9375,  793317.3750,  730189.6250],
        [1344753.2500, 1071767.8750,  830491.6250,  804338.5000,  724962.0625,
          723782.8125,  681099.8125,  655681.6875,  654122.7500,  614285.7500],
        [1297369.7500, 1120698.2500, 1029377.9375,  857312.8125,  832466.8750,
          817629.2500,  799156.0000,  756755.5625,  676918.6250,  668035.7500],
        [1236482.8750, 1047145.0000,  955877.2500,  947965.6250,  943136.7500,
          937766.2500,  936185.5000,  929090.5000,  909941.1250,  904093.7500],
        [1289958.7500, 1280340.2500, 1213988.0000, 1198212.7500, 1190592.3750,
         1174315.7500, 1149568.1250, 1133697.7500, 1130883.7500, 1116666.5000],
        [1093225.5000, 1077395.0000, 1067592.3750,  960209.8125,  820530.4375,
          792138.0000,  777214.6250,  764502.6875,  727564.8750,  707651.1875],
        [ 981245.8750,  837255.9375,  744598.3750,  734559.0625,  717389.7500,
          712944.2500,  682436.0000,  676271.4375,  636611.3125,  627169.6250],
        [1299140.3750, 1139706.6250,  836101.3750,  825262.6250,  809904.3750,
          775985.1875,  770212.2500,  769010.8125,  744999.0625,  738481.3125],
        [ 999214.6250,  954829.5000,  774782.0625,  734572.3750,  705890.6250,
          701459.5000,  651821.8125,  626416.4375,  544775.4375,  538705.4375],
        [1061065.7500, 1012304.2500,  930543.8750,  928864.6250,  921633.5000,
          894291.2500,  884369.9375,  884159.1875,  824786.6250,  805260.2500],
        [ 950914.6875,  867571.6250,  737133.1250,  727483.0000,  695534.1875,
          691280.8750,  659387.0625,  631081.5000,  614688.4375,  611788.8125],
        [1097530.3750, 1069346.0000,  981948.0625,  950169.5625,  915442.0625,
          864281.5625,  741117.8125,  715305.4375,  710136.3125,  673552.6875],
        [1057702.5000, 1037868.0625,  988964.6250,  950970.0000,  918273.3125,
          887467.2500,  866041.4375,  845028.1875,  838129.9375,  804007.9375]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 333326.8438,       0.0000],
         [ 303518.4062,       0.0000],
         [ 301277.6562,       0.0000],
         ...,
         [ 208882.8594,       0.0000],
         [      0.0000,  197155.2344],
         [ 183924.0469,       0.0000]],

        [[1477710.0000,       0.0000],
         [1431106.8750,       0.0000],
         [1409395.3750,       0.0000],
         ...,
         [1360676.0000,       0.0000],
         [1357635.2500,       0.0000],
         [1336748.8750,       0.0000]],

        [[1454116.3750,       0.0000],
         [1422139.6250,       0.0000],
         [1371342.7500,       0.0000],
         ...,
         [1229344.5000,       0.0000],
         [1223647.0000,       0.0000],
         [1215752.6250,       0.0000]],

        ...,

        [[ 950914.6875,       0.0000],
         [ 867571.6250,       0.0000],
         [ 737133.1250,       0.0000],
         ...,
         [ 631081.5000,       0.0000],
         [ 614688.4375,       0.0000],
         [ 611788.8125,       0.0000]],

        [[1097530.3750,       0.0000],
         [1069346.0000,       0.0000],
         [      0.0000,  981948.0625],
         ...,
         [ 715305.4375,       0.0000],
         [ 710136.3125,       0.0000],
         [ 673552.6875,       0.0000]],

        [[1057702.5000,       0.0000],
         [1037868.0625,       0.0000],
         [      0.0000,  988964.6250],
         ...,
         [ 845028.1875,       0.0000],
         [ 838129.9375,       0.0000],
         [ 804007.9375,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2111063.7500,   443392.3125],
        [13934535.0000,        0.0000],
        [13038862.0000,        0.0000],
        [12036244.0000,        0.0000],
        [10308134.0000,  1290752.3750],
        [13986088.0000,        0.0000],
        [13777148.0000,  1546565.0000],
        [12378890.0000,  1331646.6250],
        [12314502.0000,  1360027.3750],
        [ 7797545.0000,   732271.8750],
        [ 8469744.0000,  3870425.0000],
        [11472340.0000,        0.0000],
        [ 8471233.0000,  1095612.5000],
        [12724838.0000,  1413600.7500],
        [11908819.0000,        0.0000],
        [11477381.0000,        0.0000],
        [13908563.0000,        0.0000],
        [12181123.0000,  1263296.7500],
        [ 9575326.0000,  2394148.0000],
        [10859836.0000,  1106844.3750],
        [11914158.0000,  1261997.5000],
        [ 9035715.0000,  2373300.0000],
        [11263430.0000,        0.0000],
        [12149768.0000,        0.0000],
        [ 9827361.0000,  1081808.7500],
        [10993116.0000,        0.0000],
        [ 9430844.0000,  1241470.2500],
        [10852088.0000,        0.0000],
        [12012850.0000,        0.0000],
        [11733672.0000,        0.0000],
        [12466092.0000,        0.0000],
        [11917690.0000,        0.0000],
        [ 7920666.0000,  2049106.5000],
        [11978540.0000,        0.0000],
        [13094118.0000,        0.0000],
        [ 9815804.0000,  2479198.0000],
        [ 8367934.5000,  3265954.7500],
        [12778272.0000,        0.0000],
        [10581056.0000,  1212186.7500],
        [12669220.0000,  1347778.0000],
        [ 8182884.0000,        0.0000],
        [10410023.0000,        0.0000],
        [ 9804972.0000,        0.0000],
        [ 9236538.0000,        0.0000],
        [ 5495396.0000,  2388974.5000],
        [ 5744456.5000,   603325.5000],
        [10346697.0000,        0.0000],
        [ 4526895.0000,  1947712.3750],
        [ 5587782.5000,  3507015.5000],
        [12037934.0000,  1344645.5000],
        [ 8318177.0000,  2313523.5000],
        [ 7441861.5000,  2446021.5000],
        [ 6470456.0000,  1634830.1250],
        [ 8023254.0000,   832466.8750],
        [ 8818594.0000,   929090.5000],
        [11878224.0000,        0.0000],
        [ 4589601.5000,  4198423.0000],
        [ 6513226.0000,   837255.9375],
        [ 6269957.0000,  2438847.0000],
        [ 5606837.0000,  1625631.0000],
        [ 7250605.0000,  1896674.2500],
        [ 7186863.5000,        0.0000],
        [ 7736882.0000,   981948.0625],
        [ 7287216.0000,  1907238.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 81/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:40, 61.39s/it]  7%|▋         | 2/30 [01:02<12:00, 25.72s/it] 10%|█         | 3/30 [01:02<06:26, 14.32s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:04<02:29,  6.00s/it] 20%|██        | 6/30 [01:05<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 4.364207251866659
Epoch 82/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:04, 62.22s/it]  7%|▋         | 2/30 [01:02<12:09, 26.06s/it] 10%|█         | 3/30 [01:03<06:31, 14.51s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.07s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.07s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 4.288507827123007
Epoch 83/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:16, 60.56s/it]  7%|▋         | 2/30 [01:01<11:50, 25.38s/it] 10%|█         | 3/30 [01:02<06:21, 14.13s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 4.2917747338612875
Epoch 84/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:36, 61.24s/it]  7%|▋         | 2/30 [01:01<11:58, 25.66s/it] 10%|█         | 3/30 [01:02<06:25, 14.28s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.94s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.99s/it] 20%|██        | 6/30 [01:04<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 4.299962719281514
Epoch 85/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<30:58, 64.09s/it]  7%|▋         | 2/30 [01:05<12:41, 27.19s/it] 10%|█         | 3/30 [01:06<06:48, 15.12s/it] 13%|█▎        | 4/30 [01:06<04:05,  9.44s/it] 17%|█▋        | 5/30 [01:07<02:37,  6.31s/it] 20%|██        | 6/30 [01:08<01:46,  4.42s/it] 23%|██▎       | 7/30 [01:09<01:14,  3.22s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.43s/it] 30%|███       | 9/30 [01:10<00:40,  1.91s/it] 33%|███▎      | 10/30 [01:11<00:30,  1.55s/it] 37%|███▋      | 11/30 [01:12<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:14,  1.07it/s] 50%|█████     | 15/30 [01:15<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:18<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:21<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:24<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:26<00:00,  1.33it/s]100%|██████████| 30/30 [01:26<00:00,  2.89s/it]
Epoch loss is 4.2781410932540895
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0168,  0.0155, -0.0046,  ..., -0.0292, -0.0375,  0.0020],
        [-0.0647,  0.0343,  0.0188,  ..., -0.0075, -0.0207, -0.0261],
        [-0.0760, -0.0017, -0.0241,  ...,  0.0486,  0.0118, -0.0263],
        ...,
        [-0.0105, -0.0252, -0.0236,  ..., -0.0558, -0.0083,  0.0118],
        [-0.0743,  0.0096,  0.0206,  ..., -0.0488,  0.0334, -0.0203],
        [-0.0452,  0.0321,  0.0091,  ..., -0.0399,  0.0438, -0.0488]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8985, 0.8812, 0.8810, 0.8800, 0.8687, 0.8655, 0.8634, 0.8511, 0.8489,
         0.8473],
        [0.9940, 0.9909, 0.9907, 0.9905, 0.9904, 0.9897, 0.9891, 0.9888, 0.9880,
         0.9868],
        [0.9936, 0.9905, 0.9883, 0.9843, 0.9832, 0.9823, 0.9809, 0.9802, 0.9795,
         0.9793],
        [0.9916, 0.9811, 0.9782, 0.9779, 0.9778, 0.9768, 0.9760, 0.9752, 0.9706,
         0.9667],
        [0.9841, 0.9831, 0.9777, 0.9756, 0.9749, 0.9730, 0.9719, 0.9707, 0.9706,
         0.9704],
        [0.9917, 0.9914, 0.9894, 0.9891, 0.9888, 0.9887, 0.9878, 0.9878, 0.9877,
         0.9850],
        [0.9970, 0.9969, 0.9967, 0.9967, 0.9966, 0.9966, 0.9964, 0.9952, 0.9951,
         0.9939],
        [0.9916, 0.9899, 0.9898, 0.9874, 0.9873, 0.9865, 0.9860, 0.9856, 0.9856,
         0.9845],
        [0.9905, 0.9899, 0.9899, 0.9894, 0.9894, 0.9888, 0.9882, 0.9878, 0.9875,
         0.9871],
        [0.9794, 0.9786, 0.9578, 0.9545, 0.9545, 0.9472, 0.9469, 0.9465, 0.9465,
         0.9455],
        [0.9875, 0.9858, 0.9856, 0.9839, 0.9800, 0.9772, 0.9761, 0.9759, 0.9747,
         0.9734],
        [0.9854, 0.9849, 0.9801, 0.9800, 0.9725, 0.9699, 0.9687, 0.9651, 0.9643,
         0.9623],
        [0.9816, 0.9803, 0.9766, 0.9716, 0.9580, 0.9579, 0.9545, 0.9414, 0.9411,
         0.9336],
        [0.9928, 0.9926, 0.9918, 0.9912, 0.9901, 0.9901, 0.9900, 0.9895, 0.9893,
         0.9889],
        [0.9833, 0.9819, 0.9795, 0.9795, 0.9778, 0.9760, 0.9757, 0.9751, 0.9738,
         0.9720],
        [0.9901, 0.9853, 0.9819, 0.9780, 0.9738, 0.9708, 0.9702, 0.9696, 0.9671,
         0.9669],
        [0.9935, 0.9923, 0.9907, 0.9900, 0.9900, 0.9888, 0.9886, 0.9876, 0.9874,
         0.9864],
        [0.9926, 0.9894, 0.9893, 0.9888, 0.9881, 0.9869, 0.9854, 0.9850, 0.9825,
         0.9815],
        [0.9866, 0.9850, 0.9794, 0.9777, 0.9776, 0.9752, 0.9747, 0.9734, 0.9723,
         0.9711],
        [0.9854, 0.9824, 0.9806, 0.9791, 0.9790, 0.9785, 0.9772, 0.9764, 0.9750,
         0.9749],
        [0.9913, 0.9909, 0.9908, 0.9881, 0.9857, 0.9843, 0.9823, 0.9822, 0.9817,
         0.9809],
        [0.9881, 0.9841, 0.9830, 0.9745, 0.9727, 0.9716, 0.9702, 0.9697, 0.9656,
         0.9636],
        [0.9894, 0.9862, 0.9808, 0.9760, 0.9705, 0.9686, 0.9686, 0.9661, 0.9660,
         0.9654],
        [0.9853, 0.9849, 0.9844, 0.9841, 0.9820, 0.9796, 0.9787, 0.9768, 0.9762,
         0.9753],
        [0.9906, 0.9764, 0.9756, 0.9745, 0.9705, 0.9696, 0.9695, 0.9668, 0.9635,
         0.9606],
        [0.9839, 0.9746, 0.9743, 0.9736, 0.9717, 0.9702, 0.9681, 0.9679, 0.9663,
         0.9658],
        [0.9822, 0.9806, 0.9770, 0.9738, 0.9694, 0.9693, 0.9658, 0.9649, 0.9622,
         0.9580],
        [0.9862, 0.9818, 0.9746, 0.9737, 0.9735, 0.9706, 0.9695, 0.9637, 0.9630,
         0.9620],
        [0.9848, 0.9845, 0.9812, 0.9806, 0.9785, 0.9782, 0.9781, 0.9764, 0.9763,
         0.9744],
        [0.9845, 0.9837, 0.9819, 0.9798, 0.9770, 0.9765, 0.9745, 0.9725, 0.9719,
         0.9709],
        [0.9846, 0.9834, 0.9832, 0.9831, 0.9829, 0.9811, 0.9806, 0.9803, 0.9802,
         0.9795],
        [0.9856, 0.9849, 0.9829, 0.9807, 0.9766, 0.9753, 0.9751, 0.9744, 0.9733,
         0.9707],
        [0.9773, 0.9674, 0.9642, 0.9631, 0.9622, 0.9620, 0.9598, 0.9577, 0.9568,
         0.9559],
        [0.9825, 0.9824, 0.9789, 0.9782, 0.9780, 0.9767, 0.9761, 0.9761, 0.9760,
         0.9759],
        [0.9921, 0.9877, 0.9852, 0.9850, 0.9842, 0.9826, 0.9826, 0.9820, 0.9819,
         0.9815],
        [0.9881, 0.9861, 0.9831, 0.9819, 0.9812, 0.9810, 0.9793, 0.9777, 0.9776,
         0.9762],
        [0.9888, 0.9852, 0.9818, 0.9744, 0.9729, 0.9722, 0.9716, 0.9700, 0.9682,
         0.9677],
        [0.9863, 0.9838, 0.9828, 0.9828, 0.9821, 0.9809, 0.9807, 0.9806, 0.9804,
         0.9786],
        [0.9895, 0.9855, 0.9802, 0.9799, 0.9771, 0.9744, 0.9741, 0.9725, 0.9721,
         0.9721],
        [0.9955, 0.9931, 0.9931, 0.9920, 0.9893, 0.9889, 0.9887, 0.9885, 0.9881,
         0.9880],
        [0.9712, 0.9644, 0.9614, 0.9598, 0.9444, 0.9437, 0.9407, 0.9364, 0.9363,
         0.9314],
        [0.9752, 0.9672, 0.9671, 0.9664, 0.9656, 0.9641, 0.9637, 0.9627, 0.9620,
         0.9618],
        [0.9821, 0.9746, 0.9664, 0.9661, 0.9642, 0.9573, 0.9570, 0.9530, 0.9484,
         0.9425],
        [0.9625, 0.9618, 0.9616, 0.9605, 0.9598, 0.9572, 0.9562, 0.9556, 0.9551,
         0.9550],
        [0.9664, 0.9587, 0.9543, 0.9528, 0.9439, 0.9422, 0.9383, 0.9376, 0.9358,
         0.9358],
        [0.9337, 0.9294, 0.9292, 0.9268, 0.9244, 0.9225, 0.9160, 0.9147, 0.9140,
         0.9121],
        [0.9824, 0.9799, 0.9779, 0.9760, 0.9682, 0.9642, 0.9640, 0.9583, 0.9580,
         0.9578],
        [0.9450, 0.9422, 0.9374, 0.9374, 0.9342, 0.9288, 0.9211, 0.9178, 0.9135,
         0.9115],
        [0.9681, 0.9650, 0.9577, 0.9553, 0.9545, 0.9519, 0.9514, 0.9507, 0.9493,
         0.9490],
        [0.9917, 0.9890, 0.9870, 0.9869, 0.9858, 0.9853, 0.9850, 0.9850, 0.9847,
         0.9816],
        [0.9842, 0.9805, 0.9719, 0.9709, 0.9706, 0.9702, 0.9662, 0.9642, 0.9612,
         0.9610],
        [0.9815, 0.9805, 0.9782, 0.9661, 0.9659, 0.9635, 0.9581, 0.9535, 0.9494,
         0.9460],
        [0.9848, 0.9708, 0.9538, 0.9500, 0.9415, 0.9360, 0.9329, 0.9320, 0.9283,
         0.9276],
        [0.9828, 0.9755, 0.9648, 0.9529, 0.9522, 0.9483, 0.9446, 0.9395, 0.9355,
         0.9334],
        [0.9816, 0.9683, 0.9632, 0.9621, 0.9607, 0.9598, 0.9589, 0.9588, 0.9579,
         0.9574],
        [0.9843, 0.9839, 0.9789, 0.9776, 0.9766, 0.9760, 0.9743, 0.9737, 0.9733,
         0.9720],
        [0.9709, 0.9697, 0.9692, 0.9664, 0.9529, 0.9482, 0.9425, 0.9419, 0.9402,
         0.9361],
        [0.9655, 0.9530, 0.9448, 0.9431, 0.9388, 0.9373, 0.9371, 0.9366, 0.9353,
         0.9323],
        [0.9841, 0.9771, 0.9486, 0.9444, 0.9442, 0.9438, 0.9432, 0.9423, 0.9410,
         0.9404],
        [0.9688, 0.9552, 0.9468, 0.9405, 0.9364, 0.9360, 0.9323, 0.9303, 0.9202,
         0.9188],
        [0.9696, 0.9660, 0.9616, 0.9615, 0.9609, 0.9577, 0.9571, 0.9566, 0.9518,
         0.9514],
        [0.9616, 0.9540, 0.9475, 0.9418, 0.9382, 0.9368, 0.9327, 0.9281, 0.9272,
         0.9270],
        [0.9732, 0.9681, 0.9678, 0.9615, 0.9593, 0.9573, 0.9501, 0.9419, 0.9393,
         0.9383],
        [0.9696, 0.9680, 0.9661, 0.9637, 0.9601, 0.9591, 0.9551, 0.9542, 0.9512,
         0.9506]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 375289.5312,  293124.6250,  292534.0000,  288380.4062,  245325.5156,
          234244.4531,  227200.8125,  190696.1875,  184705.9062,  180525.1406],
        [1467873.8750, 1404564.8750, 1401278.8750, 1397830.0000, 1395764.0000,
         1380721.6250, 1369293.6250, 1363736.7500, 1347540.2500, 1325610.3750],
        [1459463.7500, 1397840.6250, 1353569.3750, 1277974.8750, 1259491.7500,
         1242038.7500, 1217973.6250, 1205816.5000, 1194633.6250, 1190297.3750],
        [1419366.1250, 1221692.8750, 1172457.1250, 1167839.3750, 1164917.2500,
         1148134.0000, 1136417.0000, 1122618.2500, 1050816.5000,  994118.0625],
        [1275673.3750, 1256697.5000, 1163899.0000, 1129427.6250, 1117533.7500,
         1087581.0000, 1070545.1250, 1053548.8750, 1051874.2500, 1048710.1250],
        [1421400.7500, 1415297.6250, 1376328.0000, 1369601.8750, 1363202.2500,
         1362493.8750, 1344499.2500, 1344445.5000, 1342335.3750, 1291814.0000],
        [1532711.5000, 1529944.1250, 1526321.1250, 1525947.1250, 1525305.5000,
         1523716.3750, 1519774.7500, 1494926.3750, 1491636.7500, 1466610.2500],
        [1420014.6250, 1385509.7500, 1384265.7500, 1337408.1250, 1335249.2500,
         1320516.5000, 1310194.0000, 1303180.6250, 1303007.8750, 1281992.1250],
        [1397852.6250, 1385383.0000, 1384511.1250, 1375388.5000, 1375274.3750,
         1363840.7500, 1351848.5000, 1345072.5000, 1338058.8750, 1331763.5000],
        [1192114.8750, 1178292.6250,  875541.7500,  835543.3750,  835507.5625,
          753122.6875,  749869.6250,  745042.3750,  744815.0625,  735091.6875],
        [1338694.3750, 1306793.3750, 1303420.5000, 1271141.1250, 1202243.1250,
         1155914.6250, 1137141.1250, 1134361.7500, 1115412.7500, 1094745.6250],
        [1299905.0000, 1289749.6250, 1203986.0000, 1202939.2500, 1080088.2500,
         1041730.5000, 1023598.3125,  971504.4375,  961276.2500,  933917.8125],
        [1229796.0000, 1207511.5000, 1146219.5000, 1066574.7500,  878078.7500,
          876905.5000,  834933.1875,  692404.5000,  689799.8125,  620010.1250],
        [1443356.3750, 1438933.6250, 1422626.6250, 1412014.8750, 1390184.5000,
         1388585.2500, 1386444.2500, 1377604.3750, 1373515.3750, 1366086.2500],
        [1260543.2500, 1236018.3750, 1194305.5000, 1193888.6250, 1165211.7500,
         1136586.1250, 1131542.8750, 1120971.7500, 1100656.0000, 1072680.8750],
        [1388520.2500, 1297986.1250, 1235412.6250, 1169268.0000, 1100026.3750,
         1054239.3750, 1045245.3125, 1036855.0625, 1000419.9375,  996681.1875],
        [1457706.8750, 1434632.5000, 1401208.1250, 1387277.5000, 1386859.5000,
         1363968.2500, 1359958.7500, 1341076.3750, 1336369.1250, 1318197.5000],
        [1439050.2500, 1376236.1250, 1373916.2500, 1364210.2500, 1350898.6250,
         1328087.8750, 1299000.3750, 1291964.2500, 1246523.0000, 1228168.0000],
        [1321161.5000, 1291555.3750, 1191879.6250, 1164513.0000, 1162866.0000,
         1122519.7500, 1115589.3750, 1094263.3750, 1077955.1250, 1058685.5000],
        [1298176.7500, 1244599.8750, 1212678.1250, 1186961.2500, 1185030.5000,
         1177220.0000, 1155651.1250, 1142687.5000, 1119947.0000, 1117445.2500],
        [1412776.0000, 1405978.8750, 1404102.8750, 1349294.3750, 1305339.8750,
         1279050.2500, 1243161.0000, 1241257.1250, 1232293.2500, 1218501.1250],
        [1349720.2500, 1275493.3750, 1255092.5000, 1111141.6250, 1084100.5000,
         1065970.7500, 1045909.4375, 1038601.7500,  978942.8750,  950935.5000],
        [1375477.7500, 1313380.2500, 1217286.2500, 1136553.6250, 1050083.1250,
         1022164.3125, 1021894.3125,  985998.1250,  984029.1875,  975935.6875],
        [1297758.3750, 1290017.7500, 1281528.8750, 1275335.2500, 1237864.3750,
         1195374.3750, 1180804.6250, 1149501.2500, 1138752.7500, 1124462.3750],
        [1400100.8750, 1142962.2500, 1129107.7500, 1110951.8750, 1049551.5000,
         1036235.1875, 1035591.0625,  996419.8750,  949632.3750,  912019.1875],
        [1271306.1250, 1113716.3750, 1108009.3750, 1098015.1250, 1068691.6250,
         1044941.3125, 1014927.7500, 1011923.0000,  989375.8750,  982258.9375],
        [1240193.5000, 1213083.0000, 1152388.1250, 1100371.7500, 1033819.8750,
         1031584.2500,  982303.8750,  968764.8750,  931922.2500,  877796.5625],
        [1313245.0000, 1233499.5000, 1112833.0000, 1099573.3750, 1096629.6250,
         1051929.3750, 1035437.0000,  953428.1875,  943609.0625,  930175.6875],
        [1288165.1250, 1281817.3750, 1223302.8750, 1213565.5000, 1176603.8750,
         1172905.6250, 1170624.8750, 1142403.1250, 1140107.8750, 1109558.5000],
        [1282509.3750, 1268221.8750, 1235019.1250, 1198775.2500, 1152325.3750,
         1143470.2500, 1111744.6250, 1081013.7500, 1071315.1250, 1056426.3750],
        [1284629.5000, 1261926.5000, 1259194.0000, 1257591.8750, 1254232.2500,
         1220822.8750, 1212592.6250, 1208117.3750, 1206760.8750, 1193425.3750],
        [1302315.8750, 1289342.6250, 1254235.7500, 1214313.3750, 1145179.2500,
         1124986.8750, 1121454.1250, 1110176.6250, 1093602.0000, 1052965.2500],
        [1156294.0000, 1004413.0000,  960140.2500,  944662.5625,  932215.5625,
          929519.4375,  900660.0625,  875110.1875,  863406.6875,  852673.3125],
        [1246740.6250, 1244417.1250, 1183711.2500, 1171847.8750, 1169563.6250,
         1147500.2500, 1138077.3750, 1137725.8750, 1136308.6250, 1133749.7500],
        [1430220.0000, 1342777.0000, 1294546.8750, 1291902.6250, 1277171.8750,
         1247607.6250, 1247447.0000, 1237687.3750, 1235617.5000, 1229500.5000],
        [1349196.5000, 1311477.7500, 1256175.0000, 1235360.7500, 1222675.3750,
         1219240.5000, 1190133.8750, 1163331.8750, 1161687.7500, 1139627.2500],
        [1364238.8750, 1295778.3750, 1234702.3750, 1110392.6250, 1087126.7500,
         1075126.6250, 1066070.3750, 1041836.6875, 1016290.5625, 1008322.1250],
        [1315323.2500, 1269337.5000, 1251922.2500, 1251056.8750, 1239701.5000,
         1218079.5000, 1214172.1250, 1212804.2500, 1208713.2500, 1178406.2500],
        [1376641.7500, 1301572.2500, 1206047.6250, 1200994.0000, 1153869.3750,
         1110247.6250, 1105958.1250, 1079887.5000, 1074451.1250, 1074050.5000],
        [1499637.1250, 1450909.6250, 1449305.3750, 1428193.2500, 1373962.1250,
         1366301.2500, 1362334.1250, 1358141.5000, 1349824.6250, 1349100.0000],
        [1060473.0000,  962807.5625,  921487.6250,  901728.3125,  723268.0625,
          715820.6875,  686201.5625,  645115.6875,  643866.1250,  600790.0000],
        [1122120.5000, 1001436.4375,  999924.8750,  989897.7500,  978339.0000,
          957830.9375,  952822.8125,  939399.8125,  930141.9375,  927415.6875],
        [1238461.8750, 1113821.5000,  989863.8125,  986045.1250,  959216.7500,
          869007.5000,  865513.0625,  817679.1875,  765657.6875,  704219.6875],
        [ 936529.3750,  926911.6250,  924258.1875,  910432.3750,  901387.8750,
          867727.1250,  856193.4375,  849187.0625,  842139.1875,  841594.0625],
        [ 990709.0625,  887043.3750,  832905.2500,  814931.1250,  717907.8750,
          700488.1250,  663304.3750,  656098.8750,  639410.1250,  639389.4375],
        [ 621091.3125,  583893.2500,  582412.2500,  562193.3750,  543370.8125,
          529145.1250,  481682.3125,  473030.2188,  468159.5312,  455582.5625],
        [1244849.1250, 1201507.2500, 1167720.2500, 1135006.7500, 1016448.5000,
          959476.5625,  957153.3750,  882552.6250,  877783.2500,  875757.1250],
        [ 729452.5625,  700873.7500,  654751.2500,  653949.9375,  625088.6250,
          578603.4375,  518122.3125,  494464.5938,  464817.1562,  452288.1250],
        [1015211.3750,  971245.0000,  874026.6875,  844885.5000,  835702.0000,
          805145.8125,  799356.4375,  790820.1250,  775603.3750,  772833.1250],
        [1422377.0000, 1367558.0000, 1329489.5000, 1327015.5000, 1306716.2500,
         1297170.6250, 1291833.7500, 1290999.8750, 1285561.0000, 1229840.5000],
        [1277712.7500, 1210402.0000, 1071737.2500, 1055281.5000, 1050742.2500,
         1045632.1250,  986798.6875,  960285.8125,  918889.1875,  916788.4375],
        [1228889.7500, 1210377.7500, 1171268.0000,  986020.6875,  983514.0625,
          950180.3750,  878979.4375,  824082.9375,  776887.0000,  739831.9375],
        [1288181.1250, 1054326.7500,  827599.5000,  783629.3750,  693663.5625,
          641112.5000,  613521.1875,  606205.7500,  574896.7500,  568637.0000],
        [1252079.7500, 1128047.6250,  968023.2500,  817012.7500,  808320.9375,
          764707.5625,  725119.0000,  674403.6875,  637158.5000,  618298.8750],
        [1230360.2500, 1016878.9375,  945783.0000,  931915.1250,  912308.8750,
          900531.2500,  889271.0000,  888020.9375,  876987.5000,  870806.0625],
        [1278827.0000, 1271475.8750, 1183450.5000, 1162347.1250, 1145926.5000,
         1135494.0000, 1108807.3750, 1098797.6250, 1093541.5000, 1071969.2500],
        [1055323.7500, 1037438.5625, 1031107.1875,  990884.7500,  816047.1875,
          764061.6875,  704149.2500,  697549.0000,  681505.9375,  642644.1250],
        [ 977381.2500,  817769.6875,  727287.3750,  709866.8125,  667775.2500,
          653151.5625,  651677.6250,  646498.9375,  635188.5625,  608756.6250],
        [1275947.1250, 1153187.2500,  767992.0625,  723616.5000,  720785.7500,
          717365.8125,  710514.3125,  702187.6875,  688917.5000,  682639.0625],
        [1025464.5625,  843452.5000,  748419.3125,  684157.6250,  645226.3750,
          640965.8125,  608043.5000,  591538.2500,  511967.8125,  501783.5312],
        [1037223.9375,  984680.6875,  925043.0000,  923518.1250,  914914.9375,
          874547.8750,  867272.9375,  861487.0000,  803942.0000,  799072.9375],
        [ 924860.4375,  829423.1250,  755628.4375,  696620.2500,  662333.4375,
          648869.6250,  612183.3750,  572946.5625,  565876.3750,  564166.0625],
        [1090608.6250, 1013970.9375, 1010183.5625,  922737.2500,  894472.9375,
          869979.3125,  784479.5625,  697441.1875,  672057.0000,  662649.9375],
        [1036918.2500, 1013031.5000,  985755.5000,  952790.1250,  904807.0625,
          892306.3125,  842157.6875,  831894.7500,  796910.8125,  790498.8750]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 375289.5312,       0.0000],
         [ 293124.6250,       0.0000],
         [ 292534.0000,       0.0000],
         ...,
         [ 190696.1875,       0.0000],
         [ 184705.9062,       0.0000],
         [      0.0000,  180525.1406]],

        [[1467873.8750,       0.0000],
         [1404564.8750,       0.0000],
         [1401278.8750,       0.0000],
         ...,
         [1363736.7500,       0.0000],
         [1347540.2500,       0.0000],
         [1325610.3750,       0.0000]],

        [[1459463.7500,       0.0000],
         [1397840.6250,       0.0000],
         [1353569.3750,       0.0000],
         ...,
         [1205816.5000,       0.0000],
         [1194633.6250,       0.0000],
         [1190297.3750,       0.0000]],

        ...,

        [[ 924860.4375,       0.0000],
         [ 829423.1250,       0.0000],
         [ 755628.4375,       0.0000],
         ...,
         [ 572946.5625,       0.0000],
         [ 565876.3750,       0.0000],
         [ 564166.0625,       0.0000]],

        [[1090608.6250,       0.0000],
         [      0.0000, 1013970.9375],
         [1010183.5625,       0.0000],
         ...,
         [ 697441.1875,       0.0000],
         [ 672057.0000,       0.0000],
         [ 662649.9375,       0.0000]],

        [[1036918.2500,       0.0000],
         [      0.0000, 1013031.5000],
         [ 985755.5000,       0.0000],
         ...,
         [ 831894.7500,       0.0000],
         [ 796910.8125,       0.0000],
         [ 790498.8750,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2104300.5000,   407725.9375],
        [13854215.0000,        0.0000],
        [12799100.0000,        0.0000],
        [11598376.0000,        0.0000],
        [ 8946918.0000,  2308571.7500],
        [13631418.0000,        0.0000],
        [13606950.0000,  1529944.1250],
        [12078158.0000,  1303180.6250],
        [12285153.0000,  1363840.7500],
        [ 7156727.0000,  1488214.3750],
        [ 8298968.0000,  3760900.5000],
        [11008695.0000,        0.0000],
        [ 8175658.5000,  1066574.7500],
        [13999352.0000,        0.0000],
        [11612406.0000,        0.0000],
        [11324654.0000,        0.0000],
        [13787255.0000,        0.0000],
        [12069886.0000,  1228168.0000],
        [ 9273609.0000,  2327379.0000],
        [10722952.0000,  1117445.2500],
        [11873254.0000,  1218501.1250],
        [ 8834846.0000,  2321063.2500],
        [11082803.0000,        0.0000],
        [12171400.0000,        0.0000],
        [ 9713021.0000,  1049551.5000],
        [10703165.0000,        0.0000],
        [ 9319145.0000,  1213083.0000],
        [10770360.0000,        0.0000],
        [11919054.0000,        0.0000],
        [11600821.0000,        0.0000],
        [12359293.0000,        0.0000],
        [11708572.0000,        0.0000],
        [ 6606612.5000,  2812482.2500],
        [11709642.0000,        0.0000],
        [12834478.0000,        0.0000],
        [ 9774097.0000,  2474809.5000],
        [ 8139334.0000,  3160551.5000],
        [12359517.0000,        0.0000],
        [10482726.0000,  1200994.0000],
        [12625375.0000,  1362334.1250],
        [ 7861558.0000,        0.0000],
        [ 9799330.0000,        0.0000],
        [ 9309486.0000,        0.0000],
        [ 8856360.0000,        0.0000],
        [ 5236748.5000,  2305439.5000],
        [ 4718148.5000,   582412.2500],
        [10318255.0000,        0.0000],
        [ 4145393.7500,  1727018.0000],
        [ 4397118.0000,  4087711.5000],
        [11856728.0000,  1291833.7500],
        [ 8229758.0000,  2264511.5000],
        [ 8046969.5000,  1703062.3750],
        [ 5471908.0000,  2179866.0000],
        [ 7584850.5000,   808320.9375],
        [ 8592057.0000,   870806.0625],
        [11550637.0000,        0.0000],
        [ 4305957.0000,  4114754.2500],
        [ 5642396.0000,  1452958.2500],
        [ 5714019.0000,  2429134.5000],
        [ 4665727.5000,  2135291.5000],
        [ 7145536.0000,  1846167.7500],
        [ 6832908.0000,        0.0000],
        [ 7604609.5000,  1013970.9375],
        [ 7129232.5000,  1917838.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 86/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:38, 59.25s/it]  7%|▋         | 2/30 [01:00<11:35, 24.84s/it] 10%|█         | 3/30 [01:00<06:13, 13.84s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.67s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.82s/it] 20%|██        | 6/30 [01:03<01:38,  4.09s/it] 23%|██▎       | 7/30 [01:03<01:08,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 4.227592285474142
Epoch 87/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:58, 59.94s/it]  7%|▋         | 2/30 [01:00<11:43, 25.12s/it] 10%|█         | 3/30 [01:01<06:17, 13.99s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.77s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 4.278825887044271
Epoch 88/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:26, 56.78s/it]  7%|▋         | 2/30 [01:01<12:11, 26.14s/it] 10%|█         | 3/30 [01:02<06:32, 14.55s/it] 13%|█▎        | 4/30 [01:02<03:56,  9.10s/it] 17%|█▋        | 5/30 [01:03<02:32,  6.09s/it] 20%|██        | 6/30 [01:04<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:05<00:52,  2.37s/it] 30%|███       | 9/30 [01:06<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 4.236699517567953
Epoch 89/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:37, 61.29s/it]  7%|▋         | 2/30 [01:03<12:24, 26.60s/it] 10%|█         | 3/30 [01:04<06:39, 14.80s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.25s/it] 17%|█▋        | 5/30 [01:05<02:34,  6.19s/it] 20%|██        | 6/30 [01:06<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.40s/it] 30%|███       | 9/30 [01:08<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Epoch loss is 4.246391717592875
Epoch 90/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:42, 61.45s/it]  7%|▋         | 2/30 [01:02<12:00, 25.75s/it] 10%|█         | 3/30 [01:02<06:26, 14.33s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.97s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.01s/it] 20%|██        | 6/30 [01:05<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 4.228298759460449
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0141,  0.0160, -0.0105,  ..., -0.0270, -0.0381,  0.0055],
        [-0.0642,  0.0343,  0.0181,  ..., -0.0050, -0.0225, -0.0240],
        [-0.0760, -0.0008, -0.0215,  ...,  0.0459,  0.0039, -0.0307],
        ...,
        [-0.0067, -0.0235, -0.0288,  ..., -0.0573, -0.0091,  0.0095],
        [-0.0703,  0.0117,  0.0144,  ..., -0.0499,  0.0330, -0.0188],
        [-0.0441,  0.0336,  0.0064,  ..., -0.0405,  0.0413, -0.0515]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9076, 0.8754, 0.8738, 0.8711, 0.8647, 0.8630, 0.8580, 0.8483, 0.8479,
         0.8400],
        [0.9936, 0.9903, 0.9896, 0.9889, 0.9889, 0.9887, 0.9881, 0.9878, 0.9865,
         0.9860],
        [0.9921, 0.9911, 0.9850, 0.9823, 0.9797, 0.9794, 0.9776, 0.9772, 0.9754,
         0.9751],
        [0.9910, 0.9789, 0.9761, 0.9757, 0.9747, 0.9740, 0.9734, 0.9721, 0.9683,
         0.9657],
        [0.9824, 0.9810, 0.9759, 0.9751, 0.9735, 0.9727, 0.9722, 0.9694, 0.9676,
         0.9673],
        [0.9902, 0.9900, 0.9874, 0.9872, 0.9870, 0.9860, 0.9860, 0.9846, 0.9842,
         0.9829],
        [0.9960, 0.9959, 0.9959, 0.9959, 0.9954, 0.9953, 0.9952, 0.9937, 0.9934,
         0.9922],
        [0.9922, 0.9883, 0.9877, 0.9849, 0.9844, 0.9841, 0.9840, 0.9837, 0.9831,
         0.9828],
        [0.9901, 0.9899, 0.9893, 0.9891, 0.9889, 0.9887, 0.9883, 0.9880, 0.9876,
         0.9875],
        [0.9804, 0.9799, 0.9554, 0.9543, 0.9540, 0.9516, 0.9492, 0.9467, 0.9467,
         0.9427],
        [0.9848, 0.9846, 0.9843, 0.9830, 0.9764, 0.9755, 0.9754, 0.9737, 0.9728,
         0.9725],
        [0.9841, 0.9837, 0.9788, 0.9788, 0.9697, 0.9662, 0.9655, 0.9623, 0.9613,
         0.9613],
        [0.9815, 0.9802, 0.9749, 0.9646, 0.9587, 0.9555, 0.9525, 0.9399, 0.9327,
         0.9323],
        [0.9929, 0.9920, 0.9911, 0.9908, 0.9908, 0.9892, 0.9892, 0.9891, 0.9883,
         0.9880],
        [0.9833, 0.9829, 0.9778, 0.9771, 0.9758, 0.9754, 0.9741, 0.9735, 0.9732,
         0.9724],
        [0.9886, 0.9843, 0.9816, 0.9756, 0.9715, 0.9705, 0.9696, 0.9682, 0.9677,
         0.9666],
        [0.9935, 0.9916, 0.9900, 0.9889, 0.9885, 0.9881, 0.9869, 0.9868, 0.9862,
         0.9857],
        [0.9937, 0.9912, 0.9899, 0.9877, 0.9857, 0.9857, 0.9853, 0.9843, 0.9829,
         0.9812],
        [0.9845, 0.9834, 0.9794, 0.9766, 0.9737, 0.9713, 0.9707, 0.9699, 0.9696,
         0.9678],
        [0.9847, 0.9826, 0.9815, 0.9793, 0.9775, 0.9774, 0.9763, 0.9763, 0.9761,
         0.9757],
        [0.9909, 0.9905, 0.9885, 0.9873, 0.9861, 0.9838, 0.9825, 0.9821, 0.9810,
         0.9810],
        [0.9858, 0.9842, 0.9823, 0.9749, 0.9730, 0.9703, 0.9689, 0.9661, 0.9648,
         0.9618],
        [0.9896, 0.9858, 0.9816, 0.9742, 0.9674, 0.9672, 0.9666, 0.9656, 0.9647,
         0.9628],
        [0.9856, 0.9855, 0.9850, 0.9846, 0.9801, 0.9795, 0.9783, 0.9766, 0.9752,
         0.9744],
        [0.9878, 0.9785, 0.9743, 0.9723, 0.9695, 0.9680, 0.9677, 0.9657, 0.9640,
         0.9593],
        [0.9825, 0.9737, 0.9717, 0.9710, 0.9695, 0.9683, 0.9659, 0.9655, 0.9651,
         0.9643],
        [0.9816, 0.9788, 0.9742, 0.9714, 0.9695, 0.9691, 0.9653, 0.9636, 0.9602,
         0.9580],
        [0.9853, 0.9816, 0.9743, 0.9725, 0.9717, 0.9711, 0.9675, 0.9625, 0.9620,
         0.9614],
        [0.9859, 0.9847, 0.9817, 0.9798, 0.9789, 0.9780, 0.9760, 0.9755, 0.9746,
         0.9725],
        [0.9840, 0.9833, 0.9800, 0.9799, 0.9787, 0.9770, 0.9764, 0.9730, 0.9729,
         0.9719],
        [0.9833, 0.9833, 0.9827, 0.9820, 0.9818, 0.9804, 0.9798, 0.9798, 0.9792,
         0.9780],
        [0.9846, 0.9840, 0.9831, 0.9794, 0.9781, 0.9759, 0.9742, 0.9740, 0.9720,
         0.9680],
        [0.9789, 0.9636, 0.9604, 0.9595, 0.9588, 0.9540, 0.9529, 0.9520, 0.9512,
         0.9510],
        [0.9839, 0.9814, 0.9791, 0.9765, 0.9752, 0.9752, 0.9749, 0.9747, 0.9738,
         0.9736],
        [0.9908, 0.9848, 0.9844, 0.9839, 0.9824, 0.9807, 0.9802, 0.9801, 0.9801,
         0.9796],
        [0.9859, 0.9853, 0.9824, 0.9818, 0.9808, 0.9792, 0.9789, 0.9785, 0.9776,
         0.9770],
        [0.9875, 0.9822, 0.9780, 0.9714, 0.9714, 0.9712, 0.9701, 0.9690, 0.9637,
         0.9633],
        [0.9826, 0.9806, 0.9789, 0.9788, 0.9785, 0.9784, 0.9779, 0.9779, 0.9769,
         0.9766],
        [0.9889, 0.9859, 0.9795, 0.9790, 0.9769, 0.9744, 0.9730, 0.9727, 0.9721,
         0.9712],
        [0.9946, 0.9934, 0.9920, 0.9916, 0.9894, 0.9891, 0.9885, 0.9884, 0.9883,
         0.9874],
        [0.9706, 0.9606, 0.9565, 0.9557, 0.9439, 0.9428, 0.9362, 0.9339, 0.9285,
         0.9280],
        [0.9707, 0.9655, 0.9626, 0.9612, 0.9599, 0.9585, 0.9577, 0.9560, 0.9556,
         0.9551],
        [0.9778, 0.9699, 0.9625, 0.9615, 0.9605, 0.9569, 0.9531, 0.9471, 0.9425,
         0.9408],
        [0.9575, 0.9572, 0.9571, 0.9566, 0.9560, 0.9558, 0.9541, 0.9525, 0.9518,
         0.9515],
        [0.9637, 0.9579, 0.9486, 0.9461, 0.9386, 0.9380, 0.9320, 0.9307, 0.9307,
         0.9296],
        [0.9229, 0.9229, 0.9146, 0.9121, 0.9084, 0.9029, 0.9015, 0.9008, 0.8949,
         0.8945],
        [0.9807, 0.9778, 0.9775, 0.9752, 0.9675, 0.9644, 0.9638, 0.9585, 0.9579,
         0.9537],
        [0.9446, 0.9405, 0.9340, 0.9323, 0.9322, 0.9185, 0.9132, 0.9059, 0.9052,
         0.9021],
        [0.9643, 0.9638, 0.9559, 0.9531, 0.9523, 0.9501, 0.9485, 0.9480, 0.9478,
         0.9446],
        [0.9909, 0.9877, 0.9863, 0.9850, 0.9841, 0.9833, 0.9832, 0.9830, 0.9817,
         0.9790],
        [0.9810, 0.9794, 0.9719, 0.9709, 0.9704, 0.9699, 0.9639, 0.9630, 0.9621,
         0.9606],
        [0.9804, 0.9779, 0.9774, 0.9653, 0.9647, 0.9617, 0.9547, 0.9543, 0.9475,
         0.9438],
        [0.9810, 0.9695, 0.9523, 0.9449, 0.9385, 0.9372, 0.9256, 0.9244, 0.9224,
         0.9214],
        [0.9786, 0.9752, 0.9581, 0.9544, 0.9463, 0.9411, 0.9410, 0.9304, 0.9281,
         0.9267],
        [0.9810, 0.9663, 0.9642, 0.9612, 0.9606, 0.9600, 0.9581, 0.9577, 0.9558,
         0.9554],
        [0.9836, 0.9829, 0.9779, 0.9767, 0.9741, 0.9738, 0.9733, 0.9730, 0.9717,
         0.9705],
        [0.9675, 0.9669, 0.9665, 0.9661, 0.9513, 0.9469, 0.9380, 0.9355, 0.9350,
         0.9348],
        [0.9659, 0.9524, 0.9444, 0.9413, 0.9370, 0.9368, 0.9367, 0.9348, 0.9343,
         0.9304],
        [0.9814, 0.9762, 0.9410, 0.9396, 0.9392, 0.9391, 0.9374, 0.9349, 0.9348,
         0.9344],
        [0.9689, 0.9467, 0.9421, 0.9340, 0.9307, 0.9300, 0.9272, 0.9231, 0.9163,
         0.9161],
        [0.9684, 0.9630, 0.9608, 0.9604, 0.9591, 0.9556, 0.9541, 0.9532, 0.9508,
         0.9495],
        [0.9578, 0.9509, 0.9464, 0.9375, 0.9371, 0.9310, 0.9257, 0.9239, 0.9223,
         0.9209],
        [0.9720, 0.9694, 0.9600, 0.9597, 0.9577, 0.9573, 0.9526, 0.9393, 0.9384,
         0.9330],
        [0.9696, 0.9690, 0.9640, 0.9612, 0.9579, 0.9576, 0.9524, 0.9522, 0.9501,
         0.9462]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 0, 1, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 1],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 427525.7500,  269972.3750,  263706.1562,  253791.4688,  231766.7969,
          225966.5312,  210472.3750,  183364.1406,  182098.4531,  162741.2969],
        [1460616.7500, 1392935.5000, 1379518.6250, 1366272.6250, 1365526.2500,
         1361139.3750, 1349205.6250, 1344730.1250, 1320385.5000, 1310445.1250],
        [1429374.7500, 1409879.3750, 1292111.0000, 1242752.0000, 1197456.6250,
         1192014.8750, 1161862.7500, 1155719.5000, 1126843.3750, 1122015.7500],
        [1407803.5000, 1183956.2500, 1136984.0000, 1131518.1250, 1114492.0000,
         1104071.7500, 1094562.8750, 1074086.3750, 1017171.8750,  980912.8750],
        [1245045.1250, 1219603.2500, 1134460.3750, 1122037.1250, 1096562.7500,
         1083220.1250, 1076064.2500, 1033656.1875, 1007913.4375, 1002760.1250],
        [1391008.1250, 1388084.7500, 1337236.0000, 1332271.5000, 1329735.5000,
         1311096.3750, 1310651.3750, 1284601.3750, 1276546.0000, 1253957.1250],
        [1511994.3750, 1510352.8750, 1509393.7500, 1508658.3750, 1498672.0000,
         1496556.8750, 1493581.1250, 1462803.8750, 1456275.7500, 1430612.8750],
        [1430582.8750, 1354804.0000, 1341581.6250, 1289841.8750, 1279771.3750,
         1275881.5000, 1274199.7500, 1268027.0000, 1256756.1250, 1251604.6250],
        [1389177.2500, 1384334.3750, 1373167.0000, 1369510.3750, 1366099.3750,
         1361077.0000, 1354822.1250, 1347754.8750, 1340663.2500, 1339444.0000],
        [1208636.0000, 1200491.2500,  845948.1875,  833518.6875,  829692.0625,
          801137.6875,  774700.7500,  747182.0000,  747109.3125,  706342.5000],
        [1287791.7500, 1284406.6250, 1278648.8750, 1254696.3750, 1142696.2500,
         1127194.8750, 1125332.3750, 1098615.3750, 1084806.8750, 1081065.2500],
        [1276015.3750, 1267691.0000, 1181718.2500, 1181375.6250, 1038471.0000,
          987801.4375,  977139.8750,  934524.5625,  920445.9375,  920017.6875],
        [1229262.5000, 1206865.6250, 1118589.3750,  965703.3750,  886718.5625,
          847635.1875,  812382.8750,  677971.6875,  611650.5625,  608729.9375],
        [1446728.5000, 1427915.5000, 1408445.5000, 1402818.0000, 1402705.5000,
         1372032.1250, 1370861.5000, 1370151.8750, 1353226.0000, 1348611.1250],
        [1261080.7500, 1252920.7500, 1165389.5000, 1153388.5000, 1133069.7500,
         1125387.1250, 1105689.1250, 1095675.2500, 1091288.0000, 1078724.3750],
        [1359865.3750, 1279477.2500, 1231190.1250, 1129442.7500, 1064478.3750,
         1050201.2500, 1036538.6250, 1016385.5000, 1008502.8125,  992863.6250],
        [1459145.0000, 1419100.8750, 1387437.5000, 1366335.1250, 1357570.3750,
         1349589.0000, 1327954.8750, 1324610.6250, 1314904.2500, 1304317.0000],
        [1463196.0000, 1411398.1250, 1385002.5000, 1342649.0000, 1305265.2500,
         1304221.1250, 1297113.6250, 1278152.7500, 1254015.7500, 1222987.8750],
        [1281546.0000, 1262322.5000, 1192001.2500, 1145469.7500, 1098495.8750,
         1061545.6250, 1053577.0000, 1040422.8125, 1036458.5625, 1010356.0000],
        [1286331.1250, 1247846.8750, 1227884.6250, 1191401.1250, 1160103.5000,
         1159395.6250, 1141450.2500, 1141063.8750, 1137853.8750, 1130525.7500],
        [1405820.6250, 1397531.5000, 1358506.8750, 1333884.8750, 1311255.2500,
         1268898.0000, 1246259.1250, 1238387.5000, 1219927.8750, 1219078.7500],
        [1306312.5000, 1276390.1250, 1243477.5000, 1118664.0000, 1088803.5000,
         1047579.5000, 1026603.5000,  986289.6875,  968401.8750,  927463.3750],
        [1380202.8750, 1306133.1250, 1230587.8750, 1106988.0000, 1004088.3125,
         1002222.8125,  992476.5000,  979165.9375,  965960.3750,  940095.2500],
        [1303413.0000, 1301323.8750, 1291689.6250, 1283835.8750, 1204968.1250,
         1194133.5000, 1174210.6250, 1145515.7500, 1123063.7500, 1110814.1250],
        [1344760.8750, 1177491.7500, 1108377.1250, 1077307.6250, 1035355.1250,
         1013808.5000, 1008701.9375,  980106.8125,  957546.9375,  895023.3750],
        [1245644.8750, 1098473.8750, 1068106.7500, 1058004.2500, 1035791.6250,
         1016867.3750,  982612.1875,  977412.9375,  971647.0625,  961221.3125],
        [1229729.1250, 1182766.7500, 1106528.8750, 1063474.8750, 1034388.8750,
         1028776.3750,  974396.5625,  950778.6875,  906066.0000,  877897.8750],
        [1297525.7500, 1231124.3750, 1109006.2500, 1080345.8750, 1068177.0000,
         1059687.5000, 1005850.8750,  936124.8125,  929326.2500,  921688.8750],
        [1308459.5000, 1286594.8750, 1232753.8750, 1199433.8750, 1184396.6250,
         1168756.3750, 1136374.7500, 1128044.3750, 1113169.5000, 1080174.8750],
        [1273602.1250, 1261314.1250, 1201901.5000, 1200080.3750, 1181029.8750,
         1152739.7500, 1141950.0000, 1088506.5000, 1086058.3750, 1071778.0000],
        [1260989.3750, 1260534.8750, 1249913.2500, 1237303.8750, 1234222.1250,
         1210006.1250, 1198955.7500, 1198595.7500, 1189301.0000, 1169487.7500],
        [1284923.6250, 1272499.6250, 1257510.2500, 1191959.2500, 1170369.2500,
         1133838.3750, 1107620.5000, 1103301.3750, 1072068.3750, 1013847.1875],
        [1183592.7500,  951074.2500,  909424.0625,  897180.3750,  888020.1250,
          829753.0000,  816464.4375,  805992.4375,  797019.5625,  795029.7500],
        [1270611.6250, 1227297.0000, 1187041.5000, 1143881.5000, 1123205.1250,
         1122950.2500, 1118021.0000, 1114635.3750, 1100890.1250, 1097287.6250],
        [1402325.6250, 1288273.3750, 1280927.6250, 1272269.1250, 1244572.6250,
         1215483.6250, 1205343.8750, 1205035.8750, 1203917.0000, 1195219.2500],
        [1309255.8750, 1297092.6250, 1243804.8750, 1233832.5000, 1217010.0000,
         1189296.5000, 1184061.2500, 1177650.1250, 1162316.1250, 1151353.2500],
        [1338117.5000, 1241017.0000, 1168894.6250, 1064247.0000, 1064023.7500,
         1060569.1250, 1044138.4375, 1027477.1875,  953117.3125,  947930.3750],
        [1248295.5000, 1212578.6250, 1183887.3750, 1181605.6250, 1176489.3750,
         1175594.3750, 1167670.1250, 1166557.1250, 1150768.1250, 1145336.5000],
        [1365609.5000, 1308726.6250, 1194163.1250, 1186147.5000, 1149696.5000,
         1110198.8750, 1087763.5000, 1083712.8750, 1074483.8750, 1060774.3750],
        [1480765.6250, 1457107.8750, 1426612.7500, 1419478.3750, 1375016.0000,
         1368764.8750, 1357091.5000, 1355900.0000, 1353513.8750, 1337269.1250],
        [1051746.8750,  912149.6875,  859085.5625,  849724.1875,  717682.6250,
          706855.9375,  643153.6250,  622054.5625,  576414.6875,  572149.9375],
        [1052348.7500,  977772.8125,  938334.3125,  919339.6875,  902152.3750,
          885052.5000,  874612.0000,  853935.4375,  849276.9375,  843138.8750],
        [1166053.2500, 1040435.7500,  937145.7500,  923355.1875,  910694.6250,
          864752.3125,  819480.1250,  751777.9375,  703824.2500,  687103.9375],
        [ 871993.6250,  868004.4375,  866768.6250,  861401.5625,  853125.5000,
          851077.7500,  831076.3750,  811521.0625,  804248.0000,  800756.5625],
        [ 953090.0625,  877273.5625,  767897.5625,  741116.4375,  665319.0625,
          660050.1875,  605768.8750,  594746.0625,  594666.6875,  585159.1875],
        [ 532191.2500,  532064.3750,  472694.6875,  455764.5938,  432682.4375,
          399771.0625,  391689.0000,  387747.8750,  356742.6875,  354451.5625],
        [1215463.8750, 1165954.2500, 1160915.7500, 1123570.3750, 1005946.7500,
          962855.3125,  954737.5000,  884816.1875,  876647.9375,  825387.7500],
        [ 725635.0625,  684023.8750,  623335.5000,  608494.2500,  607741.5000,
          499346.0625,  463261.1562,  417507.8125,  413048.0625,  395085.6875],
        [ 961059.0625,  954319.6875,  852825.3750,  818481.2500,  809933.6875,
          784198.3750,  767275.3125,  761192.5625,  759316.1250,  724914.3750],
        [1405909.1250, 1341946.2500, 1316098.6250, 1292066.5000, 1275051.8750,
         1261517.3750, 1258098.0000, 1255251.7500, 1231373.3750, 1185340.2500],
        [1220451.5000, 1192039.8750, 1070556.2500, 1055640.7500, 1047919.2500,
         1041387.8125,  955339.5625,  943259.1250,  930792.3750,  912084.5000],
        [1208886.1250, 1167245.8750, 1158775.5000,  974284.1250,  966850.6250,
          925959.1875,  837374.1250,  833374.0625,  755475.6875,  717362.4375],
        [1220130.2500, 1034485.5000,  809114.6250,  728650.8750,  664922.6250,
          652665.8750,  553195.0625,  543206.6250,  528278.9375,  520903.2500],
        [1179505.8750, 1123103.3750,  880036.2500,  834188.2500,  743139.8750,
          690365.8125,  688778.2500,  592468.6250,  572744.4375,  561976.2500],
        [1220278.1250,  989224.8750,  959600.1250,  919303.8125,  911794.0000,
          903461.9375,  879918.8125,  874501.9375,  851581.9375,  846343.5625],
        [1265467.2500, 1253903.3750, 1166621.5000, 1146590.0000, 1105804.1250,
         1101347.0000, 1092522.0000, 1088744.2500, 1068294.1250, 1049242.2500],
        [1005558.3125,  996852.2500,  991104.0625,  985571.3125,  798420.1250,
          749759.5000,  659957.0625,  636495.9375,  632454.5625,  630404.7500],
        [ 982974.9375,  810812.4375,  722930.8750,  692329.8750,  650809.3750,
          648379.6875,  648045.2500,  630321.2500,  626175.1875,  592019.0625],
        [1227304.0000, 1139693.6250,  688633.1250,  675664.1875,  671867.3750,
          670720.0625,  654318.6875,  631849.3125,  630590.6250,  626732.5625],
        [1026229.5625,  747311.6875,  699900.5625,  623119.1875,  594341.2500,
          588330.2500,  565468.0000,  533245.3750,  484308.7500,  483026.9062],
        [1018744.5625,  943838.6250,  914253.8125,  908499.1250,  892489.3125,
          849200.8125,  830934.5000,  819885.0625,  792531.6875,  778034.8125],
        [ 876238.3750,  793224.3125,  743640.3750,  654927.3750,  652002.7500,
          596994.7500,  553769.3750,  539451.9375,  527668.1250,  516925.4375],
        [1072812.8750, 1033437.3750,  904156.6250,  899838.4375,  874314.3125,
          869126.8125,  813221.6250,  672304.4375,  663796.6875,  614206.1250],
        [1036837.2500, 1028425.1875,  956236.5000,  919147.6875,  877164.7500,
          873620.0625,  811105.5000,  808613.2500,  785043.8750,  741951.5625]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 427525.7500,       0.0000],
         [ 269972.3750,       0.0000],
         [ 263706.1562,       0.0000],
         ...,
         [ 183364.1406,       0.0000],
         [ 182098.4531,       0.0000],
         [      0.0000,  162741.2969]],

        [[1460616.7500,       0.0000],
         [1392935.5000,       0.0000],
         [1379518.6250,       0.0000],
         ...,
         [1344730.1250,       0.0000],
         [1320385.5000,       0.0000],
         [1310445.1250,       0.0000]],

        [[1429374.7500,       0.0000],
         [1409879.3750,       0.0000],
         [1292111.0000,       0.0000],
         ...,
         [1155719.5000,       0.0000],
         [1126843.3750,       0.0000],
         [1122015.7500,       0.0000]],

        ...,

        [[ 876238.3750,       0.0000],
         [ 793224.3125,       0.0000],
         [ 743640.3750,       0.0000],
         ...,
         [ 539451.9375,       0.0000],
         [ 527668.1250,       0.0000],
         [ 516925.4375,       0.0000]],

        [[1072812.8750,       0.0000],
         [      0.0000, 1033437.3750],
         [ 904156.6250,       0.0000],
         ...,
         [ 672304.4375,       0.0000],
         [ 663796.6875,       0.0000],
         [ 614206.1250,       0.0000]],

        [[1036837.2500,       0.0000],
         [      0.0000, 1028425.1875],
         [ 956236.5000,       0.0000],
         ...,
         [ 808613.2500,       0.0000],
         [ 785043.8750,       0.0000],
         [ 741951.5625,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2038191.6250,   373213.6875],
        [13650776.0000,        0.0000],
        [12330030.0000,        0.0000],
        [11245560.0000,        0.0000],
        [ 8725656.0000,  2295667.5000],
        [13215188.0000,        0.0000],
        [13369507.0000,  1509393.7500],
        [11748851.0000,  1274199.7500],
        [12271227.0000,  1354822.1250],
        [ 7146438.5000,  1548319.7500],
        [ 9198814.0000,  2566440.5000],
        [10685201.0000,        0.0000],
        [ 7999805.5000,   965703.3750],
        [13903496.0000,        0.0000],
        [11462614.0000,        0.0000],
        [11168945.0000,        0.0000],
        [13610966.0000,        0.0000],
        [13264002.0000,        0.0000],
        [ 8938230.0000,  2243965.5000],
        [10693331.0000,  1130525.7500],
        [11779622.0000,  1219927.8750],
        [ 8719905.0000,  2270081.0000],
        [10907922.0000,        0.0000],
        [12132968.0000,        0.0000],
        [ 9589778.0000,  1008701.9375],
        [10415782.0000,        0.0000],
        [ 9172037.0000,  1182766.7500],
        [10638858.0000,        0.0000],
        [11838158.0000,        0.0000],
        [11658961.0000,        0.0000],
        [12209310.0000,        0.0000],
        [11607938.0000,        0.0000],
        [ 6228464.0000,  2645086.7500],
        [11505821.0000,        0.0000],
        [12513368.0000,        0.0000],
        [ 9706264.0000,  2459408.7500],
        [ 8817808.0000,  2091724.2500],
        [10633188.0000,  1175594.3750],
        [10435129.0000,  1186147.5000],
        [12556504.0000,  1375016.0000],
        [ 6934603.0000,   576414.6875],
        [ 9095963.0000,        0.0000],
        [ 8804623.0000,        0.0000],
        [ 8419974.0000,        0.0000],
        [ 4896726.0000,  2148361.5000],
        [ 3783735.2500,   532064.3750],
        [10176296.0000,        0.0000],
        [ 3916590.2500,  1520888.3750],
        [ 4211419.5000,  3982096.5000],
        [11591280.0000,  1231373.3750],
        [ 8193680.0000,  2175791.0000],
        [ 7874839.5000,  1670748.2500],
        [ 4646302.5000,  2609251.0000],
        [ 7032119.0000,   834188.2500],
        [ 9356009.0000,        0.0000],
        [11338536.0000,        0.0000],
        [ 4107492.0000,  3979085.7500],
        [ 5543176.0000,  1461621.7500],
        [ 4561743.0000,  3055630.5000],
        [ 4241684.0000,  2103597.7500],
        [ 6195604.0000,  2552808.0000],
        [ 6454843.0000,        0.0000],
        [ 7383778.0000,  1033437.3750],
        [ 6936100.5000,  1902045.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 91/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:46, 59.54s/it]  7%|▋         | 2/30 [01:00<11:38, 24.96s/it] 10%|█         | 3/30 [01:01<06:15, 13.91s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 4.229606866836548
Epoch 92/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:12, 62.50s/it]  7%|▋         | 2/30 [01:03<12:15, 26.25s/it] 10%|█         | 3/30 [01:04<06:34, 14.61s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.14s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.11s/it] 20%|██        | 6/30 [01:06<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 4.201344378789266
Epoch 93/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:43, 61.51s/it]  7%|▋         | 2/30 [01:02<12:01, 25.77s/it] 10%|█         | 3/30 [01:03<06:27, 14.35s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.98s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.01s/it] 20%|██        | 6/30 [01:05<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:06<01:10,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 4.192355036735535
Epoch 94/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:46, 59.55s/it]  7%|▋         | 2/30 [01:00<11:43, 25.14s/it] 10%|█         | 3/30 [01:01<06:18, 14.00s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.77s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 4.167520181337992
Epoch 95/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:27, 60.96s/it]  7%|▋         | 2/30 [01:01<11:55, 25.54s/it] 10%|█         | 3/30 [01:02<06:23, 14.22s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.90s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.96s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 4.1595072189966835
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0153,  0.0162, -0.0174,  ..., -0.0244, -0.0365,  0.0077],
        [-0.0622,  0.0335,  0.0162,  ..., -0.0016, -0.0242, -0.0211],
        [-0.0768, -0.0026, -0.0209,  ...,  0.0451, -0.0016, -0.0350],
        ...,
        [-0.0048, -0.0230, -0.0328,  ..., -0.0586, -0.0101,  0.0075],
        [-0.0675,  0.0116,  0.0093,  ..., -0.0493,  0.0328, -0.0174],
        [-0.0442,  0.0329,  0.0040,  ..., -0.0403,  0.0390, -0.0534]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9123, 0.8801, 0.8733, 0.8640, 0.8594, 0.8587, 0.8542, 0.8520, 0.8438,
         0.8398],
        [0.9932, 0.9895, 0.9887, 0.9886, 0.9873, 0.9867, 0.9866, 0.9863, 0.9860,
         0.9850],
        [0.9911, 0.9896, 0.9819, 0.9816, 0.9772, 0.9772, 0.9753, 0.9737, 0.9725,
         0.9723],
        [0.9901, 0.9767, 0.9744, 0.9739, 0.9724, 0.9717, 0.9708, 0.9678, 0.9656,
         0.9651],
        [0.9804, 0.9786, 0.9747, 0.9733, 0.9731, 0.9719, 0.9706, 0.9667, 0.9667,
         0.9641],
        [0.9881, 0.9878, 0.9851, 0.9848, 0.9845, 0.9840, 0.9838, 0.9812, 0.9806,
         0.9798],
        [0.9950, 0.9950, 0.9949, 0.9949, 0.9940, 0.9937, 0.9934, 0.9919, 0.9912,
         0.9899],
        [0.9926, 0.9872, 0.9855, 0.9827, 0.9822, 0.9821, 0.9814, 0.9812, 0.9805,
         0.9804],
        [0.9911, 0.9902, 0.9889, 0.9887, 0.9887, 0.9884, 0.9882, 0.9877, 0.9873,
         0.9872],
        [0.9811, 0.9805, 0.9553, 0.9547, 0.9546, 0.9543, 0.9504, 0.9490, 0.9479,
         0.9431],
        [0.9834, 0.9818, 0.9811, 0.9807, 0.9760, 0.9749, 0.9731, 0.9714, 0.9713,
         0.9696],
        [0.9829, 0.9824, 0.9778, 0.9773, 0.9662, 0.9616, 0.9615, 0.9603, 0.9597,
         0.9586],
        [0.9823, 0.9810, 0.9725, 0.9623, 0.9557, 0.9539, 0.9519, 0.9373, 0.9349,
         0.9342],
        [0.9928, 0.9914, 0.9907, 0.9903, 0.9901, 0.9894, 0.9884, 0.9883, 0.9883,
         0.9883],
        [0.9840, 0.9820, 0.9773, 0.9753, 0.9751, 0.9751, 0.9730, 0.9727, 0.9724,
         0.9718],
        [0.9864, 0.9827, 0.9807, 0.9725, 0.9702, 0.9684, 0.9679, 0.9661, 0.9653,
         0.9640],
        [0.9928, 0.9907, 0.9894, 0.9881, 0.9880, 0.9863, 0.9862, 0.9856, 0.9856,
         0.9854],
        [0.9939, 0.9913, 0.9894, 0.9860, 0.9859, 0.9859, 0.9845, 0.9839, 0.9835,
         0.9817],
        [0.9828, 0.9813, 0.9798, 0.9756, 0.9712, 0.9674, 0.9671, 0.9662, 0.9653,
         0.9639],
        [0.9840, 0.9829, 0.9817, 0.9794, 0.9775, 0.9774, 0.9772, 0.9759, 0.9759,
         0.9752],
        [0.9904, 0.9895, 0.9885, 0.9861, 0.9856, 0.9841, 0.9837, 0.9834, 0.9823,
         0.9819],
        [0.9842, 0.9822, 0.9802, 0.9762, 0.9733, 0.9697, 0.9666, 0.9634, 0.9615,
         0.9606],
        [0.9889, 0.9849, 0.9804, 0.9727, 0.9674, 0.9666, 0.9653, 0.9652, 0.9638,
         0.9634],
        [0.9862, 0.9860, 0.9858, 0.9851, 0.9813, 0.9789, 0.9781, 0.9744, 0.9740,
         0.9739],
        [0.9875, 0.9795, 0.9719, 0.9695, 0.9688, 0.9666, 0.9656, 0.9628, 0.9608,
         0.9596],
        [0.9810, 0.9727, 0.9701, 0.9662, 0.9656, 0.9653, 0.9648, 0.9642, 0.9630,
         0.9624],
        [0.9813, 0.9777, 0.9721, 0.9702, 0.9689, 0.9681, 0.9653, 0.9613, 0.9590,
         0.9582],
        [0.9817, 0.9808, 0.9721, 0.9697, 0.9683, 0.9666, 0.9623, 0.9613, 0.9607,
         0.9595],
        [0.9880, 0.9856, 0.9833, 0.9793, 0.9790, 0.9773, 0.9753, 0.9723, 0.9722,
         0.9721],
        [0.9832, 0.9825, 0.9789, 0.9785, 0.9779, 0.9778, 0.9763, 0.9744, 0.9719,
         0.9710],
        [0.9831, 0.9821, 0.9814, 0.9804, 0.9802, 0.9796, 0.9791, 0.9784, 0.9783,
         0.9772],
        [0.9838, 0.9834, 0.9834, 0.9794, 0.9779, 0.9751, 0.9746, 0.9736, 0.9706,
         0.9690],
        [0.9781, 0.9566, 0.9558, 0.9557, 0.9543, 0.9520, 0.9482, 0.9454, 0.9430,
         0.9429],
        [0.9840, 0.9808, 0.9794, 0.9752, 0.9744, 0.9742, 0.9735, 0.9727, 0.9719,
         0.9714],
        [0.9897, 0.9838, 0.9837, 0.9830, 0.9818, 0.9793, 0.9793, 0.9785, 0.9784,
         0.9782],
        [0.9833, 0.9833, 0.9822, 0.9817, 0.9815, 0.9807, 0.9803, 0.9784, 0.9768,
         0.9765],
        [0.9860, 0.9798, 0.9740, 0.9699, 0.9697, 0.9692, 0.9688, 0.9672, 0.9628,
         0.9588],
        [0.9808, 0.9789, 0.9780, 0.9766, 0.9758, 0.9749, 0.9747, 0.9746, 0.9741,
         0.9733],
        [0.9884, 0.9858, 0.9789, 0.9782, 0.9758, 0.9740, 0.9729, 0.9724, 0.9707,
         0.9701],
        [0.9941, 0.9936, 0.9915, 0.9899, 0.9892, 0.9891, 0.9888, 0.9884, 0.9878,
         0.9876],
        [0.9699, 0.9561, 0.9521, 0.9507, 0.9445, 0.9423, 0.9314, 0.9310, 0.9253,
         0.9213],
        [0.9638, 0.9633, 0.9599, 0.9588, 0.9531, 0.9528, 0.9512, 0.9510, 0.9509,
         0.9502],
        [0.9727, 0.9640, 0.9599, 0.9575, 0.9559, 0.9548, 0.9478, 0.9421, 0.9398,
         0.9357],
        [0.9574, 0.9516, 0.9512, 0.9507, 0.9498, 0.9491, 0.9489, 0.9488, 0.9484,
         0.9474],
        [0.9596, 0.9514, 0.9426, 0.9389, 0.9373, 0.9297, 0.9223, 0.9209, 0.9207,
         0.9206],
        [0.9188, 0.9106, 0.9037, 0.9014, 0.8968, 0.8914, 0.8907, 0.8899, 0.8843,
         0.8819],
        [0.9783, 0.9775, 0.9751, 0.9746, 0.9667, 0.9642, 0.9631, 0.9587, 0.9570,
         0.9512],
        [0.9442, 0.9380, 0.9358, 0.9279, 0.9243, 0.9091, 0.9044, 0.8972, 0.8936,
         0.8927],
        [0.9595, 0.9590, 0.9531, 0.9499, 0.9482, 0.9474, 0.9465, 0.9456, 0.9440,
         0.9439],
        [0.9894, 0.9867, 0.9838, 0.9830, 0.9819, 0.9811, 0.9808, 0.9794, 0.9785,
         0.9776],
        [0.9788, 0.9779, 0.9715, 0.9704, 0.9699, 0.9687, 0.9627, 0.9604, 0.9603,
         0.9592],
        [0.9779, 0.9770, 0.9739, 0.9639, 0.9630, 0.9581, 0.9533, 0.9516, 0.9471,
         0.9419],
        [0.9780, 0.9678, 0.9511, 0.9386, 0.9372, 0.9356, 0.9229, 0.9211, 0.9204,
         0.9204],
        [0.9726, 0.9726, 0.9526, 0.9480, 0.9403, 0.9369, 0.9285, 0.9208, 0.9197,
         0.9192],
        [0.9815, 0.9682, 0.9647, 0.9643, 0.9611, 0.9607, 0.9604, 0.9579, 0.9570,
         0.9558],
        [0.9839, 0.9827, 0.9789, 0.9769, 0.9736, 0.9729, 0.9722, 0.9720, 0.9718,
         0.9716],
        [0.9682, 0.9662, 0.9644, 0.9625, 0.9508, 0.9461, 0.9359, 0.9343, 0.9333,
         0.9318],
        [0.9667, 0.9507, 0.9434, 0.9402, 0.9389, 0.9370, 0.9368, 0.9330, 0.9309,
         0.9306],
        [0.9792, 0.9727, 0.9427, 0.9390, 0.9378, 0.9350, 0.9317, 0.9305, 0.9291,
         0.9288],
        [0.9688, 0.9377, 0.9349, 0.9294, 0.9288, 0.9227, 0.9167, 0.9162, 0.9148,
         0.9126],
        [0.9665, 0.9595, 0.9594, 0.9587, 0.9565, 0.9516, 0.9508, 0.9507, 0.9504,
         0.9495],
        [0.9541, 0.9466, 0.9445, 0.9357, 0.9337, 0.9250, 0.9220, 0.9218, 0.9168,
         0.9140],
        [0.9708, 0.9689, 0.9564, 0.9555, 0.9539, 0.9529, 0.9504, 0.9344, 0.9337,
         0.9313],
        [0.9685, 0.9681, 0.9647, 0.9585, 0.9564, 0.9515, 0.9498, 0.9497, 0.9493,
         0.9450]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 1, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 0, 0, 1, 0],
        [0, 0, 1, 1, 1, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 1, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 457107.5000,  288522.0938,  261970.8281,  229185.3125,  214578.9531,
          212608.7969,  199267.6094,  193118.7500,  171799.0781,  162190.4844],
        [1452082.1250, 1377516.3750, 1362336.7500, 1359022.6250, 1335642.7500,
         1323483.1250, 1321550.8750, 1316324.6250, 1309433.2500, 1291071.3750],
        [1409710.0000, 1378418.0000, 1236402.6250, 1230404.8750, 1155632.5000,
         1155035.3750, 1124231.8750, 1099553.3750, 1079956.5000, 1076907.0000],
        [1390124.8750, 1146832.7500, 1109378.6250, 1102224.3750, 1078332.3750,
         1067863.2500, 1054607.3750, 1009644.1875,  978754.3125,  972261.6250],
        [1210060.3750, 1178456.7500, 1115458.6250, 1093194.2500, 1090074.1250,
         1071112.8750, 1051916.3750,  994367.5000,  994021.3750,  957931.4375],
        [1350643.6250, 1345105.8750, 1293332.7500, 1288055.8750, 1281943.2500,
         1272845.5000, 1270426.2500, 1224203.8750, 1213398.8750, 1199230.2500],
        [1490723.6250, 1489270.0000, 1487552.5000, 1487021.8750, 1469050.2500,
         1463223.8750, 1456710.5000, 1424845.2500, 1412095.6250, 1385080.3750],
        [1440439.7500, 1332013.6250, 1300566.0000, 1249604.6250, 1241070.1250,
         1238969.8750, 1226838.1250, 1223466.2500, 1211117.8750, 1210339.6250],
        [1409067.5000, 1390965.6250, 1366560.6250, 1362422.5000, 1361951.0000,
         1355962.1250, 1351291.6250, 1341874.6250, 1335044.2500, 1332232.1250],
        [1221994.6250, 1210851.1250,  844555.2500,  837369.3750,  836553.5625,
          832573.3125,  788472.8125,  771955.8125,  760313.9375,  709481.7500],
        [1262901.6250, 1233827.7500, 1222396.7500, 1215125.5000, 1136008.5000,
         1118237.3750, 1089995.1250, 1063953.6250, 1062532.0000, 1036307.3750],
        [1253510.0000, 1245338.3750, 1165993.2500, 1157657.6250,  987844.6875,
          924366.6875,  922674.7500,  907063.7500,  899405.1250,  885566.6875],
        [1243439.6250, 1220743.6250, 1080284.0000,  934469.3125,  849895.1250,
          828604.0625,  804812.6875,  653547.2500,  631235.5625,  625459.5625],
        [1443514.7500, 1416303.6250, 1402062.3750, 1393872.3750, 1388937.5000,
         1374672.6250, 1355913.0000, 1354751.0000, 1354742.0000, 1353818.5000],
        [1273243.8750, 1237725.1250, 1157561.6250, 1124068.8750, 1121530.0000,
         1120977.1250, 1087499.0000, 1083625.0000, 1078618.3750, 1069874.5000],
        [1318240.3750, 1249392.3750, 1214076.0000, 1080102.7500, 1045441.6875,
         1019610.6250, 1011326.7500,  985532.7500,  975393.2500,  957456.5000],
        [1444037.8750, 1401734.7500, 1375252.1250, 1349316.1250, 1348777.1250,
         1315954.2500, 1314712.3750, 1303349.6250, 1303297.3750, 1298159.5000],
        [1465747.5000, 1413215.1250, 1375590.6250, 1309721.6250, 1308757.7500,
         1308319.7500, 1281703.6250, 1271411.6250, 1265014.7500, 1232812.7500],
        [1252552.7500, 1224393.0000, 1199305.7500, 1128862.3750, 1060775.5000,
         1003932.2500, 1000062.1875,  987460.4375,  974238.6250,  955673.1250],
        [1274145.1250, 1254307.6250, 1231482.5000, 1191679.6250, 1161225.8750,
         1159080.6250, 1155288.6250, 1134777.2500, 1133400.5000, 1123485.8750],
        [1395416.6250, 1377285.2500, 1358671.3750, 1311622.8750, 1302324.6250,
         1275703.7500, 1267647.3750, 1261577.6250, 1242913.2500, 1235206.3750],
        [1276356.0000, 1241418.1250, 1206900.2500, 1139701.2500, 1093293.2500,
         1038273.0000,  992433.8125,  948957.0000,  922773.3125,  910968.2500],
        [1366340.3750, 1290112.5000, 1208656.7500, 1083746.0000, 1005144.1250,
          993576.0000,  974915.2500,  972909.0625,  954519.9375,  949186.8750],
        [1313662.1250, 1310947.6250, 1306379.7500, 1293776.7500, 1225065.7500,
         1184384.2500, 1169878.1250, 1109609.2500, 1103331.7500, 1101757.7500],
        [1338236.2500, 1194401.1250, 1070780.8750, 1035643.4375, 1024968.8125,
          992448.1250,  979289.2500,  941035.2500,  914579.9375,  898958.3750],
        [1219462.5000, 1082955.6250, 1044183.2500,  988124.5625,  979422.8125,
          975296.5000,  968246.6875,  959335.7500,  943286.0625,  934776.8125],
        [1224710.6250, 1164507.3750, 1074717.6250, 1045181.5625, 1025584.8125,
         1014548.3750,  975011.9375,  920080.8750,  891199.0625,  880449.3125],
        [1232087.5000, 1216791.8750, 1074060.7500, 1037738.3750, 1017639.5625,
          993336.3125,  933355.1250,  920048.3750,  913454.5625,  896779.1875],
        [1348683.2500, 1302587.8750, 1260496.3750, 1191250.1250, 1185868.2500,
         1157335.3750, 1124489.1250, 1077063.1250, 1075102.0000, 1073930.7500],
        [1258273.2500, 1246407.7500, 1184576.2500, 1177324.3750, 1167459.6250,
         1165364.0000, 1140491.6250, 1110638.2500, 1071353.8750, 1057096.5000],
        [1256775.3750, 1239900.1250, 1227210.2500, 1210340.7500, 1206637.7500,
         1195967.2500, 1186895.5000, 1174743.7500, 1172980.5000, 1155784.6250],
        [1269992.5000, 1263190.7500, 1262678.8750, 1192187.7500, 1166483.6250,
         1120705.7500, 1113636.6250, 1097241.6250, 1051919.3750, 1027967.2500],
        [1169680.7500,  861381.0000,  851236.8125,  849985.9375,  832493.1250,
          806192.3750,  763437.5000,  733358.6250,  708633.8125,  708095.3750],
        [1273457.5000, 1216817.3750, 1191758.0000, 1123524.3750, 1109768.0000,
         1107040.7500, 1095566.5000, 1083837.8750, 1071174.1250, 1063234.5000],
        [1381401.2500, 1270524.3750, 1268109.2500, 1254728.6250, 1234172.6250,
         1191000.1250, 1190634.5000, 1177043.7500, 1175526.0000, 1172744.5000],
        [1260603.3750, 1259851.0000, 1241735.5000, 1231999.3750, 1228437.5000,
         1214978.2500, 1208385.8750, 1175481.1250, 1149237.1250, 1143160.5000],
        [1309498.1250, 1199352.6250, 1104189.7500, 1040861.5625, 1038566.1250,
         1031366.8125, 1024208.6250, 1001872.0625,  940525.6875,  887761.0000],
        [1216388.0000, 1184257.7500, 1168375.1250, 1144972.8750, 1132531.8750,
         1118557.3750, 1114949.1250, 1113588.8750, 1105430.8750, 1093065.0000],
        [1356288.1250, 1306985.3750, 1184126.7500, 1171611.0000, 1132018.8750,
         1103448.6250, 1087049.0000, 1079051.5000, 1052946.1250, 1044138.4375],
        [1470285.0000, 1461317.5000, 1416595.3750, 1386017.2500, 1371057.6250,
         1368883.6250, 1363641.7500, 1355959.5000, 1345070.0000, 1340270.8750],
        [1040620.3125,  854681.6875,  807365.6875,  791329.3750,  724590.1875,
          701363.8750,  600641.6250,  597267.5000,  550678.4375,  519935.4375],
        [ 954014.8125,  947401.6250,  902741.9375,  888298.0000,  818580.3750,
          815571.0000,  796401.0625,  794486.3750,  793556.5000,  785543.4375],
        [1083784.1250,  956236.5000,  901931.3125,  872379.5000,  851904.3750,
          838605.6250,  759193.0625,  699714.3750,  676943.8125,  639043.1875],
        [ 870196.6875,  801571.0625,  797513.7500,  791854.0000,  781001.0000,
          773535.0625,  771353.1250,  770058.0625,  765309.5000,  755277.5625],
        [ 898349.0000,  798935.8125,  704583.1250,  668276.0000,  653067.5000,
          586276.9375,  527268.7500,  517233.1250,  515781.5000,  514536.1250],
        [ 501738.0625,  446534.7500,  404397.0312,  391217.8438,  366362.8125,
          339368.3438,  335983.4688,  332115.7188,  306570.9062,  296146.9062],
        [1174472.6250, 1160446.5000, 1121648.7500, 1113757.7500,  994267.9375,
          959943.3750,  944927.4375,  886986.6875,  865980.3125,  796650.2500],
        [ 721463.8125,  660319.0000,  639929.8750,  571023.7500,  542330.7500,
          436881.0625,  408435.3125,  368489.0625,  350181.1250,  345351.6250],
        [ 896960.4375,  890843.0000,  818617.8750,  781864.0000,  763481.9375,
          754894.5000,  745161.7500,  735985.3750,  718801.2500,  718464.0000],
        [1375033.0000, 1323191.6250, 1270524.3750, 1255627.6250, 1235542.2500,
         1220847.2500, 1216049.3750, 1192655.0000, 1177589.3750, 1162153.1250],
        [1182336.0000, 1166295.6250, 1064793.1250, 1047901.2500, 1041185.1875,
         1023246.0000,  939877.4375,  908543.3125,  907437.5000,  894034.6250],
        [1167083.3750, 1151412.6250, 1102037.2500,  955115.4375,  943512.7500,
          880027.8750,  820726.0625,  802088.6875,  751985.1250,  697329.5000],
        [1168880.1250, 1010766.5625,  795280.0625,  665646.5000,  652947.3125,
          637911.1875,  531626.6250,  518659.2188,  513179.6562,  513179.6562],
        [1082695.3750, 1082479.6250,  813617.2500,  760982.0625,  681838.8125,
          649276.9375,  576137.1250,  516363.2812,  508427.1250,  504810.8125],
        [1228949.5000, 1015806.9375,  965825.8750,  960837.3125,  918332.0000,
          912276.6875,  909281.8125,  876704.8125,  866237.2500,  851517.7500],
        [1271256.3750, 1249224.5000, 1183409.8750, 1150728.6250, 1097994.1250,
         1086604.3750, 1075642.5000, 1073296.8750, 1069769.3750, 1066666.2500],
        [1016021.1250,  986901.2500,  962315.5625,  936886.6250,  791974.8750,
          741453.6250,  640043.4375,  625747.7500,  617496.3125,  603776.1875],
        [ 994185.4375,  791534.6250,  713421.7500,  680783.5625,  668766.8750,
          650617.6250,  649020.6250,  614141.6875,  596454.6875,  593592.9375],
        [1188331.6250, 1083408.0000,  706076.4375,  669367.9375,  658403.0000,
          632685.0625,  603177.6875,  592816.8125,  581577.5000,  578666.3125],
        [1025323.7500,  657056.9375,  631680.6250,  583475.1875,  578522.3125,
          530109.3750,  486597.6875,  483390.0625,  473917.4688,  458974.4688],
        [ 991521.8750,  896685.9375,  896219.1250,  886845.3750,  859091.3125,
          801298.1875,  792722.9375,  791527.8750,  787567.9375,  778219.6250],
        [ 830193.1250,  746139.5625,  724282.7500,  638811.0000,  620332.4375,
          548006.6250,  525353.3750,  523837.9688,  487387.1875,  468525.3125],
        [1054847.7500, 1026862.0000,  858518.0000,  847101.8750,  828118.1875,
          816923.1875,  787905.3125,  626481.5625,  620619.4375,  599987.2500],
        [1021029.2500, 1015070.0000,  966454.2500,  884898.8750,  858182.3125,
          800309.9375,  781720.8750,  780607.0625,  775361.5625,  729694.6250]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 457107.5000,       0.0000],
         [ 288522.0938,       0.0000],
         [ 261970.8281,       0.0000],
         ...,
         [ 193118.7500,       0.0000],
         [ 171799.0781,       0.0000],
         [      0.0000,  162190.4844]],

        [[1452082.1250,       0.0000],
         [1377516.3750,       0.0000],
         [1362336.7500,       0.0000],
         ...,
         [1316324.6250,       0.0000],
         [1309433.2500,       0.0000],
         [1291071.3750,       0.0000]],

        [[1409710.0000,       0.0000],
         [1378418.0000,       0.0000],
         [1236402.6250,       0.0000],
         ...,
         [1099553.3750,       0.0000],
         [1079956.5000,       0.0000],
         [1076907.0000,       0.0000]],

        ...,

        [[ 830193.1250,       0.0000],
         [ 746139.5625,       0.0000],
         [ 724282.7500,       0.0000],
         ...,
         [ 523837.9688,       0.0000],
         [ 487387.1875,       0.0000],
         [ 468525.3125,       0.0000]],

        [[      0.0000, 1054847.7500],
         [1026862.0000,       0.0000],
         [ 858518.0000,       0.0000],
         ...,
         [ 626481.5625,       0.0000],
         [ 620619.4375,       0.0000],
         [ 599987.2500,       0.0000]],

        [[1021029.2500,       0.0000],
         [      0.0000, 1015070.0000],
         [ 966454.2500,       0.0000],
         ...,
         [ 780607.0625,       0.0000],
         [ 775361.5625,       0.0000],
         [ 729694.6250,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2015550.0000,   374799.2812],
        [13448463.0000,        0.0000],
        [11946252.0000,        0.0000],
        [10910024.0000,        0.0000],
        [ 8484942.0000,  2271651.0000],
        [12739187.0000,        0.0000],
        [13076304.0000,  1489270.0000],
        [11433355.0000,  1241070.1250],
        [12244950.0000,  1362422.5000],
        [ 7209592.5000,  1604529.1250],
        [ 9003763.0000,  2437522.2500],
        [10349421.0000,        0.0000],
        [ 8022595.5000,   849895.1250],
        [13838588.0000,        0.0000],
        [11354724.0000,        0.0000],
        [10856574.0000,        0.0000],
        [13454592.0000,        0.0000],
        [13232295.0000,        0.0000],
        [ 8597618.0000,  2189638.0000],
        [11818873.0000,        0.0000],
        [11766792.0000,  1261577.6250],
        [ 8571740.0000,  2199334.0000],
        [10799107.0000,        0.0000],
        [12118794.0000,        0.0000],
        [ 9411052.0000,   979289.2500],
        [10095090.0000,        0.0000],
        [ 9051484.0000,  1164507.3750],
        [10235292.0000,        0.0000],
        [11796806.0000,        0.0000],
        [11578986.0000,        0.0000],
        [12027236.0000,        0.0000],
        [11566004.0000,        0.0000],
        [ 4436755.0000,  3847740.0000],
        [10265005.0000,  1071174.1250],
        [12315886.0000,        0.0000],
        [ 9704782.0000,  2409088.0000],
        [ 8513132.0000,  2065070.2500],
        [10223742.0000,  1168375.1250],
        [10346052.0000,  1171611.0000],
        [12508041.0000,  1371057.6250],
        [ 6637796.0000,   550678.4375],
        [ 8496595.0000,        0.0000],
        [ 8279736.0000,        0.0000],
        [ 7877670.0000,        0.0000],
        [ 4481862.0000,  1902445.8750],
        [ 3218697.7500,   501738.0625],
        [10019082.0000,        0.0000],
        [ 3715012.5000,  1329393.0000],
        [ 3997147.5000,  3827927.0000],
        [10089470.0000,  2339742.5000],
        [ 8084771.0000,  2090879.2500],
        [ 7648503.5000,  1622814.7500],
        [ 4515311.5000,  2492765.5000],
        [ 5846648.0000,  1329980.5000],
        [ 9505770.0000,        0.0000],
        [10251297.0000,  1073296.8750],
        [ 4020492.2500,  3902124.5000],
        [ 5511964.0000,  1440555.2500],
        [ 4316694.5000,  2977816.0000],
        [ 3841274.2500,  2067773.5000],
        [ 6792291.0000,  1689408.8750],
        [ 6112869.5000,        0.0000],
        [ 7012516.5000,  1054847.7500],
        [ 6797948.5000,  1815380.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 96/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:52, 61.81s/it]  7%|▋         | 2/30 [01:02<12:04, 25.89s/it] 10%|█         | 3/30 [01:03<06:29, 14.41s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.02s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.04s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 4.160572417577108
Epoch 97/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:33, 63.24s/it]  7%|▋         | 2/30 [01:04<12:24, 26.60s/it] 10%|█         | 3/30 [01:04<06:39, 14.80s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.25s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.19s/it] 20%|██        | 6/30 [01:07<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.40s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  2.85s/it]
Epoch loss is 4.125690881411234
Epoch 98/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:08<32:57, 68.18s/it]  7%|▋         | 2/30 [01:08<13:18, 28.52s/it] 10%|█         | 3/30 [01:09<07:07, 15.84s/it] 13%|█▎        | 4/30 [01:10<04:16,  9.88s/it] 17%|█▋        | 5/30 [01:11<02:44,  6.59s/it] 20%|██        | 6/30 [01:11<01:50,  4.61s/it] 23%|██▎       | 7/30 [01:12<01:17,  3.35s/it] 27%|██▋       | 8/30 [01:13<00:55,  2.52s/it] 30%|███       | 9/30 [01:14<00:41,  1.97s/it] 33%|███▎      | 10/30 [01:14<00:31,  1.59s/it] 37%|███▋      | 11/30 [01:15<00:25,  1.33s/it] 40%|████      | 12/30 [01:16<00:20,  1.16s/it] 43%|████▎     | 13/30 [01:17<00:17,  1.03s/it] 47%|████▋     | 14/30 [01:17<00:15,  1.05it/s] 50%|█████     | 15/30 [01:18<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:19<00:11,  1.18it/s] 57%|█████▋    | 17/30 [01:20<00:10,  1.22it/s] 60%|██████    | 18/30 [01:20<00:09,  1.25it/s] 63%|██████▎   | 19/30 [01:21<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:22<00:07,  1.29it/s] 70%|███████   | 21/30 [01:23<00:06,  1.30it/s] 73%|███████▎  | 22/30 [01:23<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:24<00:05,  1.31it/s] 80%|████████  | 24/30 [01:25<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:26<00:03,  1.32it/s] 87%|████████▋ | 26/30 [01:26<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:27<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:28<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:29<00:00,  1.33it/s]100%|██████████| 30/30 [01:29<00:00,  1.33it/s]100%|██████████| 30/30 [01:30<00:00,  3.01s/it]
Epoch loss is 4.104305632909139
Epoch 99/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:09, 62.39s/it]  7%|▋         | 2/30 [01:03<12:11, 26.13s/it] 10%|█         | 3/30 [01:03<06:32, 14.54s/it] 13%|█▎        | 4/30 [01:04<03:56,  9.10s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.09s/it] 20%|██        | 6/30 [01:06<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 4.106064136823019
Epoch 100/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:46, 59.54s/it]  7%|▋         | 2/30 [01:01<11:50, 25.39s/it] 10%|█         | 3/30 [01:01<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 4.037094950675964
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0150,  0.0153, -0.0248,  ..., -0.0210, -0.0347,  0.0102],
        [-0.0598,  0.0331,  0.0121,  ...,  0.0006, -0.0246, -0.0191],
        [-0.0782, -0.0037, -0.0203,  ...,  0.0434, -0.0052, -0.0384],
        ...,
        [-0.0021, -0.0243, -0.0357,  ..., -0.0591, -0.0121,  0.0068],
        [-0.0640,  0.0110,  0.0041,  ..., -0.0483,  0.0325, -0.0179],
        [-0.0441,  0.0307,  0.0021,  ..., -0.0383,  0.0368, -0.0553]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9150, 0.8881, 0.8691, 0.8587, 0.8582, 0.8566, 0.8554, 0.8515, 0.8422,
         0.8376],
        [0.9924, 0.9879, 0.9878, 0.9875, 0.9873, 0.9856, 0.9844, 0.9841, 0.9838,
         0.9835],
        [0.9901, 0.9888, 0.9803, 0.9787, 0.9754, 0.9740, 0.9736, 0.9717, 0.9715,
         0.9704],
        [0.9892, 0.9745, 0.9733, 0.9713, 0.9706, 0.9703, 0.9663, 0.9661, 0.9632,
         0.9629],
        [0.9776, 0.9752, 0.9716, 0.9713, 0.9692, 0.9679, 0.9655, 0.9640, 0.9613,
         0.9575],
        [0.9856, 0.9842, 0.9836, 0.9827, 0.9823, 0.9801, 0.9798, 0.9796, 0.9786,
         0.9782],
        [0.9943, 0.9943, 0.9940, 0.9940, 0.9925, 0.9920, 0.9914, 0.9898, 0.9887,
         0.9880],
        [0.9924, 0.9862, 0.9834, 0.9802, 0.9797, 0.9795, 0.9786, 0.9779, 0.9774,
         0.9764],
        [0.9906, 0.9897, 0.9892, 0.9891, 0.9885, 0.9882, 0.9880, 0.9871, 0.9871,
         0.9870],
        [0.9816, 0.9801, 0.9570, 0.9544, 0.9535, 0.9532, 0.9502, 0.9494, 0.9484,
         0.9439],
        [0.9831, 0.9809, 0.9796, 0.9789, 0.9754, 0.9754, 0.9754, 0.9712, 0.9702,
         0.9694],
        [0.9813, 0.9808, 0.9766, 0.9759, 0.9630, 0.9585, 0.9562, 0.9559, 0.9555,
         0.9553],
        [0.9825, 0.9800, 0.9694, 0.9628, 0.9515, 0.9480, 0.9434, 0.9362, 0.9341,
         0.9336],
        [0.9927, 0.9903, 0.9900, 0.9899, 0.9895, 0.9886, 0.9884, 0.9881, 0.9872,
         0.9871],
        [0.9841, 0.9799, 0.9765, 0.9764, 0.9738, 0.9729, 0.9719, 0.9714, 0.9711,
         0.9702],
        [0.9841, 0.9813, 0.9789, 0.9696, 0.9692, 0.9675, 0.9647, 0.9639, 0.9633,
         0.9632],
        [0.9924, 0.9907, 0.9887, 0.9878, 0.9876, 0.9860, 0.9856, 0.9854, 0.9854,
         0.9852],
        [0.9936, 0.9908, 0.9882, 0.9862, 0.9858, 0.9848, 0.9845, 0.9843, 0.9826,
         0.9824],
        [0.9827, 0.9808, 0.9786, 0.9742, 0.9690, 0.9651, 0.9630, 0.9627, 0.9623,
         0.9610],
        [0.9830, 0.9826, 0.9810, 0.9793, 0.9780, 0.9768, 0.9762, 0.9741, 0.9741,
         0.9731],
        [0.9895, 0.9887, 0.9886, 0.9867, 0.9853, 0.9846, 0.9842, 0.9840, 0.9823,
         0.9808],
        [0.9849, 0.9823, 0.9762, 0.9757, 0.9742, 0.9696, 0.9634, 0.9616, 0.9599,
         0.9589],
        [0.9888, 0.9841, 0.9805, 0.9713, 0.9684, 0.9673, 0.9672, 0.9635, 0.9633,
         0.9622],
        [0.9869, 0.9867, 0.9863, 0.9854, 0.9828, 0.9789, 0.9787, 0.9756, 0.9755,
         0.9738],
        [0.9882, 0.9805, 0.9692, 0.9674, 0.9661, 0.9630, 0.9622, 0.9613, 0.9609,
         0.9567],
        [0.9807, 0.9714, 0.9677, 0.9652, 0.9647, 0.9631, 0.9611, 0.9610, 0.9608,
         0.9601],
        [0.9798, 0.9774, 0.9715, 0.9706, 0.9671, 0.9666, 0.9651, 0.9603, 0.9586,
         0.9586],
        [0.9804, 0.9768, 0.9712, 0.9688, 0.9633, 0.9624, 0.9607, 0.9600, 0.9592,
         0.9588],
        [0.9887, 0.9850, 0.9840, 0.9788, 0.9787, 0.9762, 0.9739, 0.9719, 0.9708,
         0.9703],
        [0.9825, 0.9820, 0.9781, 0.9774, 0.9772, 0.9757, 0.9744, 0.9743, 0.9710,
         0.9704],
        [0.9840, 0.9823, 0.9805, 0.9794, 0.9789, 0.9787, 0.9784, 0.9773, 0.9769,
         0.9768],
        [0.9827, 0.9816, 0.9813, 0.9796, 0.9752, 0.9749, 0.9724, 0.9723, 0.9691,
         0.9668],
        [0.9757, 0.9534, 0.9503, 0.9481, 0.9471, 0.9449, 0.9432, 0.9403, 0.9391,
         0.9360],
        [0.9824, 0.9808, 0.9789, 0.9755, 0.9736, 0.9702, 0.9689, 0.9679, 0.9676,
         0.9675],
        [0.9890, 0.9830, 0.9830, 0.9819, 0.9816, 0.9796, 0.9790, 0.9785, 0.9784,
         0.9775],
        [0.9831, 0.9823, 0.9818, 0.9814, 0.9811, 0.9809, 0.9793, 0.9788, 0.9759,
         0.9755],
        [0.9834, 0.9766, 0.9717, 0.9706, 0.9668, 0.9654, 0.9644, 0.9626, 0.9599,
         0.9536],
        [0.9807, 0.9779, 0.9771, 0.9755, 0.9742, 0.9728, 0.9726, 0.9721, 0.9717,
         0.9716],
        [0.9863, 0.9843, 0.9762, 0.9756, 0.9718, 0.9714, 0.9711, 0.9704, 0.9676,
         0.9658],
        [0.9942, 0.9938, 0.9914, 0.9894, 0.9890, 0.9887, 0.9882, 0.9882, 0.9881,
         0.9873],
        [0.9674, 0.9526, 0.9486, 0.9445, 0.9444, 0.9406, 0.9293, 0.9252, 0.9220,
         0.9184],
        [0.9579, 0.9573, 0.9521, 0.9499, 0.9460, 0.9459, 0.9454, 0.9442, 0.9441,
         0.9438],
        [0.9679, 0.9590, 0.9575, 0.9544, 0.9521, 0.9478, 0.9420, 0.9360, 0.9343,
         0.9321],
        [0.9552, 0.9468, 0.9438, 0.9431, 0.9428, 0.9416, 0.9415, 0.9379, 0.9378,
         0.9375],
        [0.9573, 0.9438, 0.9369, 0.9347, 0.9337, 0.9192, 0.9167, 0.9166, 0.9163,
         0.9147],
        [0.9181, 0.8948, 0.8881, 0.8849, 0.8844, 0.8810, 0.8794, 0.8739, 0.8738,
         0.8705],
        [0.9775, 0.9764, 0.9749, 0.9738, 0.9665, 0.9636, 0.9603, 0.9594, 0.9574,
         0.9504],
        [0.9412, 0.9336, 0.9321, 0.9204, 0.9177, 0.8945, 0.8882, 0.8843, 0.8838,
         0.8804],
        [0.9559, 0.9558, 0.9485, 0.9469, 0.9460, 0.9451, 0.9420, 0.9414, 0.9411,
         0.9404],
        [0.9871, 0.9854, 0.9811, 0.9802, 0.9784, 0.9784, 0.9780, 0.9774, 0.9754,
         0.9733],
        [0.9760, 0.9757, 0.9702, 0.9702, 0.9694, 0.9672, 0.9619, 0.9545, 0.9545,
         0.9545],
        [0.9743, 0.9738, 0.9719, 0.9623, 0.9587, 0.9578, 0.9508, 0.9502, 0.9450,
         0.9394],
        [0.9762, 0.9674, 0.9510, 0.9364, 0.9346, 0.9324, 0.9213, 0.9191, 0.9171,
         0.9161],
        [0.9694, 0.9671, 0.9487, 0.9399, 0.9339, 0.9254, 0.9205, 0.9133, 0.9115,
         0.9111],
        [0.9803, 0.9691, 0.9662, 0.9640, 0.9623, 0.9622, 0.9598, 0.9597, 0.9581,
         0.9565],
        [0.9834, 0.9814, 0.9794, 0.9768, 0.9736, 0.9733, 0.9721, 0.9720, 0.9708,
         0.9708],
        [0.9696, 0.9653, 0.9627, 0.9586, 0.9495, 0.9466, 0.9355, 0.9348, 0.9323,
         0.9280],
        [0.9687, 0.9477, 0.9415, 0.9386, 0.9386, 0.9352, 0.9349, 0.9306, 0.9300,
         0.9288],
        [0.9758, 0.9713, 0.9443, 0.9370, 0.9346, 0.9327, 0.9299, 0.9256, 0.9255,
         0.9226],
        [0.9683, 0.9282, 0.9267, 0.9251, 0.9218, 0.9127, 0.9122, 0.9113, 0.9091,
         0.9070],
        [0.9639, 0.9567, 0.9563, 0.9562, 0.9525, 0.9510, 0.9499, 0.9478, 0.9466,
         0.9462],
        [0.9501, 0.9402, 0.9391, 0.9315, 0.9270, 0.9187, 0.9163, 0.9158, 0.9108,
         0.9035],
        [0.9724, 0.9653, 0.9537, 0.9537, 0.9520, 0.9508, 0.9410, 0.9317, 0.9314,
         0.9290],
        [0.9692, 0.9663, 0.9657, 0.9577, 0.9513, 0.9485, 0.9482, 0.9461, 0.9439,
         0.9438]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 1, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 1, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 0, 1, 0, 1],
        [0, 0, 1, 1, 1, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 1, 0, 0, 1, 1, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 475124.8438,  323607.4062,  246648.3906,  212552.0156,  211197.0156,
          206416.8281,  202770.5781,  191688.8125,  168020.8281,  157194.7031],
        [1435567.1250, 1346610.1250, 1345284.2500, 1339478.6250, 1334691.6250,
         1302605.2500, 1279892.1250, 1274898.6250, 1270078.5000, 1263804.1250],
        [1390071.7500, 1364189.3750, 1208591.0000, 1181210.1250, 1126487.7500,
         1104099.1250, 1097570.2500, 1068073.1250, 1064676.3750, 1048099.1875],
        [1372398.5000, 1112492.3750, 1092204.2500, 1061938.5000, 1052141.1250,
         1046464.1250,  988407.3750,  985528.0625,  946604.1875,  942391.3750],
        [1162700.8750, 1122733.8750, 1066531.0000, 1062071.1250, 1029939.5625,
         1011408.7500,  977190.1875,  957329.6250,  920391.5000,  871757.4375],
        [1303096.0000, 1276912.5000, 1266398.0000, 1249083.8750, 1242703.3750,
         1204160.5000, 1199757.6250, 1195649.1250, 1179363.0000, 1172810.5000],
        [1476031.1250, 1474664.8750, 1468715.5000, 1467950.8750, 1437871.7500,
         1428484.8750, 1415647.2500, 1383389.3750, 1361409.3750, 1347877.0000],
        [1435333.1250, 1313290.1250, 1263208.7500, 1205871.7500, 1198162.6250,
         1194493.3750, 1178162.2500, 1167287.1250, 1159432.1250, 1142798.6250],
        [1398792.8750, 1381880.8750, 1370959.6250, 1369890.6250, 1357988.6250,
         1352433.8750, 1347492.7500, 1330552.3750, 1330058.8750, 1329934.6250],
        [1231025.7500, 1204615.3750,  865946.5000,  834826.5625,  823219.6875,
          820636.0625,  785852.8125,  776798.8750,  765833.6875,  717593.6875],
        [1257288.5000, 1218089.8750, 1195858.8750, 1183048.7500, 1126889.5000,
         1126812.1250, 1126474.7500, 1060926.1250, 1046134.8750, 1034052.5625],
        [1225055.2500, 1216385.7500, 1145645.6250, 1133404.8750,  943234.8125,
          884611.1875,  856302.0000,  852562.7500,  847860.7500,  844579.3750],
        [1246338.7500, 1202035.5000, 1034256.6875,  940775.0625,  800313.0000,
          761282.5625,  713429.8750,  643073.2500,  624598.8125,  619363.0000],
        [1441185.8750, 1393395.3750, 1387022.1250, 1384453.0000, 1376641.7500,
         1359808.2500, 1356041.0000, 1350020.2500, 1332021.3750, 1330693.2500],
        [1275508.0000, 1200450.1250, 1143169.2500, 1141566.7500, 1100345.3750,
         1087361.1250, 1070514.3750, 1063945.5000, 1059242.0000, 1046041.0625],
        [1275600.3750, 1225495.7500, 1183010.3750, 1037062.6875, 1031196.6250,
         1005345.5000,  966747.3125,  954844.0625,  946802.8125,  945615.2500],
        [1435838.3750, 1400315.7500, 1361582.1250, 1344219.7500, 1339615.2500,
         1310463.8750, 1302712.1250, 1299815.7500, 1299054.7500, 1294640.7500],
        [1460297.7500, 1403262.2500, 1353002.8750, 1314051.8750, 1306734.8750,
         1288098.8750, 1283141.8750, 1277912.6250, 1248718.2500, 1245043.8750],
        [1250768.2500, 1216918.3750, 1178515.2500, 1106512.0000, 1028347.6875,
          972451.7500,  943852.1250,  939563.8125,  934412.3125,  916971.1875],
        [1255196.6250, 1247633.8750, 1220432.8750, 1190118.0000, 1168973.6250,
         1148682.7500, 1138564.8750, 1105855.8750, 1105090.5000, 1090043.0000],
        [1378086.6250, 1361823.6250, 1360280.3750, 1322823.1250, 1296588.1250,
         1283632.6250, 1277779.7500, 1273835.3750, 1243061.3750, 1217167.8750],
        [1290085.5000, 1243285.5000, 1139240.3750, 1130944.1250, 1107514.8750,
         1036444.7500,  948516.3125,  924899.2500,  902484.5000,  889190.4375],
        [1363688.6250, 1275510.3750, 1211491.0000, 1062406.3750, 1019337.4375,
         1002413.9375, 1000948.6250,  950010.0625,  946762.1875,  932835.3750],
        [1327985.2500, 1323683.7500, 1316730.1250, 1299203.5000, 1250823.1250,
         1183177.5000, 1180075.1250, 1129769.1250, 1127188.3750, 1100451.3750],
        [1351466.8750, 1210540.5000, 1031382.5000, 1005190.1250,  985972.6875,
          942767.1875,  932748.2500,  920198.4375,  915662.1250,  861706.3750],
        [1214182.5000, 1063124.0000, 1008200.8750,  973607.0000,  966103.1250,
          944151.8750,  917891.6250,  916752.6250,  914529.3750,  904807.0625],
        [1198527.1250, 1158662.7500, 1065608.8750, 1051553.2500,  999490.1250,
          993750.3125,  972019.6875,  907479.0625,  886156.3750,  886156.3750],
        [1209122.5000, 1149096.7500, 1061009.1250, 1024544.6875,  947553.4375,
          935226.2500,  913157.5625,  904116.9375,  893771.1875,  888220.0000],
        [1360820.1250, 1290769.7500, 1272456.0000, 1181672.1250, 1180692.0000,
         1139237.2500, 1101803.0000, 1071043.3750, 1054436.3750, 1047726.4375],
        [1245915.7500, 1237771.1250, 1170602.5000, 1159415.5000, 1155499.1250,
         1130430.8750, 1110576.8750, 1107767.3750, 1057942.6250, 1047996.1875],
        [1272683.0000, 1243034.1250, 1211908.1250, 1191634.1250, 1183073.6250,
         1180144.8750, 1175289.5000, 1156424.1250, 1151204.0000, 1148819.6250],
        [1249571.1250, 1231034.0000, 1225377.7500, 1196128.0000, 1123521.1250,
         1117926.0000, 1079018.6250, 1077404.1250, 1028467.3125,  996392.2500],
        [1131741.5000,  822623.1875,  786231.4375,  762098.3125,  751436.6875,
          728861.4375,  711266.1875,  682362.4375,  670328.7500,  641341.8125],
        [1244717.3750, 1216415.8750, 1183477.6250, 1127930.3750, 1096992.6250,
         1045284.1875, 1026644.6250, 1012353.4375, 1006780.8125, 1006561.8750],
        [1367028.5000, 1255098.5000, 1254714.3750, 1235217.0000, 1229994.2500,
         1195356.1250, 1185205.6250, 1176940.5000, 1175582.0000, 1160868.2500],
        [1257000.7500, 1242464.0000, 1233929.0000, 1226355.1250, 1222491.2500,
         1217923.7500, 1191466.0000, 1182873.8750, 1134543.6250, 1128386.5000],
        [1262451.3750, 1144840.7500, 1068387.8750, 1051394.7500,  996458.8125,
          976272.6875,  962804.8750,  937975.5625,  902839.1875,  825346.8125],
        [1214287.8750, 1166631.6250, 1152998.1250, 1128085.2500, 1106494.0000,
         1084462.3750, 1082572.5000, 1074972.7500, 1067701.3750, 1067074.3750],
        [1315953.1250, 1278975.8750, 1139576.2500, 1129186.3750, 1068935.1250,
         1064102.8750, 1059718.8750, 1049187.2500, 1007063.1250,  981216.9375],
        [1473613.2500, 1465177.3750, 1415147.7500, 1375611.5000, 1368263.7500,
         1362666.7500, 1352815.7500, 1352361.6250, 1350646.1250, 1335742.2500],
        [1004609.4375,  812978.8750,  767845.5625,  724210.8750,  723706.8750,
          685417.4375,  583124.7500,  549408.0000,  525045.3125,  498638.9062],
        [ 876464.0625,  869420.2500,  807339.4375,  782315.1875,  740366.2500,
          738374.2500,  733118.8125,  721314.5625,  720589.1250,  716991.0625],
        [1012051.3125,  890723.2500,  871519.6875,  834507.3125,  807767.7500,
          758961.4375,  698319.0625,  641584.6875,  625868.2500,  606629.6875],
        [ 843455.7500,  748081.0625,  717516.3750,  710250.0625,  706624.1250,
          694359.8125,  693593.4375,  659250.6250,  658497.8750,  655390.3750],
        [ 869334.9375,  717211.2500,  649851.7500,  629524.0625,  620854.4375,
          504911.4062,  486755.4688,  485974.3750,  484284.7500,  473109.1562],
        [ 496655.5938,  355844.2812,  323359.3750,  309267.2500,  306715.0938,
          292269.0625,  285754.9688,  263986.4688,  263764.2500,  251498.9375],
        [1160324.7500, 1143081.0000, 1117591.2500, 1100967.8750,  991462.3125,
          951924.5625,  908199.4375,  895522.8750,  870241.5625,  788138.3125],
        [ 690820.1875,  619789.0000,  606254.9375,  513170.8438,  494171.3750,
          354555.0312,  324245.6562,  306303.8125,  304185.7812,  289828.1875],
        [ 851865.3750,  851439.0000,  767138.5625,  749625.0625,  739723.2500,
          730916.2500,  698872.0625,  692929.6875,  690257.1250,  682683.3125],
        [1331364.7500, 1299581.5000, 1221988.7500, 1205647.5000, 1175841.1250,
         1174999.2500, 1168374.1250, 1158482.7500, 1125767.0000, 1092719.0000],
        [1135722.5000, 1131196.6250, 1046102.0000, 1045938.3750, 1033600.0000,
         1001154.8125,  928266.8750,  836008.8750,  835543.3750,  835216.7500],
        [1108048.5000, 1101056.1250, 1070850.3750,  933255.4375,  886524.0625,
          876002.8125,  791863.8750,  786203.6875,  729025.5000,  672929.8750],
        [1138548.5000, 1004081.6250,  795143.5625,  645100.2500,  628606.7500,
          609601.3125,  519575.0938,  503858.0000,  489359.1562,  482989.1562],
        [1033551.7500, 1000161.3750,  769509.6250,  678442.5625,  622535.8750,
          550930.1250,  514329.5625,  463827.4688,  451728.5938,  449673.2188],
        [1207001.5000, 1028460.4375,  986888.0625,  957368.8750,  933801.1875,
          932963.5000,  901542.5625,  900466.8125,  879727.4375,  859613.3750],
        [1262929.3750, 1226794.8750, 1192914.5000, 1149167.0000, 1097811.0000,
         1093265.1250, 1073693.1250, 1073395.1250, 1055158.6250, 1055004.7500],
        [1036142.3125,  974785.0625,  938674.4375,  885346.2500,  778386.5625,
          746094.6875,  637107.4375,  630507.6250,  608510.5000,  572110.6875],
        [1023972.3125,  757949.5000,  693527.3125,  666005.8750,  665882.6875,
          634407.0000,  631428.8125,  593989.3750,  588738.8750,  578963.8125],
        [1131906.6250, 1062065.0000,  722096.4375,  650686.5000,  628674.5000,
          611927.6875,  588000.4375,  552919.1875,  551705.6250,  529711.1250],
        [1017303.8750,  573798.5000,  561542.8750,  548891.6250,  523950.4062,
          459981.0000,  456641.7188,  450695.4062,  437068.5938,  423617.7812],
        [ 955687.6250,  862627.2500,  856620.5625,  855961.5625,  811330.6875,
          794770.5000,  781828.9375,  759561.0000,  746798.7500,  741682.8125],
        [ 785084.3125,  681203.1250,  670137.6250,  601588.0625,  564006.3750,
          500884.2188,  483851.7188,  480496.2812,  447295.5625,  403251.3125],
        [1078528.8750,  975188.6250,  826075.2500,  825960.2500,  806386.8750,
          792968.6250,  688691.5625,  603081.6250,  600520.1875,  580783.8125],
        [1030473.1250,  988682.6250,  981018.5625,  874993.3125,  797570.7500,
          766286.6875,  763427.3750,  740595.7500,  718366.0625,  717424.0000]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 475124.8438,       0.0000],
         [ 323607.4062,       0.0000],
         [ 246648.3906,       0.0000],
         ...,
         [ 191688.8125,       0.0000],
         [ 168020.8281,       0.0000],
         [ 157194.7031,       0.0000]],

        [[1435567.1250,       0.0000],
         [1346610.1250,       0.0000],
         [1345284.2500,       0.0000],
         ...,
         [1274898.6250,       0.0000],
         [1270078.5000,       0.0000],
         [1263804.1250,       0.0000]],

        [[1390071.7500,       0.0000],
         [1364189.3750,       0.0000],
         [1208591.0000,       0.0000],
         ...,
         [1068073.1250,       0.0000],
         [1064676.3750,       0.0000],
         [1048099.1875,       0.0000]],

        ...,

        [[ 785084.3125,       0.0000],
         [ 681203.1250,       0.0000],
         [ 670137.6250,       0.0000],
         ...,
         [ 480496.2812,       0.0000],
         [ 447295.5625,       0.0000],
         [ 403251.3125,       0.0000]],

        [[      0.0000, 1078528.8750],
         [ 975188.6250,       0.0000],
         [ 826075.2500,       0.0000],
         ...,
         [ 603081.6250,       0.0000],
         [ 600520.1875,       0.0000],
         [ 580783.8125,       0.0000]],

        [[      0.0000, 1030473.1250],
         [ 988682.6250,       0.0000],
         [ 981018.5625,       0.0000],
         ...,
         [ 740595.7500,       0.0000],
         [ 718366.0625,       0.0000],
         [ 717424.0000,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2182669.5000,   212552.0156],
        [13192911.0000,        0.0000],
        [11653068.0000,        0.0000],
        [10600570.0000,        0.0000],
        [ 7992789.0000,  2189265.0000],
        [12289935.0000,        0.0000],
        [12787377.0000,  1474664.8750],
        [11079878.0000,  1178162.2500],
        [12239927.0000,  1330058.8750],
        [ 7214724.0000,  1611625.5000],
        [ 8996668.0000,  2378907.5000],
        [ 9949642.0000,        0.0000],
        [ 7872036.0000,   713429.8750],
        [13711282.0000,        0.0000],
        [11188144.0000,        0.0000],
        [10571721.0000,        0.0000],
        [13388258.0000,        0.0000],
        [13180266.0000,        0.0000],
        [ 8353453.5000,  2134859.7500],
        [11670592.0000,        0.0000],
        [11731446.0000,  1283632.6250],
        [ 8533146.0000,  2079460.5000],
        [10765404.0000,        0.0000],
        [12239088.0000,        0.0000],
        [ 9214868.0000,   942767.1875],
        [ 9823350.0000,        0.0000],
        [ 8960741.0000,  1158662.7500],
        [ 9925819.0000,        0.0000],
        [11700656.0000,        0.0000],
        [11423919.0000,        0.0000],
        [11914214.0000,        0.0000],
        [11324840.0000,        0.0000],
        [ 4174395.5000,  3513896.5000],
        [ 8933733.0000,  2033425.5000],
        [12236006.0000,        0.0000],
        [ 9680399.0000,  2357035.0000],
        [ 8084112.0000,  2044660.5000],
        [ 9992282.0000,  1152998.1250],
        [ 9964729.0000,  1129186.3750],
        [12499230.0000,  1352815.7500],
        [ 5851302.0000,  1023684.2500],
        [ 7706293.0000,        0.0000],
        [ 7747933.0000,        0.0000],
        [ 7087020.0000,        0.0000],
        [ 3742295.0000,  2179516.5000],
        [ 2652460.0000,   496655.5938],
        [ 9927454.0000,        0.0000],
        [ 3058466.2500,  1444858.5000],
        [ 3775116.7500,  3680332.7500],
        [ 9693673.0000,  2261093.0000],
        [ 8693028.0000,  1135722.5000],
        [ 7377693.0000,  1578067.5000],
        [ 4403184.0000,  2413679.0000],
        [ 5250851.0000,  1283839.2500],
        [ 8728220.0000,   859613.3750],
        [10182322.0000,  1097811.0000],
        [ 3972717.5000,  3834948.0000],
        [ 4851498.0000,  1983367.6250],
        [ 4113625.2500,  2916068.0000],
        [ 3425320.7500,  2028171.0000],
        [ 6563450.0000,  1603419.2500],
        [ 5617799.0000,        0.0000],
        [ 6699657.0000,  1078528.8750],
        [ 7348365.0000,  1030473.1250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 101/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:45, 59.51s/it]  7%|▋         | 2/30 [01:00<11:38, 24.95s/it] 10%|█         | 3/30 [01:01<06:15, 13.90s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 4.061055088043213
Epoch 102/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:28, 58.90s/it]  7%|▋         | 2/30 [00:59<11:31, 24.70s/it] 10%|█         | 3/30 [01:01<06:30, 14.45s/it] 13%|█▎        | 4/30 [01:03<03:58,  9.18s/it] 17%|█▋        | 5/30 [01:03<02:33,  6.14s/it] 20%|██        | 6/30 [01:04<01:43,  4.31s/it] 23%|██▎       | 7/30 [01:05<01:12,  3.15s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.38s/it] 30%|███       | 9/30 [01:06<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.29s/it] 40%|████      | 12/30 [01:09<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.07it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.29it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 4.076784833272298
Epoch 103/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:04, 62.22s/it]  7%|▋         | 2/30 [01:02<12:09, 26.06s/it] 10%|█         | 3/30 [01:03<06:31, 14.51s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.08s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.07s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 4.067678586641947
Epoch 104/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:10, 58.29s/it]  7%|▋         | 2/30 [01:02<12:17, 26.34s/it] 10%|█         | 3/30 [01:03<06:35, 14.66s/it] 13%|█▎        | 4/30 [01:03<03:58,  9.17s/it] 17%|█▋        | 5/30 [01:04<02:33,  6.13s/it] 20%|██        | 6/30 [01:05<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.38s/it] 30%|███       | 9/30 [01:07<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.29s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 4.069760855038961
Epoch 105/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:35, 61.22s/it]  7%|▋         | 2/30 [01:02<11:59, 25.71s/it] 10%|█         | 3/30 [01:02<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:04<02:29,  6.00s/it] 20%|██        | 6/30 [01:05<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 3.9941382964452106
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0153,  0.0146, -0.0319,  ..., -0.0203, -0.0322,  0.0133],
        [-0.0576,  0.0332,  0.0068,  ...,  0.0017, -0.0247, -0.0171],
        [-0.0797, -0.0033, -0.0208,  ...,  0.0419, -0.0086, -0.0392],
        ...,
        [ 0.0007, -0.0260, -0.0382,  ..., -0.0582, -0.0154,  0.0057],
        [-0.0605,  0.0110,  0.0007,  ..., -0.0468,  0.0323, -0.0180],
        [-0.0436,  0.0286,  0.0027,  ..., -0.0341,  0.0342, -0.0576]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9184, 0.8953, 0.8667, 0.8593, 0.8582, 0.8573, 0.8492, 0.8475, 0.8442,
         0.8341],
        [0.9913, 0.9866, 0.9861, 0.9858, 0.9857, 0.9842, 0.9825, 0.9821, 0.9819,
         0.9819],
        [0.9891, 0.9879, 0.9801, 0.9773, 0.9740, 0.9725, 0.9720, 0.9719, 0.9718,
         0.9704],
        [0.9882, 0.9723, 0.9722, 0.9691, 0.9689, 0.9682, 0.9663, 0.9610, 0.9602,
         0.9575],
        [0.9749, 0.9714, 0.9684, 0.9663, 0.9650, 0.9614, 0.9601, 0.9591, 0.9556,
         0.9546],
        [0.9833, 0.9815, 0.9813, 0.9807, 0.9803, 0.9786, 0.9768, 0.9760, 0.9726,
         0.9723],
        [0.9933, 0.9932, 0.9930, 0.9927, 0.9906, 0.9896, 0.9887, 0.9868, 0.9856,
         0.9851],
        [0.9916, 0.9844, 0.9808, 0.9784, 0.9778, 0.9766, 0.9742, 0.9736, 0.9735,
         0.9731],
        [0.9896, 0.9891, 0.9885, 0.9882, 0.9881, 0.9877, 0.9873, 0.9870, 0.9862,
         0.9861],
        [0.9813, 0.9799, 0.9559, 0.9516, 0.9497, 0.9488, 0.9470, 0.9464, 0.9458,
         0.9441],
        [0.9824, 0.9815, 0.9774, 0.9768, 0.9766, 0.9758, 0.9753, 0.9729, 0.9706,
         0.9698],
        [0.9806, 0.9798, 0.9753, 0.9742, 0.9615, 0.9587, 0.9544, 0.9533, 0.9515,
         0.9514],
        [0.9824, 0.9785, 0.9669, 0.9629, 0.9490, 0.9457, 0.9388, 0.9359, 0.9350,
         0.9327],
        [0.9923, 0.9896, 0.9892, 0.9891, 0.9887, 0.9884, 0.9878, 0.9867, 0.9867,
         0.9866],
        [0.9838, 0.9776, 0.9775, 0.9746, 0.9718, 0.9714, 0.9710, 0.9710, 0.9700,
         0.9689],
        [0.9816, 0.9809, 0.9775, 0.9686, 0.9669, 0.9657, 0.9636, 0.9628, 0.9619,
         0.9615],
        [0.9928, 0.9915, 0.9882, 0.9879, 0.9862, 0.9854, 0.9852, 0.9849, 0.9846,
         0.9845],
        [0.9933, 0.9897, 0.9868, 0.9867, 0.9860, 0.9856, 0.9840, 0.9831, 0.9829,
         0.9817],
        [0.9829, 0.9808, 0.9751, 0.9726, 0.9655, 0.9629, 0.9604, 0.9570, 0.9570,
         0.9564],
        [0.9820, 0.9806, 0.9802, 0.9789, 0.9772, 0.9743, 0.9740, 0.9726, 0.9709,
         0.9708],
        [0.9893, 0.9889, 0.9874, 0.9871, 0.9850, 0.9839, 0.9838, 0.9836, 0.9814,
         0.9806],
        [0.9860, 0.9802, 0.9759, 0.9740, 0.9706, 0.9680, 0.9600, 0.9598, 0.9592,
         0.9585],
        [0.9891, 0.9824, 0.9798, 0.9696, 0.9691, 0.9683, 0.9656, 0.9640, 0.9605,
         0.9588],
        [0.9874, 0.9865, 0.9862, 0.9849, 0.9833, 0.9798, 0.9778, 0.9760, 0.9754,
         0.9734],
        [0.9877, 0.9813, 0.9657, 0.9649, 0.9630, 0.9621, 0.9609, 0.9593, 0.9589,
         0.9562],
        [0.9795, 0.9682, 0.9652, 0.9636, 0.9631, 0.9612, 0.9611, 0.9607, 0.9586,
         0.9580],
        [0.9778, 0.9773, 0.9711, 0.9702, 0.9651, 0.9649, 0.9644, 0.9590, 0.9588,
         0.9576],
        [0.9780, 0.9735, 0.9708, 0.9677, 0.9601, 0.9593, 0.9588, 0.9583, 0.9582,
         0.9576],
        [0.9879, 0.9839, 0.9838, 0.9780, 0.9771, 0.9752, 0.9723, 0.9708, 0.9693,
         0.9693],
        [0.9815, 0.9812, 0.9788, 0.9769, 0.9745, 0.9743, 0.9743, 0.9733, 0.9714,
         0.9682],
        [0.9852, 0.9827, 0.9806, 0.9793, 0.9790, 0.9777, 0.9776, 0.9773, 0.9768,
         0.9768],
        [0.9799, 0.9799, 0.9787, 0.9787, 0.9747, 0.9711, 0.9708, 0.9699, 0.9677,
         0.9663],
        [0.9710, 0.9522, 0.9408, 0.9402, 0.9396, 0.9387, 0.9365, 0.9337, 0.9316,
         0.9290],
        [0.9815, 0.9801, 0.9773, 0.9750, 0.9713, 0.9681, 0.9670, 0.9670, 0.9664,
         0.9649],
        [0.9850, 0.9826, 0.9797, 0.9790, 0.9787, 0.9779, 0.9772, 0.9769, 0.9765,
         0.9761],
        [0.9830, 0.9820, 0.9812, 0.9804, 0.9797, 0.9792, 0.9787, 0.9778, 0.9757,
         0.9752],
        [0.9806, 0.9726, 0.9721, 0.9648, 0.9642, 0.9626, 0.9590, 0.9584, 0.9532,
         0.9481],
        [0.9775, 0.9772, 0.9753, 0.9739, 0.9711, 0.9704, 0.9699, 0.9691, 0.9686,
         0.9674],
        [0.9841, 0.9818, 0.9738, 0.9731, 0.9687, 0.9679, 0.9674, 0.9667, 0.9653,
         0.9649],
        [0.9944, 0.9941, 0.9907, 0.9902, 0.9894, 0.9893, 0.9884, 0.9878, 0.9874,
         0.9872],
        [0.9654, 0.9482, 0.9468, 0.9421, 0.9367, 0.9357, 0.9264, 0.9186, 0.9168,
         0.9134],
        [0.9545, 0.9506, 0.9440, 0.9436, 0.9433, 0.9421, 0.9405, 0.9402, 0.9400,
         0.9393],
        [0.9660, 0.9568, 0.9555, 0.9517, 0.9515, 0.9411, 0.9361, 0.9323, 0.9305,
         0.9285],
        [0.9532, 0.9469, 0.9380, 0.9369, 0.9369, 0.9347, 0.9320, 0.9317, 0.9304,
         0.9304],
        [0.9566, 0.9378, 0.9360, 0.9292, 0.9284, 0.9195, 0.9184, 0.9134, 0.9107,
         0.9107],
        [0.9184, 0.8771, 0.8728, 0.8723, 0.8707, 0.8690, 0.8661, 0.8587, 0.8570,
         0.8558],
        [0.9771, 0.9748, 0.9740, 0.9719, 0.9664, 0.9615, 0.9593, 0.9576, 0.9573,
         0.9501],
        [0.9414, 0.9325, 0.9290, 0.9156, 0.9146, 0.8829, 0.8819, 0.8781, 0.8751,
         0.8748],
        [0.9580, 0.9530, 0.9483, 0.9452, 0.9425, 0.9412, 0.9391, 0.9389, 0.9369,
         0.9349],
        [0.9842, 0.9839, 0.9798, 0.9760, 0.9760, 0.9755, 0.9742, 0.9737, 0.9731,
         0.9707],
        [0.9733, 0.9730, 0.9695, 0.9691, 0.9687, 0.9646, 0.9610, 0.9528, 0.9521,
         0.9510],
        [0.9722, 0.9703, 0.9703, 0.9584, 0.9569, 0.9549, 0.9507, 0.9459, 0.9420,
         0.9369],
        [0.9735, 0.9665, 0.9500, 0.9353, 0.9337, 0.9321, 0.9207, 0.9143, 0.9139,
         0.9117],
        [0.9698, 0.9634, 0.9454, 0.9347, 0.9258, 0.9193, 0.9172, 0.9118, 0.9088,
         0.9072],
        [0.9779, 0.9685, 0.9657, 0.9640, 0.9634, 0.9628, 0.9614, 0.9605, 0.9601,
         0.9588],
        [0.9816, 0.9793, 0.9789, 0.9777, 0.9747, 0.9719, 0.9707, 0.9703, 0.9695,
         0.9688],
        [0.9707, 0.9642, 0.9598, 0.9548, 0.9473, 0.9456, 0.9357, 0.9336, 0.9314,
         0.9253],
        [0.9691, 0.9423, 0.9394, 0.9361, 0.9348, 0.9324, 0.9323, 0.9317, 0.9276,
         0.9245],
        [0.9703, 0.9662, 0.9435, 0.9348, 0.9296, 0.9291, 0.9261, 0.9211, 0.9208,
         0.9203],
        [0.9673, 0.9256, 0.9168, 0.9140, 0.9137, 0.9105, 0.9058, 0.9036, 0.8998,
         0.8983],
        [0.9616, 0.9538, 0.9531, 0.9529, 0.9508, 0.9490, 0.9472, 0.9471, 0.9440,
         0.9430],
        [0.9430, 0.9330, 0.9295, 0.9259, 0.9198, 0.9142, 0.9089, 0.9055, 0.9040,
         0.8955],
        [0.9722, 0.9626, 0.9517, 0.9517, 0.9498, 0.9485, 0.9336, 0.9315, 0.9292,
         0.9268],
        [0.9687, 0.9663, 0.9626, 0.9549, 0.9480, 0.9466, 0.9452, 0.9450, 0.9429,
         0.9410]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 1, 1, 0, 1],
        [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 1, 0, 0, 1, 1, 0, 1, 1],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 1, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 498481.0312,  358758.0312,  238285.4688,  214341.0781,  211230.0469,
          208404.3281,  185487.3750,  181104.0156,  172810.1719,  149685.5781],
        [1413323.0000, 1320893.0000, 1312217.1250, 1306899.3750, 1305272.6250,
         1277130.5000, 1246606.2500, 1238851.7500, 1236573.6250, 1235953.5000],
        [1370300.8750, 1346684.7500, 1203893.0000, 1157529.6250, 1103753.7500,
         1079847.2500, 1072737.1250, 1070701.2500, 1069697.0000, 1048723.1250],
        [1351847.1250, 1077231.5000, 1076201.6250, 1028703.7500, 1025763.8125,
         1015584.2500,  988982.5000,  916181.8125,  905701.5000,  872134.1250],
        [1117632.8750, 1063221.2500, 1019394.6875,  989490.0000,  970482.0625,
          922410.8125,  905444.9375,  892040.8125,  848164.0000,  836682.8750],
        [1261494.5000, 1229343.3750, 1225181.3750, 1214700.2500, 1207927.3750,
         1177977.0000, 1148051.8750, 1136514.5000, 1081918.2500, 1076982.0000],
        [1453574.2500, 1452723.3750, 1447345.5000, 1441683.5000, 1400008.6250,
         1379787.0000, 1361203.0000, 1324543.7500, 1303497.5000, 1294351.8750],
        [1418878.8750, 1281356.5000, 1216317.2500, 1175533.8750, 1165764.1250,
         1145431.5000, 1107062.8750, 1098047.6250, 1095374.3750, 1089022.6250],
        [1379164.7500, 1369113.3750, 1357984.8750, 1351305.7500, 1349287.8750,
         1342541.5000, 1335715.3750, 1329656.8750, 1314718.7500, 1312749.1250],
        [1225445.5000, 1201584.0000,  852648.1250,  802057.3750,  780512.5625,
          770314.4375,  750223.0000,  743743.2500,  737517.0625,  719769.7500],
        [1244413.6250, 1227833.1250, 1158030.8750, 1149274.3750, 1144960.8750,
         1131906.6250, 1124176.0000, 1087016.8750, 1052076.8750, 1039722.6250],
        [1212994.0000, 1199726.7500, 1124401.2500, 1106374.7500,  923035.5625,
          886717.6875,  834574.1875,  821198.1875,  800096.2500,  799790.3750],
        [1244755.3750, 1177586.1250,  997537.9375,  942253.0625,  771846.1875,
          736302.0000,  667257.6875,  640374.3750,  632557.1250,  611877.5000],
        [1433049.0000, 1379826.6250, 1370614.5000, 1368971.1250, 1362603.1250,
         1355154.2500, 1344784.0000, 1323547.3750, 1323379.5000, 1321466.3750],
        [1269207.8750, 1162286.1250, 1160601.5000, 1113243.7500, 1069873.3750,
         1063296.3750, 1057360.7500, 1056937.2500, 1042080.1875, 1025559.3750],
        [1230257.1250, 1217710.0000, 1160864.8750, 1021708.2500,  997601.6875,
          981083.0625,  951302.9375,  940334.6250,  928655.5625,  922824.3125],
        [1443173.2500, 1416881.7500, 1352228.8750, 1346867.1250, 1314434.1250,
         1298433.1250, 1296222.1250, 1290622.0000, 1283850.6250, 1283183.5000],
        [1455144.2500, 1380607.1250, 1325147.7500, 1323332.8750, 1309932.8750,
         1303615.6250, 1274174.2500, 1256595.6250, 1253531.3750, 1232563.5000],
        [1253359.2500, 1217273.5000, 1122029.6250, 1081873.7500,  977244.2500,
          941897.1875,  908788.5625,  866407.4375,  866376.0625,  858039.1875],
        [1237708.6250, 1212623.7500, 1205391.0000, 1184431.6250, 1155669.8750,
         1109137.3750, 1103142.3750, 1081467.3750, 1056311.5000, 1055176.7500],
        [1373395.0000, 1365332.1250, 1337244.8750, 1331513.2500, 1292353.7500,
         1270769.1250, 1269952.5000, 1266892.1250, 1227730.1250, 1213071.3750],
        [1309465.6250, 1206426.1250, 1134462.5000, 1104483.5000, 1051191.3750,
         1012465.4375,  904186.0000,  900793.1875,  893396.1875,  884723.4375],
        [1368584.7500, 1245385.8750, 1199911.0000, 1036725.5000, 1029820.7500,
         1017291.1875,  978370.6875,  956850.4375,  910311.6875,  888155.6875],
        [1337018.0000, 1319365.8750, 1314058.1250, 1289734.8750, 1260876.3750,
         1198438.0000, 1165060.6250, 1136355.1250, 1125664.0000, 1093720.8750],
        [1342713.0000, 1224297.2500,  980470.4375,  969284.1875,  943867.3750,
          931435.3750,  914831.1250,  895239.3125,  889453.3750,  855560.8125],
        [1194035.5000, 1016416.5000,  972826.5000,  951059.8125,  944113.1875,
          919136.3125,  918565.8750,  913220.2500,  885867.4375,  878036.8750],
        [1165708.5000, 1157514.2500, 1058807.6250, 1045186.5000,  971943.6250,
          969927.8125,  962744.2500,  890990.0000,  887924.4375,  873369.3125],
        [1168502.2500, 1095405.6250, 1053932.7500, 1008214.3750,  904653.4375,
          894200.0000,  888061.5625,  882594.6875,  880678.5625,  873114.4375],
        [1346754.0000, 1270643.1250, 1269704.2500, 1168902.3750, 1154167.6250,
         1123220.1250, 1076929.6250, 1054193.1250, 1031684.5625, 1031554.6250],
        [1228199.6250, 1224105.7500, 1181614.5000, 1149915.7500, 1112369.3750,
         1108313.7500, 1107905.8750, 1093595.7500, 1063772.0000, 1015647.1250],
        [1295596.7500, 1250756.2500, 1213169.7500, 1190279.1250, 1186068.3750,
         1163658.1250, 1161919.3750, 1156881.8750, 1149111.0000, 1148587.3750],
        [1201052.5000, 1200402.0000, 1181283.3750, 1181114.3750, 1115179.8750,
         1058692.5000, 1054184.0000, 1040446.6875, 1008288.4375,  988761.8125],
        [1057791.3750,  807978.0000,  687005.0000,  681321.3750,  674988.5625,
          667038.8125,  646296.1250,  620485.6875,  602512.5000,  580518.5000],
        [1229405.5000, 1203935.5000, 1156581.7500, 1119247.6250, 1061921.2500,
         1014495.1875,  999339.5000,  998852.5625,  990267.0000,  969500.5625],
        [1292097.3750, 1248361.0000, 1196785.2500, 1186004.0000, 1180400.3750,
         1167674.6250, 1155439.6250, 1150880.1250, 1143480.0000, 1137306.0000],
        [1255333.1250, 1237608.3750, 1222588.0000, 1209184.7500, 1197077.5000,
         1189219.3750, 1180874.3750, 1164953.8750, 1130505.2500, 1122791.7500],
        [1212818.0000, 1082454.8750, 1074693.0000,  968562.5625,  959754.8125,
          937562.3750,  891363.9375,  882981.0625,  820124.3750,  762578.1875],
        [1161110.7500, 1156103.1250, 1123760.1250, 1102915.2500, 1059377.3750,
         1048613.0000, 1041604.3125, 1028852.8750, 1021236.7500, 1004442.7500],
        [1275452.0000, 1233057.2500, 1100397.8750, 1090265.5000, 1023866.8125,
         1011468.5625, 1005144.1250,  994390.2500,  975492.7500,  969942.6250],
        [1477736.7500, 1471366.5000, 1400464.0000, 1390616.7500, 1376398.8750,
         1373591.3750, 1356315.2500, 1344559.6250, 1335844.1250, 1332473.6250],
        [ 976758.8125,  763208.9375,  748073.2500,  699380.7500,  647837.6250,
          639078.5000,  559504.5625,  500384.8125,  487427.1875,  464369.6250],
        [ 835179.2500,  790321.0000,  719205.0625,  714892.1875,  711536.8750,
          699610.2500,  684484.5625,  680629.7500,  679085.3750,  672477.5625],
        [ 984492.8125,  863775.6250,  848033.0000,  803153.4375,  800554.9375,
          689789.9375,  642596.3125,  608367.1875,  592773.2500,  576238.2500],
        [ 819953.1250,  749202.6875,  660276.8125,  650089.8125,  649422.4375,
          629204.6875,  605482.3750,  603273.1875,  592292.3750,  591856.4375],
        [ 861291.4375,  658101.6875,  641832.5625,  581971.9375,  575227.4375,
          506596.6562,  498515.2812,  464201.8438,  446803.1562,  446797.5938],
        [ 499169.8750,  276363.5000,  260059.3281,  258357.1250,  252503.5000,
          246304.5000,  236219.2500,  212605.7500,  207567.2812,  203983.2031],
        [1154112.6250, 1117152.2500, 1103651.7500, 1070660.3750,  990315.1875,
          923701.3125,  894218.8125,  873449.2500,  869527.2500,  784648.6875],
        [ 692496.2500,  609785.0625,  580032.6875,  479144.6250,  472798.4062,
          300251.9375,  296298.0312,  280628.8750,  268691.1562,  267510.4688],
        [ 878630.8125,  817417.2500,  764542.8125,  731131.6875,  703512.8750,
          691315.8125,  670344.1250,  668334.0000,  650069.3750,  631242.8125],
        [1276445.0000, 1271290.3750, 1199031.2500, 1136468.0000, 1135945.6250,
         1127392.6250, 1107687.0000, 1099546.0000, 1089358.1250, 1053738.7500],
        [1092257.3750, 1087491.8750, 1034379.9375, 1029365.1875, 1023384.5625,
          964901.5000,  916437.9375,  815665.1875,  807748.4375,  795066.9375],
        [1075102.0000, 1047080.0625, 1046357.3750,  883901.1875,  864200.8125,
          840429.5000,  791345.9375,  739176.7500,  698620.1250,  649565.5000],
        [1095898.8750,  991190.0625,  783342.5000,  634857.9375,  620739.0000,
          606853.0625,  515442.7188,  470173.3750,  467869.4062,  453552.8438],
        [1039681.9375,  948120.2500,  733313.1875,  629408.1875,  554283.4375,
          505405.6875,  490647.5000,  453665.3438,  434972.8438,  425126.9375],
        [1167054.5000, 1020245.7500,  979754.5000,  956650.6250,  948815.8125,
          941019.1875,  922119.6250,  909940.2500,  904670.7500,  888115.8125],
        [1229752.6250, 1190421.1250, 1183789.1250, 1163637.0000, 1115627.7500,
         1071893.5000, 1052620.8750, 1047437.6250, 1034752.0000, 1024542.6875],
        [1053087.7500,  958974.3750,  900788.0625,  839189.6875,  753537.9375,
          736059.0625,  639101.1250,  619542.5625,  600789.4375,  550506.2500],
        [1029642.0000,  701410.6875,  672904.8125,  642085.3750,  630226.2500,
          608957.5000,  608159.5625,  603173.0625,  568646.8125,  544081.7500],
        [1046556.9375,  988006.8750,  713847.1250,  630594.8125,  585436.0625,
          581069.6250,  556743.1875,  518671.0938,  516265.7500,  512635.2188],
        [1003517.7500,  552680.9375,  487593.1250,  468436.8438,  466241.8438,
          445570.0000,  416631.9688,  403778.5312,  382503.1562,  374399.0312],
        [ 924100.4375,  827115.0625,  819402.0000,  816707.4375,  792496.8750,
          772536.1250,  752687.5000,  751678.2500,  719410.8750,  709063.7500],
        [ 709182.0625,  614823.2500,  584471.5000,  555564.1250,  508939.9062,
          469897.6875,  435297.7812,  415125.6250,  406338.8750,  359826.0312],
        [1075878.5000,  938025.6250,  802622.0625,  802586.0625,  781082.9375,
          766505.2500,  619378.8750,  601640.3125,  581986.3750,  562500.6875],
        [1023723.3125,  988592.1250,  937515.8750,  840640.2500,  761445.1875,
          745849.3125,  731661.0625,  729246.6250,  708045.4375,  689317.0625]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 498481.0312,       0.0000],
         [ 358758.0312,       0.0000],
         [ 238285.4688,       0.0000],
         ...,
         [ 181104.0156,       0.0000],
         [ 172810.1719,       0.0000],
         [ 149685.5781,       0.0000]],

        [[1413323.0000,       0.0000],
         [1320893.0000,       0.0000],
         [1312217.1250,       0.0000],
         ...,
         [1238851.7500,       0.0000],
         [1236573.6250,       0.0000],
         [1235953.5000,       0.0000]],

        [[1370300.8750,       0.0000],
         [1346684.7500,       0.0000],
         [1203893.0000,       0.0000],
         ...,
         [1070701.2500,       0.0000],
         [1069697.0000,       0.0000],
         [1048723.1250,       0.0000]],

        ...,

        [[ 709182.0625,       0.0000],
         [ 614823.2500,       0.0000],
         [ 584471.5000,       0.0000],
         ...,
         [ 415125.6250,       0.0000],
         [ 406338.8750,       0.0000],
         [      0.0000,  359826.0312]],

        [[      0.0000, 1075878.5000],
         [ 938025.6250,       0.0000],
         [ 802622.0625,       0.0000],
         ...,
         [ 601640.3125,       0.0000],
         [ 581986.3750,       0.0000],
         [ 562500.6875,       0.0000]],

        [[      0.0000, 1023723.3125],
         [ 988592.1250,       0.0000],
         [ 937515.8750,       0.0000],
         ...,
         [ 729246.6250,       0.0000],
         [ 708045.4375,       0.0000],
         [      0.0000,  689317.0625]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2207357.0000,   211230.0469],
        [12893721.0000,        0.0000],
        [11523867.0000,        0.0000],
        [10258332.0000,        0.0000],
        [ 7482349.0000,  2082616.0000],
        [11760091.0000,        0.0000],
        [12405144.0000,  1453574.2500],
        [10703768.0000,  1089022.6250],
        [13442238.0000,        0.0000],
        [ 7038014.0000,  1545800.6250],
        [ 9052107.0000,  2307305.2500],
        [ 9708910.0000,        0.0000],
        [ 7789790.0000,   632557.1250],
        [13583396.0000,        0.0000],
        [11020447.0000,        0.0000],
        [10352342.0000,        0.0000],
        [13325897.0000,        0.0000],
        [13114646.0000,        0.0000],
        [ 8034170.5000,  2059118.0000],
        [11401061.0000,        0.0000],
        [11681362.0000,  1266892.1250],
        [ 8457006.0000,  1944587.5000],
        [10631408.0000,        0.0000],
        [12240292.0000,        0.0000],
        [ 9051913.0000,   895239.3125],
        [ 9593278.0000,        0.0000],
        [ 8826602.0000,  1157514.2500],
        [ 9649357.0000,        0.0000],
        [11527754.0000,        0.0000],
        [11285440.0000,        0.0000],
        [11916028.0000,        0.0000],
        [11029405.0000,        0.0000],
        [ 4454577.5000,  2571358.2500],
        [ 9729051.0000,  1014495.1875],
        [11858428.0000,        0.0000],
        [ 9598125.0000,  2312011.0000],
        [ 7580637.5000,  2012255.3750],
        [ 9591913.0000,  1156103.1250],
        [ 8609137.0000,  2070340.5000],
        [12523522.0000,  1335844.1250],
        [ 5534227.5000,   951796.8125],
        [ 7187421.5000,        0.0000],
        [ 7409774.5000,        0.0000],
        [ 6551054.0000,        0.0000],
        [ 4069838.2500,  1611501.5000],
        [ 2153963.5000,   499169.8750],
        [ 9781437.0000,        0.0000],
        [ 2930401.5000,  1317235.7500],
        [ 3665969.0000,  3540572.5000],
        [ 9271076.0000,  2225826.0000],
        [ 8474441.0000,  1092257.3750],
        [ 7105256.5000,  1530522.7500],
        [ 3812859.2500,  2827060.5000],
        [ 4975906.5000,  1238718.8750],
        [ 8750271.0000,   888115.8125],
        [ 9998846.0000,  1115627.7500],
        [ 3899536.0000,  3752040.0000],
        [ 4690760.0000,  1918527.7500],
        [ 3901415.5000,  2748411.0000],
        [ 3041375.7500,  1959977.2500],
        [ 6338672.0000,  1546526.0000],
        [ 4699641.0000,   359826.0312],
        [ 6456328.5000,  1075878.5000],
        [ 6442996.0000,  1713040.3750]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 50.0
Top1 accuracy for validation set is 50.0 size is torch.Size([64, 1])
Epoch 106/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:05<31:43, 65.65s/it]  7%|▋         | 2/30 [01:06<12:49, 27.47s/it] 10%|█         | 3/30 [01:07<06:52, 15.28s/it] 13%|█▎        | 4/30 [01:07<04:08,  9.54s/it] 17%|█▋        | 5/30 [01:08<02:39,  6.37s/it] 20%|██        | 6/30 [01:09<01:47,  4.46s/it] 23%|██▎       | 7/30 [01:10<01:14,  3.25s/it] 27%|██▋       | 8/30 [01:10<00:53,  2.45s/it] 30%|███       | 9/30 [01:11<00:40,  1.92s/it] 33%|███▎      | 10/30 [01:12<00:31,  1.56s/it] 37%|███▋      | 11/30 [01:13<00:24,  1.31s/it] 40%|████      | 12/30 [01:13<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:14<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:15<00:15,  1.06it/s] 50%|█████     | 15/30 [01:16<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:16<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:17<00:10,  1.23it/s] 60%|██████    | 18/30 [01:18<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:19<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:19<00:07,  1.30it/s] 70%|███████   | 21/30 [01:20<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:21<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:22<00:05,  1.32it/s] 80%|████████  | 24/30 [01:22<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:23<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:24<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:25<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:25<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:26<00:00,  1.33it/s]100%|██████████| 30/30 [01:27<00:00,  1.33it/s]100%|██████████| 30/30 [01:27<00:00,  2.92s/it]
Epoch loss is 3.96591477394104
Epoch 107/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:02, 58.03s/it]  7%|▋         | 2/30 [01:00<11:46, 25.23s/it] 10%|█         | 3/30 [01:01<06:19, 14.05s/it] 13%|█▎        | 4/30 [01:01<03:48,  8.80s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.90s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 3.9510072549184163
Epoch 108/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:14, 58.43s/it]  7%|▋         | 2/30 [01:00<11:46, 25.23s/it] 10%|█         | 3/30 [01:01<06:25, 14.27s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.93s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.98s/it] 20%|██        | 6/30 [01:03<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 3.978480108579
Epoch 109/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:44, 59.47s/it]  7%|▋         | 2/30 [01:00<11:38, 24.93s/it] 10%|█         | 3/30 [01:01<06:15, 13.92s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.72s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.85s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 3.9503435929616293
Epoch 110/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:04, 60.14s/it]  7%|▋         | 2/30 [01:00<11:45, 25.21s/it] 10%|█         | 3/30 [01:01<06:19, 14.04s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.79s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 3.929181933403015
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0139,  0.0138, -0.0388,  ..., -0.0184, -0.0304,  0.0166],
        [-0.0543,  0.0319,  0.0010,  ...,  0.0022, -0.0250, -0.0144],
        [-0.0783, -0.0028, -0.0207,  ...,  0.0423, -0.0113, -0.0390],
        ...,
        [ 0.0035, -0.0277, -0.0396,  ..., -0.0554, -0.0187,  0.0044],
        [-0.0595,  0.0107, -0.0039,  ..., -0.0474,  0.0303, -0.0176],
        [-0.0436,  0.0255,  0.0042,  ..., -0.0303,  0.0303, -0.0582]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9271, 0.9037, 0.8664, 0.8650, 0.8557, 0.8551, 0.8537, 0.8388, 0.8386,
         0.8333],
        [0.9905, 0.9856, 0.9851, 0.9846, 0.9846, 0.9832, 0.9820, 0.9818, 0.9814,
         0.9812],
        [0.9870, 0.9864, 0.9811, 0.9758, 0.9727, 0.9726, 0.9706, 0.9696, 0.9695,
         0.9683],
        [0.9864, 0.9716, 0.9707, 0.9680, 0.9679, 0.9656, 0.9648, 0.9568, 0.9566,
         0.9538],
        [0.9730, 0.9676, 0.9650, 0.9606, 0.9597, 0.9535, 0.9531, 0.9527, 0.9527,
         0.9489],
        [0.9809, 0.9800, 0.9794, 0.9782, 0.9776, 0.9767, 0.9750, 0.9735, 0.9694,
         0.9689],
        [0.9924, 0.9924, 0.9921, 0.9920, 0.9896, 0.9879, 0.9868, 0.9840, 0.9834,
         0.9832],
        [0.9908, 0.9818, 0.9775, 0.9766, 0.9749, 0.9723, 0.9714, 0.9711, 0.9697,
         0.9694],
        [0.9883, 0.9882, 0.9878, 0.9875, 0.9870, 0.9870, 0.9862, 0.9859, 0.9850,
         0.9849],
        [0.9821, 0.9762, 0.9546, 0.9474, 0.9465, 0.9436, 0.9429, 0.9425, 0.9425,
         0.9390],
        [0.9825, 0.9815, 0.9768, 0.9756, 0.9751, 0.9750, 0.9729, 0.9720, 0.9714,
         0.9705],
        [0.9800, 0.9791, 0.9739, 0.9720, 0.9605, 0.9595, 0.9528, 0.9520, 0.9480,
         0.9479],
        [0.9811, 0.9783, 0.9644, 0.9642, 0.9487, 0.9427, 0.9413, 0.9388, 0.9334,
         0.9303],
        [0.9920, 0.9895, 0.9886, 0.9881, 0.9876, 0.9874, 0.9874, 0.9864, 0.9863,
         0.9863],
        [0.9832, 0.9760, 0.9760, 0.9739, 0.9708, 0.9697, 0.9694, 0.9685, 0.9684,
         0.9679],
        [0.9808, 0.9787, 0.9762, 0.9661, 0.9652, 0.9631, 0.9624, 0.9618, 0.9610,
         0.9602],
        [0.9935, 0.9930, 0.9891, 0.9875, 0.9863, 0.9850, 0.9848, 0.9846, 0.9844,
         0.9844],
        [0.9931, 0.9885, 0.9862, 0.9862, 0.9858, 0.9847, 0.9829, 0.9825, 0.9815,
         0.9810],
        [0.9828, 0.9799, 0.9710, 0.9700, 0.9625, 0.9610, 0.9570, 0.9567, 0.9559,
         0.9542],
        [0.9809, 0.9793, 0.9781, 0.9773, 0.9758, 0.9721, 0.9721, 0.9714, 0.9701,
         0.9693],
        [0.9897, 0.9888, 0.9873, 0.9866, 0.9852, 0.9834, 0.9808, 0.9802, 0.9799,
         0.9795],
        [0.9825, 0.9778, 0.9759, 0.9725, 0.9666, 0.9657, 0.9582, 0.9576, 0.9575,
         0.9567],
        [0.9893, 0.9812, 0.9770, 0.9688, 0.9685, 0.9679, 0.9630, 0.9625, 0.9597,
         0.9585],
        [0.9870, 0.9859, 0.9852, 0.9834, 0.9832, 0.9804, 0.9758, 0.9756, 0.9744,
         0.9730],
        [0.9874, 0.9821, 0.9648, 0.9639, 0.9622, 0.9615, 0.9606, 0.9583, 0.9567,
         0.9554],
        [0.9772, 0.9663, 0.9634, 0.9633, 0.9625, 0.9610, 0.9602, 0.9597, 0.9590,
         0.9553],
        [0.9764, 0.9760, 0.9699, 0.9695, 0.9628, 0.9624, 0.9622, 0.9581, 0.9578,
         0.9547],
        [0.9747, 0.9711, 0.9708, 0.9658, 0.9640, 0.9608, 0.9576, 0.9572, 0.9554,
         0.9548],
        [0.9871, 0.9837, 0.9829, 0.9774, 0.9760, 0.9728, 0.9709, 0.9701, 0.9678,
         0.9677],
        [0.9815, 0.9784, 0.9779, 0.9775, 0.9736, 0.9732, 0.9729, 0.9719, 0.9704,
         0.9673],
        [0.9853, 0.9833, 0.9802, 0.9797, 0.9783, 0.9782, 0.9769, 0.9767, 0.9767,
         0.9759],
        [0.9780, 0.9778, 0.9776, 0.9772, 0.9744, 0.9690, 0.9663, 0.9660, 0.9658,
         0.9654],
        [0.9656, 0.9457, 0.9357, 0.9329, 0.9302, 0.9302, 0.9273, 0.9263, 0.9248,
         0.9229],
        [0.9803, 0.9756, 0.9746, 0.9722, 0.9670, 0.9648, 0.9646, 0.9629, 0.9626,
         0.9612],
        [0.9797, 0.9797, 0.9756, 0.9749, 0.9747, 0.9739, 0.9736, 0.9736, 0.9730,
         0.9727],
        [0.9815, 0.9811, 0.9805, 0.9801, 0.9780, 0.9766, 0.9762, 0.9756, 0.9747,
         0.9726],
        [0.9763, 0.9720, 0.9691, 0.9617, 0.9601, 0.9580, 0.9549, 0.9529, 0.9464,
         0.9437],
        [0.9769, 0.9724, 0.9718, 0.9711, 0.9674, 0.9671, 0.9652, 0.9640, 0.9639,
         0.9638],
        [0.9796, 0.9787, 0.9707, 0.9675, 0.9674, 0.9662, 0.9650, 0.9642, 0.9592,
         0.9587],
        [0.9940, 0.9939, 0.9902, 0.9891, 0.9891, 0.9889, 0.9878, 0.9878, 0.9877,
         0.9871],
        [0.9635, 0.9461, 0.9415, 0.9364, 0.9306, 0.9289, 0.9211, 0.9127, 0.9103,
         0.9102],
        [0.9498, 0.9454, 0.9445, 0.9439, 0.9433, 0.9374, 0.9373, 0.9370, 0.9369,
         0.9368],
        [0.9638, 0.9546, 0.9523, 0.9522, 0.9498, 0.9377, 0.9326, 0.9316, 0.9290,
         0.9269],
        [0.9536, 0.9453, 0.9336, 0.9314, 0.9303, 0.9256, 0.9243, 0.9243, 0.9224,
         0.9223],
        [0.9548, 0.9346, 0.9326, 0.9233, 0.9233, 0.9213, 0.9189, 0.9137, 0.9090,
         0.9071],
        [0.9175, 0.8627, 0.8594, 0.8581, 0.8566, 0.8552, 0.8540, 0.8502, 0.8490,
         0.8454],
        [0.9765, 0.9754, 0.9725, 0.9700, 0.9654, 0.9594, 0.9574, 0.9560, 0.9558,
         0.9495],
        [0.9393, 0.9283, 0.9277, 0.9151, 0.9113, 0.8887, 0.8706, 0.8702, 0.8652,
         0.8646],
        [0.9571, 0.9477, 0.9464, 0.9389, 0.9387, 0.9330, 0.9329, 0.9313, 0.9302,
         0.9294],
        [0.9823, 0.9807, 0.9790, 0.9729, 0.9723, 0.9720, 0.9715, 0.9713, 0.9700,
         0.9687],
        [0.9700, 0.9683, 0.9680, 0.9678, 0.9662, 0.9631, 0.9568, 0.9494, 0.9491,
         0.9490],
        [0.9684, 0.9680, 0.9664, 0.9558, 0.9536, 0.9507, 0.9484, 0.9414, 0.9384,
         0.9353],
        [0.9718, 0.9666, 0.9471, 0.9336, 0.9333, 0.9320, 0.9225, 0.9134, 0.9093,
         0.9069],
        [0.9685, 0.9582, 0.9431, 0.9268, 0.9182, 0.9168, 0.9110, 0.9092, 0.9065,
         0.9057],
        [0.9748, 0.9685, 0.9648, 0.9635, 0.9633, 0.9626, 0.9620, 0.9620, 0.9612,
         0.9601],
        [0.9796, 0.9764, 0.9763, 0.9756, 0.9745, 0.9695, 0.9683, 0.9676, 0.9674,
         0.9674],
        [0.9701, 0.9623, 0.9556, 0.9497, 0.9473, 0.9415, 0.9363, 0.9321, 0.9294,
         0.9271],
        [0.9677, 0.9374, 0.9354, 0.9332, 0.9318, 0.9281, 0.9277, 0.9274, 0.9223,
         0.9218],
        [0.9644, 0.9619, 0.9414, 0.9302, 0.9282, 0.9225, 0.9196, 0.9176, 0.9163,
         0.9154],
        [0.9667, 0.9209, 0.9069, 0.9045, 0.9038, 0.8983, 0.8980, 0.8971, 0.8971,
         0.8910],
        [0.9594, 0.9515, 0.9501, 0.9493, 0.9493, 0.9472, 0.9456, 0.9423, 0.9404,
         0.9398],
        [0.9357, 0.9246, 0.9219, 0.9209, 0.9145, 0.9092, 0.9011, 0.8962, 0.8959,
         0.8911],
        [0.9705, 0.9607, 0.9497, 0.9474, 0.9474, 0.9440, 0.9341, 0.9281, 0.9256,
         0.9233],
        [0.9678, 0.9643, 0.9566, 0.9532, 0.9472, 0.9427, 0.9415, 0.9404, 0.9396,
         0.9390]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 1, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 1, 1, 0, 1],
        [0, 0, 1, 1, 1, 0, 1, 0, 1, 1],
        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 1, 0, 0, 1, 1, 1, 0, 1],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 564843.8750,  404383.9375,  237438.4219,  232535.0625,  203696.6562,
          201916.8906,  197936.5156,  159985.0781,  159479.6250,  147828.7188],
        [1397730.0000, 1302056.3750, 1292898.6250, 1283638.7500, 1283401.3750,
         1258708.8750, 1237668.5000, 1233124.2500, 1227496.0000, 1224020.6250],
        [1328252.6250, 1317674.7500, 1221010.2500, 1132445.3750, 1083464.8750,
         1082663.3750, 1051040.8750, 1036400.1875, 1034713.4375, 1017216.5000],
        [1317209.8750, 1066338.7500, 1053205.2500, 1012967.6875, 1011604.5625,
          978785.0625,  967302.5000,  863840.6875,  861468.8750,  826926.5000],
        [1088039.5000, 1007069.8750,  971137.5625,  911780.0625,  899847.8750,
          823449.7500,  818954.3750,  814506.1250,  814325.9375,  770995.6875],
        [1218108.3750, 1202394.5000, 1191651.1250, 1172845.2500, 1162064.5000,
         1147507.8750, 1120375.3750, 1096367.1250, 1033835.6250, 1026546.7500],
        [1435876.6250, 1434837.7500, 1430370.1250, 1428295.5000, 1378494.1250,
         1346200.5000, 1325462.3750, 1274028.5000, 1261675.0000, 1258459.2500],
        [1403068.1250, 1234775.3750, 1160166.6250, 1145044.8750, 1118769.6250,
         1077617.0000, 1063816.6250, 1058425.0000, 1038273.0000, 1032877.6875],
        [1353071.1250, 1351554.5000, 1344267.2500, 1337940.1250, 1329683.5000,
         1328881.0000, 1313599.5000, 1308756.6250, 1291602.1250, 1290198.7500],
        [1239494.7500, 1138327.1250,  836193.0625,  755092.5000,  745646.5625,
          714900.3750,  708145.3750,  704038.3750,  703469.2500,  669133.7500],
        [1245627.0000, 1227891.6250, 1149365.3750, 1129633.3750, 1121407.0000,
         1120279.3750, 1086243.8750, 1072275.8750, 1063355.2500, 1049522.5000],
        [1203335.1250, 1187781.0000, 1102570.2500, 1073475.1250,  910555.6875,
          896696.1875,  815690.8125,  805822.6250,  761086.5625,  760776.7500],
        [1221682.3750, 1173798.5000,  962564.3125,  959146.3750,  769532.3750,
          706025.9375,  691712.1875,  667892.4375,  618404.4375,  591234.2500],
        [1426663.1250, 1377379.7500, 1360120.7500, 1350424.5000, 1340264.3750,
         1337121.2500, 1336141.0000, 1318411.2500, 1315964.2500, 1315896.5000],
        [1258621.2500, 1136306.3750, 1135411.7500, 1102535.5000, 1054832.6250,
         1038261.0625, 1033650.3125, 1020023.9375, 1019076.8750, 1011289.1250],
        [1217039.0000, 1180481.3750, 1138356.5000,  985802.5000,  973599.5625,
          945079.7500,  934604.8125,  927261.7500,  916719.3125,  906872.5625],
        [1458854.2500, 1448173.7500, 1368745.2500, 1337947.7500, 1316096.1250,
         1290725.5000, 1287629.6250, 1283723.3750, 1279971.5000, 1279958.0000],
        [1449146.5000, 1357180.7500, 1313950.3750, 1313295.1250, 1306139.3750,
         1286041.6250, 1253480.0000, 1246331.6250, 1228898.0000, 1219860.2500],
        [1251924.6250, 1201651.6250, 1057106.6250, 1042640.8750,  936344.5000,
          916482.4375,  865758.1875,  862174.8750,  852109.9375,  832096.1875],
        [1218787.0000, 1190222.3750, 1170622.6250, 1157868.6250, 1132038.2500,
         1074639.6250, 1074021.8750, 1063526.5000, 1043929.3750, 1032649.1875],
        [1380687.3750, 1363216.5000, 1335199.6250, 1322130.6250, 1296239.3750,
         1263277.5000, 1216047.0000, 1206128.1250, 1201147.3750, 1194533.2500],
        [1246689.5000, 1166084.3750, 1134581.3750, 1079886.3750,  992466.9375,
          979973.1250,  881035.5625,  873380.1250,  871829.7500,  862542.5000],
        [1374127.2500, 1222656.7500, 1152733.2500, 1025122.2500, 1021084.8125,
         1011624.8750,  943379.6250,  936660.6250,  899315.9375,  885127.6250],
        [1328426.1250, 1307536.3750, 1294586.5000, 1261871.1250, 1258100.3750,
         1208972.6250, 1132137.6250, 1128587.8750, 1109415.6250, 1088115.2500],
        [1336469.7500, 1238376.8750,  967933.7500,  955140.0625,  932389.0000,
          922984.5625,  912049.6250,  881995.6250,  862592.6875,  845786.8125],
        [1155429.6250,  989437.1875,  949237.5625,  947825.5000,  935940.0000,
          917108.5000,  906776.5625,  900273.5625,  890779.3750,  845310.2500],
        [1142287.6250, 1135436.6250, 1040824.8125, 1035436.0625,  940802.0000,
          934997.9375,  932071.5625,  879050.6875,  875632.7500,  837437.2500],
        [1115671.3750, 1058844.0000, 1054182.0000,  981220.6875,  957424.5625,
          914758.7500,  872942.1250,  867812.3750,  846135.3125,  839063.2500],
        [1330568.8750, 1267155.5000, 1253138.2500, 1158673.8750, 1135114.0000,
         1085201.1250, 1056038.5000, 1043613.7500, 1009804.0625, 1009382.3125],
        [1228753.7500, 1174917.3750, 1167562.1250, 1159860.1250, 1096813.7500,
         1091689.7500, 1087360.1250, 1071561.3750, 1048185.0625, 1003459.3750],
        [1297665.6250, 1261145.6250, 1206282.2500, 1197623.3750, 1172917.8750,
         1171949.6250, 1149952.0000, 1148024.5000, 1146767.2500, 1134894.2500],
        [1169551.3750, 1165285.1250, 1161825.1250, 1155080.5000, 1110234.8750,
         1027150.9375,  989503.2500,  984578.3125,  981948.0625,  976567.8750],
        [ 978508.8125,  736793.6875,  639049.8750,  613501.8750,  590670.6875,
          590556.8750,  566487.1250,  558343.6250,  546445.1875,  532161.3125],
        [1207255.8750, 1129057.1250, 1113179.0000, 1076044.6250,  999308.0000,
          968436.0000,  965150.0000,  941849.6875,  938129.4375,  919319.5000],
        [1197765.0000, 1197090.1250, 1129629.0000, 1118756.8750, 1114388.8750,
         1101655.8750, 1097943.8750, 1097478.1250, 1088086.2500, 1083312.0000],
        [1228123.5000, 1221508.7500, 1211775.2500, 1204895.7500, 1167976.5000,
         1144986.0000, 1138751.6250, 1129998.6250, 1115659.6250, 1082083.2500],
        [1140329.6250, 1073052.3750, 1029601.7500,  926490.1250,  904404.1875,
          878648.3750,  840137.8125,  816825.8125,  744204.4375,  716183.3125],
        [1150409.3750, 1078943.5000, 1070190.8750, 1058500.7500, 1004605.5625,
         1000219.5625,  974048.1250,  956359.5625,  955911.8750,  954815.8125],
        [1195237.5000, 1179852.2500, 1052307.6250, 1006160.7500, 1003818.3125,
          987106.4375,  970756.9375,  960247.3125,  893749.8750,  887006.1250],
        [1468359.6250, 1466274.6250, 1391764.5000, 1370172.7500, 1369355.0000,
         1364995.0000, 1345014.8750, 1343703.2500, 1341684.0000, 1330818.8750],
        [ 949571.6250,  740559.6875,  693584.8125,  645476.8750,  593663.6875,
          579800.3750,  518724.0000,  459775.7188,  444016.1875,  443940.0000],
        [ 780753.7500,  734061.1875,  724050.6875,  717744.2500,  711590.5000,
          654789.9375,  653717.3750,  651040.9375,  649760.6875,  648815.1250],
        [ 954654.6875,  836411.6250,  809787.7500,  808549.1875,  780788.0625,
          656876.5000,  610879.3750,  602086.8750,  580638.1250,  562846.2500],
        [ 824626.1250,  732626.7500,  619500.6250,  600831.2500,  591475.0625,
          553163.3750,  542949.6875,  542851.8750,  528228.5625,  527316.0000],
        [ 839429.0625,  628841.1875,  611219.6250,  535285.5625,  535192.6250,
          519901.7188,  502711.3438,  466194.2500,  436074.7812,  424446.3750],
        [ 492113.3438,  225169.5156,  214815.4219,  210691.6719,  206407.1719,
          202257.0469,  198740.4062,  188308.3281,  185176.2969,  175729.8438],
        [1144236.1250, 1125797.2500, 1079921.5000, 1042235.3125,  976256.8750,
          896455.0000,  871187.3125,  852999.4375,  851574.6250,  777293.9375],
        [ 672800.2500,  574371.7500,  569949.7500,  476019.7188,  450701.4688,
          326476.2812,  251839.5156,  250523.2188,  233310.7500,  231366.4375],
        [ 866771.0625,  758094.8125,  744190.1875,  668790.5000,  667004.5000,
          614904.7500,  613519.4375,  599605.6875,  590589.0000,  583383.9375],
        [1242735.3750, 1215138.2500, 1185954.1250, 1087314.5000, 1077419.6250,
         1073087.1250, 1064465.1250, 1062454.0000, 1042342.6250, 1023644.1875],
        [1042758.2500, 1017187.4375, 1013255.5625, 1010693.2500,  987566.9375,
          945244.7500,  862754.7500,  776701.8750,  773296.8750,  772430.8125],
        [1019492.0000, 1013039.1875,  989732.6250,  851519.3750,  825014.7500,
          791282.6250,  765258.3750,  692936.9375,  663394.8125,  635230.3750],
        [1069594.8750,  993158.1875,  752140.8125,  619404.8750,  617083.6250,
          605384.2500,  528564.1250,  464104.8750,  438238.0938,  423124.7812],
        [1020884.2500,  880680.2500,  710297.5000,  562215.3125,  497438.1875,
          487498.7500,  448570.2812,  437588.2500,  420978.2500,  416023.7500],
        [1116186.3750, 1020056.0625,  968497.0000,  950453.1875,  946706.1875,
          937975.5625,  930400.1250,  929326.2500,  919956.3125,  905555.5000],
        [1196382.5000, 1142401.0000, 1140647.1250, 1129726.0000, 1111803.0000,
         1034429.3750, 1016928.4375, 1007020.8750, 1005092.3750, 1005074.1250],
        [1043739.1875,  933412.0625,  848176.9375,  780138.9375,  754229.6250,
          693657.6250,  644179.9375,  606461.3750,  583335.0000,  564690.8750],
        [1009072.3750,  654323.0625,  635657.0000,  616467.8125,  604323.4375,
          572965.6875,  569356.0000,  566883.7500,  527603.6875,  523273.2500],
        [ 962541.3125,  928511.2500,  693176.8125,  590620.5000,  574045.9375,
          529244.0625,  507245.8750,  493054.7812,  484417.3125,  478149.5000],
        [ 995095.0625,  517179.3750,  423017.4688,  408973.9375,  405076.3438,
          374497.2500,  372817.4062,  367733.9688,  367719.9062,  337222.8750],
        [ 896473.0000,  800618.3125,  784074.1875,  775874.1875,  775300.1875,
          753019.9375,  735452.1250,  701887.7500,  682889.1250,  677600.0625],
        [ 638750.6875,  545234.3750,  524047.8125,  516865.8125,  471677.0000,
          437154.0625,  389624.6562,  363497.5625,  361586.5938,  337704.6875],
        [1050004.0000,  913442.3750,  780172.5000,  754588.5625,  754552.6250,
          719174.1875,  624058.8125,  573236.2500,  552732.0625,  534813.5625],
        [1010330.9375,  960545.9375,  860831.6250,  820643.8750,  753071.0000,
          705802.4375,  693409.6250,  682869.5625,  674850.1875,  669258.1875]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 564843.8750,       0.0000],
         [ 404383.9375,       0.0000],
         [ 237438.4219,       0.0000],
         ...,
         [ 159985.0781,       0.0000],
         [ 159479.6250,       0.0000],
         [ 147828.7188,       0.0000]],

        [[1397730.0000,       0.0000],
         [1302056.3750,       0.0000],
         [1292898.6250,       0.0000],
         ...,
         [1233124.2500,       0.0000],
         [1227496.0000,       0.0000],
         [1224020.6250,       0.0000]],

        [[1328252.6250,       0.0000],
         [1317674.7500,       0.0000],
         [1221010.2500,       0.0000],
         ...,
         [1036400.1875,       0.0000],
         [1034713.4375,       0.0000],
         [1017216.5000,       0.0000]],

        ...,

        [[ 638750.6875,       0.0000],
         [ 545234.3750,       0.0000],
         [ 524047.8125,       0.0000],
         ...,
         [ 363497.5625,       0.0000],
         [ 361586.5938,       0.0000],
         [      0.0000,  337704.6875]],

        [[      0.0000, 1050004.0000],
         [ 913442.3750,       0.0000],
         [ 780172.5000,       0.0000],
         ...,
         [ 573236.2500,       0.0000],
         [ 552732.0625,       0.0000],
         [      0.0000,  534813.5625]],

        [[      0.0000, 1010330.9375],
         [ 960545.9375,       0.0000],
         [ 860831.6250,       0.0000],
         ...,
         [ 682869.5625,       0.0000],
         [ 674850.1875,       0.0000],
         [ 669258.1875,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2306348.0000,   203696.6562],
        [12740743.0000,        0.0000],
        [11304882.0000,        0.0000],
        [ 9959650.0000,        0.0000],
        [ 6941900.0000,  1978207.5000],
        [ 9311314.0000,  2060382.3750],
        [10884870.0000,  2688829.5000],
        [11332834.0000,        0.0000],
        [13249555.0000,        0.0000],
        [ 6790215.0000,  1424226.2500],
        [ 9107082.0000,  2158519.7500],
        [ 9517790.0000,        0.0000],
        [ 8361993.0000,        0.0000],
        [13478387.0000,        0.0000],
        [10810010.0000,        0.0000],
        [10125816.0000,        0.0000],
        [13351825.0000,        0.0000],
        [12974325.0000,        0.0000],
        [ 7824839.0000,  1993451.1250],
        [11158305.0000,        0.0000],
        [11577459.0000,  1201147.3750],
        [ 9108496.0000,   979973.1250],
        [10471833.0000,        0.0000],
        [12117749.0000,        0.0000],
        [ 9009932.0000,   845786.8125],
        [ 9438118.0000,        0.0000],
        [ 8611690.0000,  1142287.6250],
        [ 9508054.0000,        0.0000],
        [11348690.0000,        0.0000],
        [11130162.0000,        0.0000],
        [11887222.0000,        0.0000],
        [10721725.0000,        0.0000],
        [ 4056477.0000,  2296042.0000],
        [ 9258421.0000,   999308.0000],
        [11226106.0000,        0.0000],
        [ 9400101.0000,  2245658.2500],
        [ 7092421.0000,  1977456.5000],
        [ 8098780.0000,  2105225.2500],
        [ 8113178.5000,  2023064.5000],
        [13792142.0000,        0.0000],
        [ 5181156.5000,   887956.1875],
        [ 6926324.0000,        0.0000],
        [ 7203518.5000,        0.0000],
        [ 6063569.0000,        0.0000],
        [ 3943728.7500,  1555567.7500],
        [ 1807295.7500,   492113.3438],
        [ 9617958.0000,        0.0000],
        [ 2852928.5000,  1184430.6250],
        [ 2839376.2500,  3867477.5000],
        [ 8914153.0000,  2160401.5000],
        [ 8184703.0000,  1017187.4375],
        [ 6818248.0000,  1428653.2500],
        [ 3737479.5000,  2773319.0000],
        [ 4674439.0000,  1207735.7500],
        [ 9625112.0000,        0.0000],
        [ 9677702.0000,  1111803.0000],
        [ 3846554.5000,  3605467.2500],
        [ 4473062.0000,  1806864.2500],
        [ 3656777.7500,  2584229.2500],
        [ 2682562.0000,  1886771.7500],
        [ 6099681.5000,  1483507.5000],
        [ 4248438.5000,   337704.6875],
        [ 5671957.0000,  1584817.5000],
        [ 6127873.0000,  1703740.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 51.5625
Top1 accuracy for validation set is 51.5625 size is torch.Size([64, 1])
Epoch 111/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:57, 57.84s/it]  7%|▋         | 2/30 [00:58<11:19, 24.26s/it] 10%|█         | 3/30 [00:59<06:05, 13.52s/it] 13%|█▎        | 4/30 [01:00<03:43,  8.59s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.76s/it] 20%|██        | 6/30 [01:01<01:37,  4.06s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 3.9128923336664836
Epoch 112/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:28, 56.83s/it]  7%|▋         | 2/30 [00:58<11:24, 24.43s/it] 10%|█         | 3/30 [00:59<06:07, 13.62s/it] 13%|█▎        | 4/30 [01:00<03:42,  8.54s/it] 17%|█▋        | 5/30 [01:00<02:23,  5.73s/it] 20%|██        | 6/30 [01:01<01:36,  4.04s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.96s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.26s/it] 30%|███       | 9/30 [01:03<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.66s/it]
Epoch loss is 3.9075350761413574
Epoch 113/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:17, 60.60s/it]  7%|▋         | 2/30 [01:02<12:03, 25.83s/it] 10%|█         | 3/30 [01:02<06:28, 14.38s/it] 13%|█▎        | 4/30 [01:04<04:02,  9.31s/it] 17%|█▋        | 5/30 [01:05<02:35,  6.22s/it] 20%|██        | 6/30 [01:05<01:44,  4.36s/it] 23%|██▎       | 7/30 [01:06<01:13,  3.18s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.41s/it] 30%|███       | 9/30 [01:08<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.30s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 3.9241795539855957
Epoch 114/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:51, 59.70s/it]  7%|▋         | 2/30 [01:00<11:40, 25.03s/it] 10%|█         | 3/30 [01:01<06:16, 13.94s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.29it/s] 70%|███████   | 21/30 [01:14<00:06,  1.30it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 3.9124818563461305
Epoch 115/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:32, 56.99s/it]  7%|▋         | 2/30 [00:58<11:16, 24.15s/it] 10%|█         | 3/30 [00:58<06:03, 13.47s/it] 13%|█▎        | 4/30 [01:01<03:58,  9.18s/it] 17%|█▋        | 5/30 [01:02<02:33,  6.14s/it] 20%|██        | 6/30 [01:03<01:43,  4.31s/it] 23%|██▎       | 7/30 [01:03<01:12,  3.15s/it] 27%|██▋       | 8/30 [01:04<00:52,  2.38s/it] 30%|███       | 9/30 [01:05<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.29s/it] 40%|████      | 12/30 [01:07<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.07it/s] 50%|█████     | 15/30 [01:09<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 3.9135800917943318
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0104,  0.0122, -0.0465,  ..., -0.0151, -0.0248,  0.0232],
        [-0.0516,  0.0297, -0.0035,  ...,  0.0032, -0.0244, -0.0113],
        [-0.0763, -0.0026, -0.0184,  ...,  0.0432, -0.0121, -0.0389],
        ...,
        [ 0.0061, -0.0290, -0.0411,  ..., -0.0528, -0.0213,  0.0021],
        [-0.0585,  0.0101, -0.0071,  ..., -0.0471,  0.0294, -0.0181],
        [-0.0440,  0.0222,  0.0049,  ..., -0.0290,  0.0281, -0.0580]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9258, 0.9093, 0.8664, 0.8646, 0.8582, 0.8504, 0.8503, 0.8336, 0.8324,
         0.8280],
        [0.9901, 0.9843, 0.9842, 0.9840, 0.9827, 0.9821, 0.9817, 0.9812, 0.9812,
         0.9800],
        [0.9843, 0.9840, 0.9809, 0.9726, 0.9706, 0.9704, 0.9682, 0.9682, 0.9670,
         0.9644],
        [0.9861, 0.9707, 0.9698, 0.9665, 0.9652, 0.9642, 0.9604, 0.9567, 0.9521,
         0.9488],
        [0.9706, 0.9630, 0.9607, 0.9547, 0.9546, 0.9504, 0.9466, 0.9464, 0.9460,
         0.9430],
        [0.9782, 0.9781, 0.9781, 0.9754, 0.9752, 0.9747, 0.9729, 0.9707, 0.9667,
         0.9637],
        [0.9918, 0.9916, 0.9916, 0.9915, 0.9886, 0.9863, 0.9851, 0.9822, 0.9816,
         0.9811],
        [0.9897, 0.9790, 0.9745, 0.9736, 0.9710, 0.9706, 0.9683, 0.9662, 0.9645,
         0.9637],
        [0.9866, 0.9864, 0.9862, 0.9861, 0.9858, 0.9843, 0.9841, 0.9841, 0.9838,
         0.9838],
        [0.9826, 0.9718, 0.9532, 0.9429, 0.9426, 0.9420, 0.9380, 0.9380, 0.9377,
         0.9374],
        [0.9826, 0.9794, 0.9771, 0.9761, 0.9752, 0.9746, 0.9733, 0.9711, 0.9701,
         0.9699],
        [0.9789, 0.9782, 0.9719, 0.9699, 0.9593, 0.9589, 0.9510, 0.9504, 0.9440,
         0.9438],
        [0.9814, 0.9775, 0.9657, 0.9617, 0.9484, 0.9433, 0.9410, 0.9408, 0.9365,
         0.9325],
        [0.9911, 0.9883, 0.9873, 0.9871, 0.9865, 0.9859, 0.9858, 0.9858, 0.9856,
         0.9850],
        [0.9831, 0.9748, 0.9736, 0.9730, 0.9700, 0.9694, 0.9690, 0.9681, 0.9667,
         0.9660],
        [0.9801, 0.9761, 0.9746, 0.9632, 0.9617, 0.9608, 0.9608, 0.9601, 0.9588,
         0.9582],
        [0.9940, 0.9938, 0.9894, 0.9863, 0.9862, 0.9850, 0.9847, 0.9845, 0.9839,
         0.9839],
        [0.9929, 0.9885, 0.9861, 0.9849, 0.9846, 0.9825, 0.9823, 0.9816, 0.9811,
         0.9809],
        [0.9814, 0.9808, 0.9698, 0.9661, 0.9633, 0.9632, 0.9613, 0.9577, 0.9573,
         0.9561],
        [0.9787, 0.9786, 0.9754, 0.9747, 0.9743, 0.9713, 0.9702, 0.9699, 0.9696,
         0.9688],
        [0.9910, 0.9882, 0.9874, 0.9862, 0.9855, 0.9833, 0.9798, 0.9785, 0.9775,
         0.9769],
        [0.9832, 0.9772, 0.9761, 0.9715, 0.9678, 0.9586, 0.9584, 0.9577, 0.9573,
         0.9572],
        [0.9894, 0.9804, 0.9751, 0.9695, 0.9681, 0.9660, 0.9615, 0.9614, 0.9592,
         0.9588],
        [0.9869, 0.9855, 0.9834, 0.9833, 0.9817, 0.9791, 0.9766, 0.9742, 0.9738,
         0.9737],
        [0.9861, 0.9832, 0.9684, 0.9654, 0.9625, 0.9610, 0.9600, 0.9591, 0.9588,
         0.9558],
        [0.9718, 0.9649, 0.9632, 0.9622, 0.9612, 0.9610, 0.9604, 0.9573, 0.9554,
         0.9552],
        [0.9738, 0.9737, 0.9699, 0.9688, 0.9602, 0.9599, 0.9590, 0.9579, 0.9578,
         0.9534],
        [0.9718, 0.9715, 0.9684, 0.9666, 0.9635, 0.9595, 0.9580, 0.9575, 0.9557,
         0.9535],
        [0.9868, 0.9839, 0.9821, 0.9769, 0.9753, 0.9720, 0.9701, 0.9698, 0.9670,
         0.9665],
        [0.9810, 0.9773, 0.9772, 0.9731, 0.9712, 0.9709, 0.9707, 0.9706, 0.9688,
         0.9660],
        [0.9848, 0.9833, 0.9804, 0.9804, 0.9800, 0.9779, 0.9769, 0.9759, 0.9759,
         0.9755],
        [0.9764, 0.9747, 0.9746, 0.9742, 0.9740, 0.9675, 0.9656, 0.9640, 0.9629,
         0.9622],
        [0.9591, 0.9369, 0.9321, 0.9277, 0.9243, 0.9242, 0.9228, 0.9226, 0.9208,
         0.9195],
        [0.9780, 0.9712, 0.9705, 0.9694, 0.9641, 0.9600, 0.9596, 0.9587, 0.9577,
         0.9576],
        [0.9763, 0.9754, 0.9752, 0.9726, 0.9723, 0.9721, 0.9718, 0.9690, 0.9680,
         0.9677],
        [0.9805, 0.9791, 0.9788, 0.9781, 0.9775, 0.9755, 0.9747, 0.9724, 0.9720,
         0.9713],
        [0.9709, 0.9702, 0.9661, 0.9616, 0.9570, 0.9521, 0.9496, 0.9486, 0.9458,
         0.9414],
        [0.9764, 0.9687, 0.9677, 0.9676, 0.9650, 0.9631, 0.9629, 0.9613, 0.9603,
         0.9597],
        [0.9736, 0.9705, 0.9671, 0.9671, 0.9636, 0.9594, 0.9592, 0.9581, 0.9531,
         0.9523],
        [0.9929, 0.9925, 0.9884, 0.9884, 0.9884, 0.9875, 0.9871, 0.9868, 0.9864,
         0.9857],
        [0.9599, 0.9423, 0.9371, 0.9343, 0.9281, 0.9214, 0.9193, 0.9051, 0.9047,
         0.9044],
        [0.9445, 0.9418, 0.9399, 0.9379, 0.9340, 0.9340, 0.9331, 0.9322, 0.9322,
         0.9319],
        [0.9605, 0.9532, 0.9526, 0.9475, 0.9469, 0.9368, 0.9329, 0.9304, 0.9274,
         0.9270],
        [0.9557, 0.9421, 0.9307, 0.9287, 0.9246, 0.9207, 0.9190, 0.9188, 0.9176,
         0.9167],
        [0.9524, 0.9333, 0.9282, 0.9202, 0.9178, 0.9176, 0.9156, 0.9112, 0.9102,
         0.9091],
        [0.9165, 0.8562, 0.8485, 0.8434, 0.8430, 0.8398, 0.8385, 0.8316, 0.8299,
         0.8257],
        [0.9749, 0.9748, 0.9704, 0.9671, 0.9632, 0.9574, 0.9548, 0.9536, 0.9528,
         0.9492],
        [0.9348, 0.9254, 0.9239, 0.9111, 0.9081, 0.8881, 0.8769, 0.8687, 0.8677,
         0.8582],
        [0.9568, 0.9422, 0.9412, 0.9347, 0.9336, 0.9284, 0.9270, 0.9253, 0.9214,
         0.9204],
        [0.9807, 0.9790, 0.9770, 0.9708, 0.9698, 0.9695, 0.9695, 0.9685, 0.9668,
         0.9663],
        [0.9671, 0.9653, 0.9599, 0.9599, 0.9592, 0.9590, 0.9486, 0.9454, 0.9432,
         0.9431],
        [0.9632, 0.9622, 0.9613, 0.9538, 0.9517, 0.9429, 0.9428, 0.9421, 0.9331,
         0.9326],
        [0.9710, 0.9643, 0.9439, 0.9338, 0.9315, 0.9314, 0.9242, 0.9123, 0.9051,
         0.9049],
        [0.9642, 0.9526, 0.9397, 0.9169, 0.9165, 0.9093, 0.9077, 0.9057, 0.9011,
         0.9004],
        [0.9715, 0.9673, 0.9649, 0.9643, 0.9640, 0.9638, 0.9627, 0.9621, 0.9597,
         0.9593],
        [0.9765, 0.9745, 0.9726, 0.9724, 0.9716, 0.9663, 0.9659, 0.9656, 0.9650,
         0.9642],
        [0.9674, 0.9593, 0.9507, 0.9477, 0.9454, 0.9373, 0.9364, 0.9307, 0.9303,
         0.9255],
        [0.9659, 0.9333, 0.9301, 0.9285, 0.9258, 0.9225, 0.9219, 0.9175, 0.9168,
         0.9158],
        [0.9600, 0.9600, 0.9367, 0.9293, 0.9257, 0.9152, 0.9145, 0.9143, 0.9140,
         0.9101],
        [0.9654, 0.9160, 0.8975, 0.8971, 0.8968, 0.8950, 0.8931, 0.8901, 0.8868,
         0.8852],
        [0.9567, 0.9486, 0.9473, 0.9458, 0.9451, 0.9449, 0.9417, 0.9388, 0.9366,
         0.9359],
        [0.9292, 0.9163, 0.9161, 0.9153, 0.9090, 0.9030, 0.8931, 0.8871, 0.8868,
         0.8859],
        [0.9690, 0.9586, 0.9481, 0.9456, 0.9456, 0.9404, 0.9352, 0.9279, 0.9255,
         0.9231],
        [0.9681, 0.9607, 0.9502, 0.9495, 0.9450, 0.9419, 0.9389, 0.9361, 0.9354,
         0.9338]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 1, 0],
        [0, 0, 1, 1, 1, 0, 1, 1, 1, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 1, 0, 0, 1, 1, 1, 0, 1],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 554246.4375,  437798.6250,  237286.9844,  231325.3906,  211208.4844,
          188783.2031,  188655.2344,  148623.7188,  146049.6094,  137190.0625],
        [1388399.8750, 1278402.6250, 1276505.8750, 1272707.1250, 1250614.3750,
         1239577.3750, 1231950.0000, 1224017.1250, 1223774.2500, 1203164.1250],
        [1279256.3750, 1274022.3750, 1217941.1250, 1082028.6250, 1051152.1250,
         1048638.1250, 1016296.3125, 1015392.5000,  998651.6250,  961717.4375],
        [1312308.6250, 1053348.8750, 1039662.1250,  991434.0000,  973115.0000,
          959347.6250,  908486.1250,  861623.3750,  807128.5625,  770281.3125],
        [1051218.3750,  943296.0000,  913337.0000,  837783.9375,  836678.8125,
          787627.3125,  746699.7500,  744609.7500,  740382.4375,  708907.5625],
        [1171341.7500, 1171160.8750, 1171005.5000, 1125422.5000, 1122154.7500,
         1115474.5000, 1087120.6250, 1052824.6250,  994395.9375,  953389.1250],
        [1423739.5000, 1420047.1250, 1419092.7500, 1416640.0000, 1360183.0000,
         1316573.1250, 1294053.2500, 1240569.6250, 1230900.1250, 1221210.6250],
        [1380737.3750, 1186088.7500, 1110980.5000, 1098132.5000, 1056816.2500,
         1050890.6250, 1017427.0000,  988129.3125,  963238.3125,  952380.4375],
        [1320849.0000, 1318391.2500, 1313319.0000, 1312928.2500, 1306149.2500,
         1278449.0000, 1275999.5000, 1275739.1250, 1270422.5000, 1269849.6250],
        [1248645.6250, 1068928.0000,  820539.8125,  707398.1875,  704532.0625,
          699250.6875,  660290.6875,  660150.2500,  657631.1875,  654049.1875],
        [1248595.6250, 1192280.8750, 1153262.1250, 1137270.2500, 1122577.6250,
         1114068.0000, 1093198.5000, 1058795.6250, 1044304.7500, 1040698.7500],
        [1184299.5000, 1171611.0000, 1071754.5000, 1040819.8750,  894293.8750,
          889134.5000,  794409.8125,  787543.9375,  719114.5625,  717470.5000],
        [1226556.2500, 1160786.2500,  979991.8125,  926233.8125,  765828.5625,
          711816.5000,  689128.4375,  686577.3125,  646378.1250,  610371.0000],
        [1408577.1250, 1354907.3750, 1334424.3750, 1331094.2500, 1320010.3750,
         1307769.6250, 1306938.0000, 1306245.2500, 1302102.3750, 1292355.0000],
        [1257047.5000, 1116480.1250, 1097111.8750, 1088455.7500, 1042066.3125,
         1034182.6875, 1027318.4375, 1015172.6875,  994347.5625,  984876.9375],
        [1205075.0000, 1138164.2500, 1112679.1250,  945964.3750,  926395.5625,
          913932.1250,  913620.1250,  905216.1250,  888841.9375,  881052.3750],
        [1467952.2500, 1465532.3750, 1374569.0000, 1315953.1250, 1314465.3750,
         1292199.6250, 1285882.2500, 1282754.0000, 1272182.8750, 1271264.8750],
        [1446866.6250, 1357069.5000, 1312298.5000, 1289519.6250, 1284008.6250,
         1247183.0000, 1242204.6250, 1230489.3750, 1221367.8750, 1218158.3750],
        [1226787.8750, 1215946.1250, 1040217.5000,  986222.8750,  946848.8750,
          945414.2500,  920701.3750,  873987.5625,  869005.8750,  854158.6250],
        [1180615.5000, 1179262.8750, 1125571.7500, 1115704.3750, 1107804.3750,
         1062740.8750, 1045430.7500, 1041500.0000, 1036168.0000, 1024857.3750],
        [1407780.7500, 1351813.6250, 1337288.2500, 1314521.8750, 1299986.8750,
         1260091.2500, 1198640.2500, 1176740.7500, 1160318.1250, 1151027.2500],
        [1258593.6250, 1155109.1250, 1136942.7500, 1064464.1250, 1009888.7500,
          886021.1875,  883323.9375,  874756.3750,  869822.5000,  868031.7500],
        [1375930.3750, 1210092.6250, 1120814.6250, 1034392.7500, 1014884.1250,
          985088.3125,  923509.3125,  922602.6250,  893841.9375,  888983.5625],
        [1328108.2500, 1300476.6250, 1262190.0000, 1261484.8750, 1232830.3750,
         1187545.3750, 1145264.3750, 1106795.8750, 1101383.7500, 1099502.1250],
        [1311715.5000, 1259554.2500, 1018783.4375,  976824.0625,  936440.0625,
          916596.9375,  903742.0000,  891960.0625,  888425.8750,  851178.3750],
        [1069199.2500,  969047.6250,  946257.5625,  932944.8750,  919755.3750,
          916735.1250,  908436.7500,  870000.0625,  845981.2500,  843326.2500],
        [1100595.2500, 1099127.7500, 1040344.5000, 1025242.5625,  905883.7500,
          902113.6875,  890336.8125,  876770.0625,  876015.3125,  822070.2500],
        [1069787.7500, 1065075.5000, 1019652.3750,  992751.0000,  949738.3125,
          896843.2500,  878797.5000,  871848.0625,  850007.0625,  823295.7500],
        [1325652.0000, 1271706.2500, 1239452.1250, 1150164.7500, 1125253.0000,
         1072722.8750, 1043789.9375, 1039123.8750,  998360.1875,  991619.3125],
        [1220417.7500, 1157248.1250, 1155763.6250, 1089453.7500, 1060796.6250,
         1055775.7500, 1052408.0000, 1052221.3750, 1024982.5000,  984144.5625],
        [1287482.2500, 1259948.3750, 1209874.5000, 1208697.1250, 1203420.0000,
         1167498.6250, 1149705.2500, 1134013.5000, 1133960.6250, 1127126.1250],
        [1141947.7500, 1114705.6250, 1113209.8750, 1106321.0000, 1104340.2500,
         1005898.8125,  978483.5625,  956412.5625,  941997.8750,  932269.8125],
        [ 892137.8750,  650169.7500,  606572.4375,  569823.6875,  542855.4375,
          542240.2500,  531505.5000,  529920.3125,  515883.8438,  506921.4062],
        [1168763.0000, 1060413.3750, 1049344.3750, 1033837.6250,  957679.3125,
          903234.5000,  898757.7500,  887099.1875,  874721.3125,  873063.6875],
        [1141348.0000, 1125327.0000, 1122579.7500, 1081576.6250, 1076857.7500,
         1074612.0000, 1069590.8750, 1028390.8125, 1012502.1875, 1008317.2500],
        [1210956.1250, 1188042.7500, 1182781.5000, 1170560.0000, 1160241.8750,
         1127285.1250, 1114380.3750, 1078233.6250, 1072112.2500, 1061908.1250],
        [1056241.0000, 1044854.6250,  985351.3750,  924326.1250,  866391.7500,
          807500.4375,  778815.7500,  768343.6875,  738303.8750,  692629.7500],
        [1142418.3750, 1023580.7500, 1008220.1250, 1007540.5625,  970357.1875,
          944551.6875,  941811.0000,  920903.4375,  907287.8125,  900378.3125],
        [1096872.2500, 1049838.7500, 1000083.1875,  999896.1875,  951847.4375,
          895949.9375,  892871.5625,  879851.6250,  819209.0625,  810068.8750],
        [1445191.0000, 1436943.7500, 1356052.6250, 1355204.6250, 1355121.8750,
         1339440.2500, 1331468.8750, 1324994.7500, 1317143.2500, 1305327.5000],
        [ 902639.4375,  701402.6250,  651155.1250,  625662.3750,  572595.3750,
          520506.9375,  505195.1250,  412487.0938,  410304.1250,  408581.4062],
        [ 724403.6250,  696557.8125,  677839.8750,  658671.1875,  623688.1875,
          623354.5625,  615729.8125,  607445.3750,  607231.6875,  604758.1875],
        [ 910586.0625,  820036.8125,  812667.3125,  755441.8750,  749221.3125,
          648439.6250,  613293.6250,  592343.1875,  567023.2500,  564372.7500],
        [ 849293.1875,  699965.2500,  594637.7500,  578084.3750,  544825.8125,
          515297.2500,  503113.7188,  501528.0938,  493130.0000,  487034.0625],
        [ 811271.8750,  616974.8125,  573908.5000,  511921.4688,  494328.8125,
          493400.9688,  479014.8438,  450231.9062,  443564.1875,  436914.3750],
        [ 485568.5312,  205239.0625,  183850.2188,  170914.5000,  169858.2969,
          162234.8906,  159192.1250,  144379.2031,  140814.6562,  132696.6719],
        [1117505.0000, 1116363.1250, 1049096.2500, 1000449.4375,  945990.5625,
          871176.5000,  839385.0000,  824614.3750,  815684.5625,  774148.3750],
        [ 630843.8125,  551282.7500,  539498.1875,  449118.6250,  430464.0312,
          323617.2812,  275895.5625,  245091.2031,  241768.4531,  211017.0156],
        [ 863796.1875,  700858.3125,  691122.0000,  629545.6250,  619535.5000,
          575107.8750,  564146.1875,  550497.8750,  520467.7500,  513475.8125],
        [1215269.1250, 1184894.8750, 1152758.5000, 1055215.0000, 1039386.5000,
         1035705.6875, 1035447.8750, 1019950.9375,  996316.2500,  988416.7500],
        [1000737.6250,  974680.0625,  902516.3750,  901808.2500,  894038.8750,
          890439.5625,  768337.8125,  733728.7500,  710576.0000,  709743.6250],
        [ 945823.6875,  932863.9375,  920718.9375,  827450.3125,  803127.4375,
          707815.8750,  707081.1250,  699917.8750,  615151.1250,  610711.0000],
        [1057647.0000,  960544.1250,  718183.8750,  621530.3750,  601653.5000,
          600390.1875,  541713.6250,  456947.9688,  412355.7500,  411464.0000],
        [ 959980.0000,  813300.6875,  676138.6250,  488280.4375,  485550.0312,
          438081.7812,  428079.3438,  416254.2812,  389869.9375,  385586.6562],
        [1064457.0000, 1002561.2500,  969772.4375,  961546.7500,  957273.0000,
          954460.8125,  939199.1250,  930841.2500,  899915.6250,  894612.8750],
        [1143621.8750, 1111831.6250, 1082000.7500, 1078211.1250, 1066620.5000,
          988913.6875,  983621.0000,  979504.0625,  970435.8125,  959372.2500],
        [1004042.3750,  895215.4375,  790958.8750,  757909.7500,  733921.1875,
          653453.7500,  644883.7500,  594846.5000,  590999.6875,  552193.6250],
        [ 983273.0625,  617355.0000,  589757.1250,  576573.5625,  554824.5000,
          528531.8750,  524747.9375,  492749.6875,  487805.7188,  480853.9062],
        [ 903800.6250,  903728.1875,  647510.2500,  583146.4375,  553715.5000,
          476844.3750,  471623.9375,  470197.6250,  468473.0312,  443252.0938],
        [ 976483.0625,  481812.3125,  370305.0938,  368114.6562,  366543.1562,
          357261.1875,  347455.0000,  332993.5625,  317713.7812,  310366.9688],
        [ 862174.8750,  768290.1875,  753316.6250,  737832.2500,  730067.0625,
          728197.9375,  695845.3750,  667636.4375,  646773.3750,  640908.3125],
        [ 581983.5625,  483948.1562,  483027.3750,  477134.1562,  436153.3750,
          400566.0000,  347608.1250,  318771.7812,  317828.3438,  313426.2188],
        [1027632.9375,  885605.5625,  762752.7500,  735227.0000,  735178.6250,
          682612.3750,  634267.2500,  570937.6875,  551750.8750,  533299.8125],
        [1014289.1250,  912507.3125,  785939.0625,  778226.2500,  729035.2500,
          697656.0625,  668301.5000,  641938.4375,  636328.4375,  621827.4375]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 554246.4375,       0.0000],
         [ 437798.6250,       0.0000],
         [ 237286.9844,       0.0000],
         ...,
         [ 148623.7188,       0.0000],
         [      0.0000,  146049.6094],
         [ 137190.0625,       0.0000]],

        [[1388399.8750,       0.0000],
         [1278402.6250,       0.0000],
         [1276505.8750,       0.0000],
         ...,
         [1224017.1250,       0.0000],
         [1223774.2500,       0.0000],
         [1203164.1250,       0.0000]],

        [[1279256.3750,       0.0000],
         [1274022.3750,       0.0000],
         [1217941.1250,       0.0000],
         ...,
         [1015392.5000,       0.0000],
         [ 998651.6250,       0.0000],
         [ 961717.4375,       0.0000]],

        ...,

        [[ 581983.5625,       0.0000],
         [ 483948.1562,       0.0000],
         [ 483027.3750,       0.0000],
         ...,
         [ 318771.7812,       0.0000],
         [      0.0000,  317828.3438],
         [ 313426.2188,       0.0000]],

        [[      0.0000, 1027632.9375],
         [ 885605.5625,       0.0000],
         [ 762752.7500,       0.0000],
         ...,
         [ 570937.6875,       0.0000],
         [      0.0000,  551750.8750],
         [ 533299.8125,       0.0000]],

        [[      0.0000, 1014289.1250],
         [ 912507.3125,       0.0000],
         [ 785939.0625,       0.0000],
         ...,
         [ 641938.4375,       0.0000],
         [ 636328.4375,       0.0000],
         [ 621827.4375,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2146335.0000,   334832.8125],
        [12589112.0000,        0.0000],
        [10945097.0000,        0.0000],
        [ 9676735.0000,        0.0000],
        [ 6453908.0000,  1856633.0000],
        [ 9969894.0000,   994395.9375],
        [10683347.0000,  2659662.5000],
        [10804822.0000,        0.0000],
        [12942098.0000,        0.0000],
        [ 7182165.0000,   699250.6875],
        [11205052.0000,        0.0000],
        [ 9270452.0000,        0.0000],
        [ 8403668.0000,        0.0000],
        [13264424.0000,        0.0000],
        [10657060.0000,        0.0000],
        [ 9830940.0000,        0.0000],
        [13342756.0000,        0.0000],
        [12849166.0000,        0.0000],
        [ 7918372.0000,  1960918.8750],
        [10919656.0000,        0.0000],
        [12658208.0000,        0.0000],
        [ 9120934.0000,   886021.1875],
        [10370140.0000,        0.0000],
        [12025581.0000,        0.0000],
        [ 9955221.0000,        0.0000],
        [ 9221684.0000,        0.0000],
        [ 8439372.0000,  1099127.7500],
        [ 9417796.0000,        0.0000],
        [11257844.0000,        0.0000],
        [10853212.0000,        0.0000],
        [11881726.0000,        0.0000],
        [10395587.0000,        0.0000],
        [ 4235653.5000,  1652376.5000],
        [ 7874513.0000,  1832400.6250],
        [ 9659525.0000,  1081576.6250],
        [ 9167104.0000,  2199397.5000],
        [ 6740125.0000,  1922632.7500],
        [ 7680079.0000,  2086970.0000],
        [ 5815280.0000,  3581208.7500],
        [13566890.0000,        0.0000],
        [ 4891644.0000,   818885.5000],
        [ 6439680.5000,        0.0000],
        [ 7033426.0000,        0.0000],
        [ 5766909.5000,        0.0000],
        [ 3788787.5000,  1522744.3750],
        [ 1469179.5000,   485568.5312],
        [ 9354413.0000,        0.0000],
        [ 2950468.7500,   948128.0625],
        [ 2653238.2500,  3575315.0000],
        [ 8628760.0000,  2094601.5000],
        [ 6885591.5000,  1601015.5000],
        [ 6448429.0000,  1322232.2500],
        [ 3653731.0000,  2728699.7500],
        [ 4316702.5000,  1164419.0000],
        [ 9574640.0000,        0.0000],
        [ 9282132.0000,  1082000.7500],
        [ 3794287.0000,  3424138.0000],
        [ 4176542.2500,  1659930.0000],
        [ 3024000.7500,  2898291.2500],
        [ 2423298.5000,  1805750.3750],
        [ 5821844.0000,  1409198.5000],
        [ 3842618.7500,   317828.3438],
        [ 5539881.0000,  1579383.7500],
        [ 5774104.0000,  1711945.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 51.5625
Top1 accuracy for validation set is 51.5625 size is torch.Size([64, 1])
Epoch 116/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:37, 57.17s/it]  7%|▋         | 2/30 [01:01<12:02, 25.80s/it] 10%|█         | 3/30 [01:01<06:27, 14.36s/it] 13%|█▎        | 4/30 [01:02<03:53,  8.99s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.02s/it] 20%|██        | 6/30 [01:04<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.35s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 3.8477937539418536
Epoch 117/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:19, 62.73s/it]  7%|▋         | 2/30 [01:03<12:22, 26.53s/it] 10%|█         | 3/30 [01:04<06:38, 14.76s/it] 13%|█▎        | 4/30 [01:05<03:59,  9.23s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.17s/it] 20%|██        | 6/30 [01:06<01:43,  4.33s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  2.84s/it]
Epoch loss is 3.8044703722000124
Epoch 118/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:47, 61.65s/it]  7%|▋         | 2/30 [01:02<12:04, 25.88s/it] 10%|█         | 3/30 [01:03<06:28, 14.40s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.03s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 3.8314966917037965
Epoch 119/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:40, 59.33s/it]  7%|▋         | 2/30 [01:01<12:06, 25.95s/it] 10%|█         | 3/30 [01:02<06:29, 14.44s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.04s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.05s/it] 20%|██        | 6/30 [01:04<01:41,  4.25s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 3.863956085840861
Epoch 120/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:35, 63.30s/it]  7%|▋         | 2/30 [01:04<12:22, 26.51s/it] 10%|█         | 3/30 [01:04<06:38, 14.75s/it] 13%|█▎        | 4/30 [01:05<03:59,  9.22s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.17s/it] 20%|██        | 6/30 [01:07<01:43,  4.32s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  2.84s/it]
Epoch loss is 3.8217695077260334
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0055,  0.0118, -0.0523,  ..., -0.0122, -0.0229,  0.0283],
        [-0.0483,  0.0278, -0.0070,  ...,  0.0040, -0.0254, -0.0085],
        [-0.0742, -0.0044, -0.0133,  ...,  0.0447, -0.0119, -0.0415],
        ...,
        [ 0.0105, -0.0288, -0.0438,  ..., -0.0496, -0.0224, -0.0012],
        [-0.0561,  0.0096, -0.0096,  ..., -0.0470,  0.0291, -0.0177],
        [-0.0432,  0.0194,  0.0024,  ..., -0.0262,  0.0271, -0.0575]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9221, 0.9096, 0.8617, 0.8573, 0.8539, 0.8465, 0.8410, 0.8288, 0.8277,
         0.8275],
        [0.9896, 0.9829, 0.9826, 0.9826, 0.9820, 0.9816, 0.9808, 0.9806, 0.9787,
         0.9782],
        [0.9797, 0.9786, 0.9784, 0.9653, 0.9650, 0.9649, 0.9643, 0.9632, 0.9630,
         0.9582],
        [0.9858, 0.9702, 0.9695, 0.9647, 0.9623, 0.9620, 0.9564, 0.9533, 0.9480,
         0.9465],
        [0.9678, 0.9593, 0.9556, 0.9497, 0.9474, 0.9452, 0.9414, 0.9388, 0.9382,
         0.9360],
        [0.9771, 0.9759, 0.9752, 0.9725, 0.9724, 0.9719, 0.9700, 0.9673, 0.9639,
         0.9608],
        [0.9914, 0.9913, 0.9909, 0.9908, 0.9877, 0.9846, 0.9829, 0.9807, 0.9796,
         0.9794],
        [0.9885, 0.9756, 0.9718, 0.9703, 0.9686, 0.9647, 0.9644, 0.9589, 0.9573,
         0.9572],
        [0.9859, 0.9852, 0.9844, 0.9842, 0.9838, 0.9831, 0.9829, 0.9827, 0.9826,
         0.9824],
        [0.9816, 0.9691, 0.9524, 0.9420, 0.9395, 0.9388, 0.9373, 0.9361, 0.9341,
         0.9333],
        [0.9820, 0.9770, 0.9757, 0.9752, 0.9752, 0.9744, 0.9729, 0.9707, 0.9703,
         0.9697],
        [0.9775, 0.9770, 0.9703, 0.9681, 0.9577, 0.9560, 0.9485, 0.9481, 0.9391,
         0.9388],
        [0.9812, 0.9761, 0.9650, 0.9563, 0.9419, 0.9413, 0.9376, 0.9370, 0.9338,
         0.9327],
        [0.9900, 0.9871, 0.9870, 0.9852, 0.9852, 0.9848, 0.9847, 0.9841, 0.9837,
         0.9836],
        [0.9824, 0.9722, 0.9720, 0.9693, 0.9677, 0.9677, 0.9674, 0.9668, 0.9664,
         0.9646],
        [0.9768, 0.9724, 0.9717, 0.9595, 0.9594, 0.9590, 0.9581, 0.9575, 0.9564,
         0.9563],
        [0.9940, 0.9938, 0.9883, 0.9852, 0.9841, 0.9835, 0.9835, 0.9835, 0.9833,
         0.9832],
        [0.9926, 0.9887, 0.9848, 0.9827, 0.9814, 0.9811, 0.9808, 0.9806, 0.9806,
         0.9804],
        [0.9816, 0.9812, 0.9677, 0.9655, 0.9633, 0.9616, 0.9584, 0.9583, 0.9581,
         0.9570],
        [0.9781, 0.9765, 0.9732, 0.9720, 0.9711, 0.9699, 0.9683, 0.9681, 0.9681,
         0.9675],
        [0.9920, 0.9877, 0.9872, 0.9857, 0.9841, 0.9834, 0.9802, 0.9801, 0.9766,
         0.9760],
        [0.9859, 0.9766, 0.9739, 0.9682, 0.9681, 0.9601, 0.9585, 0.9568, 0.9542,
         0.9539],
        [0.9894, 0.9791, 0.9740, 0.9699, 0.9682, 0.9653, 0.9634, 0.9605, 0.9593,
         0.9586],
        [0.9854, 0.9833, 0.9823, 0.9806, 0.9794, 0.9778, 0.9762, 0.9729, 0.9724,
         0.9718],
        [0.9841, 0.9827, 0.9678, 0.9657, 0.9605, 0.9592, 0.9592, 0.9591, 0.9579,
         0.9552],
        [0.9697, 0.9626, 0.9604, 0.9604, 0.9586, 0.9581, 0.9580, 0.9554, 0.9535,
         0.9508],
        [0.9709, 0.9708, 0.9698, 0.9682, 0.9588, 0.9586, 0.9578, 0.9573, 0.9542,
         0.9534],
        [0.9716, 0.9688, 0.9676, 0.9664, 0.9614, 0.9582, 0.9582, 0.9579, 0.9559,
         0.9508],
        [0.9862, 0.9836, 0.9811, 0.9762, 0.9747, 0.9721, 0.9697, 0.9673, 0.9662,
         0.9655],
        [0.9787, 0.9761, 0.9743, 0.9689, 0.9680, 0.9677, 0.9663, 0.9662, 0.9653,
         0.9645],
        [0.9842, 0.9830, 0.9814, 0.9809, 0.9799, 0.9771, 0.9768, 0.9757, 0.9751,
         0.9742],
        [0.9734, 0.9731, 0.9704, 0.9693, 0.9688, 0.9647, 0.9636, 0.9610, 0.9589,
         0.9580],
        [0.9510, 0.9293, 0.9252, 0.9198, 0.9194, 0.9186, 0.9184, 0.9169, 0.9158,
         0.9152],
        [0.9750, 0.9667, 0.9660, 0.9609, 0.9596, 0.9596, 0.9570, 0.9535, 0.9522,
         0.9511],
        [0.9734, 0.9718, 0.9716, 0.9712, 0.9698, 0.9678, 0.9678, 0.9658, 0.9639,
         0.9616],
        [0.9786, 0.9785, 0.9776, 0.9766, 0.9762, 0.9749, 0.9717, 0.9717, 0.9695,
         0.9695],
        [0.9684, 0.9665, 0.9661, 0.9620, 0.9555, 0.9489, 0.9485, 0.9438, 0.9385,
         0.9358],
        [0.9751, 0.9660, 0.9630, 0.9616, 0.9611, 0.9604, 0.9574, 0.9568, 0.9556,
         0.9553],
        [0.9669, 0.9637, 0.9635, 0.9611, 0.9606, 0.9526, 0.9505, 0.9501, 0.9500,
         0.9466],
        [0.9923, 0.9909, 0.9876, 0.9874, 0.9872, 0.9864, 0.9852, 0.9850, 0.9848,
         0.9845],
        [0.9584, 0.9367, 0.9344, 0.9328, 0.9262, 0.9185, 0.9126, 0.9039, 0.9015,
         0.9007],
        [0.9415, 0.9406, 0.9362, 0.9351, 0.9329, 0.9316, 0.9310, 0.9307, 0.9304,
         0.9295],
        [0.9549, 0.9503, 0.9495, 0.9404, 0.9402, 0.9334, 0.9334, 0.9324, 0.9232,
         0.9225],
        [0.9582, 0.9401, 0.9287, 0.9252, 0.9191, 0.9137, 0.9137, 0.9131, 0.9126,
         0.9114],
        [0.9504, 0.9293, 0.9215, 0.9163, 0.9156, 0.9098, 0.9096, 0.9093, 0.9075,
         0.9063],
        [0.9152, 0.8499, 0.8427, 0.8388, 0.8333, 0.8283, 0.8227, 0.8225, 0.8221,
         0.8056],
        [0.9742, 0.9726, 0.9680, 0.9640, 0.9599, 0.9548, 0.9514, 0.9504, 0.9495,
         0.9474],
        [0.9307, 0.9199, 0.9179, 0.9050, 0.9030, 0.8839, 0.8794, 0.8654, 0.8604,
         0.8564],
        [0.9551, 0.9403, 0.9369, 0.9312, 0.9301, 0.9263, 0.9217, 0.9216, 0.9171,
         0.9120],
        [0.9791, 0.9789, 0.9731, 0.9700, 0.9679, 0.9676, 0.9665, 0.9664, 0.9654,
         0.9639],
        [0.9608, 0.9584, 0.9541, 0.9483, 0.9476, 0.9471, 0.9383, 0.9380, 0.9366,
         0.9347],
        [0.9576, 0.9563, 0.9541, 0.9488, 0.9481, 0.9403, 0.9389, 0.9344, 0.9303,
         0.9294],
        [0.9694, 0.9613, 0.9397, 0.9362, 0.9319, 0.9314, 0.9260, 0.9114, 0.9074,
         0.9061],
        [0.9613, 0.9474, 0.9359, 0.9164, 0.9109, 0.9088, 0.9058, 0.9011, 0.8969,
         0.8957],
        [0.9692, 0.9648, 0.9643, 0.9629, 0.9624, 0.9614, 0.9609, 0.9604, 0.9600,
         0.9592],
        [0.9747, 0.9729, 0.9700, 0.9694, 0.9692, 0.9655, 0.9647, 0.9636, 0.9622,
         0.9622],
        [0.9643, 0.9544, 0.9475, 0.9474, 0.9409, 0.9353, 0.9339, 0.9315, 0.9276,
         0.9214],
        [0.9638, 0.9269, 0.9259, 0.9234, 0.9176, 0.9166, 0.9140, 0.9100, 0.9091,
         0.9076],
        [0.9600, 0.9592, 0.9330, 0.9318, 0.9221, 0.9144, 0.9138, 0.9132, 0.9127,
         0.9087],
        [0.9632, 0.9126, 0.8935, 0.8892, 0.8880, 0.8875, 0.8870, 0.8850, 0.8793,
         0.8763],
        [0.9529, 0.9456, 0.9455, 0.9430, 0.9421, 0.9406, 0.9366, 0.9357, 0.9345,
         0.9320],
        [0.9210, 0.9092, 0.9056, 0.9055, 0.9015, 0.8913, 0.8824, 0.8809, 0.8763,
         0.8762],
        [0.9670, 0.9560, 0.9454, 0.9442, 0.9419, 0.9358, 0.9347, 0.9269, 0.9253,
         0.9193],
        [0.9658, 0.9601, 0.9451, 0.9442, 0.9435, 0.9375, 0.9337, 0.9317, 0.9315,
         0.9291]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 1, 1, 0, 1, 1, 1],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 1, 0, 1, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 525960.9375,  440004.4688,  222005.8750,  208380.4844,  198449.8594,
          178680.0938,  165060.3906,  138592.6719,  136611.5625,  136113.0938],
        [1379185.8750, 1253082.1250, 1248813.5000, 1248031.2500, 1236821.2500,
         1229694.0000, 1215869.6250, 1213336.3750, 1180911.6250, 1172778.0000],
        [1197132.2500, 1178831.1250, 1175706.5000,  975415.5625,  970483.9375,
          969011.6250,  961558.7500,  945425.0000,  942954.1875,  880320.8125],
        [1306680.0000, 1045180.5000, 1034801.3125,  965834.0625,  934048.8125,
          930155.3125,  859038.8750,  820676.7500,  761684.1875,  745643.6875],
        [1009939.8125,  895126.6250,  848498.1875,  780091.3750,  755389.2500,
          732022.6250,  692640.9375,  667123.4375,  661505.8750,  641575.5625],
        [1153224.7500, 1134231.0000, 1122430.8750, 1080134.6250, 1078162.7500,
         1071449.0000, 1042593.1875, 1003724.5000,  954938.7500,  914443.8750],
        [1415294.8750, 1414212.8750, 1405110.2500, 1403275.6250, 1341971.8750,
         1284727.6250, 1254018.1250, 1215003.7500, 1196154.3750, 1191677.3750],
        [1358302.1250, 1130047.1250, 1070275.6250, 1047133.0000, 1022054.1875,
          967154.0625,  962704.8125,  890001.5625,  869402.8750,  868688.4375],
        [1308087.6250, 1294651.8750, 1281212.3750, 1277706.7500, 1269079.6250,
         1256569.2500, 1253129.7500, 1249409.1250, 1247679.1250, 1244805.2500],
        [1231258.1250, 1029200.3125,  810218.0625,  699066.0000,  673907.3750,
          667446.0625,  653292.3750,  642120.3125,  624401.6875,  616750.0625],
        [1237898.7500, 1151975.8750, 1131647.6250, 1122655.7500, 1122118.3750,
         1110021.0000, 1087311.2500, 1053472.3750, 1046841.5000, 1038571.0625],
        [1160242.8750, 1151507.0000, 1046274.5625, 1014256.2500,  874894.0000,
          852949.0000,  767306.1250,  762687.2500,  670619.6875,  667292.6875],
        [1223864.1250, 1138101.2500,  970167.5000,  857119.8750,  697947.5625,
          691561.7500,  656423.6875,  650755.4375,  621554.6875,  612005.3125],
        [1387493.1250, 1330364.6250, 1328699.8750, 1295756.2500, 1294521.0000,
         1287827.3750, 1286472.1250, 1274738.2500, 1268527.8750, 1265317.6250],
        [1245318.1250, 1075966.6250, 1073192.5000, 1031632.3750, 1008789.5000,
         1008275.9375, 1003968.6250,  995485.2500,  990791.2500,  965800.9375],
        [1149198.7500, 1078735.6250, 1068114.8750,  897842.8125,  896135.3750,
          890710.5000,  879629.3125,  872167.4375,  857951.6250,  857526.1875],
        [1468927.0000, 1463699.7500, 1354882.7500, 1294790.1250, 1275304.7500,
         1264756.6250, 1264586.5000, 1264058.3750, 1259861.8750, 1259417.2500],
        [1440140.3750, 1361082.3750, 1288625.8750, 1249055.2500, 1227293.3750,
         1221510.0000, 1215779.2500, 1212645.7500, 1212549.7500, 1209230.8750],
        [1230042.2500, 1224094.1250, 1008595.1875,  978053.5625,  947654.6875,
          924349.0000,  883412.3750,  881417.1250,  878904.8125,  866058.8125],
        [1170680.6250, 1144209.8750, 1091215.1250, 1073057.5000, 1058756.2500,
         1040905.1875, 1017604.5625, 1014854.1250, 1014188.5000, 1006620.4375],
        [1427296.0000, 1342773.2500, 1332217.0000, 1304151.6250, 1275501.8750,
         1262599.3750, 1206430.7500, 1204315.5000, 1145043.8750, 1136603.3750],
        [1308454.6250, 1145279.7500, 1101496.1250, 1015713.0625, 1014171.1250,
          905400.8750,  884498.1250,  863165.4375,  831485.4375,  827934.1875],
        [1375845.1250, 1188043.7500, 1104183.3750, 1041380.8125, 1015783.6875,
          975122.6250,  948051.5625,  909751.0625,  894142.9375,  886282.3750],
        [1299536.8750, 1260782.6250, 1242843.2500, 1212815.7500, 1192541.3750,
         1165635.1250, 1138968.8750, 1086695.6250, 1079407.6250, 1069764.2500],
        [1274949.7500, 1250452.1250, 1010108.4375,  980325.5000,  909799.6250,
          893993.6875,  893265.0000,  892068.1250,  877250.1250,  843874.9375],
        [1037394.1250,  938391.6250,  909156.0625,  909011.3125,  886019.5000,
          879841.5625,  878761.5000,  846853.0625,  823822.0000,  792304.1875],
        [1055924.7500, 1054620.3750, 1039558.0000, 1015824.4375,  888450.4375,
          886338.0625,  875208.6875,  870062.3125,  831950.2500,  822073.4375],
        [1067372.5000, 1024843.7500, 1007889.4375,  990620.2500,  921520.1250,
          881239.6875,  880845.6250,  877471.8125,  851863.7500,  791907.6250],
        [1313694.7500, 1266679.5000, 1222488.7500, 1139386.0000, 1115458.6250,
         1074035.2500, 1038137.3125, 1003145.6250,  986823.1250,  978279.3125],
        [1181201.0000, 1137500.1250, 1108965.0000, 1026191.4375, 1012907.7500,
         1008354.7500,  988855.1875,  987441.6875,  974443.0625,  964034.1250],
        [1277458.1250, 1255381.0000, 1226801.8750, 1218028.3750, 1201350.2500,
         1153314.8750, 1149113.3750, 1131190.0000, 1121612.3750, 1107229.7500],
        [1095098.6250, 1088952.0000, 1047910.2500, 1032651.1875, 1024886.7500,
          966217.3750,  951339.1875,  916715.8750,  889661.1875,  878696.1250],
        [ 794140.1875,  583252.1250,  549361.8750,  508658.4375,  505763.9375,
          500269.8125,  498678.3750,  488572.9688,  480440.8750,  476791.1562],
        [1120309.2500,  994914.7500,  984864.7500,  915721.5000,  899016.6875,
          898281.3125,  866404.1250,  823482.6875,  808226.9375,  795597.8750],
        [1094816.6250, 1069113.6250, 1066617.3750, 1060066.5000, 1039044.6250,
         1010193.1250, 1009802.0625,  981893.6875,  955869.0625,  924968.0625],
        [1178927.7500, 1177058.3750, 1161497.2500, 1145224.0000, 1139255.6250,
         1117421.8750, 1068862.8750, 1068717.0000, 1035490.3750, 1035173.4375],
        [1019120.6250,  990962.3125,  986652.8125,  929595.7500,  847907.6875,
          771167.0625,  766392.6250,  716526.1875,  664604.9375,  639312.0000],
        [1121835.8750,  983932.5000,  943639.6875,  924214.1250,  918564.1250,
          909463.9375,  871266.2500,  862981.9375,  849043.7500,  845072.5000],
        [ 996880.7500,  952613.8750,  949783.5625,  918668.3750,  911909.6250,
          812603.6875,  789432.1250,  784051.0000,  783573.3750,  746511.8125],
        [1432829.0000, 1405259.0000, 1340975.2500, 1337377.5000, 1333176.5000,
         1317475.0000, 1295089.0000, 1291060.2500, 1288248.7500, 1282301.5000],
        [ 882753.7500,  647415.8125,  626744.5000,  612442.0000,  557726.3125,
          499419.4062,  459457.9688,  405675.1875,  392082.9062,  387337.2500],
        [ 693504.8125,  685052.1250,  643229.6250,  633369.6250,  613200.0625,
          602748.6875,  597187.1875,  594360.5625,  592500.8125,  584213.0000],
        [ 839644.3750,  786445.1250,  777776.6250,  683043.4375,  680932.2500,
          618329.5625,  618059.0000,  609575.1250,  534576.9375,  529181.0000],
        [ 880202.5000,  679699.5625,  577777.9375,  549835.7500,  504180.0312,
          466575.9062,  466421.5000,  462730.8438,  459259.5000,  451445.2188],
        [ 788139.0000,  582796.1875,  521667.3438,  484415.9375,  479353.4688,
          441080.8750,  439843.8125,  437827.0312,  427148.7812,  419415.0000],
        [ 476530.6875,  187352.8438,  169115.1094,  159878.7656,  147848.4531,
          137643.8906,  127168.1328,  126775.9219,  125958.9688,   99594.5312],
        [1106966.8750, 1081505.5000, 1012939.6875,  957326.8125,  903013.1875,
          838647.2500,  799335.8750,  787884.1875,  777304.2500,  754757.0000],
        [ 595005.9375,  509800.6875,  495093.5938,  412038.5312,  400049.8438,
          304835.0938,  285913.3750,  233958.4531,  217789.9844,  205748.7812],
        [ 842839.0625,  682359.1875,  650049.5000,  598680.0625,  589847.1250,
          558687.2500,  523234.3125,  522402.6875,  489938.6562,  455028.0312],
        [1188063.1250, 1184484.7500, 1089439.2500, 1043088.3750, 1011932.6250,
         1006970.0000,  991961.6875,  990156.5000,  975861.2500,  954996.1250],
        [ 913984.3750,  883785.6875,  830828.3125,  765160.6250,  756883.3125,
          752018.1250,  663011.5625,  660055.1875,  646710.4375,  629425.0000],
        [ 873624.2500,  857347.9375,  830383.8750,  770298.2500,  761966.7500,
          681961.6875,  668115.4375,  627271.3125,  590902.2500,  583486.8750],
        [1033259.0000,  920052.7500,  675766.6250,  643087.3125,  605037.9375,
          600513.3125,  556201.9375,  451573.1250,  426087.3125,  418140.9688],
        [ 921114.1875,  754354.7500,  640439.0625,  484827.2812,  448265.7812,
          434654.0000,  416866.5000,  389622.0312,  366929.2812,  360450.4375],
        [1030708.0000,  967901.4375,  960854.6875,  942172.1250,  934702.0000,
          922132.0000,  914908.7500,  909333.8750,  903653.2500,  894068.7500],
        [1114432.5000, 1087154.7500, 1042626.9375, 1033999.2500, 1030841.7500,
          977163.1875,  966100.3750,  951466.1875,  932999.1250,  932328.5000],
        [ 960676.0625,  834611.5625,  756108.5000,  754939.1250,  688141.3750,
          635470.9375,  622626.7500,  601823.3125,  568790.5000,  520927.0938],
        [ 954624.6250,  563243.0625,  554949.8750,  536109.0625,  492921.7188,
          486477.4688,  468719.2500,  442162.4375,  436629.4688,  427377.7500],
        [ 903249.9375,  893277.8125,  614907.1250,  603878.6875,  525562.3125,
          471040.0312,  467065.5938,  462978.9375,  459494.7500,  434380.4688],
        [ 945819.1250,  459363.3438,  349318.9062,  328596.0000,  323052.3750,
          320577.5000,  318388.0625,  309716.1875,  285460.2812,  273371.8125],
        [ 816717.5625,  735621.8750,  734192.8125,  708826.4375,  699528.8750,
          684639.3125,  647138.6250,  638652.0000,  627467.0000,  606171.1250],
        [ 517330.8438,  437406.7812,  415181.5000,  414870.0000,  391980.0938,
          338514.3438,  298125.1250,  292011.9062,  273353.8125,  273144.8438],
        [ 998320.3125,  853989.1875,  733832.3125,  721302.1875,  697910.9375,
          639407.0625,  629413.5625,  562946.6250,  550332.0000,  505469.3125],
        [ 981605.3125,  905535.6250,  730561.5625,  721278.0625,  713738.8750,
          655560.3750,  620278.6250,  603105.7500,  601755.6250,  581157.7500]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 525960.9375,       0.0000],
         [ 440004.4688,       0.0000],
         [ 222005.8750,       0.0000],
         ...,
         [ 138592.6719,       0.0000],
         [ 136611.5625,       0.0000],
         [      0.0000,  136113.0938]],

        [[1379185.8750,       0.0000],
         [1253082.1250,       0.0000],
         [1248813.5000,       0.0000],
         ...,
         [1213336.3750,       0.0000],
         [1180911.6250,       0.0000],
         [1172778.0000,       0.0000]],

        [[1197132.2500,       0.0000],
         [1178831.1250,       0.0000],
         [1175706.5000,       0.0000],
         ...,
         [ 945425.0000,       0.0000],
         [ 942954.1875,       0.0000],
         [ 880320.8125,       0.0000]],

        ...,

        [[ 517330.8438,       0.0000],
         [ 437406.7812,       0.0000],
         [ 415181.5000,       0.0000],
         ...,
         [      0.0000,  292011.9062],
         [      0.0000,  273353.8125],
         [ 273144.8438,       0.0000]],

        [[      0.0000,  998320.3125],
         [ 853989.1875,       0.0000],
         [ 733832.3125,       0.0000],
         ...,
         [ 562946.6250,       0.0000],
         [      0.0000,  550332.0000],
         [ 505469.3125,       0.0000]],

        [[      0.0000,  981605.3125],
         [ 905535.6250,       0.0000],
         [ 730561.5625,       0.0000],
         ...,
         [ 603105.7500,       0.0000],
         [ 601755.6250,       0.0000],
         [ 581157.7500,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2048686.0000,   301173.5000],
        [12378524.0000,        0.0000],
        [10196840.0000,        0.0000],
        [ 9403744.0000,        0.0000],
        [ 5940289.0000,  1743624.7500],
        [ 9600395.0000,   954938.7500],
        [ 9294994.0000,  3826453.0000],
        [10185764.0000,        0.0000],
        [12682331.0000,        0.0000],
        [ 7005540.5000,   642120.3125],
        [11102514.0000,        0.0000],
        [ 8968030.0000,        0.0000],
        [ 8119501.0000,        0.0000],
        [13019718.0000,        0.0000],
        [10399221.0000,        0.0000],
        [ 9448012.0000,        0.0000],
        [13170284.0000,        0.0000],
        [12637913.0000,        0.0000],
        [ 7932569.5000,  1890012.2500],
        [10632093.0000,        0.0000],
        [12636933.0000,        0.0000],
        [ 9897598.0000,        0.0000],
        [10338588.0000,        0.0000],
        [11748992.0000,        0.0000],
        [ 9826087.0000,        0.0000],
        [ 8901555.0000,        0.0000],
        [ 8284087.0000,  1055924.7500],
        [ 9295574.0000,        0.0000],
        [11138128.0000,        0.0000],
        [10389894.0000,        0.0000],
        [11841480.0000,        0.0000],
        [ 9892128.0000,        0.0000],
        [ 3879336.0000,  1506593.8750],
        [ 7309522.0000,  1797298.0000],
        [ 9117568.0000,  1094816.6250],
        [ 8941490.0000,  2186139.0000],
        [ 6465214.0000,  1867028.2500],
        [ 7189615.0000,  2040400.0000],
        [ 5201262.5000,  3444765.5000],
        [13323792.0000,        0.0000],
        [ 4691635.0000,   779420.1250],
        [ 6239366.0000,        0.0000],
        [ 6677563.5000,        0.0000],
        [ 5498128.5000,        0.0000],
        [ 3580822.7500,  1440864.6250],
        [ 1155377.7500,   602489.6250],
        [ 9019680.0000,        0.0000],
        [ 2740312.5000,   919921.6875],
        [ 2048432.5000,  3864633.2500],
        [ 8401904.0000,  2035050.0000],
        [ 6098269.0000,  1403593.7500],
        [ 5986341.0000,  1259017.7500],
        [ 4041140.5000,  2288579.5000],
        [ 4092257.0000,  1125266.3750],
        [ 9380435.0000,        0.0000],
        [ 9035113.0000,  1033999.2500],
        [ 3704577.7500,  3239537.5000],
        [ 3847706.5000,  1515508.2500],
        [ 2957335.0000,  2878500.5000],
        [ 2190093.0000,  1723570.5000],
        [ 6163334.0000,   735621.8750],
        [ 3086553.5000,   565365.7500],
        [ 5344271.0000,  1548652.2500],
        [ 5477412.0000,  1637165.7500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 51.5625
Top1 accuracy for validation set is 51.5625 size is torch.Size([64, 1])
Epoch 121/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:00, 60.01s/it]  7%|▋         | 2/30 [01:01<11:57, 25.63s/it] 10%|█         | 3/30 [01:02<06:25, 14.27s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.93s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.98s/it] 20%|██        | 6/30 [01:04<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 3.82245291074117
Epoch 122/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:16, 56.44s/it]  7%|▋         | 2/30 [00:57<11:03, 23.68s/it] 10%|█         | 3/30 [00:57<05:56, 13.21s/it] 13%|█▎        | 4/30 [00:58<03:35,  8.29s/it] 17%|█▋        | 5/30 [00:59<02:19,  5.57s/it] 20%|██        | 6/30 [01:00<01:34,  3.93s/it] 23%|██▎       | 7/30 [01:00<01:06,  2.89s/it] 27%|██▋       | 8/30 [01:01<00:48,  2.21s/it] 30%|███       | 9/30 [01:02<00:36,  1.75s/it] 33%|███▎      | 10/30 [01:03<00:28,  1.44s/it] 37%|███▋      | 11/30 [01:03<00:23,  1.23s/it] 40%|████      | 12/30 [01:04<00:19,  1.08s/it] 43%|████▎     | 13/30 [01:05<00:16,  1.02it/s] 47%|████▋     | 14/30 [01:06<00:14,  1.10it/s] 50%|█████     | 15/30 [01:06<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:07<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:08<00:10,  1.24it/s] 60%|██████    | 18/30 [01:09<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:09<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:10<00:07,  1.30it/s] 70%|███████   | 21/30 [01:11<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:12<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:12<00:05,  1.32it/s] 80%|████████  | 24/30 [01:13<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:14<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:15<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:15<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:16<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:17<00:00,  1.34it/s]100%|██████████| 30/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:18<00:00,  2.61s/it]
Epoch loss is 3.7949934879938763
Epoch 123/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:23, 58.73s/it]  7%|▋         | 2/30 [01:02<12:15, 26.25s/it] 10%|█         | 3/30 [01:03<06:34, 14.61s/it] 13%|█▎        | 4/30 [01:03<03:57,  9.14s/it] 17%|█▋        | 5/30 [01:04<02:32,  6.11s/it] 20%|██        | 6/30 [01:05<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:05<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.37s/it] 30%|███       | 9/30 [01:07<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 3.841718244552612
Epoch 124/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:08, 62.35s/it]  7%|▋         | 2/30 [01:03<12:22, 26.50s/it] 10%|█         | 3/30 [01:04<06:38, 14.74s/it] 13%|█▎        | 4/30 [01:05<03:59,  9.22s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.16s/it] 20%|██        | 6/30 [01:06<01:43,  4.32s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.15s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Epoch loss is 3.7562885840733844
Epoch 125/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:47, 57.48s/it]  7%|▋         | 2/30 [01:01<12:11, 26.12s/it] 10%|█         | 3/30 [01:02<06:32, 14.54s/it] 13%|█▎        | 4/30 [01:03<03:56,  9.09s/it] 17%|█▋        | 5/30 [01:03<02:32,  6.08s/it] 20%|██        | 6/30 [01:04<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.36s/it] 30%|███       | 9/30 [01:06<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 3.7612762769063313
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[-0.0010,  0.0104, -0.0538,  ..., -0.0102, -0.0226,  0.0306],
        [-0.0450,  0.0266, -0.0096,  ...,  0.0050, -0.0263, -0.0067],
        [-0.0702, -0.0060, -0.0101,  ...,  0.0463, -0.0122, -0.0439],
        ...,
        [ 0.0153, -0.0295, -0.0452,  ..., -0.0462, -0.0236, -0.0048],
        [-0.0528,  0.0096, -0.0107,  ..., -0.0455,  0.0286, -0.0184],
        [-0.0422,  0.0169, -0.0004,  ..., -0.0238,  0.0250, -0.0578]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9195, 0.9090, 0.8594, 0.8499, 0.8483, 0.8412, 0.8321, 0.8252, 0.8235,
         0.8235],
        [0.9883, 0.9822, 0.9818, 0.9817, 0.9816, 0.9810, 0.9803, 0.9785, 0.9770,
         0.9768],
        [0.9746, 0.9737, 0.9714, 0.9600, 0.9583, 0.9565, 0.9564, 0.9558, 0.9558,
         0.9551],
        [0.9845, 0.9695, 0.9691, 0.9625, 0.9610, 0.9593, 0.9571, 0.9474, 0.9442,
         0.9439],
        [0.9643, 0.9567, 0.9493, 0.9450, 0.9417, 0.9396, 0.9359, 0.9304, 0.9300,
         0.9297],
        [0.9758, 0.9738, 0.9728, 0.9703, 0.9694, 0.9690, 0.9669, 0.9648, 0.9612,
         0.9591],
        [0.9910, 0.9907, 0.9902, 0.9901, 0.9867, 0.9832, 0.9810, 0.9790, 0.9782,
         0.9778],
        [0.9874, 0.9731, 0.9702, 0.9687, 0.9633, 0.9611, 0.9584, 0.9535, 0.9525,
         0.9511],
        [0.9846, 0.9837, 0.9833, 0.9824, 0.9822, 0.9822, 0.9812, 0.9812, 0.9810,
         0.9800],
        [0.9803, 0.9673, 0.9523, 0.9419, 0.9390, 0.9377, 0.9335, 0.9321, 0.9312,
         0.9312],
        [0.9807, 0.9770, 0.9755, 0.9737, 0.9722, 0.9708, 0.9705, 0.9704, 0.9698,
         0.9692],
        [0.9753, 0.9748, 0.9684, 0.9666, 0.9545, 0.9515, 0.9452, 0.9447, 0.9325,
         0.9317],
        [0.9821, 0.9746, 0.9617, 0.9496, 0.9471, 0.9342, 0.9335, 0.9334, 0.9307,
         0.9300],
        [0.9887, 0.9861, 0.9855, 0.9840, 0.9839, 0.9839, 0.9834, 0.9832, 0.9830,
         0.9823],
        [0.9806, 0.9726, 0.9704, 0.9678, 0.9668, 0.9667, 0.9667, 0.9641, 0.9636,
         0.9636],
        [0.9742, 0.9709, 0.9682, 0.9596, 0.9588, 0.9563, 0.9555, 0.9545, 0.9529,
         0.9529],
        [0.9936, 0.9928, 0.9869, 0.9842, 0.9830, 0.9825, 0.9823, 0.9821, 0.9812,
         0.9812],
        [0.9916, 0.9887, 0.9828, 0.9803, 0.9802, 0.9800, 0.9789, 0.9788, 0.9786,
         0.9783],
        [0.9832, 0.9811, 0.9665, 0.9646, 0.9626, 0.9587, 0.9569, 0.9566, 0.9563,
         0.9548],
        [0.9776, 0.9737, 0.9728, 0.9679, 0.9674, 0.9671, 0.9670, 0.9666, 0.9658,
         0.9656],
        [0.9919, 0.9863, 0.9861, 0.9838, 0.9836, 0.9836, 0.9795, 0.9795, 0.9754,
         0.9751],
        [0.9846, 0.9752, 0.9734, 0.9659, 0.9619, 0.9590, 0.9573, 0.9561, 0.9539,
         0.9534],
        [0.9894, 0.9775, 0.9728, 0.9695, 0.9679, 0.9665, 0.9642, 0.9605, 0.9599,
         0.9580],
        [0.9834, 0.9811, 0.9800, 0.9785, 0.9773, 0.9766, 0.9740, 0.9715, 0.9704,
         0.9703],
        [0.9839, 0.9821, 0.9665, 0.9640, 0.9601, 0.9591, 0.9583, 0.9583, 0.9571,
         0.9552],
        [0.9691, 0.9614, 0.9590, 0.9564, 0.9562, 0.9556, 0.9554, 0.9528, 0.9496,
         0.9486],
        [0.9708, 0.9685, 0.9674, 0.9670, 0.9596, 0.9575, 0.9568, 0.9561, 0.9530,
         0.9488],
        [0.9706, 0.9699, 0.9669, 0.9635, 0.9583, 0.9582, 0.9578, 0.9571, 0.9521,
         0.9474],
        [0.9858, 0.9833, 0.9808, 0.9763, 0.9740, 0.9697, 0.9686, 0.9653, 0.9651,
         0.9650],
        [0.9759, 0.9752, 0.9704, 0.9666, 0.9662, 0.9658, 0.9627, 0.9627, 0.9626,
         0.9616],
        [0.9833, 0.9813, 0.9813, 0.9813, 0.9782, 0.9770, 0.9764, 0.9759, 0.9743,
         0.9743],
        [0.9715, 0.9699, 0.9663, 0.9641, 0.9640, 0.9612, 0.9609, 0.9585, 0.9554,
         0.9550],
        [0.9430, 0.9218, 0.9192, 0.9175, 0.9138, 0.9115, 0.9110, 0.9100, 0.9080,
         0.9060],
        [0.9723, 0.9648, 0.9598, 0.9597, 0.9576, 0.9550, 0.9549, 0.9483, 0.9467,
         0.9466],
        [0.9736, 0.9695, 0.9679, 0.9672, 0.9661, 0.9644, 0.9632, 0.9602, 0.9596,
         0.9574],
        [0.9774, 0.9758, 0.9751, 0.9742, 0.9741, 0.9734, 0.9707, 0.9680, 0.9669,
         0.9668],
        [0.9661, 0.9647, 0.9618, 0.9602, 0.9525, 0.9477, 0.9456, 0.9380, 0.9351,
         0.9306],
        [0.9755, 0.9642, 0.9619, 0.9592, 0.9585, 0.9550, 0.9533, 0.9531, 0.9530,
         0.9519],
        [0.9613, 0.9608, 0.9606, 0.9603, 0.9548, 0.9478, 0.9472, 0.9440, 0.9429,
         0.9421],
        [0.9919, 0.9900, 0.9869, 0.9868, 0.9862, 0.9860, 0.9838, 0.9837, 0.9835,
         0.9828],
        [0.9579, 0.9344, 0.9312, 0.9308, 0.9283, 0.9171, 0.9051, 0.9030, 0.9002,
         0.8994],
        [0.9400, 0.9384, 0.9332, 0.9317, 0.9311, 0.9307, 0.9306, 0.9295, 0.9289,
         0.9263],
        [0.9503, 0.9464, 0.9461, 0.9356, 0.9335, 0.9330, 0.9317, 0.9293, 0.9197,
         0.9190],
        [0.9586, 0.9389, 0.9254, 0.9206, 0.9137, 0.9129, 0.9106, 0.9074, 0.9047,
         0.9046],
        [0.9481, 0.9259, 0.9152, 0.9140, 0.9110, 0.9092, 0.9081, 0.9018, 0.9000,
         0.8981],
        [0.9111, 0.8443, 0.8363, 0.8335, 0.8327, 0.8165, 0.8099, 0.8068, 0.8034,
         0.8007],
        [0.9741, 0.9706, 0.9652, 0.9629, 0.9572, 0.9522, 0.9494, 0.9493, 0.9479,
         0.9443],
        [0.9245, 0.9160, 0.9135, 0.9003, 0.8974, 0.8817, 0.8769, 0.8615, 0.8556,
         0.8550],
        [0.9500, 0.9377, 0.9348, 0.9265, 0.9259, 0.9243, 0.9200, 0.9131, 0.9116,
         0.9090],
        [0.9781, 0.9760, 0.9711, 0.9681, 0.9661, 0.9654, 0.9638, 0.9627, 0.9616,
         0.9614],
        [0.9547, 0.9515, 0.9477, 0.9394, 0.9388, 0.9358, 0.9329, 0.9308, 0.9308,
         0.9268],
        [0.9517, 0.9499, 0.9481, 0.9464, 0.9415, 0.9384, 0.9328, 0.9289, 0.9281,
         0.9262],
        [0.9670, 0.9580, 0.9377, 0.9333, 0.9326, 0.9316, 0.9269, 0.9085, 0.9084,
         0.9029],
        [0.9601, 0.9448, 0.9358, 0.9173, 0.9100, 0.9084, 0.9002, 0.8988, 0.8973,
         0.8920],
        [0.9673, 0.9640, 0.9627, 0.9612, 0.9605, 0.9604, 0.9585, 0.9583, 0.9581,
         0.9571],
        [0.9733, 0.9699, 0.9663, 0.9661, 0.9658, 0.9635, 0.9632, 0.9590, 0.9581,
         0.9578],
        [0.9616, 0.9511, 0.9468, 0.9465, 0.9367, 0.9345, 0.9313, 0.9296, 0.9248,
         0.9196],
        [0.9607, 0.9205, 0.9195, 0.9179, 0.9105, 0.9101, 0.9054, 0.9040, 0.9034,
         0.9026],
        [0.9599, 0.9562, 0.9325, 0.9309, 0.9153, 0.9151, 0.9146, 0.9144, 0.9066,
         0.9005],
        [0.9604, 0.9090, 0.8938, 0.8848, 0.8833, 0.8812, 0.8800, 0.8792, 0.8733,
         0.8678],
        [0.9473, 0.9440, 0.9416, 0.9410, 0.9390, 0.9367, 0.9349, 0.9326, 0.9295,
         0.9293],
        [0.9150, 0.9027, 0.8973, 0.8964, 0.8925, 0.8794, 0.8761, 0.8735, 0.8732,
         0.8693],
        [0.9643, 0.9533, 0.9414, 0.9408, 0.9373, 0.9318, 0.9290, 0.9268, 0.9251,
         0.9180],
        [0.9608, 0.9602, 0.9447, 0.9395, 0.9377, 0.9305, 0.9291, 0.9289, 0.9273,
         0.9248]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 1, 1, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 0, 1, 1, 1, 1, 1, 0, 1, 1],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 1, 1, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 1, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 507062.5938,  435945.8750,  214662.2500,  187603.6875,  183240.0312,
          165493.5312,  145293.8750,  131733.7344,  128627.0312,  128564.0000],
        [1353278.8750, 1241580.3750, 1234054.8750, 1232628.1250, 1230357.8750,
         1219637.0000, 1207123.5000, 1176593.7500, 1152390.2500, 1148608.2500],
        [1113276.7500, 1099699.2500, 1063761.0000,  903470.5000,  882458.3750,
          859373.9375,  858376.3750,  851509.6250,  850803.4375,  842295.8750],
        [1282848.1250, 1034803.3125, 1029757.8750,  937020.6875,  916646.7500,
          894178.7500,  867519.5000,  755359.6875,  721179.6875,  718103.6875],
        [ 961419.3125,  861828.0000,  775837.1250,  729808.0625,  695922.3750,
          675183.0000,  640506.8750,  592446.0000,  588874.7500,  586523.0000],
        [1132084.7500, 1101288.1250, 1085138.0000, 1047704.4375, 1033010.6875,
         1027981.0000,  996690.6875,  968432.3125,  918981.1875,  892600.8125],
        [1407485.3750, 1401534.2500, 1391780.3750, 1389751.0000, 1323166.3750,
         1258621.2500, 1219125.3750, 1185384.2500, 1171537.2500, 1164726.2500],
        [1335979.1250, 1089946.3750, 1046156.8125, 1023514.3750,  947395.3125,
          918485.3125,  883697.1875,  823633.5000,  812078.4375,  795693.5000],
        [1284427.3750, 1267487.8750, 1260619.0000, 1244366.1250, 1241624.1250,
         1241463.2500, 1223526.8750, 1222689.3750, 1220699.5000, 1202854.3750],
        [1207797.1250, 1003645.0625,  809977.7500,  697478.4375,  669045.0000,
          657616.7500,  618523.0000,  606959.5625,  598998.1250,  598788.0000],
        [1215276.1250, 1152540.8750, 1128346.7500, 1098873.1250, 1076477.7500,
         1055199.8750, 1050221.3750, 1048822.0000, 1040162.8750, 1030066.3125],
        [1124265.0000, 1117115.0000, 1019133.3125,  992815.3750,  835028.0000,
          800206.8750,  731237.6875,  726585.8125,  610493.2500,  602932.6875],
        [1238863.5000, 1113465.7500,  926364.6250,  778937.5625,  751577.1875,
          624966.4375,  618638.0625,  618232.2500,  594341.2500,  588799.5000],
        [1362556.2500, 1312037.0000, 1301621.7500, 1273331.2500, 1271309.7500,
         1270719.3750, 1262434.3750, 1259035.3750, 1255484.0000, 1243380.2500],
        [1213233.3750, 1082520.8750, 1047942.2500, 1009874.3125,  995227.0000,
          994528.6875,  993808.1250,  958440.5000,  951612.3125,  950774.1250],
        [1107599.3750, 1056570.3750, 1015638.4375,  898829.8125,  888680.1250,
          857663.6250,  847877.7500,  835676.4375,  816705.0625,  816564.0625],
        [1460602.7500, 1443108.6250, 1326414.5000, 1277569.0000, 1255965.3750,
         1246888.1250, 1242550.5000, 1239822.1250, 1222976.2500, 1222626.3750],
        [1419377.0000, 1361845.7500, 1252346.1250, 1207216.7500, 1206715.0000,
         1201903.7500, 1184391.0000, 1181629.2500, 1178776.0000, 1173277.0000],
        [1258582.8750, 1221569.3750,  990992.5000,  964494.8125,  938065.0000,
          887418.1875,  864961.8125,  860438.4375,  856774.9375,  839485.0625],
        [1162465.7500, 1099814.6250, 1085679.3750, 1012412.3125, 1004870.0000,
         1000243.3750,  998281.1875,  993255.6875,  981437.7500,  979610.5625],
        [1425828.0000, 1315303.1250, 1312640.2500, 1269642.5000, 1265958.5000,
         1265676.0000, 1194501.3750, 1193250.1250, 1126152.6250, 1120703.5000],
        [1283785.6250, 1122392.5000, 1094568.1250,  982759.3750,  928314.6875,
          890490.5000,  869606.0625,  855218.2500,  828090.5625,  821844.5625],
        [1375284.8750, 1159805.8750, 1085043.8750, 1035594.0625, 1012240.5000,
          991194.8125,  959096.9375,  910794.5000,  901989.8125,  877693.6250],
        [1263253.3750, 1221740.6250, 1202962.2500, 1176472.5000, 1157368.3750,
         1145775.7500, 1104339.2500, 1065255.2500, 1048368.0000, 1047680.4375],
        [1270709.6250, 1238642.6250,  991996.6875,  956408.8750,  905537.3750,
          891754.2500,  881803.8750,  881442.3125,  866970.3125,  844012.5625],
        [1028968.6250,  921655.5000,  891360.5625,  857912.3125,  855955.0625,
          848546.6875,  846738.3750,  815033.7500,  778626.3750,  767915.1250],
        [1054978.5000, 1020321.6875, 1004308.6250,  998293.5625,  898378.1250,
          872168.1875,  863195.8750,  855131.7500,  817653.4375,  770195.3750],
        [1051487.1250, 1041445.3750,  997444.6875,  950588.2500,  882173.9375,
          880633.1250,  876045.3750,  867445.8125,  807231.7500,  754757.0000],
        [1305669.8750, 1261155.3750, 1217073.8750, 1141331.6250, 1104009.6250,
         1038586.8750, 1021734.5000,  975158.8750,  971438.6250,  971136.6875],
        [1134295.8750, 1122252.2500, 1048296.0625,  993733.2500,  987292.8750,
          981582.8125,  939362.1875,  939022.7500,  937637.4375,  925011.2500],
        [1261410.2500, 1225744.7500, 1225252.7500, 1224585.7500, 1171449.0000,
         1152339.7500, 1142827.0000, 1134614.0000, 1109205.1250, 1108073.8750],
        [1065841.6250, 1041741.3125,  988446.0000,  957788.9375,  957349.6875,
          919153.8750,  914835.5000,  884817.8750,  846804.5625,  841590.8125],
        [ 709148.9375,  523508.8438,  504352.6875,  492646.8125,  467096.3438,
          451739.3750,  448972.1562,  442556.4375,  429846.2188,  417987.5000],
        [1077782.3750,  968266.1250,  901491.0000,  899816.0625,  873387.6250,
          841249.8125,  840556.1875,  764620.0625,  747673.1250,  746284.0000],
        [1097442.5000, 1035404.5000, 1011500.3750, 1001787.0625,  985677.5000,
          962458.6875,  945572.8750,  906143.8125,  898894.9375,  870214.9375],
        [1159541.6250, 1132701.3750, 1121648.7500, 1107482.1250, 1105947.6250,
         1093924.3750, 1053519.6250, 1012793.8125,  997970.9375,  995415.0000],
        [ 986147.6250,  967048.0000,  927220.1875,  906926.1875,  812082.3750,
          757843.2500,  736049.2500,  660214.5000,  632972.8750,  593476.9375],
        [1127413.1250,  959380.5000,  928740.5625,  893414.1250,  885161.4375,
          841737.7500,  821276.5000,  819231.6875,  818136.2500,  804700.6250],
        [ 920284.4375,  913895.5000,  912152.3125,  908049.5625,  839131.2500,
          759264.7500,  752854.1250,  719318.9375,  707740.2500,  699432.1250],
        [1425515.2500, 1387150.5000, 1327196.6250, 1325868.2500, 1314564.5000,
         1310593.7500, 1269706.7500, 1267797.2500, 1264252.5000, 1252440.5000],
        [ 877128.8125,  627236.0000,  599269.0000,  595723.0625,  574553.6250,
          489604.2500,  412450.9375,  400409.7812,  384491.2812,  380156.0625],
        [ 679367.7500,  663746.6875,  616353.7500,  602906.1875,  597807.1250,
          594874.8750,  593946.8750,  584188.4375,  579255.4375,  558182.8750],
        [ 787206.8125,  744026.3125,  740774.4375,  637328.6250,  619342.3125,
          614742.9375,  603551.1250,  583052.4375,  507929.8750,  503197.6875],
        [ 886349.9375,  668925.1250,  551378.9375,  515104.1250,  466203.1562,
          461367.0625,  446510.9375,  426472.6875,  409885.2812,  409282.9688],
        [ 762219.7500,  554977.3750,  476418.4375,  468224.2500,  448641.3125,
          437418.4375,  430711.6562,  393479.3750,  383756.7812,  373161.7500],
        [ 449682.6250,  173166.8281,  154413.2500,  148362.7188,  146693.2812,
          116261.7109,  105894.0391,  101344.1562,   96428.5625,   92785.0469],
        [1105000.8750, 1051691.7500,  972935.9375,  941494.9375,  868033.4375,
          808465.8750,  777184.9375,  775474.6875,  759880.4375,  722388.4375],
        [ 544286.2500,  482277.0938,  465258.4062,  385337.4062,  369273.5625,
          295261.3750,  275896.3750,  221319.9062,  203457.4688,  201707.4844],
        [ 783912.6875,  656896.5000,  630797.5000,  559990.3750,  554993.3125,
          542734.3750,  510175.6875,  462339.1875,  452930.4062,  435890.1562],
        [1169897.1250, 1135951.0000, 1058566.3750, 1015289.8125,  985991.5000,
          975775.6250,  954504.5000,  938864.2500,  924997.1875,  921537.6875],
        [ 838154.6875,  799846.0000,  758467.1875,  673189.1875,  667842.7500,
          639807.8750,  613231.6250,  595234.6250,  595164.8125,  562327.4375],
        [ 802593.0000,  782046.6250,  762065.6250,  744381.8750,  693678.7500,
          663800.5000,  612871.5000,  579143.3125,  572919.2500,  557232.4375],
        [ 998230.7500,  878560.3750,  657246.8125,  616892.4375,  611315.8125,
          601953.6250,  563210.2500,  433063.0625,  432139.3438,  399480.6562],
        [ 905313.6875,  726973.9375,  639729.1250,  491315.6875,  442515.5000,
          432511.6562,  384428.9688,  377253.4688,  369143.3125,  342341.6875],
        [1003672.8125,  957070.3750,  938632.3125,  919383.5625,  909987.0625,
          908905.5625,  884423.0625,  881438.0625,  879922.1250,  866561.1250],
        [1092641.7500, 1041458.2500,  989273.9375,  985989.6250,  982272.0625,
          949832.4375,  945489.9375,  891517.0000,  879655.2500,  876237.5625],
        [ 925037.7500,  795883.2500,  748292.3125,  744778.1250,  647454.6875,
          627913.5625,  600073.6250,  585375.1875,  546922.7500,  507325.2188],
        [ 912789.2500,  513931.4375,  506912.6875,  494935.9062,  445703.8750,
          442891.2812,  414472.9375,  406106.0312,  402772.0625,  398245.5312],
        [ 902515.5625,  856416.3750,  609843.7500,  596705.5625,  477437.7500,
          475521.5000,  472470.7188,  471114.5938,  421390.0000,  386407.5312],
        [ 909413.6250,  436271.0938,  350960.4375,  308659.6875,  302180.0312,
          293157.8750,  288049.7500,  285033.7500,  261869.9062,  242145.0312],
        [ 753732.0000,  718945.1875,  694416.0625,  689355.2500,  669268.3750,
          648234.3750,  630971.9375,  611337.3750,  584628.1875,  582613.3750],
        [ 475467.5312,  398590.5625,  369123.5625,  364257.5312,  344445.4375,
          285679.5000,  272645.9375,  262498.7500,  261574.8750,  247443.2969],
        [ 961393.6875,  821701.0625,  692529.3125,  687069.1875,  653807.8125,
          604394.3750,  580161.0000,  562722.2500,  548599.6250,  496017.0312],
        [ 914497.0625,  905888.8750,  726520.0000,  674554.8750,  656889.0625,
          592566.9375,  580839.1875,  579619.5625,  566775.0625,  546955.1250]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 507062.5938,       0.0000],
         [ 435945.8750,       0.0000],
         [ 214662.2500,       0.0000],
         ...,
         [ 131733.7344,       0.0000],
         [ 128627.0312,       0.0000],
         [      0.0000,  128564.0000]],

        [[1353278.8750,       0.0000],
         [1241580.3750,       0.0000],
         [1234054.8750,       0.0000],
         ...,
         [1176593.7500,       0.0000],
         [1152390.2500,       0.0000],
         [1148608.2500,       0.0000]],

        [[1113276.7500,       0.0000],
         [1099699.2500,       0.0000],
         [1063761.0000,       0.0000],
         ...,
         [ 851509.6250,       0.0000],
         [ 850803.4375,       0.0000],
         [ 842295.8750,       0.0000]],

        ...,

        [[ 475467.5312,       0.0000],
         [ 398590.5625,       0.0000],
         [ 369123.5625,       0.0000],
         ...,
         [      0.0000,  262498.7500],
         [ 261574.8750,       0.0000],
         [      0.0000,  247443.2969]],

        [[      0.0000,  961393.6875],
         [ 821701.0625,       0.0000],
         [ 692529.3125,       0.0000],
         ...,
         [ 562722.2500,       0.0000],
         [      0.0000,  548599.6250],
         [ 496017.0312,       0.0000]],

        [[      0.0000,  914497.0625],
         [ 905888.8750,       0.0000],
         [ 726520.0000,       0.0000],
         ...,
         [ 579619.5625,       0.0000],
         [ 566775.0625,       0.0000],
         [ 546955.1250,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 1954368.7500,   273857.8750],
        [12196254.0000,        0.0000],
        [ 9325025.0000,        0.0000],
        [ 9157418.0000,        0.0000],
        [ 4884160.5000,  2224188.0000],
        [ 9284931.0000,   918981.1875],
        [ 9154656.0000,  3758455.7500],
        [ 9676580.0000,        0.0000],
        [12409758.0000,        0.0000],
        [ 7468829.0000,        0.0000],
        [10895987.0000,        0.0000],
        [ 8559813.0000,        0.0000],
        [ 7854187.0000,        0.0000],
        [12811910.0000,        0.0000],
        [10197961.0000,        0.0000],
        [ 9141805.0000,        0.0000],
        [12938523.0000,        0.0000],
        [12367478.0000,        0.0000],
        [ 7878803.5000,  1803979.8750],
        [10318070.0000,        0.0000],
        [12489656.0000,        0.0000],
        [ 9677070.0000,        0.0000],
        [10308740.0000,        0.0000],
        [11433216.0000,        0.0000],
        [ 9729279.0000,        0.0000],
        [ 8612712.0000,        0.0000],
        [ 8150316.0000,  1004308.6250],
        [ 9109252.0000,        0.0000],
        [11007296.0000,        0.0000],
        [10008487.0000,        0.0000],
        [11755503.0000,        0.0000],
        [ 9418370.0000,        0.0000],
        [ 3535669.0000,  1352186.5000],
        [ 6123303.0000,  2537823.7500],
        [ 7747440.0000,  1967657.5000],
        [ 8621478.0000,  2159467.2500],
        [ 6181751.0000,  1798230.0000],
        [ 6843039.0000,  2056153.7500],
        [ 4094524.5000,  4037599.0000],
        [13145086.0000,        0.0000],
        [ 4576375.5000,   764647.3750],
        [ 6070629.5000,        0.0000],
        [ 6341152.5000,        0.0000],
        [ 5241480.5000,        0.0000],
        [ 3381432.0000,  1347577.2500],
        [ 1019087.8750,   565944.3125],
        [ 8782552.0000,        0.0000],
        [ 2558220.5000,   885854.8750],
        [ 1903148.5000,  3687512.0000],
        [ 8127221.0000,  1954154.0000],
        [ 6070076.5000,   673189.1875],
        [ 5584942.0000,  1185790.7500],
        [ 3967611.5000,  2224481.5000],
        [ 3980482.2500,  1131044.7500],
        [ 9149996.0000,        0.0000],
        [ 8648378.0000,   985989.6250],
        [ 3612388.2500,  3116668.0000],
        [ 3149124.0000,  1789637.0000],
        [ 2879657.5000,  2790165.7500],
        [ 2044006.7500,  1633734.5000],
        [ 5889086.0000,   694416.0625],
        [ 2499139.0000,   782588.0000],
        [ 5098402.0000,  1509993.2500],
        [ 5249769.5000,  1495336.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 51.5625
Top1 accuracy for validation set is 51.5625 size is torch.Size([64, 1])
Epoch 126/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:51, 59.70s/it]  7%|▋         | 2/30 [01:00<11:48, 25.29s/it] 10%|█         | 3/30 [01:01<06:20, 14.09s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.82s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.91s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.73s/it]
Epoch loss is 3.7548377831776936
Epoch 127/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:48, 63.75s/it]  7%|▋         | 2/30 [01:04<12:27, 26.69s/it] 10%|█         | 3/30 [01:05<06:40, 14.85s/it] 13%|█▎        | 4/30 [01:05<04:01,  9.28s/it] 17%|█▋        | 5/30 [01:06<02:35,  6.20s/it] 20%|██        | 6/30 [01:07<01:44,  4.35s/it] 23%|██▎       | 7/30 [01:08<01:12,  3.17s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.40s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.86s/it]
Epoch loss is 3.7312264998753863
Epoch 128/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:49, 57.58s/it]  7%|▋         | 2/30 [00:58<11:16, 24.15s/it] 10%|█         | 3/30 [00:59<06:03, 13.47s/it] 13%|█▎        | 4/30 [01:01<03:53,  8.97s/it] 17%|█▋        | 5/30 [01:01<02:30,  6.00s/it] 20%|██        | 6/30 [01:02<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:03<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.34s/it] 30%|███       | 9/30 [01:04<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:05<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:07<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.7749274333318072
Epoch 129/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:30, 56.92s/it]  7%|▋         | 2/30 [01:00<11:54, 25.53s/it] 10%|█         | 3/30 [01:01<06:23, 14.22s/it] 13%|█▎        | 4/30 [01:01<03:51,  8.90s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.96s/it] 20%|██        | 6/30 [01:03<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.33s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 3.7179190874099732
Epoch 130/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:10, 58.29s/it]  7%|▋         | 2/30 [00:59<11:24, 24.44s/it] 10%|█         | 3/30 [00:59<06:07, 13.63s/it] 13%|█▎        | 4/30 [01:00<03:42,  8.54s/it] 17%|█▋        | 5/30 [01:01<02:23,  5.73s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 3.6931306521097818
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0012,  0.0092, -0.0531,  ..., -0.0098, -0.0235,  0.0316],
        [-0.0421,  0.0260, -0.0114,  ...,  0.0059, -0.0283, -0.0058],
        [-0.0665, -0.0075, -0.0087,  ...,  0.0480, -0.0142, -0.0466],
        ...,
        [ 0.0187, -0.0299, -0.0459,  ..., -0.0429, -0.0244, -0.0078],
        [-0.0488,  0.0095, -0.0113,  ..., -0.0427,  0.0280, -0.0190],
        [-0.0418,  0.0153, -0.0020,  ..., -0.0207,  0.0229, -0.0584]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9180, 0.9093, 0.8590, 0.8477, 0.8388, 0.8362, 0.8299, 0.8230, 0.8211,
         0.8207],
        [0.9864, 0.9820, 0.9812, 0.9811, 0.9810, 0.9802, 0.9798, 0.9768, 0.9767,
         0.9763],
        [0.9703, 0.9675, 0.9633, 0.9546, 0.9534, 0.9501, 0.9495, 0.9494, 0.9442,
         0.9434],
        [0.9839, 0.9688, 0.9688, 0.9602, 0.9591, 0.9582, 0.9563, 0.9429, 0.9424,
         0.9423],
        [0.9605, 0.9533, 0.9435, 0.9408, 0.9368, 0.9361, 0.9310, 0.9243, 0.9227,
         0.9219],
        [0.9738, 0.9712, 0.9696, 0.9672, 0.9669, 0.9665, 0.9636, 0.9622, 0.9584,
         0.9565],
        [0.9909, 0.9899, 0.9895, 0.9895, 0.9856, 0.9815, 0.9787, 0.9766, 0.9761,
         0.9760],
        [0.9855, 0.9707, 0.9700, 0.9663, 0.9581, 0.9568, 0.9522, 0.9489, 0.9480,
         0.9460],
        [0.9833, 0.9829, 0.9820, 0.9808, 0.9808, 0.9794, 0.9793, 0.9791, 0.9791,
         0.9786],
        [0.9800, 0.9653, 0.9526, 0.9415, 0.9394, 0.9359, 0.9318, 0.9307, 0.9302,
         0.9300],
        [0.9784, 0.9773, 0.9732, 0.9720, 0.9703, 0.9693, 0.9690, 0.9689, 0.9682,
         0.9674],
        [0.9726, 0.9721, 0.9660, 0.9650, 0.9514, 0.9474, 0.9420, 0.9413, 0.9260,
         0.9253],
        [0.9823, 0.9727, 0.9589, 0.9494, 0.9443, 0.9321, 0.9314, 0.9310, 0.9308,
         0.9298],
        [0.9869, 0.9843, 0.9841, 0.9827, 0.9825, 0.9825, 0.9819, 0.9818, 0.9813,
         0.9808],
        [0.9784, 0.9741, 0.9692, 0.9688, 0.9670, 0.9656, 0.9645, 0.9645, 0.9622,
         0.9610],
        [0.9718, 0.9693, 0.9655, 0.9590, 0.9575, 0.9545, 0.9530, 0.9516, 0.9492,
         0.9489],
        [0.9926, 0.9915, 0.9856, 0.9827, 0.9824, 0.9822, 0.9807, 0.9801, 0.9799,
         0.9798],
        [0.9904, 0.9884, 0.9811, 0.9797, 0.9789, 0.9789, 0.9785, 0.9782, 0.9782,
         0.9756],
        [0.9822, 0.9806, 0.9659, 0.9612, 0.9598, 0.9569, 0.9566, 0.9559, 0.9506,
         0.9500],
        [0.9775, 0.9723, 0.9716, 0.9675, 0.9654, 0.9640, 0.9640, 0.9638, 0.9638,
         0.9635],
        [0.9915, 0.9867, 0.9845, 0.9826, 0.9816, 0.9789, 0.9784, 0.9764, 0.9755,
         0.9741],
        [0.9819, 0.9733, 0.9723, 0.9620, 0.9573, 0.9566, 0.9550, 0.9541, 0.9538,
         0.9522],
        [0.9891, 0.9753, 0.9715, 0.9686, 0.9676, 0.9675, 0.9648, 0.9612, 0.9608,
         0.9568],
        [0.9809, 0.9792, 0.9774, 0.9751, 0.9749, 0.9742, 0.9707, 0.9687, 0.9681,
         0.9680],
        [0.9849, 0.9807, 0.9658, 0.9606, 0.9603, 0.9595, 0.9591, 0.9571, 0.9569,
         0.9540],
        [0.9675, 0.9597, 0.9582, 0.9555, 0.9540, 0.9540, 0.9524, 0.9500, 0.9489,
         0.9468],
        [0.9722, 0.9670, 0.9669, 0.9628, 0.9588, 0.9555, 0.9548, 0.9532, 0.9524,
         0.9468],
        [0.9703, 0.9695, 0.9644, 0.9616, 0.9583, 0.9579, 0.9565, 0.9556, 0.9505,
         0.9463],
        [0.9849, 0.9823, 0.9795, 0.9755, 0.9719, 0.9680, 0.9654, 0.9647, 0.9631,
         0.9630],
        [0.9748, 0.9734, 0.9665, 0.9664, 0.9651, 0.9640, 0.9617, 0.9615, 0.9601,
         0.9590],
        [0.9828, 0.9805, 0.9804, 0.9798, 0.9773, 0.9767, 0.9767, 0.9746, 0.9737,
         0.9735],
        [0.9691, 0.9664, 0.9637, 0.9626, 0.9586, 0.9584, 0.9565, 0.9565, 0.9549,
         0.9537],
        [0.9358, 0.9177, 0.9153, 0.9153, 0.9108, 0.9102, 0.9059, 0.9054, 0.9053,
         0.9051],
        [0.9696, 0.9640, 0.9586, 0.9561, 0.9541, 0.9522, 0.9517, 0.9464, 0.9463,
         0.9453],
        [0.9739, 0.9668, 0.9637, 0.9633, 0.9625, 0.9588, 0.9572, 0.9556, 0.9553,
         0.9546],
        [0.9760, 0.9729, 0.9712, 0.9709, 0.9703, 0.9683, 0.9678, 0.9667, 0.9625,
         0.9619],
        [0.9638, 0.9611, 0.9586, 0.9583, 0.9486, 0.9478, 0.9413, 0.9334, 0.9327,
         0.9285],
        [0.9751, 0.9643, 0.9625, 0.9569, 0.9545, 0.9539, 0.9511, 0.9490, 0.9484,
         0.9479],
        [0.9616, 0.9613, 0.9607, 0.9552, 0.9475, 0.9445, 0.9442, 0.9402, 0.9380,
         0.9337],
        [0.9917, 0.9889, 0.9863, 0.9861, 0.9858, 0.9852, 0.9836, 0.9828, 0.9823,
         0.9821],
        [0.9577, 0.9323, 0.9306, 0.9301, 0.9267, 0.9163, 0.9025, 0.8986, 0.8978,
         0.8974],
        [0.9408, 0.9384, 0.9340, 0.9315, 0.9303, 0.9298, 0.9296, 0.9289, 0.9271,
         0.9246],
        [0.9466, 0.9432, 0.9419, 0.9374, 0.9292, 0.9280, 0.9276, 0.9266, 0.9169,
         0.9156],
        [0.9556, 0.9385, 0.9212, 0.9181, 0.9147, 0.9105, 0.9078, 0.9024, 0.9015,
         0.8997],
        [0.9441, 0.9240, 0.9157, 0.9104, 0.9087, 0.9080, 0.9072, 0.9029, 0.8970,
         0.8925],
        [0.9022, 0.8361, 0.8291, 0.8233, 0.8224, 0.8070, 0.7951, 0.7918, 0.7916,
         0.7911],
        [0.9740, 0.9689, 0.9623, 0.9612, 0.9556, 0.9504, 0.9491, 0.9483, 0.9463,
         0.9428],
        [0.9189, 0.9124, 0.9123, 0.8943, 0.8937, 0.8853, 0.8717, 0.8573, 0.8538,
         0.8537],
        [0.9434, 0.9376, 0.9355, 0.9230, 0.9226, 0.9196, 0.9188, 0.9104, 0.9059,
         0.9032],
        [0.9760, 0.9724, 0.9690, 0.9663, 0.9640, 0.9628, 0.9618, 0.9594, 0.9588,
         0.9572],
        [0.9481, 0.9461, 0.9425, 0.9282, 0.9273, 0.9271, 0.9268, 0.9250, 0.9244,
         0.9184],
        [0.9449, 0.9438, 0.9422, 0.9385, 0.9351, 0.9350, 0.9275, 0.9266, 0.9225,
         0.9203],
        [0.9664, 0.9543, 0.9393, 0.9355, 0.9308, 0.9297, 0.9251, 0.9071, 0.9043,
         0.9016],
        [0.9577, 0.9414, 0.9344, 0.9164, 0.9092, 0.9078, 0.9000, 0.8938, 0.8902,
         0.8866],
        [0.9651, 0.9626, 0.9616, 0.9613, 0.9597, 0.9587, 0.9584, 0.9581, 0.9555,
         0.9548],
        [0.9713, 0.9662, 0.9643, 0.9638, 0.9637, 0.9591, 0.9590, 0.9570, 0.9562,
         0.9535],
        [0.9609, 0.9505, 0.9464, 0.9458, 0.9331, 0.9322, 0.9292, 0.9270, 0.9229,
         0.9166],
        [0.9583, 0.9167, 0.9144, 0.9135, 0.9037, 0.9036, 0.9029, 0.9004, 0.8994,
         0.8986],
        [0.9579, 0.9526, 0.9324, 0.9302, 0.9153, 0.9138, 0.9138, 0.9093, 0.9004,
         0.8953],
        [0.9587, 0.9034, 0.8946, 0.8844, 0.8778, 0.8770, 0.8762, 0.8751, 0.8725,
         0.8641],
        [0.9428, 0.9397, 0.9391, 0.9383, 0.9365, 0.9332, 0.9332, 0.9307, 0.9282,
         0.9268],
        [0.9066, 0.8964, 0.8895, 0.8861, 0.8829, 0.8720, 0.8712, 0.8682, 0.8641,
         0.8630],
        [0.9623, 0.9499, 0.9383, 0.9349, 0.9314, 0.9283, 0.9267, 0.9241, 0.9202,
         0.9191],
        [0.9593, 0.9561, 0.9444, 0.9337, 0.9298, 0.9274, 0.9257, 0.9234, 0.9234,
         0.9231]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 1, 0, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 1, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 495887.4688,  438187.0938,  213521.5312,  181773.1250,  160076.6406,
          154176.9219,  140982.8906,  127706.6484,  124226.3125,  123605.8984],
        [1317895.8750, 1237398.2500, 1222759.3750, 1221667.2500, 1220422.5000,
         1206812.7500, 1199969.2500, 1149586.7500, 1146749.6250, 1140369.8750],
        [1046986.1875, 1005575.6250,  947747.8125,  836163.5000,  822431.8125,
          784753.4375,  777768.4375,  776983.3750,  721252.6250,  712628.1875],
        [1270896.3750, 1024917.0000, 1024584.6875,  905730.8125,  892464.6250,
          880783.5000,  857298.9375,  707357.0000,  702678.0625,  701323.6875],
        [ 909627.0625,  821028.2500,  713494.5625,  686960.5000,  649042.8750,
          642116.6250,  596854.1250,  542629.3125,  530746.1875,  524374.3125],
        [1101050.8750, 1060284.8750, 1036644.3750, 1001625.6250,  997537.0000,
          991624.0625,  951825.6250,  932025.3125,  882985.3125,  860029.9375],
        [1405389.0000, 1384397.7500, 1377868.5000, 1377551.8750, 1302093.6250,
         1228868.6250, 1180233.7500, 1145044.8750, 1137172.6250, 1135142.1250],
        [1300433.2500, 1053714.6250, 1042645.8125,  989402.3125,  879129.5000,
          863371.2500,  808525.3125,  770827.3750,  761170.0625,  740080.3125],
        [1260925.6250, 1252606.5000, 1236620.7500, 1216124.7500, 1215821.0000,
         1192789.2500, 1189868.2500, 1187621.2500, 1187458.2500, 1178565.7500],
        [1201989.7500,  974406.8125,  813530.3125,  694295.6250,  672958.1250,
          640676.7500,  603829.7500,  594735.2500,  590585.0625,  588411.0625],
        [1176267.2500, 1157334.2500, 1091307.8750, 1072503.0000, 1047166.0000,
         1032377.4375, 1028050.5625, 1026383.2500, 1016352.5000, 1004255.9375],
        [1082248.3750, 1074551.6250,  984676.0000,  970762.5625,  799394.6250,
          755080.2500,  698466.2500,  691387.6250,  555793.6250,  550429.0625],
        [1242506.7500, 1083438.0000,  889870.8125,  777191.6250,  722001.3750,
          606672.5625,  600927.5000,  596985.0625,  595475.3750,  587081.5000],
        [1326596.7500, 1277993.1250, 1275376.5000, 1249869.1250, 1247219.8750,
         1246495.7500, 1235155.7500, 1233101.8750, 1225174.5000, 1215717.7500],
        [1175151.6250, 1105231.6250, 1030320.8125, 1024841.7500,  998283.1875,
          978624.5000,  963975.2500,  963270.4375,  933149.5625,  917128.5625],
        [1069222.7500, 1032500.5625,  977797.9375,  890900.0000,  871693.4375,
          835402.3750,  817807.1250,  801476.2500,  774587.0625,  771598.8750],
        [1438834.8750, 1418308.0000, 1302725.7500, 1249512.8750, 1245156.7500,
         1241602.8750, 1215009.6250, 1205158.8750, 1200913.8750, 1199417.8750],
        [1395417.8750, 1354969.3750, 1221782.5000, 1197984.2500, 1183486.6250,
         1183379.5000, 1176275.1250, 1172170.8750, 1171881.3750, 1129897.3750],
        [1240424.1250, 1212590.2500,  982899.0000,  918762.1250,  900664.3750,
          864269.1875,  860636.2500,  851964.5000,  789902.0625,  783361.1875],
        [1160954.5000, 1077630.3750, 1066980.7500, 1006459.1875,  976654.5000,
          957540.5625,  956292.1250,  953793.8125,  953774.6875,  950013.6875],
        [1416525.1250, 1322467.3750, 1281701.1250, 1248469.3750, 1230856.6250,
         1183538.5000, 1176099.0000, 1141915.1250, 1127585.1250, 1105892.7500],
        [1235655.3750, 1093461.1250, 1076995.3750,  929851.9375,  869281.0000,
          861462.3750,  841113.4375,  831179.3750,  826846.0625,  808183.0625],
        [1369146.1250, 1124805.5000, 1064391.1250, 1021546.5000, 1007592.5000,
         1005530.5000,  967361.6250,  918781.3750,  913844.0625,  863009.0000],
        [1218802.1250, 1188718.2500, 1159047.3750, 1121752.5000, 1117964.3750,
         1107720.8750, 1053238.3750, 1023782.8750, 1014947.0625, 1013022.7500],
        [1290624.5000, 1215011.8750,  982281.3750,  911852.2500,  907169.2500,
          897816.3125,  891624.1250,  866736.3750,  865094.6250,  828989.7500],
        [1005850.8750,  899670.1875,  880986.0000,  847488.0000,  829646.9375,
          829106.7500,  810982.5625,  783063.9375,  771635.6875,  748470.0000],
        [1075143.0000,  998846.0000,  996964.5000,  940119.4375,  888706.3750,
          847682.8750,  838585.6875,  819681.0625,  810445.9375,  748462.8750],
        [1046255.6250, 1035139.8125,  962107.2500,  924478.5625,  882470.1250,
          877336.2500,  859094.5625,  848728.8125,  788821.8125,  743607.0625],
        [1290588.7500, 1242746.0000, 1193382.1250, 1126943.2500, 1070636.8750,
         1012620.8750,  975755.1875,  966102.1875,  945272.6875,  943858.3750],
        [1116762.3750, 1094842.7500,  991608.8750,  990147.0000,  971593.3750,
          956249.2500,  926120.8125,  923092.7500,  904885.5625,  890927.9375],
        [1252174.1250, 1210449.2500, 1210000.3750, 1199152.5000, 1157634.5000,
         1147293.3750, 1147201.5000, 1112911.5000, 1099048.1250, 1096723.7500],
        [1029084.4375,  989813.7500,  952595.6875,  937997.8750,  886148.8125,
          883403.9375,  859417.4375,  859377.3125,  839649.1875,  825862.5625],
        [ 640030.0000,  493593.0000,  477316.6562,  477050.8750,  447780.0000,
          443962.8438,  417114.2188,  414355.1562,  413443.7500,  412748.8125],
        [1036359.7500,  957326.8125,  885993.2500,  854326.4375,  831032.7500,
          808842.3125,  802996.4375,  744324.3125,  743613.4375,  732834.3125],
        [1102196.0000,  995895.4375,  953447.2500,  947008.7500,  936612.3750,
          887832.9375,  868359.6250,  848460.9375,  844684.1250,  836236.9375],
        [1136349.8750, 1087372.5000, 1060090.7500, 1056638.8750, 1046405.2500,
         1018117.1250, 1010512.1250,  994383.6250,  936411.4375,  929237.6250],
        [ 954481.6875,  918008.8750,  885850.5000,  881850.9375,  768181.7500,
          758846.3750,  691708.8750,  618393.8125,  611472.6875,  575937.6875],
        [1120956.8750,  960813.5000,  936979.6250,  864371.3750,  835075.7500,
          828563.6875,  795862.7500,  772587.7500,  765916.2500,  760577.2500],
        [ 924068.7500,  920697.0625,  913010.4375,  843722.0000,  755908.0625,
          724018.1875,  721534.6875,  680727.0625,  660030.0625,  620459.6250],
        [1420440.0000, 1364919.3750, 1315150.1250, 1311505.3750, 1306925.6250,
         1295842.6250, 1265653.0000, 1251918.6250, 1243349.5000, 1238490.2500],
        [ 874008.3750,  608143.3125,  594072.6250,  589893.8125,  561694.4375,
          483976.8125,  397596.6250,  376152.7188,  371724.2812,  369540.2500],
        [ 686935.5625,  663367.0625,  623610.8125,  601566.3125,  591439.5000,
          586896.7500,  585475.1250,  579437.7500,  564792.1875,  545281.1875],
        [ 745926.0625,  710613.2500,  697467.1250,  654389.1875,  581673.4375,
          571825.3750,  568606.1250,  561083.0625,  488026.7188,  478970.0938],
        [ 848343.5625,  664890.2500,  518812.0625,  496694.9062,  473094.7500,
          445698.7500,  428533.1562,  396652.4688,  391615.4062,  381801.2500],
        [ 720603.6250,  540097.4375,  480009.4688,  444799.0312,  434375.0938,
          429889.6875,  424901.1875,  399686.8125,  367364.5000,  344336.0625],
        [ 395489.0938,  153879.9062,  139354.0781,  128130.9453,  126600.5000,
          101572.1250,   85668.9609,   81794.7891,   81519.4219,   80952.4844],
        [1103686.5000, 1026966.8125,  933778.0000,  919852.7500,  848597.7500,
          788099.9375,  773356.5625,  764318.2500,  742656.6875,  707319.9375],
        [ 502527.2500,  457766.2188,  456924.0000,  353396.7812,  350683.0938,
          310666.0938,  255815.1562,  208268.8125,  198169.3906,  198010.5312],
        [ 713201.3125,  656001.3125,  637077.7500,  532389.1875,  529563.1250,
          507377.5000,  501756.2500,  444751.5000,  417258.6562,  401621.3438],
        [1135404.1250, 1078939.3750, 1027649.6250,  988680.7500,  956372.3750,
          940393.0000,  927478.4375,  896033.6250,  888044.6875,  867785.0625],
        [ 762749.8125,  741063.3750,  703916.8750,  573372.3750,  566319.6250,
          564729.6875,  562806.0000,  547901.6250,  543801.6250,  498973.3438],
        [ 728848.9375,  717256.4375,  701016.8125,  664295.7500,  633181.8125,
          632390.6875,  568374.0625,  560805.3750,  528787.0000,  512294.0938],
        [ 990061.1250,  833188.8125,  672441.6875,  636451.0000,  595518.5625,
          585824.7500,  548620.5000,  424250.4688,  408015.6250,  392474.2188],
        [ 874515.3125,  693254.1875,  627079.3125,  484558.2500,  437647.5000,
          428862.2812,  383486.7812,  350930.6562,  333578.4062,  316562.6562],
        [ 971562.7500,  937987.1875,  924826.8750,  920286.1875,  900417.0000,
          886891.9375,  882795.0625,  880020.3125,  847981.2500,  838838.4375],
        [1062251.3750,  986844.7500,  961008.6250,  954229.5625,  952731.0625,
          892135.2500,  890872.7500,  865380.1250,  855779.5000,  823609.9375],
        [ 915896.1875,  789201.0625,  744622.5625,  738152.4375,  615689.8750,
          607260.6250,  581938.1250,  563877.8125,  532006.0000,  486467.7500],
        [ 881950.1875,  486631.5312,  471152.8125,  465378.2188,  404559.4062,
          404007.3125,  399899.9375,  385652.0938,  380474.8750,  376016.7500],
        [ 877031.7500,  812694.4375,  609517.5625,  590582.1875,  477070.4375,
          466912.3750,  466885.6875,  438279.0312,  385455.0000,  358807.6250],
        [ 887462.1875,  402845.4062,  355220.4062,  307024.4375,  279179.1562,
          275984.0000,  272975.8125,  268812.4062,  258913.0938,  229766.1094],
        [ 706551.3125,  676040.5625,  670235.4375,  662434.5625,  645685.5625,
          616461.9375,  615872.5000,  595026.3125,  573456.6250,  562164.4375],
        [ 421395.2188,  364447.9062,  330282.4688,  314338.5938,  300604.9375,
          257150.9219,  254146.5469,  243377.8594,  229741.5625,  225996.2812],
        [ 934336.5625,  781856.5000,  662522.3750,  631074.2500,  600240.1875,
          574528.4375,  561820.8750,  541445.0000,  512088.4375,  503600.0000],
        [ 894246.0625,  855338.1250,  723169.4375,  620981.7500,  586870.4375,
          567548.5625,  553380.2500,  536065.6250,  535886.1875,  533247.5000]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 495887.4688,       0.0000],
         [ 438187.0938,       0.0000],
         [ 213521.5312,       0.0000],
         ...,
         [ 127706.6484,       0.0000],
         [      0.0000,  124226.3125],
         [ 123605.8984,       0.0000]],

        [[1317895.8750,       0.0000],
         [1237398.2500,       0.0000],
         [1222759.3750,       0.0000],
         ...,
         [1149586.7500,       0.0000],
         [1146749.6250,       0.0000],
         [1140369.8750,       0.0000]],

        [[1046986.1875,       0.0000],
         [1005575.6250,       0.0000],
         [ 947747.8125,       0.0000],
         ...,
         [ 776983.3750,       0.0000],
         [ 721252.6250,       0.0000],
         [ 712628.1875,       0.0000]],

        ...,

        [[ 421395.2188,       0.0000],
         [ 364447.9062,       0.0000],
         [ 330282.4688,       0.0000],
         ...,
         [ 243377.8594,       0.0000],
         [ 229741.5625,       0.0000],
         [      0.0000,  225996.2812]],

        [[      0.0000,  934336.5625],
         [ 781856.5000,       0.0000],
         [ 662522.3750,       0.0000],
         ...,
         [      0.0000,  541445.0000],
         [ 512088.4375,       0.0000],
         [ 503600.0000,       0.0000]],

        [[ 894246.0625,       0.0000],
         [      0.0000,  855338.1250],
         [ 723169.4375,       0.0000],
         ...,
         [ 536065.6250,       0.0000],
         [      0.0000,  535886.1875],
         [ 533247.5000,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 1894935.2500,   265209.1875],
        [12063632.0000,        0.0000],
        [ 8432291.0000,        0.0000],
        [ 8968034.0000,        0.0000],
        [ 4539722.0000,  2077152.0000],
        [ 8932648.0000,   882985.3125],
        [ 9009178.0000,  3664585.0000],
        [ 9209300.0000,        0.0000],
        [12118401.0000,        0.0000],
        [ 7375418.5000,        0.0000],
        [10651998.0000,        0.0000],
        [ 8162790.0000,        0.0000],
        [ 7702151.0000,        0.0000],
        [12532701.0000,        0.0000],
        [10089977.0000,        0.0000],
        [ 8842986.0000,        0.0000],
        [12716642.0000,        0.0000],
        [12187245.0000,        0.0000],
        [ 7721447.5000,  1684025.5000],
        [10060094.0000,        0.0000],
        [12235050.0000,        0.0000],
        [ 9374029.0000,        0.0000],
        [10256008.0000,        0.0000],
        [11018996.0000,        0.0000],
        [ 9657201.0000,        0.0000],
        [ 8406901.0000,        0.0000],
        [ 8024518.0000,   940119.4375],
        [ 8968039.0000,        0.0000],
        [10767906.0000,        0.0000],
        [ 9766231.0000,        0.0000],
        [11632589.0000,        0.0000],
        [ 9063351.0000,        0.0000],
        [ 4160078.7500,   477316.6562],
        [ 5913717.0000,  2483933.2500],
        [ 7282302.0000,  1938433.0000],
        [ 8128055.5000,  2147463.2500],
        [ 5942070.0000,  1722663.5000],
        [ 6559934.5000,  2081770.3750],
        [ 3800817.0000,  3963359.0000],
        [13014194.0000,        0.0000],
        [ 4481110.0000,   745693.0000],
        [ 6028802.0000,        0.0000],
        [ 6058580.0000,        0.0000],
        [ 5046136.5000,        0.0000],
        [ 3276999.0000,  1309063.7500],
        [  796381.6250,   578580.6250],
        [ 8608634.0000,        0.0000],
        [ 2422609.2500,   869618.0000],
        [ 2169159.0000,  3171838.7500],
        [ 7830055.5000,  1876725.5000],
        [ 4993288.5000,  1072345.7500],
        [ 5118072.0000,  1129179.5000],
        [ 3882006.2500,  2204840.5000],
        [ 3818837.7500,  1111637.5000],
        [ 8991607.0000,        0.0000],
        [ 7418454.5000,  1826388.7500],
        [ 3518132.0000,  3056980.5000],
        [ 2981878.0000,  1673845.0000],
        [ 2798537.5000,  2684698.7500],
        [ 1974899.6250,  1563283.5000],
        [ 5661495.0000,   662434.5625],
        [ 2204188.5000,   737293.7500],
        [ 4827731.0000,  1475781.5000],
        [ 5015509.5000,  1391224.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 53.125
Top1 accuracy for validation set is 53.125 size is torch.Size([64, 1])
Epoch 131/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:28, 60.98s/it]  7%|▋         | 2/30 [01:02<12:14, 26.22s/it] 10%|█         | 3/30 [01:03<06:34, 14.59s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.13s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.11s/it] 20%|██        | 6/30 [01:05<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 3.6763613780339557
Epoch 132/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:27, 58.86s/it]  7%|▋         | 2/30 [00:59<11:31, 24.68s/it] 10%|█         | 3/30 [01:00<06:11, 13.75s/it] 13%|█▎        | 4/30 [01:01<03:44,  8.62s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.78s/it] 20%|██        | 6/30 [01:02<01:37,  4.07s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:04<00:49,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.6590691486994427
Epoch 133/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:32, 61.13s/it]  7%|▋         | 2/30 [01:01<11:57, 25.61s/it] 10%|█         | 3/30 [01:02<06:25, 14.26s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.93s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.98s/it] 20%|██        | 6/30 [01:04<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 3.6788847128550213
Epoch 134/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:11, 60.40s/it]  7%|▋         | 2/30 [01:02<12:07, 26.00s/it] 10%|█         | 3/30 [01:04<06:43, 14.94s/it] 13%|█▎        | 4/30 [01:04<04:02,  9.34s/it] 17%|█▋        | 5/30 [01:05<02:35,  6.24s/it] 20%|██        | 6/30 [01:06<01:44,  4.37s/it] 23%|██▎       | 7/30 [01:07<01:13,  3.19s/it] 27%|██▋       | 8/30 [01:07<00:53,  2.41s/it] 30%|███       | 9/30 [01:08<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.30s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 3.649236234029134
Epoch 135/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:39, 57.24s/it]  7%|▋         | 2/30 [01:01<12:12, 26.16s/it] 10%|█         | 3/30 [01:02<06:32, 14.55s/it] 13%|█▎        | 4/30 [01:03<03:56,  9.10s/it] 17%|█▋        | 5/30 [01:03<02:32,  6.09s/it] 20%|██        | 6/30 [01:04<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.37s/it] 30%|███       | 9/30 [01:06<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 3.587244812647502
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0023,  0.0093, -0.0502,  ..., -0.0123, -0.0246,  0.0314],
        [-0.0387,  0.0250, -0.0120,  ...,  0.0069, -0.0303, -0.0052],
        [-0.0630, -0.0092, -0.0061,  ...,  0.0500, -0.0159, -0.0479],
        ...,
        [ 0.0225, -0.0301, -0.0461,  ..., -0.0403, -0.0257, -0.0104],
        [-0.0454,  0.0088, -0.0121,  ..., -0.0396,  0.0261, -0.0200],
        [-0.0403,  0.0120, -0.0025,  ..., -0.0169,  0.0209, -0.0577]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9179, 0.9082, 0.8558, 0.8477, 0.8324, 0.8304, 0.8289, 0.8229, 0.8209,
         0.8161],
        [0.9852, 0.9810, 0.9809, 0.9805, 0.9804, 0.9804, 0.9789, 0.9767, 0.9757,
         0.9752],
        [0.9663, 0.9594, 0.9558, 0.9541, 0.9457, 0.9449, 0.9421, 0.9382, 0.9308,
         0.9299],
        [0.9844, 0.9683, 0.9681, 0.9583, 0.9579, 0.9543, 0.9529, 0.9417, 0.9404,
         0.9399],
        [0.9535, 0.9460, 0.9349, 0.9344, 0.9307, 0.9293, 0.9227, 0.9202, 0.9159,
         0.9149],
        [0.9711, 0.9677, 0.9653, 0.9645, 0.9631, 0.9626, 0.9601, 0.9587, 0.9556,
         0.9533],
        [0.9912, 0.9891, 0.9886, 0.9886, 0.9843, 0.9795, 0.9761, 0.9747, 0.9744,
         0.9740],
        [0.9832, 0.9700, 0.9688, 0.9638, 0.9527, 0.9521, 0.9451, 0.9451, 0.9441,
         0.9427],
        [0.9822, 0.9817, 0.9792, 0.9787, 0.9787, 0.9780, 0.9761, 0.9759, 0.9758,
         0.9752],
        [0.9798, 0.9616, 0.9515, 0.9403, 0.9391, 0.9327, 0.9286, 0.9278, 0.9278,
         0.9272],
        [0.9771, 0.9767, 0.9712, 0.9702, 0.9696, 0.9685, 0.9676, 0.9663, 0.9661,
         0.9659],
        [0.9700, 0.9695, 0.9638, 0.9628, 0.9497, 0.9457, 0.9399, 0.9390, 0.9209,
         0.9205],
        [0.9813, 0.9718, 0.9562, 0.9512, 0.9397, 0.9304, 0.9290, 0.9281, 0.9280,
         0.9279],
        [0.9861, 0.9832, 0.9829, 0.9826, 0.9818, 0.9812, 0.9806, 0.9803, 0.9802,
         0.9792],
        [0.9781, 0.9754, 0.9692, 0.9687, 0.9666, 0.9664, 0.9663, 0.9658, 0.9648,
         0.9627],
        [0.9695, 0.9672, 0.9631, 0.9559, 0.9558, 0.9535, 0.9504, 0.9484, 0.9452,
         0.9444],
        [0.9919, 0.9898, 0.9844, 0.9824, 0.9822, 0.9809, 0.9801, 0.9787, 0.9784,
         0.9780],
        [0.9887, 0.9870, 0.9794, 0.9788, 0.9784, 0.9780, 0.9777, 0.9777, 0.9767,
         0.9752],
        [0.9805, 0.9794, 0.9652, 0.9593, 0.9574, 0.9553, 0.9537, 0.9525, 0.9494,
         0.9467],
        [0.9773, 0.9727, 0.9711, 0.9686, 0.9645, 0.9635, 0.9629, 0.9628, 0.9621,
         0.9613],
        [0.9918, 0.9866, 0.9847, 0.9817, 0.9791, 0.9785, 0.9753, 0.9744, 0.9737,
         0.9731],
        [0.9811, 0.9715, 0.9713, 0.9593, 0.9558, 0.9557, 0.9543, 0.9539, 0.9510,
         0.9472],
        [0.9881, 0.9737, 0.9697, 0.9677, 0.9667, 0.9667, 0.9645, 0.9627, 0.9605,
         0.9563],
        [0.9757, 0.9756, 0.9738, 0.9725, 0.9703, 0.9694, 0.9670, 0.9655, 0.9654,
         0.9638],
        [0.9852, 0.9792, 0.9627, 0.9610, 0.9608, 0.9593, 0.9560, 0.9540, 0.9538,
         0.9530],
        [0.9638, 0.9588, 0.9567, 0.9533, 0.9523, 0.9517, 0.9496, 0.9494, 0.9476,
         0.9465],
        [0.9729, 0.9669, 0.9663, 0.9586, 0.9562, 0.9542, 0.9517, 0.9516, 0.9483,
         0.9459],
        [0.9701, 0.9689, 0.9617, 0.9595, 0.9590, 0.9564, 0.9547, 0.9545, 0.9489,
         0.9483],
        [0.9830, 0.9796, 0.9762, 0.9722, 0.9694, 0.9654, 0.9637, 0.9623, 0.9619,
         0.9612],
        [0.9735, 0.9717, 0.9666, 0.9641, 0.9630, 0.9627, 0.9618, 0.9615, 0.9591,
         0.9584],
        [0.9821, 0.9792, 0.9790, 0.9788, 0.9772, 0.9752, 0.9751, 0.9738, 0.9730,
         0.9725],
        [0.9677, 0.9642, 0.9614, 0.9613, 0.9565, 0.9553, 0.9541, 0.9539, 0.9534,
         0.9518],
        [0.9274, 0.9177, 0.9105, 0.9086, 0.9080, 0.9067, 0.9064, 0.9055, 0.9015,
         0.8973],
        [0.9666, 0.9640, 0.9579, 0.9550, 0.9503, 0.9470, 0.9462, 0.9456, 0.9450,
         0.9449],
        [0.9742, 0.9645, 0.9615, 0.9609, 0.9596, 0.9540, 0.9522, 0.9520, 0.9514,
         0.9508],
        [0.9743, 0.9719, 0.9718, 0.9664, 0.9634, 0.9634, 0.9616, 0.9602, 0.9586,
         0.9567],
        [0.9627, 0.9564, 0.9563, 0.9561, 0.9461, 0.9450, 0.9367, 0.9304, 0.9295,
         0.9295],
        [0.9747, 0.9644, 0.9595, 0.9544, 0.9522, 0.9521, 0.9487, 0.9464, 0.9449,
         0.9447],
        [0.9603, 0.9592, 0.9584, 0.9497, 0.9413, 0.9410, 0.9406, 0.9368, 0.9344,
         0.9296],
        [0.9914, 0.9878, 0.9861, 0.9856, 0.9854, 0.9851, 0.9835, 0.9825, 0.9813,
         0.9805],
        [0.9570, 0.9314, 0.9308, 0.9275, 0.9218, 0.9160, 0.9016, 0.8972, 0.8960,
         0.8955],
        [0.9428, 0.9383, 0.9369, 0.9333, 0.9316, 0.9293, 0.9278, 0.9276, 0.9266,
         0.9264],
        [0.9419, 0.9394, 0.9369, 0.9365, 0.9247, 0.9233, 0.9216, 0.9205, 0.9150,
         0.9148],
        [0.9523, 0.9348, 0.9172, 0.9150, 0.9140, 0.9075, 0.9054, 0.9014, 0.8959,
         0.8957],
        [0.9414, 0.9237, 0.9181, 0.9115, 0.9089, 0.9066, 0.9061, 0.9044, 0.8967,
         0.8933],
        [0.8950, 0.8240, 0.8179, 0.8121, 0.8111, 0.7924, 0.7917, 0.7913, 0.7895,
         0.7853],
        [0.9734, 0.9682, 0.9596, 0.9580, 0.9541, 0.9495, 0.9482, 0.9472, 0.9457,
         0.9435],
        [0.9132, 0.9072, 0.9070, 0.8911, 0.8879, 0.8848, 0.8697, 0.8543, 0.8543,
         0.8537],
        [0.9397, 0.9364, 0.9347, 0.9224, 0.9206, 0.9132, 0.9132, 0.9090, 0.9037,
         0.9015],
        [0.9728, 0.9680, 0.9671, 0.9638, 0.9599, 0.9597, 0.9588, 0.9569, 0.9558,
         0.9540],
        [0.9429, 0.9415, 0.9372, 0.9252, 0.9205, 0.9180, 0.9170, 0.9149, 0.9147,
         0.9131],
        [0.9437, 0.9389, 0.9337, 0.9318, 0.9318, 0.9296, 0.9265, 0.9223, 0.9213,
         0.9127],
        [0.9670, 0.9493, 0.9401, 0.9354, 0.9316, 0.9270, 0.9210, 0.9065, 0.8999,
         0.8998],
        [0.9538, 0.9379, 0.9315, 0.9133, 0.9070, 0.9052, 0.8986, 0.8901, 0.8900,
         0.8788],
        [0.9628, 0.9620, 0.9616, 0.9612, 0.9599, 0.9584, 0.9566, 0.9559, 0.9547,
         0.9533],
        [0.9689, 0.9626, 0.9625, 0.9620, 0.9617, 0.9551, 0.9550, 0.9544, 0.9543,
         0.9517],
        [0.9612, 0.9499, 0.9456, 0.9442, 0.9301, 0.9256, 0.9250, 0.9230, 0.9214,
         0.9110],
        [0.9566, 0.9137, 0.9130, 0.9098, 0.9038, 0.8989, 0.8966, 0.8966, 0.8950,
         0.8941],
        [0.9555, 0.9487, 0.9319, 0.9292, 0.9173, 0.9142, 0.9125, 0.9021, 0.8950,
         0.8931],
        [0.9579, 0.8972, 0.8951, 0.8845, 0.8776, 0.8742, 0.8711, 0.8705, 0.8699,
         0.8626],
        [0.9409, 0.9368, 0.9353, 0.9349, 0.9339, 0.9316, 0.9300, 0.9267, 0.9259,
         0.9239],
        [0.8992, 0.8918, 0.8825, 0.8777, 0.8725, 0.8696, 0.8646, 0.8597, 0.8585,
         0.8564],
        [0.9586, 0.9459, 0.9353, 0.9296, 0.9262, 0.9252, 0.9252, 0.9232, 0.9187,
         0.9102],
        [0.9558, 0.9476, 0.9416, 0.9253, 0.9226, 0.9226, 0.9211, 0.9197, 0.9190,
         0.9177]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 1, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [0, 1, 0, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 1, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 1, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 495460.1250,  430890.3750,  204039.6250,  181795.6719,  146046.9688,
          141817.3281,  138870.9062,  127450.6641,  123956.0156,  115632.3047],
        [1295042.1250, 1220088.2500, 1217366.3750, 1210433.1250, 1210006.1250,
         1209748.7500, 1183606.2500, 1148020.1250, 1130637.8750, 1122994.1250],
        [ 988146.3125,  896255.8750,  850901.6250,  830902.0625,  736929.2500,
          727870.1875,  699850.5000,  661608.6875,  595411.7500,  587724.0000],
        [1280754.2500, 1016819.8125, 1015077.8125,  882068.7500,  876750.7500,
          833186.5000,  816471.4375,  695988.7500,  683395.3125,  677880.5625],
        [ 823007.7500,  739389.6250,  631710.7500,  626792.3125,  594749.5000,
          583117.5625,  530084.0625,  512006.8750,  481634.0625,  474447.9688],
        [1058286.7500, 1009079.1250,  974196.7500,  963553.4375,  944286.9375,
          938274.3750,  904715.5625,  887009.5000,  848892.3750,  821417.4375],
        [1411068.5000, 1369950.6250, 1360336.1250, 1359963.8750, 1279175.8750,
         1193384.3750, 1138212.1250, 1115521.3750, 1109662.1250, 1104142.2500],
        [1258628.5000, 1042416.1875, 1024217.4375,  954292.4375,  814322.0625,
          807545.0625,  730173.6250,  730039.9375,  719774.5625,  705570.2500],
        [1240808.6250, 1231502.5000, 1189590.2500, 1180392.5000, 1180291.2500,
         1168203.6250, 1137056.6250, 1133423.1250, 1131833.2500, 1122755.2500],
        [1199999.0000,  924929.2500,  799987.1250,  682154.8750,  670566.5625,
          612265.0625,  577439.6875,  570814.6250,  570764.0625,  565431.3750],
        [1154505.6250, 1146411.8750, 1060171.6250, 1045351.0000, 1036985.5625,
         1020786.8125, 1006788.4375,  989522.1250,  985864.6250,  983228.0625],
        [1042725.4375, 1035796.5625,  953699.1875,  940618.1250,  779807.2500,
          737068.5000,  678420.5625,  669232.0000,  517143.8750,  514289.8438],
        [1225640.7500, 1070112.2500,  855528.1875,  797203.5000,  676134.1250,
          592242.1250,  580285.0000,  573267.9375,  571740.8750,  571179.0000],
        [1311974.3750, 1259292.3750, 1253391.6250, 1248417.0000, 1234338.5000,
         1223493.1250, 1212422.5000, 1208438.8750, 1205373.7500, 1189650.3750],
        [1169815.6250, 1125682.2500, 1029979.9375, 1023578.7500,  993468.8750,
          990452.0625,  988992.0000,  981773.8750,  967594.0625,  939698.1875],
        [1035030.2500, 1001569.2500,  945178.8750,  852040.0625,  851301.0000,
          823797.6250,  787764.7500,  765954.1875,  731034.8125,  723665.4375],
        [1425913.7500, 1382936.8750, 1280564.8750, 1245037.8750, 1241827.8750,
         1217553.2500, 1203858.5000, 1180189.8750, 1175021.6250, 1168171.2500],
        [1361667.8750, 1329672.1250, 1191542.1250, 1181741.8750, 1175012.6250,
         1169394.0000, 1163926.7500, 1163346.3750, 1147394.0000, 1123215.8750],
        [1211922.0000, 1192494.7500,  974073.2500,  895165.8750,  870483.0625,
          844755.0000,  825820.0625,  811343.8125,  776310.8125,  747340.9375],
        [1157555.0000, 1083197.3750, 1058929.8750, 1021419.8125,  963520.3125,
          949937.5625,  942362.6875,  940871.0625,  930965.5625,  920789.3125],
        [1423973.2500, 1320705.5000, 1285863.8750, 1231321.6250, 1186481.3750,
         1176337.8750, 1124552.3750, 1109878.1250, 1098614.2500, 1089023.6250],
        [1220896.1250, 1064899.7500, 1061890.7500,  894902.1250,  851427.6250,
          850460.3125,  832837.7500,  827939.7500,  794319.6875,  752699.7500],
        [1350450.3750, 1099183.3750, 1037636.4375, 1008397.0625,  994045.1250,
          994016.6250,  963720.6875,  938936.8125,  909875.1250,  857567.9375],
        [1131665.8750, 1129910.2500, 1099889.1250, 1080465.3750, 1046306.5625,
         1033118.0625,  998785.8750,  977429.7500,  976050.1250,  954300.5625],
        [1296103.5000, 1189400.8750,  939109.5625,  917061.2500,  914387.2500,
          895148.0000,  853232.9375,  829172.3750,  826803.5000,  817719.0000],
        [ 954229.5625,  887765.2500,  861646.3750,  821574.9375,  810157.0000,
          803098.2500,  778847.7500,  776644.0625,  757531.7500,  744939.3125],
        [1086324.6250,  997002.5000,  989129.6875,  886167.3750,  855603.2500,
          831573.4375,  802367.1875,  801816.4375,  765090.6250,  738532.7500],
        [1043646.6250, 1026288.3125,  926316.0000,  897059.6875,  890492.2500,
          858547.4375,  838057.9375,  835334.6250,  771502.5000,  764366.3750],
        [1255870.7500, 1196490.8750, 1139579.5000, 1076067.2500, 1032957.5000,
          976451.4375,  953250.0000,  934495.1875,  929005.4375,  919558.8750],
        [1096623.3750, 1067665.7500,  993567.4375,  957704.9375,  943968.1875,
          939734.9375,  927133.5625,  922671.2500,  892697.8125,  882895.2500],
        [1238810.3750, 1188114.1250, 1184929.8750, 1181904.1250, 1155518.8750,
         1122422.3750, 1121704.2500, 1100494.5000, 1088594.7500, 1079956.5000],
        [1008953.0625,  959295.4375,  922310.5000,  921147.6250,  860104.5625,
          844689.7500,  830292.1250,  828346.4375,  822301.5625,  803803.2500],
        [ 567436.0000,  494194.9375,  445300.6875,  433917.5938,  429956.1250,
          421853.1562,  419999.0000,  414877.5000,  391994.6875,  369254.5625],
        [ 992544.6250,  957122.3750,  876823.5000,  841898.3125,  786897.5625,
          750659.5625,  742129.1875,  735627.4375,  729562.4375,  728568.8750],
        [1107603.6250,  964155.5625,  922814.6250,  915053.6250,  898646.3750,
          829246.6875,  808974.1875,  806366.0625,  798686.6250,  792482.5625],
        [1108558.8750, 1070727.8750, 1069416.5000,  990438.8750,  948718.1250,
          948395.1250,  925163.8750,  906742.8750,  886169.0625,  861708.8125],
        [ 938920.6250,  858807.8750,  857613.7500,  855118.7500,  740872.6250,
          729143.0625,  648116.3125,  592436.4375,  584762.5625,  584697.8750],
        [1114209.2500,  961902.6250,  896885.1875,  834821.7500,  807889.3750,
          807151.6250,  768790.0625,  744643.8750,  728780.1250,  726449.3125],
        [ 907810.5625,  893974.1250,  883195.9375,  780063.8750,  692017.0000,
          689128.4375,  685269.0625,  648716.7500,  627303.6250,  585506.9375],
        [1415413.7500, 1344553.1250, 1311840.5000, 1301995.5000, 1298521.0000,
         1292584.2500, 1264060.7500, 1245555.7500, 1225692.1250, 1211735.8750],
        [ 865293.5625,  600423.9375,  595479.3125,  568221.2500,  523846.9688,
          482161.6562,  392497.0312,  368734.8125,  362260.0000,  359886.0625],
        [ 706552.6875,  662947.6875,  649762.5000,  617342.0625,  601949.6250,
          582671.1250,  570852.2500,  568685.8125,  560959.9375,  558894.5000],
        [ 697421.8750,  673424.1875,  649996.1875,  645588.9375,  545626.0625,
          534802.3125,  522098.8125,  514238.8125,  475366.9062,  474067.5625],
        [ 809694.3125,  630871.5000,  490525.9062,  475000.7188,  468503.4375,
          426835.5938,  414303.3750,  391191.7188,  361511.4375,  360796.7812],
        [ 692585.4375,  538324.3750,  496769.2812,  451875.9688,  435250.8750,
          421355.4062,  418451.7500,  408623.8750,  365884.8125,  348433.5625],
        [ 357122.9062,  129577.2812,  118612.8906,  109217.0312,  107779.5234,
           82444.3359,   81638.0625,   81111.2891,   79065.2344,   74533.4062],
        [1093950.3750, 1015382.7500,  898664.3125,  878224.4375,  830115.5000,
          777305.7500,  763653.0625,  752236.1875,  736544.9375,  713657.1875],
        [ 463355.2500,  424925.5000,  423886.9375,  337737.5312,  322807.5625,
          308697.3750,  248925.6562,  199630.7344,  199501.7031,  197971.4375],
        [ 676235.9375,  644658.0625,  629748.0625,  528292.0000,  514801.1250,
          463058.0000,  462782.0312,  436088.5000,  404332.6250,  391736.4375],
        [1085480.5000, 1012619.0000, 1000478.1250,  953612.8125,  902908.9375,
          899756.0625,  888780.9375,  865050.1250,  851050.9375,  829192.9375],
        [ 708182.5000,  694128.7500,  652738.1250,  549948.4375,  513802.0625,
          495802.8125,  488739.8125,  474680.5625,  473032.9375,  462523.5000],
        [ 716283.6875,  668262.6250,  620329.4375,  604192.6875,  603896.5625,
          585286.4375,  560068.3750,  527542.3125,  519869.9688,  459600.8125],
        [ 998434.5625,  775687.6875,  680508.3750,  635943.8125,  601971.4375,
          564328.6250,  517666.4375,  420684.0625,  382992.2500,  382575.4062],
        [ 826559.8750,  659118.6250,  601494.0000,  464095.1250,  423586.6562,
          412891.3438,  376059.0938,  332808.5000,  332557.2188,  283410.4062],
        [ 941217.5000,  929547.8750,  924484.7500,  919731.6875,  901960.5000,
          883345.8125,  860314.5625,  852513.9375,  837470.7500,  821766.1250],
        [1025850.9375,  938142.8125,  936973.3125,  930383.3125,  925573.3125,
          842221.1250,  841916.7500,  833878.8750,  833427.3125,  802806.5625],
        [ 919907.1250,  782496.5625,  736167.8750,  721361.3125,  589975.3750,
          553223.5625,  547893.2500,  532402.3750,  520807.3750,  449015.4062],
        [ 861249.5625,  466136.4688,  462065.4062,  440959.7500,  404897.9375,
          377530.2188,  365430.4062,  365397.3125,  357302.0938,  352670.2500],
        [ 847143.8750,  769361.4375,  605158.0000,  581886.5000,  491368.6562,
          469774.9375,  458327.0938,  395295.2500,  357243.8438,  347692.9688],
        [ 877159.7500,  368479.5938,  357781.5000,  307366.9062,  278552.5938,
          265276.5938,  253935.7656,  251704.5625,  249646.9531,  224846.1406],
        [ 687877.5625,  648581.9375,  634753.8125,  630973.8125,  622627.3125,
          602339.0000,  589018.0000,  561773.6875,  555237.8750,  539731.3750],
        [ 379020.1250,  341034.6562,  298609.1250,  279020.7812,  259028.1719,
          248540.8906,  231437.7031,  215656.8125,  211878.8906,  205763.3125],
        [ 886444.6250,  739157.6875,  634679.3125,  585778.3750,  557357.8750,
          549775.3750,  549359.2500,  534035.2500,  500798.7188,  443907.3750],
        [ 850585.1875,  756893.4375,  695202.0000,  550566.6250,  529870.7500,
          529542.4375,  518577.0938,  508346.6250,  502930.4688,  494035.6562]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 495460.1250,       0.0000],
         [ 430890.3750,       0.0000],
         [ 204039.6250,       0.0000],
         ...,
         [ 127450.6641,       0.0000],
         [ 123956.0156,       0.0000],
         [      0.0000,  115632.3047]],

        [[1295042.1250,       0.0000],
         [1220088.2500,       0.0000],
         [1217366.3750,       0.0000],
         ...,
         [1148020.1250,       0.0000],
         [1130637.8750,       0.0000],
         [1122994.1250,       0.0000]],

        [[ 988146.3125,       0.0000],
         [ 896255.8750,       0.0000],
         [ 850901.6250,       0.0000],
         ...,
         [ 661608.6875,       0.0000],
         [ 595411.7500,       0.0000],
         [ 587724.0000,       0.0000]],

        ...,

        [[ 379020.1250,       0.0000],
         [ 341034.6562,       0.0000],
         [ 298609.1250,       0.0000],
         ...,
         [ 215656.8125,       0.0000],
         [ 211878.8906,       0.0000],
         [      0.0000,  205763.3125]],

        [[      0.0000,  886444.6250],
         [ 739157.6875,       0.0000],
         [ 634679.3125,       0.0000],
         ...,
         [      0.0000,  534035.2500],
         [ 500798.7188,       0.0000],
         [ 443907.3750,       0.0000]],

        [[ 850585.1875,       0.0000],
         [      0.0000,  756893.4375],
         [ 695202.0000,       0.0000],
         ...,
         [ 508346.6250,       0.0000],
         [ 502930.4688,       0.0000],
         [ 494035.6562,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 1851456.7500,   254503.2188],
        [11947943.0000,        0.0000],
        [ 7575600.0000,        0.0000],
        [ 8778394.0000,        0.0000],
        [ 4113833.0000,  1883107.2500],
        [ 8500820.0000,   848892.3750],
        [ 9965932.0000,  2475485.2500],
        [ 8786980.0000,        0.0000],
        [11715858.0000,        0.0000],
        [ 7174351.5000,        0.0000],
        [10429616.0000,        0.0000],
        [ 7868801.0000,        0.0000],
        [ 7513333.5000,        0.0000],
        [12346793.0000,        0.0000],
        [10211036.0000,        0.0000],
        [ 8517336.0000,        0.0000],
        [12521076.0000,        0.0000],
        [12006914.0000,        0.0000],
        [ 7562055.0000,  1587654.6250],
        [ 9969548.0000,        0.0000],
        [12046752.0000,        0.0000],
        [ 9152273.0000,        0.0000],
        [10153829.0000,        0.0000],
        [10427921.0000,        0.0000],
        [ 9478138.0000,        0.0000],
        [ 8196434.0000,        0.0000],
        [ 7867440.5000,   886167.3750],
        [ 8851612.0000,        0.0000],
        [10413726.0000,        0.0000],
        [ 9624662.0000,        0.0000],
        [11462450.0000,        0.0000],
        [ 8801244.0000,        0.0000],
        [ 3943483.5000,   445300.6875],
        [ 5694543.5000,  2447290.7500],
        [ 6927452.0000,  1916577.7500],
        [ 7575896.0000,  2140144.5000],
        [ 5722426.0000,  1668063.7500],
        [ 6315411.5000,  2076111.8750],
        [ 3561339.5000,  3831646.7500],
        [12911953.0000,        0.0000],
        [ 4756544.5000,   362260.0000],
        [ 6080618.0000,        0.0000],
        [ 5732632.0000,        0.0000],
        [ 4829234.5000,        0.0000],
        [ 2915919.7500,  1661635.5000],
        [  702469.5000,   518632.4688],
        [ 8459735.0000,        0.0000],
        [ 2283033.2500,   844406.3750],
        [ 2102053.0000,  3049679.5000],
        [ 7484266.5000,  1804663.7500],
        [ 4551806.5000,   961772.7500],
        [ 4777722.0000,  1087610.7500],
        [ 3784527.0000,  2176265.7500],
        [ 3646991.5000,  1065589.1250],
        [ 8872354.0000,        0.0000],
        [ 7131980.0000,  1779194.5000],
        [ 3361455.2500,  2991795.0000],
        [ 2855848.2500,  1597791.0000],
        [ 2744345.5000,  2578907.2500],
        [ 1923834.5000,  1510916.0000],
        [ 5438160.5000,   634753.8125],
        [ 1984248.6250,   685741.8750],
        [ 4560814.0000,  1420479.8750],
        [ 5179657.0000,   756893.4375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 53.125
Top1 accuracy for validation set is 53.125 size is torch.Size([64, 1])
Epoch 136/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:58, 57.87s/it]  7%|▋         | 2/30 [00:59<11:27, 24.55s/it] 10%|█         | 3/30 [00:59<06:09, 13.68s/it] 13%|█▎        | 4/30 [01:00<03:46,  8.72s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.85s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.6259400208791095
Epoch 137/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:45, 59.51s/it]  7%|▋         | 2/30 [01:02<12:07, 25.99s/it] 10%|█         | 3/30 [01:02<06:30, 14.47s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.05s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 3.5907112042109173
Epoch 138/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<31:12, 64.55s/it]  7%|▋         | 2/30 [01:05<12:36, 27.02s/it] 10%|█         | 3/30 [01:06<06:45, 15.02s/it] 13%|█▎        | 4/30 [01:06<04:04,  9.39s/it] 17%|█▋        | 5/30 [01:07<02:36,  6.27s/it] 20%|██        | 6/30 [01:08<01:45,  4.39s/it] 23%|██▎       | 7/30 [01:09<01:13,  3.20s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.42s/it] 30%|███       | 9/30 [01:10<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:11<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:12<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:14,  1.07it/s] 50%|█████     | 15/30 [01:15<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:18<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:21<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:24<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.88s/it]
Epoch loss is 3.5779500802357993
Epoch 139/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:24, 56.69s/it]  7%|▋         | 2/30 [00:58<11:24, 24.46s/it] 10%|█         | 3/30 [00:59<06:15, 13.90s/it] 13%|█▎        | 4/30 [01:00<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.84s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:02<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 3.5733888785044354
Epoch 140/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:18, 56.50s/it]  7%|▋         | 2/30 [01:00<12:00, 25.73s/it] 10%|█         | 3/30 [01:01<06:26, 14.33s/it] 13%|█▎        | 4/30 [01:02<03:53,  8.97s/it] 17%|█▋        | 5/30 [01:02<02:30,  6.00s/it] 20%|██        | 6/30 [01:03<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 3.6279613256454466
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0039,  0.0096, -0.0456,  ..., -0.0133, -0.0255,  0.0325],
        [-0.0350,  0.0236, -0.0112,  ...,  0.0086, -0.0318, -0.0047],
        [-0.0605, -0.0109, -0.0036,  ...,  0.0523, -0.0174, -0.0492],
        ...,
        [ 0.0254, -0.0295, -0.0450,  ..., -0.0375, -0.0279, -0.0114],
        [-0.0433,  0.0084, -0.0114,  ..., -0.0362,  0.0244, -0.0208],
        [-0.0397,  0.0102, -0.0013,  ..., -0.0136,  0.0186, -0.0575]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9155, 0.9080, 0.8532, 0.8474, 0.8304, 0.8261, 0.8259, 0.8224, 0.8217,
         0.8135],
        [0.9843, 0.9815, 0.9810, 0.9797, 0.9793, 0.9792, 0.9782, 0.9767, 0.9754,
         0.9738],
        [0.9622, 0.9563, 0.9514, 0.9463, 0.9418, 0.9392, 0.9358, 0.9284, 0.9269,
         0.9232],
        [0.9839, 0.9680, 0.9669, 0.9574, 0.9554, 0.9492, 0.9485, 0.9405, 0.9374,
         0.9366],
        [0.9457, 0.9382, 0.9266, 0.9236, 0.9234, 0.9220, 0.9152, 0.9151, 0.9094,
         0.9077],
        [0.9684, 0.9633, 0.9619, 0.9602, 0.9584, 0.9564, 0.9562, 0.9550, 0.9527,
         0.9503],
        [0.9908, 0.9881, 0.9869, 0.9867, 0.9822, 0.9764, 0.9733, 0.9726, 0.9725,
         0.9724],
        [0.9794, 0.9700, 0.9662, 0.9602, 0.9485, 0.9457, 0.9422, 0.9403, 0.9379,
         0.9373],
        [0.9796, 0.9791, 0.9761, 0.9761, 0.9757, 0.9750, 0.9748, 0.9738, 0.9734,
         0.9727],
        [0.9789, 0.9569, 0.9484, 0.9399, 0.9392, 0.9285, 0.9257, 0.9252, 0.9239,
         0.9232],
        [0.9765, 0.9755, 0.9695, 0.9684, 0.9683, 0.9672, 0.9663, 0.9659, 0.9647,
         0.9633],
        [0.9668, 0.9662, 0.9612, 0.9594, 0.9495, 0.9453, 0.9375, 0.9364, 0.9162,
         0.9147],
        [0.9813, 0.9712, 0.9550, 0.9516, 0.9357, 0.9291, 0.9272, 0.9260, 0.9256,
         0.9251],
        [0.9861, 0.9831, 0.9831, 0.9828, 0.9815, 0.9814, 0.9795, 0.9794, 0.9786,
         0.9785],
        [0.9770, 0.9742, 0.9671, 0.9669, 0.9665, 0.9665, 0.9661, 0.9651, 0.9637,
         0.9631],
        [0.9670, 0.9669, 0.9614, 0.9558, 0.9539, 0.9529, 0.9495, 0.9478, 0.9433,
         0.9423],
        [0.9915, 0.9886, 0.9829, 0.9819, 0.9817, 0.9801, 0.9800, 0.9782, 0.9778,
         0.9768],
        [0.9872, 0.9852, 0.9778, 0.9777, 0.9771, 0.9770, 0.9758, 0.9754, 0.9748,
         0.9733],
        [0.9785, 0.9782, 0.9650, 0.9589, 0.9577, 0.9555, 0.9508, 0.9485, 0.9445,
         0.9434],
        [0.9764, 0.9729, 0.9710, 0.9698, 0.9646, 0.9638, 0.9624, 0.9610, 0.9609,
         0.9596],
        [0.9917, 0.9861, 0.9851, 0.9818, 0.9776, 0.9758, 0.9752, 0.9733, 0.9715,
         0.9707],
        [0.9807, 0.9699, 0.9693, 0.9570, 0.9556, 0.9554, 0.9534, 0.9533, 0.9502,
         0.9460],
        [0.9868, 0.9721, 0.9694, 0.9677, 0.9661, 0.9648, 0.9648, 0.9642, 0.9591,
         0.9574],
        [0.9737, 0.9713, 0.9707, 0.9688, 0.9666, 0.9664, 0.9653, 0.9623, 0.9603,
         0.9600],
        [0.9844, 0.9783, 0.9604, 0.9590, 0.9579, 0.9561, 0.9529, 0.9521, 0.9501,
         0.9497],
        [0.9608, 0.9595, 0.9534, 0.9512, 0.9505, 0.9497, 0.9485, 0.9480, 0.9462,
         0.9460],
        [0.9724, 0.9674, 0.9663, 0.9558, 0.9532, 0.9531, 0.9505, 0.9481, 0.9454,
         0.9453],
        [0.9686, 0.9650, 0.9588, 0.9578, 0.9544, 0.9542, 0.9516, 0.9512, 0.9500,
         0.9466],
        [0.9812, 0.9763, 0.9721, 0.9667, 0.9663, 0.9615, 0.9608, 0.9604, 0.9603,
         0.9603],
        [0.9721, 0.9708, 0.9664, 0.9648, 0.9632, 0.9627, 0.9612, 0.9599, 0.9598,
         0.9587],
        [0.9811, 0.9787, 0.9771, 0.9770, 0.9759, 0.9732, 0.9728, 0.9728, 0.9725,
         0.9714],
        [0.9669, 0.9646, 0.9590, 0.9576, 0.9541, 0.9540, 0.9511, 0.9502, 0.9495,
         0.9489],
        [0.9190, 0.9153, 0.9080, 0.9079, 0.9059, 0.9055, 0.9033, 0.9001, 0.8981,
         0.8963],
        [0.9646, 0.9633, 0.9568, 0.9567, 0.9488, 0.9445, 0.9438, 0.9422, 0.9411,
         0.9404],
        [0.9731, 0.9638, 0.9600, 0.9599, 0.9566, 0.9513, 0.9499, 0.9492, 0.9483,
         0.9477],
        [0.9719, 0.9704, 0.9695, 0.9609, 0.9577, 0.9559, 0.9544, 0.9533, 0.9512,
         0.9511],
        [0.9609, 0.9545, 0.9532, 0.9497, 0.9425, 0.9424, 0.9325, 0.9295, 0.9267,
         0.9236],
        [0.9750, 0.9640, 0.9574, 0.9504, 0.9499, 0.9494, 0.9449, 0.9444, 0.9423,
         0.9419],
        [0.9590, 0.9578, 0.9540, 0.9439, 0.9382, 0.9372, 0.9344, 0.9337, 0.9327,
         0.9310],
        [0.9912, 0.9875, 0.9859, 0.9848, 0.9847, 0.9846, 0.9831, 0.9819, 0.9806,
         0.9795],
        [0.9566, 0.9308, 0.9294, 0.9228, 0.9183, 0.9150, 0.9002, 0.8962, 0.8945,
         0.8943],
        [0.9436, 0.9378, 0.9371, 0.9324, 0.9319, 0.9288, 0.9261, 0.9250, 0.9249,
         0.9235],
        [0.9352, 0.9343, 0.9327, 0.9309, 0.9201, 0.9193, 0.9135, 0.9124, 0.9120,
         0.9118],
        [0.9454, 0.9291, 0.9145, 0.9127, 0.9105, 0.9073, 0.9035, 0.9007, 0.8970,
         0.8910],
        [0.9380, 0.9241, 0.9195, 0.9101, 0.9089, 0.9075, 0.9058, 0.9014, 0.8998,
         0.8907],
        [0.8884, 0.8187, 0.8146, 0.8042, 0.8004, 0.7952, 0.7944, 0.7926, 0.7859,
         0.7843],
        [0.9722, 0.9679, 0.9568, 0.9540, 0.9528, 0.9498, 0.9472, 0.9461, 0.9457,
         0.9436],
        [0.9088, 0.9023, 0.8993, 0.8876, 0.8796, 0.8790, 0.8682, 0.8556, 0.8549,
         0.8490],
        [0.9388, 0.9349, 0.9335, 0.9235, 0.9173, 0.9099, 0.9087, 0.9064, 0.9019,
         0.9016],
        [0.9695, 0.9648, 0.9634, 0.9610, 0.9559, 0.9553, 0.9551, 0.9537, 0.9520,
         0.9513],
        [0.9391, 0.9351, 0.9310, 0.9225, 0.9169, 0.9166, 0.9125, 0.9103, 0.9053,
         0.9035],
        [0.9430, 0.9355, 0.9284, 0.9265, 0.9247, 0.9246, 0.9230, 0.9201, 0.9194,
         0.9127],
        [0.9683, 0.9423, 0.9395, 0.9338, 0.9316, 0.9263, 0.9179, 0.9041, 0.8999,
         0.8972],
        [0.9494, 0.9353, 0.9295, 0.9113, 0.9039, 0.9021, 0.8959, 0.8898, 0.8895,
         0.8775],
        [0.9640, 0.9606, 0.9604, 0.9598, 0.9593, 0.9578, 0.9547, 0.9545, 0.9540,
         0.9522],
        [0.9675, 0.9604, 0.9589, 0.9583, 0.9583, 0.9521, 0.9512, 0.9509, 0.9499,
         0.9492],
        [0.9616, 0.9488, 0.9444, 0.9420, 0.9273, 0.9242, 0.9235, 0.9204, 0.9152,
         0.9069],
        [0.9558, 0.9128, 0.9110, 0.9054, 0.9052, 0.8972, 0.8927, 0.8927, 0.8899,
         0.8898],
        [0.9537, 0.9443, 0.9308, 0.9265, 0.9190, 0.9144, 0.9115, 0.8935, 0.8919,
         0.8918],
        [0.9581, 0.8941, 0.8932, 0.8855, 0.8790, 0.8721, 0.8663, 0.8659, 0.8626,
         0.8617],
        [0.9403, 0.9351, 0.9328, 0.9318, 0.9317, 0.9303, 0.9280, 0.9238, 0.9217,
         0.9217],
        [0.8937, 0.8870, 0.8755, 0.8712, 0.8633, 0.8632, 0.8589, 0.8542, 0.8519,
         0.8519],
        [0.9550, 0.9427, 0.9314, 0.9250, 0.9241, 0.9234, 0.9208, 0.9197, 0.9171,
         0.9080],
        [0.9541, 0.9418, 0.9389, 0.9224, 0.9205, 0.9187, 0.9176, 0.9162, 0.9133,
         0.9132]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 1, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [0, 1, 0, 1, 1, 1, 1, 1, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 1, 1, 0, 1, 1, 1, 1],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 1, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 478371.1562,  429714.6875,  196445.9844,  180803.4062,  141858.8750,
          133510.0938,  133029.2812,  126628.0234,  125259.1562,  111511.3359],
        [1278478.2500, 1228431.6250, 1220384.0000, 1198248.2500, 1190994.5000,
         1188211.6250, 1171900.3750, 1147967.5000, 1125834.7500, 1101432.1250],
        [ 933105.0625,  856632.8125,  798681.3750,  742870.5625,  697077.5000,
          671436.8750,  639434.5625,  575360.1875,  563624.0000,  534326.6875],
        [1272184.1250, 1013503.0000,  996888.4375,  870641.6875,  846594.6875,
          774068.6250,  766500.1250,  683523.0625,  653922.5625,  646923.2500],
        [ 736255.6250,  661796.7500,  560868.4375,  537207.9375,  536051.8125,
          525015.7500,  476560.6875,  475697.9688,  438690.5312,  428399.9688],
        [1018646.4375,  947911.3750,  928589.1875,  906824.1250,  883517.6250,
          858360.0000,  856467.0000,  841305.1250,  814548.0625,  786734.0000],
        [1403841.7500, 1350592.0000, 1326930.7500, 1323652.1250, 1240480.8750,
         1141512.3750, 1093400.7500, 1082603.5000, 1080282.0000, 1078649.2500],
        [1192524.2500, 1041981.8750,  987235.4375,  905894.9375,  766636.0625,
          737104.3125,  700360.5625,  682146.4375,  659264.4375,  653453.7500],
        [1195331.0000, 1186645.5000, 1137418.8750, 1137000.2500, 1130244.2500,
         1119033.2500, 1115710.6250, 1100035.8750, 1093982.7500, 1083312.0000],
        [1184331.1250,  863979.9375,  766033.8125,  677707.3750,  671468.2500,
          576588.4375,  553449.4375,  549651.6875,  539892.4375,  534521.8750],
        [1143240.1250, 1127484.0000, 1035601.9375, 1019005.9375, 1016777.1250,
         1001096.5625,  988557.2500,  983373.3750,  966050.6250,  947136.0625],
        [ 995434.8750,  987529.1875,  918965.4375,  896247.3750,  777567.5000,
          732593.9375,  655137.2500,  644832.0625,  483355.4688,  473425.0938],
        [1225397.6250, 1059897.7500,  841068.5625,  801268.3750,  638356.0625,
          581392.2500,  565298.6875,  556119.6875,  552716.7500,  548748.1875],
        [1312064.5000, 1257709.3750, 1256826.8750, 1251430.3750, 1227809.7500,
         1226863.8750, 1194544.6250, 1192432.1250, 1178198.3750, 1176575.7500],
        [1151999.1250, 1107023.8750, 1000011.6250,  996914.0625,  992230.3750,
          991791.4375,  985668.1250,  972024.2500,  952570.2500,  945084.2500],
        [ 998125.1250,  997893.8125,  921476.1875,  851363.4375,  827836.3125,
          816789.9375,  777543.7500,  758777.6250,  712126.1250,  702113.3750],
        [1418168.7500, 1358918.8750, 1252766.6250, 1235029.7500, 1232182.7500,
         1203480.8750, 1202766.0000, 1172013.3750, 1165436.1250, 1148709.0000],
        [1333449.8750, 1296149.2500, 1164805.1250, 1164546.2500, 1153549.1250,
         1151597.0000, 1132630.1250, 1126558.6250, 1116258.7500, 1093610.3750],
        [1177115.6250, 1171798.7500,  970624.6250,  890098.2500,  874848.9375,
          846920.0625,  792683.6250,  767069.0000,  724700.7500,  712843.0000],
        [1141536.2500, 1086583.6250, 1057952.7500, 1039619.5000,  964834.3125,
          954200.5000,  935450.1250,  917356.9375,  916001.8125,  898365.3125],
        [1422218.2500, 1312896.8750, 1292816.0000, 1234802.5000, 1161409.7500,
         1131948.7500, 1123174.1250, 1093012.8750, 1065243.1250, 1053384.1250],
        [1214670.1250, 1041283.5000, 1032222.8750,  865268.7500,  848100.9375,
          845878.0000,  822021.6875,  821315.6875,  785779.3750,  739496.8750],
        [1325638.1250, 1074968.7500, 1033351.6250, 1009452.6250,  986199.3750,
          968167.3125,  968113.7500,  959268.8750,  892169.3125,  870609.2500],
        [1099079.5000, 1062333.5000, 1053449.3750, 1024510.4375,  993285.0625,
          990634.4375,  975471.3750,  934050.5625,  908196.8125,  904341.1875],
        [1280993.6250, 1173086.8750,  908628.2500,  890737.6875,  876814.3125,
          854468.1875,  816356.2500,  807165.5000,  784986.2500,  779793.8125],
        [ 914114.2500,  896867.1875,  822759.7500,  796997.5000,  789348.5625,
          779813.9375,  766973.9375,  761231.7500,  741770.5000,  739992.1250],
        [1078271.7500, 1004423.5625,  989210.7500,  851680.2500,  819573.9375,
          818870.0000,  788523.1875,  762381.1250,  733419.5000,  732828.6875],
        [1021170.5000,  970474.6250,  888483.5000,  875597.6875,  834645.0000,
          831795.5625,  801885.2500,  797151.0625,  783461.3125,  746231.3125],
        [1223433.5000, 1140139.2500, 1074277.0000,  994766.8125,  988142.5625,
          923518.1250,  914357.5625,  909044.2500,  908165.5625,  907882.4375],
        [1074630.5000, 1054738.1250,  989752.4375,  967453.8750,  945737.0625,
          939531.5000,  918777.0000,  902615.3125,  901354.2500,  887089.8750],
        [1222054.0000, 1180812.3750, 1153360.0000, 1152643.0000, 1134044.8750,
         1090960.2500, 1085160.7500, 1085126.6250, 1080675.6250, 1063020.6250],
        [ 996875.1250,  965573.5000,  891179.5625,  872750.6875,  830214.5000,
          829956.3750,  796145.1875,  785851.3125,  777943.5625,  771507.6250],
        [ 503073.4375,  477333.9688,  429772.0312,  429479.0938,  417374.0625,
          414705.4062,  402185.9375,  384004.6250,  373464.3750,  363971.7188],
        [ 964910.6875,  947452.3125,  863881.8750,  862189.6875,  770330.5625,
          724063.8125,  716802.3125,  700495.5000,  689940.5625,  683397.2500],
        [1090136.5000,  953601.8125,  903347.3750,  902801.3125,  860697.8125,
          798610.5000,  782791.3750,  774936.5000,  764341.5625,  758129.5000],
        [1071795.3750, 1047946.2500, 1034737.1250,  915748.5625,  874430.1875,
          852296.0625,  833890.8125,  821112.8125,  797481.0625,  795899.1875],
        [ 915100.8125,  835052.6250,  819567.6875,  780173.9375,  703684.6875,
          702684.1250,  610466.4375,  584811.6250,  561846.5625,  537140.2500],
        [1119225.2500,  956386.0625,  870673.1875,  788097.6250,  781919.8750,
          777027.1250,  728479.2500,  723152.2500,  701609.3750,  697690.0000],
        [ 890383.5000,  876262.6250,  829526.7500,  717538.9375,  662024.6250,
          652240.3125,  626886.8125,  620502.2500,  611698.4375,  597530.1250],
        [1411838.5000, 1338759.6250, 1309213.5000, 1288173.7500, 1286701.6250,
         1284339.2500, 1257073.8750, 1236079.6250, 1212200.6250, 1194737.2500],
        [ 860642.0000,  595906.5625,  583531.9375,  531324.0625,  497985.4375,
          474955.4375,  384357.8125,  363076.6250,  354481.6562,  353467.2500],
        [ 715471.9375,  658431.9375,  652007.0625,  609446.1250,  604825.1250,
          579041.6875,  556609.9375,  548219.8750,  547715.6250,  536290.1250],
        [ 634200.7500,  625830.6875,  611799.3125,  596616.2500,  511335.9375,
          505595.1562,  464821.1250,  457909.4688,  455162.1562,  453846.1875],
        [ 733507.6875,  580879.6250,  471906.9062,  459762.1250,  445364.3750,
          425488.3438,  402946.0938,  387298.1250,  367366.9688,  337248.3125],
        [ 659993.5000,  541304.6250,  506828.0938,  443241.9688,  435423.1875,
          426630.1250,  416864.9062,  391514.1875,  382427.2812,  335880.0312],
        [ 325140.2188,  120014.2656,  113248.1562,   97570.6328,   92390.8828,
           85810.2500,   84852.5234,   82719.9766,   75110.6719,   73492.9375],
        [1076481.8750, 1012077.4375,  862969.5000,  829087.0000,  815226.5625,
          780741.1250,  752687.5000,  740690.3125,  736290.0625,  715329.3125],
        [ 434633.6562,  396234.6562,  379668.3750,  321133.1875,  286618.0000,
          284062.0000,  243502.0469,  203423.5156,  201323.7188,  185099.6719],
        [ 667512.9375,  631515.0000,  618528.3125,  536188.8125,  490721.9062,
          442078.0938,  434315.0312,  419980.5625,  394269.6875,  392370.1875],
        [1035383.6875,  967202.0000,  949090.0000,  916645.9375,  852386.3125,
          845093.4375,  843092.2500,  826531.5000,  806223.8750,  797628.6250],
        [ 670583.8750,  632907.7500,  597295.9375,  528793.0000,  488094.6875,
          485902.0625,  458640.1875,  444114.0312,  413491.0625,  403361.3125],
        [ 709276.8125,  637249.6875,  575502.3125,  560184.8125,  545757.2500,
          545086.7500,  532546.1250,  510936.6875,  505688.6875,  459934.4688],
        [1016799.4375,  701495.5625,  674615.3125,  621884.3750,  602033.4375,
          558549.7500,  495293.3438,  406541.9688,  383080.3125,  368640.5625],
        [ 776741.8125,  635083.1250,  584774.8125,  450779.6562,  405553.7500,
          395071.7500,  361629.7188,  331631.1562,  330196.5000,  278225.5000],
        [ 956751.8750,  911467.9375,  909213.3125,  901559.7500,  894623.9375,
          876000.2500,  837412.5000,  835874.9375,  829260.9375,  808431.2500],
        [1005913.1875,  908508.6250,  889892.8750,  882655.3125,  881663.3750,
          806886.8750,  796575.8125,  794109.8750,  782473.3750,  774414.1875],
        [ 924795.1875,  770225.5000,  722685.4375,  698405.6875,  566372.5625,
          541720.3125,  536572.0000,  513361.2188,  476686.5938,  423453.3438],
        [ 851037.1875,  460480.4688,  449017.5312,  414420.3750,  413325.8750,
          368498.1875,  345609.5938,  345335.8125,  332073.9062,  331726.6875],
        [ 825976.0000,  722247.9375,  595331.1250,  560404.3750,  503313.3750,
          471090.7812,  452067.7812,  349534.1875,  341377.0000,  340972.8750],
        [ 879272.8750,  352493.6875,  348065.8750,  311978.3750,  284279.8750,
          257541.6406,  236854.4844,  235603.0312,  224746.6719,  221813.0938],
        [ 681788.6875,  633117.8125,  612748.1875,  604074.5000,  603287.5625,
          591101.1875,  572041.9375,  538506.6875,  522783.9375,  522550.1562],
        [ 350496.1875,  318509.5312,  270118.4062,  254294.6719,  226940.9531,
          226851.3750,  213125.6406,  199264.5938,  192820.8125,  192809.4062],
        [ 841280.3125,  706286.5625,  600567.6875,  548489.7500,  541153.3750,
          535516.8125,  516319.4062,  508278.3125,  489322.7812,  430116.8750],
        [ 831033.5625,  697258.9375,  668428.3125,  528360.5000,  514077.0312,
          500928.6562,  493196.7812,  483225.9688,  463948.2188,  463345.1250]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 478371.1562,       0.0000],
         [ 429714.6875,       0.0000],
         [ 196445.9844,       0.0000],
         ...,
         [ 126628.0234,       0.0000],
         [ 125259.1562,       0.0000],
         [ 111511.3359,       0.0000]],

        [[1278478.2500,       0.0000],
         [1228431.6250,       0.0000],
         [1220384.0000,       0.0000],
         ...,
         [1147967.5000,       0.0000],
         [1125834.7500,       0.0000],
         [1101432.1250,       0.0000]],

        [[ 933105.0625,       0.0000],
         [ 856632.8125,       0.0000],
         [ 798681.3750,       0.0000],
         ...,
         [ 575360.1875,       0.0000],
         [ 563624.0000,       0.0000],
         [ 534326.6875,       0.0000]],

        ...,

        [[ 350496.1875,       0.0000],
         [ 318509.5312,       0.0000],
         [ 270118.4062,       0.0000],
         ...,
         [ 199264.5938,       0.0000],
         [ 192820.8125,       0.0000],
         [      0.0000,  192809.4062]],

        [[      0.0000,  841280.3125],
         [ 706286.5625,       0.0000],
         [ 600567.6875,       0.0000],
         ...,
         [      0.0000,  508278.3125],
         [ 489322.7812,       0.0000],
         [ 430116.8750,       0.0000]],

        [[ 831033.5625,       0.0000],
         [      0.0000,  697258.9375],
         [ 668428.3125,       0.0000],
         ...,
         [ 483225.9688,       0.0000],
         [ 463948.2188,       0.0000],
         [ 463345.1250,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 1924102.7500,   133029.2812],
        [11851884.0000,        0.0000],
        [ 7012550.0000,        0.0000],
        [ 8524749.0000,        0.0000],
        [ 3701843.0000,  1674702.6250],
        [ 8028355.0000,   814548.0625],
        [ 9704892.0000,  2417053.0000],
        [ 8326602.5000,        0.0000],
        [11298714.0000,        0.0000],
        [ 6917624.0000,        0.0000],
        [10228323.0000,        0.0000],
        [ 7565088.5000,        0.0000],
        [ 7370264.0000,        0.0000],
        [12274456.0000,        0.0000],
        [10095317.0000,        0.0000],
        [ 8364045.5000,        0.0000],
        [12389471.0000,        0.0000],
        [11733154.0000,        0.0000],
        [ 8161633.0000,   767069.0000],
        [ 9911902.0000,        0.0000],
        [11890906.0000,        0.0000],
        [ 9016038.0000,        0.0000],
        [10087939.0000,        0.0000],
        [ 9945352.0000,        0.0000],
        [ 9173031.0000,        0.0000],
        [ 8009869.0000,        0.0000],
        [ 7727502.5000,   851680.2500],
        [ 8550896.0000,        0.0000],
        [ 9983726.0000,        0.0000],
        [ 9581680.0000,        0.0000],
        [11247858.0000,        0.0000],
        [ 8517998.0000,        0.0000],
        [ 3780659.2500,   414705.4062],
        [ 5473329.0000,  2450135.5000],
        [ 5952125.0000,  2637269.5000],
        [ 6938805.5000,  2106532.5000],
        [ 5431743.0000,  1618785.5000],
        [ 6068649.0000,  2075611.2500],
        [ 3383181.0000,  3701413.2500],
        [12819118.0000,        0.0000],
        [ 4646261.5000,   353467.2500],
        [ 6008059.5000,        0.0000],
        [ 5317117.0000,        0.0000],
        [ 4611768.5000,        0.0000],
        [ 2887501.2500,  1652606.6250],
        [  590796.4375,   559554.0625],
        [ 8321580.0000,        0.0000],
        [ 2145404.0000,   790294.8750],
        [ 1680311.0000,  3347169.5000],
        [ 7116407.5000,  1722869.7500],
        [ 4223790.5000,   899393.1250],
        [ 4543929.0000,  1038234.8125],
        [ 2951460.0000,  2877474.0000],
        [ 3514133.5000,  1035554.5000],
        [ 8760597.0000,        0.0000],
        [ 6818009.0000,  1705084.5000],
        [ 3243210.5000,  2931067.5000],
        [ 2773918.7500,  1537606.8750],
        [ 2677383.2500,  2484932.0000],
        [ 1643022.5000,  1709627.0000],
        [ 5269253.0000,   612748.1875],
        [ 1812445.1250,   632786.4375],
        [ 4367773.0000,  1349558.6250],
        [ 4946544.0000,   697258.9375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 54.6875
Top1 accuracy for validation set is 54.6875 size is torch.Size([64, 1])
Epoch 141/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:03, 60.12s/it]  7%|▋         | 2/30 [01:01<11:54, 25.53s/it] 10%|█         | 3/30 [01:04<06:48, 15.15s/it] 13%|█▎        | 4/30 [01:04<04:06,  9.46s/it] 17%|█▋        | 5/30 [01:05<02:37,  6.32s/it] 20%|██        | 6/30 [01:06<01:46,  4.43s/it] 23%|██▎       | 7/30 [01:07<01:14,  3.22s/it] 27%|██▋       | 8/30 [01:07<00:53,  2.44s/it] 30%|███       | 9/30 [01:08<00:40,  1.91s/it] 33%|███▎      | 10/30 [01:09<00:31,  1.55s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.30s/it] 40%|████      | 12/30 [01:10<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 3.5735231161117555
Epoch 142/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:29, 61.01s/it]  7%|▋         | 2/30 [01:01<11:55, 25.56s/it] 10%|█         | 3/30 [01:02<06:24, 14.23s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.91s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.97s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 3.5299689610799154
Epoch 143/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:33, 59.08s/it]  7%|▋         | 2/30 [00:59<11:33, 24.77s/it] 10%|█         | 3/30 [01:00<06:12, 13.80s/it] 13%|█▎        | 4/30 [01:01<03:44,  8.65s/it] 17%|█▋        | 5/30 [01:02<02:24,  5.80s/it] 20%|██        | 6/30 [01:02<01:37,  4.08s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 3.5381671746571857
Epoch 144/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:49, 59.65s/it]  7%|▋         | 2/30 [01:00<11:40, 25.01s/it] 10%|█         | 3/30 [01:01<06:16, 13.93s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.73s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.85s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 3.5373870452245075
Epoch 145/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:28, 60.97s/it]  7%|▋         | 2/30 [01:03<12:22, 26.53s/it] 10%|█         | 3/30 [01:04<06:38, 14.76s/it] 13%|█▎        | 4/30 [01:04<03:59,  9.23s/it] 17%|█▋        | 5/30 [01:05<02:34,  6.17s/it] 20%|██        | 6/30 [01:06<01:43,  4.33s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.39s/it] 30%|███       | 9/30 [01:08<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 3.484442186355591
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0059,  0.0100, -0.0409,  ..., -0.0129, -0.0256,  0.0340],
        [-0.0323,  0.0206, -0.0095,  ...,  0.0111, -0.0330, -0.0044],
        [-0.0584, -0.0129, -0.0012,  ...,  0.0539, -0.0191, -0.0493],
        ...,
        [ 0.0279, -0.0286, -0.0440,  ..., -0.0351, -0.0298, -0.0119],
        [-0.0410,  0.0072, -0.0107,  ..., -0.0330,  0.0229, -0.0209],
        [-0.0390,  0.0088, -0.0005,  ..., -0.0114,  0.0166, -0.0570]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9103, 0.9071, 0.8518, 0.8451, 0.8303, 0.8299, 0.8265, 0.8233, 0.8165,
         0.8140],
        [0.9837, 0.9794, 0.9792, 0.9788, 0.9784, 0.9782, 0.9773, 0.9768, 0.9748,
         0.9743],
        [0.9585, 0.9557, 0.9439, 0.9412, 0.9382, 0.9347, 0.9315, 0.9248, 0.9209,
         0.9155],
        [0.9830, 0.9675, 0.9653, 0.9568, 0.9529, 0.9449, 0.9435, 0.9391, 0.9343,
         0.9341],
        [0.9383, 0.9303, 0.9188, 0.9151, 0.9150, 0.9101, 0.9100, 0.9096, 0.9027,
         0.9015],
        [0.9658, 0.9590, 0.9580, 0.9550, 0.9536, 0.9532, 0.9508, 0.9504, 0.9503,
         0.9464],
        [0.9899, 0.9866, 0.9847, 0.9845, 0.9797, 0.9723, 0.9718, 0.9703, 0.9702,
         0.9685],
        [0.9711, 0.9692, 0.9637, 0.9564, 0.9453, 0.9392, 0.9383, 0.9361, 0.9351,
         0.9307],
        [0.9762, 0.9761, 0.9757, 0.9744, 0.9740, 0.9737, 0.9723, 0.9720, 0.9716,
         0.9706],
        [0.9780, 0.9516, 0.9444, 0.9390, 0.9382, 0.9242, 0.9219, 0.9216, 0.9211,
         0.9202],
        [0.9755, 0.9738, 0.9671, 0.9667, 0.9662, 0.9657, 0.9656, 0.9654, 0.9634,
         0.9621],
        [0.9615, 0.9608, 0.9574, 0.9540, 0.9476, 0.9428, 0.9323, 0.9311, 0.9082,
         0.9063],
        [0.9820, 0.9702, 0.9544, 0.9520, 0.9348, 0.9284, 0.9269, 0.9255, 0.9235,
         0.9225],
        [0.9865, 0.9839, 0.9833, 0.9831, 0.9822, 0.9814, 0.9792, 0.9790, 0.9788,
         0.9785],
        [0.9756, 0.9725, 0.9653, 0.9648, 0.9646, 0.9644, 0.9640, 0.9636, 0.9629,
         0.9607],
        [0.9666, 0.9633, 0.9596, 0.9562, 0.9541, 0.9517, 0.9490, 0.9477, 0.9414,
         0.9412],
        [0.9910, 0.9876, 0.9813, 0.9812, 0.9808, 0.9797, 0.9795, 0.9784, 0.9761,
         0.9752],
        [0.9865, 0.9834, 0.9771, 0.9769, 0.9751, 0.9751, 0.9746, 0.9730, 0.9728,
         0.9720],
        [0.9772, 0.9761, 0.9649, 0.9588, 0.9564, 0.9552, 0.9485, 0.9464, 0.9412,
         0.9411],
        [0.9752, 0.9714, 0.9703, 0.9690, 0.9657, 0.9623, 0.9616, 0.9582, 0.9579,
         0.9578],
        [0.9913, 0.9847, 0.9847, 0.9820, 0.9765, 0.9747, 0.9735, 0.9728, 0.9705,
         0.9699],
        [0.9807, 0.9673, 0.9670, 0.9569, 0.9546, 0.9529, 0.9527, 0.9526, 0.9501,
         0.9451],
        [0.9866, 0.9708, 0.9699, 0.9680, 0.9660, 0.9652, 0.9650, 0.9633, 0.9583,
         0.9579],
        [0.9721, 0.9680, 0.9680, 0.9658, 0.9645, 0.9644, 0.9637, 0.9603, 0.9570,
         0.9567],
        [0.9827, 0.9783, 0.9587, 0.9553, 0.9545, 0.9520, 0.9510, 0.9498, 0.9465,
         0.9465],
        [0.9598, 0.9553, 0.9515, 0.9508, 0.9497, 0.9485, 0.9472, 0.9470, 0.9455,
         0.9440],
        [0.9722, 0.9682, 0.9659, 0.9545, 0.9524, 0.9512, 0.9511, 0.9455, 0.9446,
         0.9430],
        [0.9657, 0.9611, 0.9584, 0.9556, 0.9534, 0.9518, 0.9513, 0.9502, 0.9482,
         0.9453],
        [0.9799, 0.9743, 0.9698, 0.9646, 0.9643, 0.9612, 0.9604, 0.9594, 0.9593,
         0.9592],
        [0.9709, 0.9702, 0.9675, 0.9654, 0.9646, 0.9627, 0.9606, 0.9597, 0.9578,
         0.9577],
        [0.9803, 0.9776, 0.9750, 0.9750, 0.9740, 0.9731, 0.9722, 0.9716, 0.9694,
         0.9693],
        [0.9656, 0.9639, 0.9563, 0.9539, 0.9522, 0.9513, 0.9492, 0.9476, 0.9469,
         0.9469],
        [0.9126, 0.9106, 0.9096, 0.9083, 0.9051, 0.9011, 0.8998, 0.8972, 0.8966,
         0.8945],
        [0.9636, 0.9595, 0.9551, 0.9546, 0.9443, 0.9424, 0.9392, 0.9383, 0.9372,
         0.9372],
        [0.9715, 0.9621, 0.9597, 0.9585, 0.9539, 0.9492, 0.9491, 0.9478, 0.9472,
         0.9461],
        [0.9700, 0.9676, 0.9674, 0.9576, 0.9551, 0.9531, 0.9517, 0.9515, 0.9480,
         0.9459],
        [0.9585, 0.9530, 0.9494, 0.9433, 0.9402, 0.9364, 0.9293, 0.9276, 0.9209,
         0.9177],
        [0.9761, 0.9660, 0.9554, 0.9475, 0.9438, 0.9434, 0.9420, 0.9417, 0.9411,
         0.9409],
        [0.9586, 0.9572, 0.9513, 0.9399, 0.9344, 0.9327, 0.9326, 0.9309, 0.9302,
         0.9302],
        [0.9905, 0.9868, 0.9858, 0.9845, 0.9840, 0.9834, 0.9831, 0.9803, 0.9799,
         0.9785],
        [0.9563, 0.9309, 0.9263, 0.9176, 0.9153, 0.9124, 0.8967, 0.8936, 0.8930,
         0.8910],
        [0.9441, 0.9369, 0.9351, 0.9297, 0.9295, 0.9282, 0.9254, 0.9239, 0.9230,
         0.9212],
        [0.9307, 0.9304, 0.9296, 0.9257, 0.9157, 0.9154, 0.9083, 0.9071, 0.9060,
         0.9053],
        [0.9408, 0.9290, 0.9158, 0.9095, 0.9085, 0.9072, 0.9023, 0.9015, 0.8972,
         0.8889],
        [0.9350, 0.9234, 0.9189, 0.9112, 0.9085, 0.9078, 0.9030, 0.8999, 0.8985,
         0.8897],
        [0.8828, 0.8137, 0.8137, 0.8019, 0.7964, 0.7958, 0.7922, 0.7903, 0.7882,
         0.7809],
        [0.9704, 0.9675, 0.9545, 0.9508, 0.9503, 0.9493, 0.9454, 0.9453, 0.9448,
         0.9426],
        [0.9054, 0.8974, 0.8887, 0.8843, 0.8763, 0.8687, 0.8668, 0.8557, 0.8545,
         0.8444],
        [0.9372, 0.9329, 0.9323, 0.9244, 0.9159, 0.9073, 0.9060, 0.9048, 0.9046,
         0.8991],
        [0.9664, 0.9612, 0.9596, 0.9588, 0.9521, 0.9515, 0.9508, 0.9507, 0.9485,
         0.9477],
        [0.9336, 0.9280, 0.9226, 0.9168, 0.9154, 0.9121, 0.9074, 0.9067, 0.8987,
         0.8970],
        [0.9395, 0.9313, 0.9239, 0.9232, 0.9191, 0.9179, 0.9177, 0.9167, 0.9161,
         0.9129],
        [0.9685, 0.9386, 0.9379, 0.9333, 0.9305, 0.9265, 0.9148, 0.9010, 0.8988,
         0.8973],
        [0.9450, 0.9307, 0.9271, 0.9106, 0.9033, 0.8995, 0.8938, 0.8882, 0.8878,
         0.8807],
        [0.9658, 0.9585, 0.9576, 0.9571, 0.9571, 0.9567, 0.9553, 0.9525, 0.9524,
         0.9519],
        [0.9660, 0.9574, 0.9557, 0.9548, 0.9542, 0.9486, 0.9469, 0.9468, 0.9459,
         0.9458],
        [0.9614, 0.9482, 0.9426, 0.9398, 0.9254, 0.9234, 0.9222, 0.9137, 0.9074,
         0.9035],
        [0.9560, 0.9149, 0.9077, 0.9070, 0.9017, 0.8957, 0.8937, 0.8904, 0.8862,
         0.8855],
        [0.9544, 0.9396, 0.9293, 0.9240, 0.9179, 0.9123, 0.9114, 0.8902, 0.8894,
         0.8889],
        [0.9584, 0.8914, 0.8908, 0.8854, 0.8796, 0.8690, 0.8663, 0.8625, 0.8625,
         0.8576],
        [0.9398, 0.9332, 0.9302, 0.9298, 0.9294, 0.9294, 0.9258, 0.9214, 0.9208,
         0.9194],
        [0.8873, 0.8807, 0.8669, 0.8643, 0.8586, 0.8539, 0.8537, 0.8509, 0.8484,
         0.8443],
        [0.9514, 0.9388, 0.9266, 0.9225, 0.9222, 0.9168, 0.9163, 0.9162, 0.9148,
         0.9071],
        [0.9501, 0.9368, 0.9350, 0.9204, 0.9182, 0.9153, 0.9111, 0.9106, 0.9082,
         0.9080]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 0, 1, 0, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 1, 1, 0],
        [1, 0, 0, 1, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 1, 1, 1, 1],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 444136.9062,  424556.8750,  192545.0000,  175087.9688,  141788.1250,
          140901.7031,  134284.9375,  128301.7656,  116306.2812,  112253.3203],
        [1267220.7500, 1191516.0000, 1189597.0000, 1181859.1250, 1175286.0000,
         1172020.0000, 1156449.3750, 1148750.6250, 1115855.3750, 1107960.7500],
        [ 884777.3750,  849731.5000,  718364.6875,  690600.1875,  662224.8125,
          629878.3750,  601574.8750,  546827.8750,  517011.7188,  478437.3125],
        [1254725.1250, 1006025.5000,  974741.3125,  863539.2500,  816958.2500,
          728224.3750,  713850.5000,  670697.6875,  625594.3750,  624590.5000],
        [ 662521.0625,  591546.1250,  501506.5312,  475590.4375,  475215.0312,
          442989.6875,  442172.9688,  439580.4688,  398545.7188,  392024.5625],
        [ 981471.5000,  891386.0625,  878755.6250,  841126.3125,  824266.0625,
          819762.3750,  792793.2500,  787710.6875,  786243.4375,  744314.4375],
        [1386075.5000, 1322335.0000, 1286917.6250, 1283230.0000, 1197204.2500,
         1077050.7500, 1070428.6250, 1047362.7500, 1046047.1250, 1021014.6875],
        [1058841.0000, 1030934.1250,  952930.0625,  858713.6875,  732211.1250,
          671381.1875,  663263.3125,  642763.0000,  632903.5000,  594966.7500],
        [1138445.5000, 1137902.7500, 1130849.1250, 1110437.0000, 1103924.3750,
         1099836.5000, 1078060.8750, 1072843.6250, 1067115.0000, 1051835.1250],
        [1169422.0000,  801964.0625,  723099.1250,  669481.6250,  661560.1250,
          542238.1875,  524602.3750,  522505.2812,  518624.0625,  511792.5938],
        [1128380.1250, 1101409.0000,  999526.3125,  995131.1875,  987561.1875,
          980419.0625,  978701.1250,  976104.1875,  948707.1875,  930593.5625],
        [ 923359.6250,  914343.5625,  870988.7500,  830005.5000,  756918.7500,
          707246.3750,  607968.1875,  597934.3125,  431134.1250,  419559.8438],
        [1237188.2500, 1045859.5625,  834369.6875,  806134.6875,  630724.1250,
          575082.0625,  563142.6250,  552222.5000,  536233.3125,  528570.6875],
        [1319629.0000, 1270786.1250, 1261260.0000, 1257711.7500, 1241589.8750,
         1227061.7500, 1188329.3750, 1184780.7500, 1181542.3750, 1177604.0000],
        [1128782.6250, 1081072.5000,  974603.7500,  967202.0000,  965742.0000,
          962260.4375,  957302.2500,  950924.6250,  942178.4375,  913223.8125],
        [ 992852.3125,  947645.6250,  898439.0000,  856043.1875,  830262.8125,
          803112.8750,  772346.8125,  757635.8125,  692412.4375,  691066.0000],
        [1407590.1250, 1341309.0000, 1224929.1250, 1223391.6250, 1216873.1250,
         1197492.0000, 1193639.3750, 1175475.5000, 1137840.8750, 1123381.8750],
        [1319320.6250, 1263054.6250, 1154065.2500, 1150623.2500, 1121573.8750,
         1121271.1250, 1112699.2500, 1088045.7500, 1085733.2500, 1072948.0000],
        [1155729.5000, 1137286.5000,  968590.3125,  888608.8750,  858784.9375,
          843732.4375,  767306.1250,  743622.6875,  691253.8750,  689439.3750],
        [1122998.3750, 1064272.2500, 1046822.5625, 1027558.5000,  979905.8125,
          934246.5625,  924534.1250,  880386.3125,  877313.6875,  875178.5625],
        [1412598.0000, 1286455.0000, 1285437.1250, 1237320.3750, 1144298.2500,
         1114869.3750, 1095580.1250, 1084521.3750, 1050407.6250, 1040331.5625],
        [1214425.7500, 1003225.0000,  998690.6250,  864441.4375,  836519.3125,
          816652.0625,  814122.5625,  812604.5000,  784847.0000,  730026.0000],
        [1321805.3750, 1054523.8750, 1040426.8125, 1012747.4375,  985072.3125,
          973854.0000,  970733.8750,  946749.5000,  881452.3750,  877207.4375],
        [1074258.5000, 1013252.7500, 1012557.2500,  981869.3125,  963240.1250,
          962320.1875,  952374.9375,  907649.5625,  866365.3125,  862647.0000],
        [1249301.8750, 1173237.8750,  886597.6875,  845151.5000,  835845.4375,
          806425.3125,  794674.2500,  780869.1875,  745435.3750,  745190.1875],
        [ 901297.6250,  844887.1250,  800907.7500,  792840.1250,  780389.0000,
          766950.5000,  752966.8125,  750493.5000,  734968.3125,  718924.6250],
        [1075306.0000, 1016133.5000,  983104.3125,  834943.5625,  810794.6875,
          796655.5000,  795880.2500,  734246.0000,  724912.9375,  709371.5000],
        [ 980532.1875,  918084.1875,  883360.1250,  848392.1250,  822721.2500,
          804246.4375,  798094.3125,  785633.3125,  763759.4375,  732902.1250],
        [1200194.7500, 1107919.5000, 1040248.2500,  964538.1250,  960369.1250,
          919600.1250,  908487.8750,  895846.5625,  894389.3125,  894041.3750],
        [1055997.2500, 1045006.0625, 1005602.4375,  976313.6875,  964449.8125,
          939426.6875,  911961.8125,  899715.6875,  875277.9375,  874088.3750],
        [1208103.6250, 1162183.1250, 1119286.1250, 1118994.7500, 1103926.5000,
         1089465.1250, 1075826.1250, 1066391.6250, 1033356.5000, 1031551.7500],
        [ 979517.1875,  954881.3750,  856825.6250,  828374.8750,  808634.0000,
          798232.0625,  774429.6875,  757420.5625,  749751.6250,  749586.5000],
        [ 458962.2188,  446226.1562,  439615.6562,  432012.0000,  412499.3125,
          389824.5938,  382342.6875,  368456.3750,  365179.2500,  354459.6875],
        [ 951349.1250,  897063.0625,  842113.5000,  836288.0000,  722435.3125,
          703206.3750,  671884.6875,  662638.0000,  652794.7500,  652438.1250],
        [1065274.6250,  931116.5000,  900227.2500,  884931.8750,  828481.5625,
          774824.9375,  773726.1875,  759129.3750,  752997.6875,  740925.6250],
        [1043061.5625, 1007869.1875, 1004775.1250,  873219.3750,  843006.1875,
          818335.3125,  802575.3750,  800031.4375,  761391.4375,  738522.8750],
        [ 885190.9375,  817918.6250,  776457.4375,  712136.3125,  681411.6875,
          645392.5625,  582896.2500,  568803.0000,  516787.4375,  493746.5000],
        [1137664.0000,  984291.9375,  845750.5000,  756354.4375,  717320.0000,
          713149.6250,  699111.3750,  695632.3750,  690043.1875,  687531.3125],
        [ 886419.2500,  868232.1250,  797542.6250,  677779.6875,  626599.8750,
          611804.0000,  611081.5625,  596081.0625,  590691.5000,  590579.9375],
        [1396935.8750, 1325250.0000, 1306399.6250, 1283127.2500, 1273277.7500,
         1263350.8750, 1256163.0000, 1208163.5000, 1201431.6250, 1176921.3750],
        [ 857761.0000,  595999.8125,  558449.6250,  493281.5000,  477253.3750,
          457655.3438,  365918.3125,  350160.0625,  347100.9688,  337386.0000],
        [ 719710.7500,  649852.3750,  633100.8750,  586595.6875,  584812.1875,
          573502.5625,  550993.6875,  539894.0625,  532611.1250,  518851.6562],
        [ 594813.5625,  592330.1875,  585223.9375,  553615.1875,  479920.1875,
          478103.4375,  431629.0312,  424576.7188,  417772.2812,  413424.0312],
        [ 687383.7500,  580440.5000,  480777.3125,  438983.9062,  432852.0938,
          424959.5625,  396379.8125,  392078.8125,  368262.4688,  327222.2188],
        [ 631869.8125,  535387.1250,  502155.9688,  450375.3438,  433173.7500,
          428471.4375,  400065.5000,  382886.0000,  375183.9375,  331028.9062],
        [ 299843.0000,  111843.8281,  111768.5547,   94493.4219,   87326.0625,
           86512.5625,   82203.6172,   80057.6719,   77615.7188,   69966.5234],
        [1048869.1250, 1005846.0000,  835879.6875,  792207.5000,  786265.9375,
          775628.5625,  733991.1875,  732602.3125,  727056.4375,  704862.7500],
        [ 414128.7812,  369291.1875,  326565.0312,  306393.5312,  273326.4375,
          245214.4219,  238613.8438,  203617.4219,  200230.0000,  173273.2188],
        [ 652079.2500,  613929.1250,  608268.5625,  543444.4375,  481119.9375,
          425662.0625,  417549.6250,  411020.4688,  409758.2500,  378497.4375],
        [ 990063.0000,  919601.8750,  898424.4375,  888347.0625,  807187.8750,
          800289.3750,  792834.0625,  791296.1250,  767027.3125,  758285.0000],
        [ 619444.4375,  571876.1250,  529865.1875,  487765.2500,  477737.4375,
          455721.1562,  425986.1250,  422307.6250,  376727.0938,  367333.3125],
        [ 673999.8750,  599590.2500,  539746.3125,  534142.7500,  503686.0000,
          495468.1250,  494039.4688,  487110.2500,  483016.3438,  461049.5000],
        [1019682.5625,  665747.4375,  659505.9375,  617577.0000,  592559.0625,
          560092.3750,  473568.2500,  389058.7812,  376937.6875,  369079.5938],
        [ 729097.1250,  594549.3125,  564884.8125,  446432.1562,  401781.5000,
          380771.8125,  350898.8438,  323935.3438,  322117.7812,  291280.4375],
        [ 981437.7500,  884737.7500,  873235.2500,  867424.3125,  866582.6250,
          862448.8125,  845179.6875,  812044.3750,  810951.6875,  805049.8750],
        [ 984563.3125,  871074.3750,  849993.2500,  839125.6250,  831709.0625,
          768235.2500,  749005.5625,  747927.7500,  739339.6250,  737308.9375],
        [ 921763.5625,  763252.6250,  704828.4375,  676948.3125,  551555.1875,
          536003.2500,  526889.6875,  466424.5938,  426402.3125,  403229.7812],
        [ 853154.8125,  474736.6875,  427880.1875,  423878.4062,  392968.5938,
          360726.5625,  350436.6875,  334523.0000,  314963.6562,  311776.7188],
        [ 834368.1250,  675566.2500,  582541.6875,  540659.1875,  495570.2188,
          457152.8125,  451267.0312,  333463.9062,  329607.8438,  327504.7500],
        [ 882709.1250,  339320.1250,  336057.1875,  311455.1562,  286527.2500,
          246137.3125,  236989.8281,  224409.9844,  224335.0938,  209280.2500],
        [ 676983.1875,  616298.5000,  590245.5000,  587427.6250,  583922.7500,
          583487.4375,  554283.4375,  520719.4375,  516186.0312,  506133.0625],
        [ 319668.9375,  291101.8750,  239093.0938,  230373.2344,  212185.4375,
          198514.9688,  197866.6875,  190299.4062,  183625.2344,  173040.0625],
        [ 799138.5000,  667356.3125,  560948.1875,  528692.6875,  526712.3750,
          487340.7188,  484389.5938,  483196.4375,  473715.5000,  424543.9375],
        [ 784414.5000,  648521.8750,  632590.3125,  513570.3438,  497677.8125,
          476989.9062,  449551.4375,  446240.1875,  430955.7188,  429686.0000]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 444136.9062,       0.0000],
         [ 424556.8750,       0.0000],
         [ 192545.0000,       0.0000],
         ...,
         [ 128301.7656,       0.0000],
         [ 116306.2812,       0.0000],
         [ 112253.3203,       0.0000]],

        [[1267220.7500,       0.0000],
         [1191516.0000,       0.0000],
         [1189597.0000,       0.0000],
         ...,
         [1148750.6250,       0.0000],
         [1115855.3750,       0.0000],
         [1107960.7500,       0.0000]],

        [[ 884777.3750,       0.0000],
         [ 849731.5000,       0.0000],
         [ 718364.6875,       0.0000],
         ...,
         [ 546827.8750,       0.0000],
         [ 517011.7188,       0.0000],
         [ 478437.3125,       0.0000]],

        ...,

        [[ 319668.9375,       0.0000],
         [ 291101.8750,       0.0000],
         [ 239093.0938,       0.0000],
         ...,
         [      0.0000,  190299.4062],
         [ 183625.2344,       0.0000],
         [ 173040.0625,       0.0000]],

        [[      0.0000,  799138.5000],
         [ 667356.3125,       0.0000],
         [ 560948.1875,       0.0000],
         ...,
         [ 483196.4375,       0.0000],
         [ 473715.5000,       0.0000],
         [ 424543.9375,       0.0000]],

        [[ 784414.5000,       0.0000],
         [      0.0000,  648521.8750],
         [ 632590.3125,       0.0000],
         ...,
         [ 446240.1875,       0.0000],
         [      0.0000,  430955.7188],
         [ 429686.0000,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 1875877.8750,   134284.9375],
        [11706516.0000,        0.0000],
        [ 6579429.0000,        0.0000],
        [ 8278947.0000,        0.0000],
        [ 3344984.0000,  1476708.7500],
        [ 7560119.5000,   787710.6875],
        [ 9384008.0000,  2353658.5000],
        [ 7838908.0000,        0.0000],
        [ 9918407.0000,  1072843.6250],
        [ 6645289.5000,        0.0000],
        [10026533.0000,        0.0000],
        [ 7059459.0000,        0.0000],
        [ 7309527.0000,        0.0000],
        [12310296.0000,        0.0000],
        [ 9843293.0000,        0.0000],
        [ 8241817.0000,        0.0000],
        [12241923.0000,        0.0000],
        [11489335.0000,        0.0000],
        [ 8000732.5000,   743622.6875],
        [ 9733217.0000,        0.0000],
        [11751819.0000,        0.0000],
        [ 8875554.0000,        0.0000],
        [10064573.0000,        0.0000],
        [ 9596535.0000,        0.0000],
        [ 8862729.0000,        0.0000],
        [ 7844625.0000,        0.0000],
        [ 7646404.5000,   834943.5625],
        [ 8337726.0000,        0.0000],
        [ 9785635.0000,        0.0000],
        [ 9547839.0000,        0.0000],
        [11009086.0000,        0.0000],
        [ 8257653.5000,        0.0000],
        [ 3667235.5000,   382342.6875],
        [ 4558165.0000,  3034046.0000],
        [ 5813505.5000,  2598130.0000],
        [ 6641857.0000,  2050930.7500],
        [ 4620392.0000,  2060349.1250],
        [ 5105781.5000,  2821067.5000],
        [ 3268397.7500,  3588413.7500],
        [12691021.0000,        0.0000],
        [ 4503580.0000,   337386.0000],
        [ 5889925.0000,        0.0000],
        [ 4971408.5000,        0.0000],
        [ 4529340.0000,        0.0000],
        [ 2833681.0000,  1636916.7500],
        [  557270.3750,   544360.6250],
        [ 8143210.0000,        0.0000],
        [ 1997660.6250,   752993.1875],
        [ 1260347.7500,  3680981.5000],
        [ 6757981.5000,  1655374.3750],
        [ 3880299.0000,   854464.5000],
        [ 4281052.5000,   990796.2500],
        [ 2905028.2500,  2818780.5000],
        [ 3394432.0000,  1011317.0000],
        [ 8609092.0000,        0.0000],
        [ 6499280.5000,  1619002.1250],
        [ 3121028.5000,  2856269.2500],
        [ 3346430.0000,   898615.1250],
        [ 2607721.0000,  2419980.7500],
        [ 1595327.8750,  1701893.5000],
        [ 5145441.5000,   590245.5000],
        [ 1634769.1250,   600999.8125],
        [ 4152506.5000,  1283528.1250],
        [ 4230720.0000,  1079477.6250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 54.6875
Top1 accuracy for validation set is 54.6875 size is torch.Size([64, 1])
Epoch 146/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:51, 61.79s/it]  7%|▋         | 2/30 [01:02<12:04, 25.88s/it] 10%|█         | 3/30 [01:03<06:28, 14.41s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.03s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 3.461069361368815
Epoch 147/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:37, 59.22s/it]  7%|▋         | 2/30 [00:59<11:35, 24.83s/it] 10%|█         | 3/30 [01:00<06:13, 13.84s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.67s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.81s/it] 20%|██        | 6/30 [01:02<01:38,  4.09s/it] 23%|██▎       | 7/30 [01:03<01:08,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.70s/it]
Epoch loss is 3.494254644711812
Epoch 148/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:42, 63.55s/it]  7%|▋         | 2/30 [01:04<12:25, 26.61s/it] 10%|█         | 3/30 [01:05<06:39, 14.80s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.25s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.19s/it] 20%|██        | 6/30 [01:07<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:08<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.85s/it]
Epoch loss is 3.4919837792714437
Epoch 149/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:57, 59.91s/it]  7%|▋         | 2/30 [01:00<11:43, 25.11s/it] 10%|█         | 3/30 [01:01<06:17, 13.99s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.76s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 3.4619102160135906
Epoch 150/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:50, 59.68s/it]  7%|▋         | 2/30 [01:00<11:40, 25.01s/it] 10%|█         | 3/30 [01:01<06:16, 13.93s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.73s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.85s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 3.4798214197158814
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0080,  0.0104, -0.0360,  ..., -0.0119, -0.0248,  0.0348],
        [-0.0293,  0.0180, -0.0081,  ...,  0.0123, -0.0336, -0.0036],
        [-0.0568, -0.0153,  0.0005,  ...,  0.0559, -0.0216, -0.0492],
        ...,
        [ 0.0297, -0.0274, -0.0425,  ..., -0.0331, -0.0314, -0.0124],
        [-0.0389,  0.0050, -0.0098,  ..., -0.0295,  0.0216, -0.0208],
        [-0.0380,  0.0064,  0.0012,  ..., -0.0087,  0.0151, -0.0566]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9076, 0.9064, 0.8503, 0.8407, 0.8329, 0.8305, 0.8292, 0.8242, 0.8181,
         0.8152],
        [0.9813, 0.9778, 0.9768, 0.9765, 0.9760, 0.9757, 0.9756, 0.9754, 0.9748,
         0.9740],
        [0.9558, 0.9553, 0.9413, 0.9381, 0.9358, 0.9335, 0.9325, 0.9243, 0.9170,
         0.9113],
        [0.9823, 0.9666, 0.9633, 0.9562, 0.9505, 0.9395, 0.9391, 0.9363, 0.9342,
         0.9333],
        [0.9298, 0.9202, 0.9111, 0.9072, 0.9068, 0.9056, 0.9031, 0.8970, 0.8967,
         0.8962],
        [0.9629, 0.9557, 0.9516, 0.9507, 0.9494, 0.9485, 0.9471, 0.9457, 0.9441,
         0.9423],
        [0.9886, 0.9848, 0.9823, 0.9822, 0.9770, 0.9703, 0.9680, 0.9676, 0.9673,
         0.9641],
        [0.9719, 0.9671, 0.9606, 0.9514, 0.9438, 0.9342, 0.9320, 0.9298, 0.9285,
         0.9214],
        [0.9756, 0.9726, 0.9725, 0.9724, 0.9721, 0.9719, 0.9704, 0.9699, 0.9694,
         0.9670],
        [0.9768, 0.9467, 0.9403, 0.9377, 0.9353, 0.9245, 0.9189, 0.9167, 0.9162,
         0.9157],
        [0.9755, 0.9725, 0.9665, 0.9657, 0.9650, 0.9645, 0.9636, 0.9629, 0.9628,
         0.9624],
        [0.9549, 0.9538, 0.9530, 0.9471, 0.9440, 0.9390, 0.9252, 0.9239, 0.8999,
         0.8979],
        [0.9832, 0.9692, 0.9525, 0.9519, 0.9315, 0.9282, 0.9266, 0.9265, 0.9213,
         0.9202],
        [0.9869, 0.9836, 0.9829, 0.9824, 0.9824, 0.9809, 0.9786, 0.9785, 0.9785,
         0.9776],
        [0.9737, 0.9706, 0.9636, 0.9622, 0.9621, 0.9619, 0.9613, 0.9611, 0.9610,
         0.9593],
        [0.9662, 0.9589, 0.9573, 0.9551, 0.9538, 0.9507, 0.9482, 0.9475, 0.9394,
         0.9390],
        [0.9906, 0.9877, 0.9812, 0.9803, 0.9797, 0.9791, 0.9786, 0.9786, 0.9741,
         0.9740],
        [0.9860, 0.9817, 0.9764, 0.9761, 0.9757, 0.9730, 0.9726, 0.9712, 0.9707,
         0.9705],
        [0.9765, 0.9735, 0.9633, 0.9589, 0.9555, 0.9551, 0.9462, 0.9437, 0.9421,
         0.9409],
        [0.9730, 0.9696, 0.9692, 0.9663, 0.9636, 0.9607, 0.9601, 0.9557, 0.9556,
         0.9556],
        [0.9908, 0.9840, 0.9828, 0.9813, 0.9745, 0.9737, 0.9737, 0.9700, 0.9697,
         0.9685],
        [0.9801, 0.9650, 0.9643, 0.9578, 0.9529, 0.9518, 0.9510, 0.9493, 0.9491,
         0.9451],
        [0.9866, 0.9692, 0.9688, 0.9685, 0.9646, 0.9642, 0.9642, 0.9595, 0.9587,
         0.9554],
        [0.9708, 0.9669, 0.9663, 0.9646, 0.9625, 0.9619, 0.9618, 0.9583, 0.9563,
         0.9558],
        [0.9802, 0.9782, 0.9562, 0.9513, 0.9509, 0.9477, 0.9472, 0.9455, 0.9436,
         0.9429],
        [0.9588, 0.9528, 0.9523, 0.9491, 0.9488, 0.9484, 0.9458, 0.9457, 0.9438,
         0.9433],
        [0.9716, 0.9684, 0.9653, 0.9529, 0.9510, 0.9507, 0.9490, 0.9440, 0.9436,
         0.9430],
        [0.9630, 0.9590, 0.9576, 0.9556, 0.9524, 0.9521, 0.9516, 0.9482, 0.9459,
         0.9452],
        [0.9790, 0.9731, 0.9669, 0.9639, 0.9625, 0.9614, 0.9609, 0.9584, 0.9582,
         0.9579],
        [0.9690, 0.9680, 0.9671, 0.9662, 0.9647, 0.9619, 0.9597, 0.9586, 0.9573,
         0.9545],
        [0.9793, 0.9759, 0.9738, 0.9729, 0.9725, 0.9722, 0.9717, 0.9696, 0.9684,
         0.9659],
        [0.9645, 0.9620, 0.9537, 0.9501, 0.9498, 0.9485, 0.9472, 0.9461, 0.9452,
         0.9445],
        [0.9092, 0.9089, 0.9077, 0.9009, 0.9001, 0.8976, 0.8971, 0.8938, 0.8935,
         0.8929],
        [0.9627, 0.9570, 0.9539, 0.9504, 0.9393, 0.9361, 0.9347, 0.9342, 0.9341,
         0.9338],
        [0.9709, 0.9600, 0.9592, 0.9566, 0.9522, 0.9496, 0.9471, 0.9458, 0.9448,
         0.9439],
        [0.9679, 0.9653, 0.9636, 0.9523, 0.9521, 0.9492, 0.9480, 0.9464, 0.9456,
         0.9434],
        [0.9569, 0.9489, 0.9445, 0.9377, 0.9360, 0.9291, 0.9246, 0.9246, 0.9159,
         0.9154],
        [0.9760, 0.9675, 0.9537, 0.9460, 0.9429, 0.9395, 0.9385, 0.9384, 0.9381,
         0.9372],
        [0.9583, 0.9567, 0.9477, 0.9357, 0.9340, 0.9321, 0.9291, 0.9277, 0.9271,
         0.9266],
        [0.9888, 0.9856, 0.9852, 0.9837, 0.9825, 0.9818, 0.9817, 0.9788, 0.9787,
         0.9781],
        [0.9557, 0.9303, 0.9245, 0.9117, 0.9111, 0.9109, 0.8935, 0.8924, 0.8913,
         0.8906],
        [0.9437, 0.9369, 0.9318, 0.9269, 0.9265, 0.9261, 0.9256, 0.9224, 0.9186,
         0.9179],
        [0.9267, 0.9263, 0.9239, 0.9211, 0.9132, 0.9107, 0.9037, 0.9025, 0.8999,
         0.8991],
        [0.9405, 0.9291, 0.9155, 0.9082, 0.9060, 0.9043, 0.9037, 0.8995, 0.8961,
         0.8881],
        [0.9313, 0.9226, 0.9173, 0.9101, 0.9083, 0.9043, 0.9003, 0.8996, 0.8953,
         0.8894],
        [0.8795, 0.8064, 0.8056, 0.8043, 0.7965, 0.7936, 0.7883, 0.7871, 0.7756,
         0.7755],
        [0.9684, 0.9673, 0.9517, 0.9486, 0.9483, 0.9475, 0.9446, 0.9438, 0.9431,
         0.9404],
        [0.8986, 0.8922, 0.8806, 0.8777, 0.8731, 0.8643, 0.8609, 0.8520, 0.8506,
         0.8385],
        [0.9331, 0.9286, 0.9276, 0.9207, 0.9117, 0.9052, 0.9051, 0.9029, 0.9012,
         0.8936],
        [0.9628, 0.9569, 0.9563, 0.9561, 0.9489, 0.9470, 0.9470, 0.9468, 0.9467,
         0.9463],
        [0.9289, 0.9201, 0.9139, 0.9120, 0.9083, 0.9070, 0.9005, 0.8999, 0.8899,
         0.8867],
        [0.9370, 0.9274, 0.9201, 0.9185, 0.9171, 0.9154, 0.9143, 0.9136, 0.9122,
         0.9056],
        [0.9676, 0.9374, 0.9354, 0.9332, 0.9310, 0.9260, 0.9113, 0.8983, 0.8970,
         0.8960],
        [0.9429, 0.9269, 0.9229, 0.9121, 0.9022, 0.8984, 0.8926, 0.8852, 0.8830,
         0.8814],
        [0.9650, 0.9574, 0.9573, 0.9557, 0.9545, 0.9529, 0.9519, 0.9516, 0.9503,
         0.9502],
        [0.9622, 0.9520, 0.9514, 0.9512, 0.9504, 0.9445, 0.9428, 0.9421, 0.9415,
         0.9412],
        [0.9606, 0.9461, 0.9399, 0.9373, 0.9245, 0.9200, 0.9197, 0.9061, 0.9010,
         0.9006],
        [0.9554, 0.9157, 0.9065, 0.9038, 0.8975, 0.8937, 0.8933, 0.8876, 0.8833,
         0.8813],
        [0.9560, 0.9339, 0.9272, 0.9219, 0.9181, 0.9110, 0.9102, 0.8900, 0.8883,
         0.8839],
        [0.9583, 0.8877, 0.8875, 0.8838, 0.8790, 0.8686, 0.8648, 0.8587, 0.8579,
         0.8576],
        [0.9384, 0.9310, 0.9295, 0.9276, 0.9275, 0.9260, 0.9243, 0.9186, 0.9185,
         0.9173],
        [0.8829, 0.8742, 0.8604, 0.8595, 0.8509, 0.8508, 0.8504, 0.8461, 0.8456,
         0.8394],
        [0.9474, 0.9349, 0.9232, 0.9204, 0.9183, 0.9129, 0.9126, 0.9125, 0.9082,
         0.9054],
        [0.9440, 0.9326, 0.9289, 0.9176, 0.9144, 0.9112, 0.9092, 0.9054, 0.9041,
         0.9038]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 1, 1, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 1, 0, 1, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 1, 1, 1, 1],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 427387.9375,  420029.0625,  188577.7188,  164307.4688,  146976.9844,
          142009.1250,  139569.6719,  129805.2344,  118969.7500,  114193.1016],
        [1225140.5000, 1165357.2500, 1148712.2500, 1143135.5000, 1136537.2500,
         1130922.5000, 1129893.0000, 1125874.5000, 1116588.8750, 1103863.3750],
        [ 850897.5000,  845054.7500,  691491.2500,  660933.9375,  639191.3125,
          618478.7500,  610495.5625,  542701.1875,  488791.5312,  450970.1562],
        [1242993.7500,  993197.0000,  947324.0000,  855827.6875,  789408.7500,
          674623.0000,  670787.2500,  643791.1875,  624876.5000,  617390.9375],
        [ 586918.5625,  511947.8125,  449508.1250,  425093.2812,  422820.6250,
          415726.2500,  401031.5625,  367220.5312,  365899.1250,  363220.6875],
        [ 941330.6250,  850028.1250,  801780.5000,  791192.0625,  776589.3125,
          766600.2500,  751243.9375,  736483.1875,  720434.5625,  701373.8750],
        [1359023.8750, 1287567.0000, 1242089.6250, 1241470.2500, 1152748.5000,
         1046691.7500, 1012710.7500, 1007432.0000, 1003748.4375,  958508.1250],
        [1070949.3750,  999572.0625,  911700.0625,  799199.4375,  716825.6250,
          625342.0625,  605436.1875,  586935.3750,  576647.8125,  520867.9688],
        [1129108.8750, 1081304.3750, 1080772.5000, 1079302.6250, 1074899.0000,
         1070863.6250, 1047750.3750, 1041243.7500, 1034187.6875,  999251.8125],
        [1149013.5000,  747040.9375,  682438.5625,  656845.7500,  635432.1250,
          544510.5000,  502571.8125,  487071.2188,  483124.0938,  480197.1875],
        [1127293.7500, 1080240.7500,  991839.6875,  980218.0000,  970236.8125,
          963092.2500,  952007.1875,  941356.6250,  940041.4375,  934963.1250],
        [ 839670.0625,  826548.8750,  817299.5625,  751884.6875,  718815.6250,
          669260.6875,  549598.7500,  539819.8750,  383004.6875,  372323.9062],
        [1259077.5000, 1029964.1875,  812217.1250,  805515.3125,  601842.8750,
          573952.8750,  560695.1875,  560072.0625,  519886.8125,  512042.0312],
        [1327001.6250, 1265837.7500, 1253553.0000, 1244584.5000, 1243967.3750,
         1218049.1250, 1179237.0000, 1177923.0000, 1176412.0000, 1162880.5000],
        [1098406.8750, 1051763.8750,  952019.9375,  932573.0625,  930697.3750,
          928031.3750,  920721.6250,  917877.5625,  917202.0625,  894635.9375],
        [ 988092.5625,  889039.5000,  869660.7500,  842376.1875,  826590.6250,
          791567.8750,  763280.2500,  755707.6875,  673091.6250,  669334.7500],
        [1398632.7500, 1343080.6250, 1222906.2500, 1208551.7500, 1197315.0000,
         1186570.7500, 1179229.1250, 1178193.8750, 1105170.5000, 1103791.7500],
        [1309872.8750, 1231954.7500, 1142623.2500, 1137959.1250, 1131643.2500,
         1088253.2500, 1082441.3750, 1060128.2500, 1053158.0000, 1050685.1250],
        [1144004.7500, 1095446.3750,  946860.6250,  890071.9375,  847736.2500,
          842282.1250,  742070.4375,  716027.5625,  699616.9375,  688113.8125],
        [1087582.0000, 1036980.5625, 1031316.6250,  988459.2500,  951405.4375,
          913425.0000,  904389.4375,  849921.9375,  848816.2500,  848532.1875],
        [1402551.7500, 1273340.8750, 1251201.2500, 1225514.5000, 1111078.0000,
         1098986.2500, 1098525.2500, 1042131.8750, 1037732.5000, 1020610.6250],
        [1204523.3750,  970883.8750,  960903.2500,  875144.3750,  816707.4375,
          803556.4375,  794246.1875,  775776.5000,  773618.4375,  730303.1250],
        [1320685.2500, 1031013.6875, 1024862.2500, 1019909.1875,  964821.5000,
          959948.8750,  959896.6875,  897379.7500,  886582.4375,  845746.5000],
        [1054512.8750,  996975.8125,  989051.3750,  965797.3125,  936707.0625,
          928983.3750,  927744.6875,  881686.1250,  856926.1875,  851663.9375],
        [1205539.3750, 1171903.7500,  855423.0000,  797589.8125,  793198.6250,
          758190.2500,  752485.1875,  734158.5000,  715004.6875,  708260.8750],
        [ 887830.5000,  815263.8750,  809258.9375,  772918.6250,  770477.5000,
          766049.9375,  737538.1250,  736361.6875,  716602.0625,  712411.4375],
        [1065992.0000, 1019362.6875,  975104.0000,  815995.8125,  794523.5000,
          791822.3125,  772434.5000,  718843.6875,  714784.4375,  708583.8125],
        [ 943970.8750,  890776.7500,  872830.6250,  848807.3125,  810357.1250,
          806736.1250,  801518.2500,  763249.0000,  738700.3750,  731589.9375],
        [1186073.0000, 1089838.1250,  997958.5625,  955571.0000,  937097.5000,
          921898.1250,  915277.1250,  883164.6875,  880582.8125,  877295.3125],
        [1028439.8750, 1012555.3125, 1000583.0000,  987295.6875,  967114.3750,
          928875.2500,  899245.5625,  885810.0000,  869572.0625,  835611.8750],
        [1189843.3750, 1133654.5000, 1100114.6250, 1086283.1250, 1080058.5000,
         1075460.8750, 1067489.6250, 1036011.9375, 1018975.8125,  983018.0625],
        [ 963201.5000,  930509.3125,  826461.3750,  784656.1250,  781255.0000,
          767306.1250,  752918.7500,  740624.6875,  731003.4375,  723806.3125],
        [ 437127.3750,  435716.8750,  428350.5312,  388375.9062,  384339.5000,
          370738.6562,  368162.4062,  350799.4688,  349327.8750,  346571.0625],
        [ 939588.8750,  865798.6875,  828152.1250,  787861.6875,  672756.6250,
          642298.5000,  629308.5625,  624937.2500,  624150.4375,  621845.8125],
        [1056197.6250,  903930.7500,  893523.1875,  860272.6875,  807876.3125,
          778469.0000,  751587.9375,  738129.2500,  727065.4375,  718219.4375],
        [1011265.0625,  974362.1875,  951788.4375,  809681.1875,  807280.2500,
          774765.0625,  761588.2500,  743770.8750,  735935.5000,  712554.7500],
        [ 864299.6875,  771214.8125,  724307.5625,  657082.6250,  641806.2500,
          581153.8750,  545158.4375,  544840.8750,  481658.4375,  477804.4062],
        [1135033.8750, 1005820.1875,  826496.0000,  739623.8125,  707877.3125,
          673884.1875,  664857.8750,  663341.6875,  660596.8125,  652130.8750],
        [ 882355.6875,  862503.0000,  758286.4375,  638222.1875,  623758.3750,
          606648.2500,  581239.8125,  570071.5625,  564570.3125,  560672.2500],
        [1363119.1250, 1301878.8750, 1295939.1250, 1267280.0000, 1245724.5000,
         1234091.5000, 1231992.3750, 1182873.8750, 1180123.5000, 1170623.6250],
        [ 849605.8750,  591410.1875,  544157.0000,  453556.7500,  449166.1562,
          448038.4062,  349594.1875,  344248.7500,  338486.2812,  335365.9688],
        [ 716080.1875,  649828.2500,  604394.3750,  562863.9375,  559973.2500,
          556755.9375,  552625.0625,  528496.0625,  500043.2500,  495138.4375],
        [ 561250.5000,  558317.5625,  539586.1875,  518690.8750,  462863.2812,
          446657.8750,  404466.8438,  397729.3438,  382978.4062,  378371.8438],
        [ 683596.7500,  580863.0000,  478904.7500,  431276.8125,  417983.1250,
          407730.0938,  404374.6562,  380581.5938,  362487.0625,  323733.6562],
        [ 600026.6875,  529293.0625,  490760.3125,  443245.7500,  431857.5312,
          408072.0625,  385234.9062,  381080.5938,  358402.7188,  329585.2188],
        [ 286219.1875,  100729.2188,   99588.8281,   97774.2500,   87461.8281,
           83880.4375,   77733.8047,   76413.5703,   64830.8867,   64722.4062],
        [1018367.6875, 1002999.2500,  802697.8125,  768251.3750,  764136.0625,
          756335.6875,  725471.7500,  717387.7500,  709560.8750,  682861.7500],
        [ 376086.0000,  342943.2500,  290741.1875,  278927.9375,  261275.4688,
          230343.1250,  219435.1250,  193119.8750,  189285.4531,  159215.2031],
        [ 615680.5000,  577065.9375,  568528.0000,  515225.0000,  452965.8125,
          413113.0938,  412620.1250,  399878.9688,  390110.5938,  350048.2188],
        [ 941128.6875,  864227.1250,  857565.4375,  854262.8750,  770844.2500,
          751041.9375,  750773.3750,  748096.7500,  747262.5625,  743431.2500],
        [ 579711.3750,  511433.4688,  468081.8438,  455315.3750,  431503.0938,
          423581.8125,  386315.4062,  383039.7812,  332195.8438,  317356.7500],
        [ 650362.6875,  567221.1875,  511239.3750,  499710.0000,  489602.3750,
          478114.3750,  470681.6875,  465440.3750,  456788.0625,  415738.1562],
        [1007067.9375,  653988.0000,  636387.9375,  615953.0000,  597014.0625,
          556120.1875,  450499.9062,  374116.7188,  367589.1250,  362281.7812],
        [ 707398.1875,  563477.8125,  531598.7500,  455601.6250,  395932.5000,
          374892.0938,  345099.4375,  310608.5938,  301000.8125,  294085.6250],
        [ 971195.0000,  870611.7500,  869706.3750,  849401.6875,  835490.0000,
          817047.0625,  805150.5000,  801450.2500,  786237.4375,  785366.6250],
        [ 931963.1250,  806187.7500,  798784.9375,  796793.0625,  787485.3750,
          724626.7500,  706897.0625,  699384.1250,  693781.3125,  690731.9375],
        [ 911818.3125,  740741.9375,  678539.6250,  653035.0625,  544079.6875,
          510383.4688,  507828.6562,  418490.8438,  389038.0000,  386739.7188],
        [ 845935.2500,  479634.2500,  420662.4062,  404897.9375,  370133.1250,
          350289.0312,  348757.0312,  321260.0000,  302263.0312,  293718.7188],
        [ 853110.0625,  622862.5000,  565934.1250,  524664.9375,  496808.5938,
          448621.1562,  443763.9062,  332524.2188,  324565.5625,  304696.7500],
        [ 882296.7500,  321503.0312,  320848.7812,  304152.4062,  284119.9688,
          244838.4219,  231912.2969,  212467.5000,  210267.3281,  209161.5312],
        [ 664180.4375,  597297.1250,  584779.3125,  568508.5000,  567855.5625,
          555824.8750,  542631.8750,  500328.0000,  499724.3438,  491161.5625],
        [ 300565.3750,  265370.6875,  217774.2031,  215051.3750,  190112.0312,
          189791.2031,  188915.0469,  177491.2500,  176310.3125,  161309.0625],
        [ 754707.3125,  631667.9375,  534239.5000,  513048.4688,  498026.7812,
          461312.0938,  459028.2812,  458691.3438,  431046.5625,  414500.2188],
        [ 718881.4375,  611435.9375,  579579.2500,  493485.2188,  470999.1562,
          450234.0312,  437382.1562,  414241.3438,  406676.9375,  404777.0938]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 427387.9375,       0.0000],
         [ 420029.0625,       0.0000],
         [ 188577.7188,       0.0000],
         ...,
         [ 129805.2344,       0.0000],
         [ 118969.7500,       0.0000],
         [ 114193.1016,       0.0000]],

        [[1225140.5000,       0.0000],
         [1165357.2500,       0.0000],
         [1148712.2500,       0.0000],
         ...,
         [1125874.5000,       0.0000],
         [1116588.8750,       0.0000],
         [1103863.3750,       0.0000]],

        [[ 850897.5000,       0.0000],
         [ 845054.7500,       0.0000],
         [ 691491.2500,       0.0000],
         ...,
         [ 542701.1875,       0.0000],
         [ 488791.5312,       0.0000],
         [ 450970.1562,       0.0000]],

        ...,

        [[ 300565.3750,       0.0000],
         [ 265370.6875,       0.0000],
         [ 217774.2031,       0.0000],
         ...,
         [ 177491.2500,       0.0000],
         [ 176310.3125,       0.0000],
         [ 161309.0625,       0.0000]],

        [[      0.0000,  754707.3125],
         [ 631667.9375,       0.0000],
         [ 534239.5000,       0.0000],
         ...,
         [ 458691.3438,       0.0000],
         [ 431046.5625,       0.0000],
         [ 414500.2188,       0.0000]],

        [[ 718881.4375,       0.0000],
         [      0.0000,  611435.9375],
         [ 579579.2500,       0.0000],
         ...,
         [ 414241.3438,       0.0000],
         [ 406676.9375,       0.0000],
         [ 404777.0938,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 1849817.0000,   142009.1250],
        [11426025.0000,        0.0000],
        [ 6399006.0000,        0.0000],
        [ 8060220.0000,        0.0000],
        [ 3015813.2500,  1293573.2500],
        [ 7070456.0000,   766600.2500],
        [ 9023209.0000,  2288781.5000],
        [ 7413476.0000,        0.0000],
        [ 9563786.0000,  1074899.0000],
        [ 6368246.0000,        0.0000],
        [ 9881290.0000,        0.0000],
        [ 6468226.5000,        0.0000],
        [ 7235265.5000,        0.0000],
        [12249446.0000,        0.0000],
        [ 9543930.0000,        0.0000],
        [ 8068742.0000,        0.0000],
        [12123442.0000,        0.0000],
        [11288720.0000,        0.0000],
        [ 7924117.0000,   688113.8125],
        [ 9460829.0000,        0.0000],
        [11561672.0000,        0.0000],
        [ 8705663.0000,        0.0000],
        [ 9910846.0000,        0.0000],
        [ 9390049.0000,        0.0000],
        [ 8491754.0000,        0.0000],
        [ 7724712.0000,        0.0000],
        [ 7561451.0000,   815995.8125],
        [ 8208536.0000,        0.0000],
        [ 9644757.0000,        0.0000],
        [ 9415103.0000,        0.0000],
        [10770910.0000,        0.0000],
        [ 8001743.0000,        0.0000],
        [ 3510181.5000,   349327.8750],
        [ 4326082.0000,  2910616.2500],
        [ 5662475.5000,  2572796.0000],
        [ 6297364.0000,  1985627.2500],
        [ 4286286.5000,  2003040.7500],
        [ 4880931.0000,  2848731.5000],
        [ 3166077.2500,  3482250.7500],
        [12473646.0000,        0.0000],
        [ 4703630.0000,        0.0000],
        [ 5726199.0000,        0.0000],
        [ 4650913.0000,        0.0000],
        [ 4471531.5000,        0.0000],
        [ 2762603.2500,  1594955.3750],
        [  506758.1250,   532596.2500],
        [ 7948069.5000,        0.0000],
        [ 1841953.8750,   699418.6250],
        [ 1192746.5000,  3502490.0000],
        [ 6413633.5000,  1615000.5000],
        [ 3515862.5000,   772672.1250],
        [ 4039747.7500,   965150.3750],
        [ 2853564.2500,  2767454.5000],
        [ 3292495.0000,   987200.3750],
        [ 8391657.0000,        0.0000],
        [ 6830448.0000,   806187.7500],
        [ 2602066.5000,  3138628.7500],
        [ 3237254.0000,   900296.6250],
        [ 2570948.2500,  2346603.5000],
        [ 1541017.5000,  1680550.5000],
        [ 5016467.0000,   555824.8750],
        [ 1513872.2500,   568818.3125],
        [ 3942532.7500,  1213735.6250],
        [ 3938874.2500,  1048818.1250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 57.8125
Top1 accuracy for validation set is 57.8125 size is torch.Size([64, 1])
Epoch 151/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:24, 58.78s/it]  7%|▋         | 2/30 [00:59<11:30, 24.64s/it] 10%|█         | 3/30 [01:01<06:25, 14.26s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.93s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.98s/it] 20%|██        | 6/30 [01:03<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 3.436114462216695
Epoch 152/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:55, 61.92s/it]  7%|▋         | 2/30 [01:02<12:06, 25.94s/it] 10%|█         | 3/30 [01:03<06:29, 14.44s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.03s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.04s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 3.4278515338897706
Epoch 153/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:03, 58.06s/it]  7%|▋         | 2/30 [00:58<11:21, 24.35s/it] 10%|█         | 3/30 [00:59<06:06, 13.57s/it] 13%|█▎        | 4/30 [01:00<03:43,  8.60s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.77s/it] 20%|██        | 6/30 [01:02<01:37,  4.06s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 3.4276984135309854
Epoch 154/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:42, 57.32s/it]  7%|▋         | 2/30 [00:58<11:13, 24.04s/it] 10%|█         | 3/30 [01:00<06:17, 13.99s/it] 13%|█▎        | 4/30 [01:00<03:47,  8.76s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.87s/it] 20%|██        | 6/30 [01:02<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.30s/it] 30%|███       | 9/30 [01:04<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 3.4121567328770954
Epoch 155/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:49, 59.65s/it]  7%|▋         | 2/30 [01:00<11:40, 25.01s/it] 10%|█         | 3/30 [01:01<06:16, 13.93s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.73s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.85s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 3.4101810296376547
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0090,  0.0107, -0.0314,  ..., -0.0100, -0.0240,  0.0344],
        [-0.0261,  0.0176, -0.0070,  ...,  0.0123, -0.0337, -0.0033],
        [-0.0557, -0.0174,  0.0020,  ...,  0.0574, -0.0240, -0.0486],
        ...,
        [ 0.0316, -0.0258, -0.0414,  ..., -0.0316, -0.0324, -0.0126],
        [-0.0371,  0.0021, -0.0090,  ..., -0.0264,  0.0201, -0.0214],
        [-0.0371,  0.0047,  0.0030,  ..., -0.0059,  0.0137, -0.0571]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9081, 0.9020, 0.8494, 0.8387, 0.8365, 0.8329, 0.8313, 0.8256, 0.8210,
         0.8209],
        [0.9785, 0.9769, 0.9759, 0.9757, 0.9754, 0.9745, 0.9743, 0.9739, 0.9735,
         0.9735],
        [0.9561, 0.9528, 0.9399, 0.9355, 0.9346, 0.9338, 0.9324, 0.9231, 0.9153,
         0.9096],
        [0.9820, 0.9654, 0.9620, 0.9550, 0.9470, 0.9399, 0.9342, 0.9335, 0.9322,
         0.9308],
        [0.9214, 0.9099, 0.9043, 0.9013, 0.9000, 0.8989, 0.8986, 0.8929, 0.8925,
         0.8847],
        [0.9590, 0.9521, 0.9471, 0.9461, 0.9444, 0.9427, 0.9401, 0.9391, 0.9386,
         0.9377],
        [0.9866, 0.9825, 0.9792, 0.9792, 0.9743, 0.9684, 0.9653, 0.9644, 0.9618,
         0.9618],
        [0.9701, 0.9667, 0.9588, 0.9479, 0.9425, 0.9319, 0.9272, 0.9227, 0.9221,
         0.9179],
        [0.9748, 0.9720, 0.9706, 0.9694, 0.9685, 0.9676, 0.9671, 0.9660, 0.9660,
         0.9642],
        [0.9748, 0.9419, 0.9365, 0.9365, 0.9336, 0.9269, 0.9145, 0.9131, 0.9126,
         0.9102],
        [0.9758, 0.9718, 0.9665, 0.9637, 0.9631, 0.9629, 0.9626, 0.9613, 0.9613,
         0.9610],
        [0.9507, 0.9505, 0.9490, 0.9438, 0.9418, 0.9386, 0.9209, 0.9194, 0.8956,
         0.8955],
        [0.9835, 0.9681, 0.9520, 0.9506, 0.9296, 0.9290, 0.9276, 0.9264, 0.9192,
         0.9188],
        [0.9866, 0.9827, 0.9822, 0.9820, 0.9815, 0.9798, 0.9784, 0.9783, 0.9778,
         0.9769],
        [0.9716, 0.9698, 0.9625, 0.9607, 0.9601, 0.9583, 0.9575, 0.9574, 0.9574,
         0.9570],
        [0.9653, 0.9552, 0.9551, 0.9533, 0.9525, 0.9485, 0.9474, 0.9469, 0.9387,
         0.9381],
        [0.9906, 0.9874, 0.9809, 0.9797, 0.9783, 0.9782, 0.9770, 0.9768, 0.9723,
         0.9720],
        [0.9856, 0.9800, 0.9766, 0.9751, 0.9741, 0.9707, 0.9704, 0.9691, 0.9688,
         0.9687],
        [0.9769, 0.9729, 0.9619, 0.9603, 0.9555, 0.9547, 0.9454, 0.9441, 0.9426,
         0.9392],
        [0.9697, 0.9676, 0.9649, 0.9630, 0.9595, 0.9594, 0.9564, 0.9538, 0.9537,
         0.9523],
        [0.9903, 0.9827, 0.9804, 0.9793, 0.9736, 0.9728, 0.9727, 0.9677, 0.9672,
         0.9669],
        [0.9788, 0.9639, 0.9637, 0.9584, 0.9526, 0.9507, 0.9503, 0.9487, 0.9476,
         0.9459],
        [0.9862, 0.9693, 0.9680, 0.9671, 0.9638, 0.9623, 0.9616, 0.9592, 0.9549,
         0.9519],
        [0.9703, 0.9669, 0.9657, 0.9648, 0.9617, 0.9608, 0.9596, 0.9556, 0.9549,
         0.9547],
        [0.9779, 0.9778, 0.9531, 0.9487, 0.9465, 0.9431, 0.9430, 0.9409, 0.9400,
         0.9397],
        [0.9578, 0.9534, 0.9504, 0.9485, 0.9473, 0.9470, 0.9458, 0.9440, 0.9438,
         0.9430],
        [0.9700, 0.9676, 0.9642, 0.9516, 0.9505, 0.9485, 0.9473, 0.9437, 0.9431,
         0.9423],
        [0.9617, 0.9568, 0.9563, 0.9555, 0.9528, 0.9517, 0.9492, 0.9466, 0.9463,
         0.9454],
        [0.9791, 0.9726, 0.9655, 0.9640, 0.9619, 0.9613, 0.9611, 0.9574, 0.9565,
         0.9561],
        [0.9668, 0.9660, 0.9656, 0.9646, 0.9640, 0.9610, 0.9602, 0.9560, 0.9549,
         0.9519],
        [0.9784, 0.9744, 0.9733, 0.9715, 0.9710, 0.9705, 0.9704, 0.9686, 0.9678,
         0.9638],
        [0.9628, 0.9595, 0.9510, 0.9468, 0.9463, 0.9460, 0.9456, 0.9447, 0.9425,
         0.9424],
        [0.9058, 0.9053, 0.9050, 0.8974, 0.8951, 0.8949, 0.8949, 0.8929, 0.8905,
         0.8894],
        [0.9618, 0.9543, 0.9539, 0.9461, 0.9371, 0.9333, 0.9330, 0.9329, 0.9327,
         0.9322],
        [0.9700, 0.9580, 0.9570, 0.9547, 0.9499, 0.9459, 0.9459, 0.9426, 0.9418,
         0.9411],
        [0.9674, 0.9610, 0.9582, 0.9508, 0.9492, 0.9450, 0.9425, 0.9411, 0.9395,
         0.9391],
        [0.9541, 0.9420, 0.9393, 0.9343, 0.9289, 0.9198, 0.9195, 0.9190, 0.9127,
         0.9080],
        [0.9756, 0.9681, 0.9548, 0.9453, 0.9417, 0.9382, 0.9373, 0.9365, 0.9355,
         0.9347],
        [0.9574, 0.9551, 0.9447, 0.9318, 0.9294, 0.9290, 0.9253, 0.9235, 0.9227,
         0.9204],
        [0.9861, 0.9839, 0.9837, 0.9811, 0.9799, 0.9798, 0.9788, 0.9781, 0.9770,
         0.9768],
        [0.9544, 0.9301, 0.9241, 0.9100, 0.9094, 0.9046, 0.8939, 0.8921, 0.8919,
         0.8896],
        [0.9430, 0.9354, 0.9249, 0.9246, 0.9246, 0.9232, 0.9200, 0.9191, 0.9166,
         0.9144],
        [0.9225, 0.9221, 0.9188, 0.9169, 0.9108, 0.9050, 0.9003, 0.8966, 0.8964,
         0.8917],
        [0.9427, 0.9277, 0.9165, 0.9113, 0.9068, 0.9015, 0.9003, 0.8961, 0.8955,
         0.8857],
        [0.9282, 0.9214, 0.9154, 0.9091, 0.9073, 0.9017, 0.8996, 0.8960, 0.8921,
         0.8888],
        [0.8766, 0.8074, 0.7991, 0.7986, 0.7983, 0.7981, 0.7794, 0.7773, 0.7722,
         0.7690],
        [0.9668, 0.9662, 0.9477, 0.9460, 0.9459, 0.9442, 0.9435, 0.9420, 0.9406,
         0.9379],
        [0.8890, 0.8880, 0.8735, 0.8718, 0.8711, 0.8633, 0.8545, 0.8456, 0.8431,
         0.8339],
        [0.9307, 0.9250, 0.9231, 0.9153, 0.9080, 0.9047, 0.8995, 0.8982, 0.8957,
         0.8868],
        [0.9585, 0.9551, 0.9532, 0.9500, 0.9471, 0.9469, 0.9453, 0.9429, 0.9425,
         0.9418],
        [0.9260, 0.9133, 0.9090, 0.9085, 0.9019, 0.8996, 0.8977, 0.8933, 0.8843,
         0.8831],
        [0.9348, 0.9249, 0.9195, 0.9194, 0.9152, 0.9125, 0.9119, 0.9095, 0.9083,
         0.8978],
        [0.9655, 0.9364, 0.9320, 0.9314, 0.9298, 0.9246, 0.9071, 0.8985, 0.8935,
         0.8931],
        [0.9407, 0.9242, 0.9154, 0.9144, 0.9013, 0.8977, 0.8929, 0.8827, 0.8806,
         0.8795],
        [0.9640, 0.9568, 0.9566, 0.9544, 0.9543, 0.9509, 0.9499, 0.9496, 0.9490,
         0.9468],
        [0.9593, 0.9488, 0.9481, 0.9465, 0.9456, 0.9404, 0.9400, 0.9398, 0.9396,
         0.9394],
        [0.9597, 0.9449, 0.9368, 0.9354, 0.9215, 0.9162, 0.9156, 0.8985, 0.8975,
         0.8975],
        [0.9540, 0.9156, 0.9056, 0.8995, 0.8931, 0.8926, 0.8899, 0.8846, 0.8798,
         0.8782],
        [0.9570, 0.9303, 0.9252, 0.9207, 0.9196, 0.9100, 0.9058, 0.8893, 0.8858,
         0.8802],
        [0.9578, 0.8835, 0.8827, 0.8821, 0.8788, 0.8682, 0.8623, 0.8588, 0.8550,
         0.8520],
        [0.9363, 0.9300, 0.9290, 0.9253, 0.9248, 0.9227, 0.9210, 0.9160, 0.9159,
         0.9153],
        [0.8776, 0.8682, 0.8555, 0.8541, 0.8480, 0.8475, 0.8444, 0.8424, 0.8400,
         0.8347],
        [0.9456, 0.9321, 0.9203, 0.9203, 0.9153, 0.9099, 0.9096, 0.9092, 0.9048,
         0.9018],
        [0.9379, 0.9302, 0.9209, 0.9139, 0.9096, 0.9084, 0.9060, 0.8995, 0.8986,
         0.8985]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 1, 1, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 1, 1, 1, 1, 1, 1],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 1],
        [0, 0, 0, 1, 1, 0, 1, 1, 1, 1],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 430536.7188,  394578.1250,  186111.6250,  159744.6406,  154743.7500,
          147046.2344,  143793.2812,  132498.6562,  124103.6328,  123853.1094],
        [1177183.0000, 1150336.8750, 1134793.6250, 1131166.2500, 1126357.7500,
         1111259.2500, 1108000.8750, 1102410.5000, 1096329.5000, 1096089.0000],
        [ 854856.1250,  815169.0000,  677730.5625,  636690.1875,  628669.6875,
          621685.6875,  609019.0625,  533481.9375,  477378.0938,  439858.9062],
        [1236853.1250,  975961.7500,  930424.9375,  840918.5000,  750192.9375,
          678561.6250,  625438.1250,  619221.2500,  607853.3750,  595592.9375],
        [ 520471.1875,  442077.2500,  408064.6250,  390555.0938,  383749.4688,
          377592.8750,  376076.6562,  346625.9375,  344574.5625,  308117.6562],
        [ 890996.0000,  806911.5000,  751971.5000,  740784.3125,  723584.7500,
          705541.3125,  680372.0625,  670476.4375,  666004.6250,  657028.7500],
        [1321617.6250, 1246858.2500, 1189786.6250, 1189370.2500, 1108845.5000,
         1018917.5625,  974918.9375,  962088.8750,  927395.2500,  927255.5625],
        [1044122.5000,  994284.0000,  887979.5000,  760686.0000,  703341.8125,
          604747.8125,  565910.9375,  530127.0625,  525724.7500,  495119.0938],
        [1116634.6250, 1072310.6250, 1051028.8750, 1034321.7500, 1020774.1875,
         1007521.3125, 1000436.1250,  984482.5000,  984382.0625,  959151.8125],
        [1116487.6250,  698079.3125,  646210.4375,  646015.6875,  620000.6250,
          563018.0625,  471587.5000,  462274.8125,  459062.8750,  443979.3438],
        [1132094.3750, 1070221.5000,  991632.5625,  953269.0625,  944866.1250,
          941466.1250,  937513.1875,  921154.6250,  920469.6250,  916465.8750],
        [ 791455.4375,  789451.0000,  772203.1875,  716842.0000,  696963.1250,
          666052.8750,  517319.4688,  506000.3438,  360234.9688,  359852.8125],
        [1264359.8750, 1015046.7500,  806169.3125,  789612.0625,  585467.8750,
          580521.3125,  568769.9375,  559377.6250,  504894.0625,  501875.4375],
        [1321518.1250, 1250420.0000, 1241258.3750, 1237003.0000, 1229077.2500,
         1199602.0000, 1175705.3750, 1173479.6250, 1164590.7500, 1150755.0000],
        [1066551.3750, 1039053.5000,  937068.9375,  913234.1875,  904908.8125,
          882022.5625,  871519.6875,  871092.6250,  870430.7500,  865859.0000],
        [ 974615.0000,  843556.3125,  842612.3750,  821094.8125,  811425.8750,
          767268.0000,  754392.8125,  749494.2500,  666349.6250,  661224.5625],
        [1398562.1250, 1336928.7500, 1218462.7500, 1197593.6250, 1173597.1250,
         1172738.8750, 1152169.3750, 1149067.2500, 1076996.3750, 1072796.6250],
        [1302066.2500, 1203149.2500, 1146327.6250, 1121846.6250, 1105055.6250,
         1053264.5000, 1048629.1250, 1029070.6875, 1024543.7500, 1023529.9375],
        [1149924.5000, 1086057.3750,  929079.0000,  907465.1875,  847856.6875,
          837517.1250,  733193.5625,  719947.5625,  705317.9375,  671787.9375],
        [1037358.4375, 1007641.4375,  969242.6250,  942830.0625,  896818.5000,
          896554.1875,  858405.8125,  827126.0625,  825995.6875,  809556.0625],
        [1392681.8750, 1249326.8750, 1208968.0000, 1190799.1250, 1097089.8750,
         1085399.8750, 1083642.6250, 1008806.8125, 1001658.0625,  997994.6250],
        [1182639.2500,  955725.9375,  953011.8750,  882775.6875,  812580.5000,
          791240.3125,  786412.8750,  768814.2500,  757309.3750,  739048.4375],
        [1314884.2500, 1032168.6875, 1013659.6250,  999839.0000,  954434.3750,
          933677.4375,  924799.5625,  893530.8750,  839993.5625,  804475.8125],
        [1047501.6250,  996835.1250,  980684.5625,  967610.6875,  926468.8750,
          914600.8750,  898380.7500,  848164.0000,  840691.6250,  838263.4375],
        [1167386.2500, 1165262.8750,  819440.3125,  768676.4375,  745286.1250,
          710267.0625,  708408.1250,  688251.6250,  679107.3750,  675949.0000],
        [ 875947.6250,  822983.3750,  788052.5625,  766372.9375,  753281.4375,
          750096.3750,  737723.1875,  718910.8750,  717057.3750,  709329.5625],
        [1042289.8750, 1007908.6875,  959277.1250,  801860.8125,  789302.6250,
          766377.3125,  753395.6250,  716178.5000,  710322.5625,  701717.7500],
        [ 926445.0000,  863525.2500,  856813.3750,  847558.4375,  814945.9375,
          802759.1250,  774090.0000,  746167.3125,  742783.5000,  733467.7500],
        [1187326.8750, 1081312.7500,  976985.1875,  957294.0000,  928712.2500,
          920110.6875,  917921.3125,  871095.0625,  859435.4375,  854303.6250],
        [ 995232.7500,  984849.6875,  978311.0000,  965550.5000,  956265.6875,
          916728.9375,  906552.6250,  853179.2500,  840417.5000,  804897.9375],
        [1175965.6250, 1110656.3750, 1092993.0000, 1064498.6250, 1057373.8750,
         1050382.6250, 1048012.2500, 1022179.9375, 1009767.4375,  954302.3750],
        [ 940839.6250,  897914.7500,  794144.7500,  748573.5000,  742755.1250,
          740009.6875,  736120.8125,  726235.2500,  704057.1875,  702842.3125],
        [ 416604.9688,  413554.5625,  411753.7188,  369599.4688,  357745.6875,
          356553.5625,  356524.3125,  346483.1562,  334729.8125,  329561.0000],
        [ 927702.2500,  832666.1875,  828243.7500,  741323.5625,  651418.5000,
          617453.9375,  614769.8750,  613537.0000,  611742.1250,  607666.1875],
        [1041925.1875,  878283.8750,  866145.5000,  837958.8750,  781772.2500,
          738770.8125,  738353.8125,  704774.0000,  696642.1875,  689607.7500],
        [1004315.3750,  917386.6250,  880398.8750,  792068.5000,  775003.7500,
          729684.2500,  703541.0625,  689719.5625,  674131.0625,  670056.5000],
        [ 831233.3125,  698782.1250,  672128.1250,  626449.3125,  579223.3750,
          508914.6250,  507075.1562,  503391.1250,  459862.5938,  429958.5625],
        [1129521.3750, 1014754.5000,  839345.0000,  732944.0000,  695351.1875,
          661896.5000,  653583.3750,  646213.5000,  636772.8125,  629904.1875],
        [ 870458.9375,  842644.5625,  726508.2500,  603749.1875,  583748.5000,
          580693.5000,  550574.5000,  536688.6250,  530101.7500,  513117.9688],
        [1312840.6250, 1270866.0000, 1267110.7500, 1222246.3750, 1201317.0000,
         1199245.1250, 1182649.5000, 1170949.6250, 1152667.2500, 1148804.2500],
        [ 833997.3125,  589333.1875,  541323.1875,  442485.5312,  438877.1875,
          409546.1250,  351430.0000,  342441.5938,  341380.5625,  330590.6875],
        [ 708975.1250,  635729.7500,  547698.8750,  544884.0000,  544878.3125,
          534290.0000,  510530.5000,  503765.2500,  486211.7188,  471158.2188],
        [ 528765.3125,  525691.6875,  501988.4062,  488334.9062,  447611.3438,
          411659.9062,  385418.2500,  365196.3125,  364179.6875,  340529.2812],
        [ 705478.7500,  569945.4375,  485438.0000,  450512.8125,  422679.5312,
          392095.6250,  384913.9062,  362817.6875,  359393.9062,  312847.4375],
        [ 573700.0625,  520688.6875,  477716.0312,  436566.1875,  425813.5000,
          393192.0312,  381311.0938,  362053.1562,  342578.1562,  326947.0625],
        [ 274616.6562,  102172.8203,   90693.4375,   90050.9219,   89664.2734,
           89503.9141,   68526.6719,   66455.7109,   61786.4766,   59005.3281],
        [ 995891.5625,  987684.5625,  758152.6875,  739427.0625,  739243.6875,
          721020.8125,  713573.4375,  699206.7500,  685380.1250,  659449.9375],
        [ 327954.8125,  322871.2812,  262625.4688,  256249.0469,  253677.5000,
          226925.3750,  200146.2031,  176342.1094,  170150.9375,  149147.3750],
        [ 595003.6875,  547906.3125,  533365.9375,  477362.6250,  429647.4688,
          410114.0000,  380535.1250,  374016.8438,  360419.1562,  317711.0625],
        [ 885184.2500,  842802.0625,  819952.3750,  783284.1875,  751919.1250,
          749984.0625,  732651.9375,  707664.0000,  703378.6875,  697183.8125],
        [ 556088.8750,  463623.5938,  436091.0000,  432935.4688,  393916.7812,
          381474.0312,  371041.4375,  348412.5938,  306661.8750,  301205.5312],
        [ 630773.4375,  547031.7500,  506632.9062,  505688.2188,  476532.5000,
          458455.1875,  454527.0938,  439173.1562,  431528.1875,  371611.2188],
        [ 977258.2500,  645452.8750,  605439.1250,  600483.5625,  586790.3750,
          545051.8750,  424723.3438,  375174.6562,  349717.5625,  347585.5625],
        [ 686052.3750,  541959.5000,  478199.1875,  471255.2500,  390694.3750,
          371147.9375,  346490.7500,  299652.6562,  290829.3750,  286332.5000],
        [ 956959.9375,  863091.3750,  861429.4375,  833977.5000,  833382.8125,
          793883.5000,  782774.1875,  778813.5625,  772137.6875,  748682.7500],
        [ 894139.5000,  770335.6875,  762312.0625,  744785.8750,  735492.1250,
          682608.5000,  679387.1875,  676901.8125,  675118.0000,  672884.3125],
        [ 900286.4375,  728595.3125,  648331.4375,  636217.3750,  521489.7812,
          483685.1875,  479437.5938,  375278.0625,  370043.1250,  369956.7188],
        [ 828988.1250,  479186.1875,  415286.0312,  380776.5625,  347630.9688,
          344878.0000,  331944.0938,  307859.4688,  287300.2500,  281081.5312],
        [ 866204.1875,  591445.1250,  549377.5625,  515333.1250,  507597.7188,
          442322.2812,  416409.5312,  329132.0000,  313037.2812,  289134.1250],
        [ 875858.2500,  302831.7500,  299520.0625,  296888.9062,  283242.8750,
          243472.7969,  223684.3594,  212818.7344,  201569.6250,  193122.7969],
        [ 644137.0000,  588467.6875,  580605.4375,  550602.8750,  546664.6250,
          530060.3125,  517538.5938,  481685.5312,  481472.4375,  477375.3750],
        [ 278465.4688,  243431.0156,  202974.1250,  199110.3281,  182565.3281,
          181204.7344,  173211.2656,  168458.0312,  162781.7969,  150793.8438],
        [ 735923.6250,  606951.5000,  512697.3125,  512674.8125,  476964.8750,
          441568.2500,  440177.8438,  437215.3125,  410614.1562,  393442.2188],
        [ 659432.3125,  590741.0625,  516787.9062,  467917.1562,  439612.3125,
          432597.8750,  418032.9062,  381006.8438,  375792.7188,  375603.8750]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 430536.7188,       0.0000],
         [ 394578.1250,       0.0000],
         [ 186111.6250,       0.0000],
         ...,
         [ 132498.6562,       0.0000],
         [ 124103.6328,       0.0000],
         [ 123853.1094,       0.0000]],

        [[1177183.0000,       0.0000],
         [1150336.8750,       0.0000],
         [1134793.6250,       0.0000],
         ...,
         [1102410.5000,       0.0000],
         [1096329.5000,       0.0000],
         [1096089.0000,       0.0000]],

        [[ 854856.1250,       0.0000],
         [ 815169.0000,       0.0000],
         [ 677730.5625,       0.0000],
         ...,
         [ 533481.9375,       0.0000],
         [ 477378.0938,       0.0000],
         [ 439858.9062,       0.0000]],

        ...,

        [[ 278465.4688,       0.0000],
         [ 243431.0156,       0.0000],
         [ 202974.1250,       0.0000],
         ...,
         [ 168458.0312,       0.0000],
         [ 162781.7969,       0.0000],
         [ 150793.8438,       0.0000]],

        [[      0.0000,  735923.6250],
         [ 606951.5000,       0.0000],
         [ 512697.3125,       0.0000],
         ...,
         [ 437215.3125,       0.0000],
         [ 410614.1562,       0.0000],
         [ 393442.2188,       0.0000]],

        [[ 659432.3125,       0.0000],
         [      0.0000,  590741.0625],
         [ 516787.9062,       0.0000],
         ...,
         [ 381006.8438,       0.0000],
         [ 375792.7188,       0.0000],
         [ 375603.8750,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 1849963.5000,   147046.2344],
        [11233927.0000,        0.0000],
        [ 6294539.0000,        0.0000],
        [ 7861019.0000,        0.0000],
        [ 2757155.5000,  1140750.0000],
        [ 6552887.0000,   740784.3125],
        [ 8658350.0000,  2208704.2500],
        [ 7112043.5000,        0.0000],
        [ 9158733.0000,  1072310.6250],
        [ 6126716.5000,        0.0000],
        [ 9729153.0000,        0.0000],
        [ 6176375.5000,        0.0000],
        [ 7176094.0000,        0.0000],
        [12143410.0000,        0.0000],
        [ 9221741.0000,        0.0000],
        [ 7892033.0000,        0.0000],
        [11948913.0000,        0.0000],
        [11057483.0000,        0.0000],
        [ 8588147.0000,        0.0000],
        [ 9071529.0000,        0.0000],
        [11316368.0000,        0.0000],
        [ 8629559.0000,        0.0000],
        [ 9711463.0000,        0.0000],
        [ 9259202.0000,        0.0000],
        [ 8128035.5000,        0.0000],
        [ 7639756.0000,        0.0000],
        [ 7446770.5000,   801860.8125],
        [ 8108556.0000,        0.0000],
        [ 9554497.0000,        0.0000],
        [ 9201986.0000,        0.0000],
        [10586132.0000,        0.0000],
        [ 7733493.0000,        0.0000],
        [ 3358380.5000,   334729.8125],
        [ 4210767.5000,  2835755.7500],
        [ 5489181.5000,  2485053.0000],
        [ 5914603.5000,  1921702.0000],
        [ 3899473.0000,  1917545.2500],
        [ 4800659.5000,  2839627.0000],
        [ 3001269.0000,  3337016.7500],
        [12128696.0000,        0.0000],
        [ 4621405.5000,        0.0000],
        [ 5488122.0000,        0.0000],
        [ 4359375.0000,        0.0000],
        [ 4446123.0000,        0.0000],
        [ 2686918.5000,  1553647.5000],
        [  464396.3125,   528079.8750],
        [ 7699031.0000,        0.0000],
        [ 1687016.1250,   659073.9375],
        [ 1128369.6250,  3297712.5000],
        [ 6079283.0000,  1594721.2500],
        [ 3555360.0000,   436091.0000],
        [ 3490127.5000,  1331826.5000],
        [ 2773202.0000,  2684475.0000],
        [ 3213159.5000,   949454.4375],
        [ 8225132.5000,        0.0000],
        [ 6531653.0000,   762312.0625],
        [ 2490786.5000,  3022534.2500],
        [ 3110459.0000,   894472.2500],
        [ 2523832.0000,  2296161.0000],
        [ 1490474.7500,  1642535.5000],
        [ 4881071.5000,   517538.5938],
        [ 1406014.6250,   536981.3125],
        [ 3790738.0000,  1177491.8750],
        [ 3627171.5000,  1030353.3750]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 57.8125
Top1 accuracy for validation set is 57.8125 size is torch.Size([64, 1])
Epoch 156/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:11, 56.27s/it]  7%|▋         | 2/30 [00:59<11:42, 25.08s/it] 10%|█         | 3/30 [01:00<06:17, 13.97s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.75s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.87s/it] 20%|██        | 6/30 [01:02<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:04<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.4113311052322386
Epoch 157/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:07, 56.12s/it]  7%|▋         | 2/30 [00:57<11:12, 24.01s/it] 10%|█         | 3/30 [00:58<06:01, 13.39s/it] 13%|█▎        | 4/30 [01:01<03:58,  9.18s/it] 17%|█▋        | 5/30 [01:01<02:33,  6.14s/it] 20%|██        | 6/30 [01:02<01:43,  4.31s/it] 23%|██▎       | 7/30 [01:03<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:04<00:52,  2.38s/it] 30%|███       | 9/30 [01:04<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:05<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.29s/it] 40%|████      | 12/30 [01:07<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:07<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.23it/s] 60%|██████    | 18/30 [01:11<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.382898227373759
Epoch 158/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:18, 56.51s/it]  7%|▋         | 2/30 [00:59<11:39, 24.99s/it] 10%|█         | 3/30 [01:00<06:15, 13.92s/it] 13%|█▎        | 4/30 [01:00<03:46,  8.72s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.85s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.4192638794581094
Epoch 159/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:10, 62.41s/it]  7%|▋         | 2/30 [01:03<12:11, 26.14s/it] 10%|█         | 3/30 [01:04<06:41, 14.88s/it] 13%|█▎        | 4/30 [01:05<04:01,  9.30s/it] 17%|█▋        | 5/30 [01:07<02:45,  6.60s/it] 20%|██        | 6/30 [01:07<01:50,  4.61s/it] 23%|██▎       | 7/30 [01:08<01:17,  3.35s/it] 27%|██▋       | 8/30 [01:09<00:55,  2.52s/it] 30%|███       | 9/30 [01:10<00:41,  1.97s/it] 33%|███▎      | 10/30 [01:10<00:31,  1.59s/it] 37%|███▋      | 11/30 [01:11<00:25,  1.33s/it] 40%|████      | 12/30 [01:12<00:20,  1.15s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.03s/it] 47%|████▋     | 14/30 [01:13<00:15,  1.06it/s] 50%|█████     | 15/30 [01:14<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.18it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.87s/it]
Epoch loss is 3.3528745730717975
Epoch 160/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:07, 60.27s/it]  7%|▋         | 2/30 [01:01<11:47, 25.26s/it] 10%|█         | 3/30 [01:01<06:19, 14.07s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.81s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.90s/it] 20%|██        | 6/30 [01:04<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 3.330580981572469
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0106,  0.0107, -0.0283,  ..., -0.0071, -0.0231,  0.0338],
        [-0.0231,  0.0178, -0.0050,  ...,  0.0126, -0.0329, -0.0034],
        [-0.0551, -0.0198,  0.0031,  ...,  0.0590, -0.0263, -0.0468],
        ...,
        [ 0.0330, -0.0245, -0.0399,  ..., -0.0303, -0.0335, -0.0134],
        [-0.0361,  0.0001, -0.0089,  ..., -0.0238,  0.0192, -0.0212],
        [-0.0367,  0.0030,  0.0048,  ..., -0.0033,  0.0128, -0.0576]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9091, 0.8969, 0.8509, 0.8412, 0.8407, 0.8355, 0.8350, 0.8279, 0.8258,
         0.8215],
        [0.9761, 0.9755, 0.9751, 0.9747, 0.9746, 0.9738, 0.9738, 0.9732, 0.9732,
         0.9730],
        [0.9563, 0.9490, 0.9377, 0.9367, 0.9311, 0.9305, 0.9294, 0.9200, 0.9128,
         0.9103],
        [0.9817, 0.9633, 0.9607, 0.9535, 0.9421, 0.9392, 0.9338, 0.9299, 0.9297,
         0.9289],
        [0.9123, 0.8984, 0.8968, 0.8948, 0.8931, 0.8901, 0.8901, 0.8886, 0.8881,
         0.8782],
        [0.9546, 0.9471, 0.9441, 0.9426, 0.9370, 0.9362, 0.9342, 0.9327, 0.9327,
         0.9322],
        [0.9841, 0.9799, 0.9763, 0.9754, 0.9707, 0.9663, 0.9620, 0.9611, 0.9591,
         0.9578],
        [0.9681, 0.9659, 0.9570, 0.9448, 0.9408, 0.9297, 0.9227, 0.9163, 0.9154,
         0.9146],
        [0.9733, 0.9712, 0.9686, 0.9662, 0.9647, 0.9646, 0.9644, 0.9627, 0.9614,
         0.9612],
        [0.9728, 0.9375, 0.9363, 0.9321, 0.9321, 0.9275, 0.9132, 0.9098, 0.9082,
         0.9080],
        [0.9748, 0.9696, 0.9655, 0.9629, 0.9617, 0.9614, 0.9610, 0.9602, 0.9592,
         0.9592],
        [0.9485, 0.9483, 0.9458, 0.9417, 0.9403, 0.9388, 0.9174, 0.9159, 0.8955,
         0.8917],
        [0.9830, 0.9658, 0.9509, 0.9490, 0.9302, 0.9275, 0.9255, 0.9248, 0.9182,
         0.9156],
        [0.9854, 0.9818, 0.9818, 0.9806, 0.9806, 0.9785, 0.9784, 0.9781, 0.9765,
         0.9759],
        [0.9694, 0.9680, 0.9601, 0.9596, 0.9586, 0.9547, 0.9547, 0.9540, 0.9538,
         0.9531],
        [0.9640, 0.9524, 0.9517, 0.9516, 0.9496, 0.9458, 0.9456, 0.9438, 0.9382,
         0.9373],
        [0.9906, 0.9869, 0.9806, 0.9787, 0.9782, 0.9772, 0.9745, 0.9740, 0.9707,
         0.9701],
        [0.9850, 0.9782, 0.9765, 0.9741, 0.9719, 0.9686, 0.9680, 0.9675, 0.9672,
         0.9671],
        [0.9760, 0.9718, 0.9615, 0.9600, 0.9562, 0.9537, 0.9480, 0.9418, 0.9418,
         0.9384],
        [0.9660, 0.9658, 0.9597, 0.9582, 0.9572, 0.9538, 0.9522, 0.9509, 0.9501,
         0.9486],
        [0.9899, 0.9804, 0.9794, 0.9762, 0.9733, 0.9714, 0.9712, 0.9665, 0.9650,
         0.9642],
        [0.9774, 0.9633, 0.9621, 0.9586, 0.9503, 0.9497, 0.9481, 0.9477, 0.9463,
         0.9459],
        [0.9858, 0.9694, 0.9656, 0.9643, 0.9628, 0.9594, 0.9590, 0.9580, 0.9502,
         0.9498],
        [0.9702, 0.9661, 0.9653, 0.9635, 0.9618, 0.9605, 0.9588, 0.9536, 0.9535,
         0.9526],
        [0.9775, 0.9753, 0.9510, 0.9464, 0.9418, 0.9397, 0.9384, 0.9376, 0.9360,
         0.9354],
        [0.9554, 0.9523, 0.9475, 0.9471, 0.9471, 0.9454, 0.9449, 0.9443, 0.9441,
         0.9435],
        [0.9685, 0.9663, 0.9628, 0.9504, 0.9490, 0.9464, 0.9459, 0.9434, 0.9414,
         0.9407],
        [0.9600, 0.9574, 0.9543, 0.9540, 0.9535, 0.9516, 0.9457, 0.9455, 0.9451,
         0.9448],
        [0.9785, 0.9714, 0.9654, 0.9641, 0.9615, 0.9610, 0.9594, 0.9572, 0.9562,
         0.9561],
        [0.9648, 0.9637, 0.9635, 0.9629, 0.9611, 0.9604, 0.9587, 0.9531, 0.9530,
         0.9527],
        [0.9776, 0.9732, 0.9726, 0.9706, 0.9697, 0.9690, 0.9683, 0.9680, 0.9678,
         0.9628],
        [0.9612, 0.9571, 0.9485, 0.9457, 0.9448, 0.9434, 0.9416, 0.9411, 0.9408,
         0.9398],
        [0.9031, 0.9027, 0.9026, 0.8964, 0.8936, 0.8933, 0.8930, 0.8867, 0.8862,
         0.8862],
        [0.9602, 0.9527, 0.9521, 0.9427, 0.9348, 0.9328, 0.9314, 0.9312, 0.9308,
         0.9296],
        [0.9685, 0.9567, 0.9536, 0.9524, 0.9468, 0.9452, 0.9415, 0.9406, 0.9397,
         0.9388],
        [0.9661, 0.9569, 0.9525, 0.9524, 0.9449, 0.9379, 0.9374, 0.9370, 0.9365,
         0.9345],
        [0.9539, 0.9348, 0.9319, 0.9300, 0.9214, 0.9148, 0.9109, 0.9099, 0.9068,
         0.9053],
        [0.9748, 0.9664, 0.9537, 0.9414, 0.9384, 0.9361, 0.9356, 0.9345, 0.9337,
         0.9318],
        [0.9563, 0.9535, 0.9433, 0.9280, 0.9270, 0.9233, 0.9216, 0.9213, 0.9178,
         0.9174],
        [0.9839, 0.9832, 0.9824, 0.9789, 0.9785, 0.9781, 0.9779, 0.9758, 0.9756,
         0.9756],
        [0.9519, 0.9305, 0.9224, 0.9107, 0.9066, 0.8989, 0.8969, 0.8950, 0.8919,
         0.8896],
        [0.9429, 0.9329, 0.9245, 0.9234, 0.9200, 0.9193, 0.9182, 0.9138, 0.9131,
         0.9126],
        [0.9186, 0.9184, 0.9165, 0.9090, 0.9071, 0.8989, 0.8974, 0.8933, 0.8910,
         0.8906],
        [0.9430, 0.9251, 0.9174, 0.9125, 0.9080, 0.8970, 0.8965, 0.8931, 0.8902,
         0.8884],
        [0.9245, 0.9187, 0.9127, 0.9083, 0.9048, 0.9011, 0.8937, 0.8909, 0.8887,
         0.8873],
        [0.8733, 0.8090, 0.8024, 0.7980, 0.7943, 0.7921, 0.7720, 0.7685, 0.7677,
         0.7659],
        [0.9654, 0.9637, 0.9437, 0.9431, 0.9429, 0.9421, 0.9410, 0.9401, 0.9377,
         0.9349],
        [0.8849, 0.8794, 0.8716, 0.8688, 0.8664, 0.8606, 0.8513, 0.8393, 0.8359,
         0.8280],
        [0.9272, 0.9237, 0.9164, 0.9091, 0.9034, 0.9032, 0.8961, 0.8935, 0.8872,
         0.8826],
        [0.9542, 0.9529, 0.9493, 0.9472, 0.9457, 0.9423, 0.9413, 0.9390, 0.9383,
         0.9377],
        [0.9210, 0.9064, 0.9048, 0.9044, 0.8982, 0.8976, 0.8938, 0.8862, 0.8830,
         0.8781],
        [0.9329, 0.9201, 0.9198, 0.9192, 0.9153, 0.9113, 0.9091, 0.9041, 0.9009,
         0.8990],
        [0.9641, 0.9353, 0.9302, 0.9286, 0.9252, 0.9243, 0.9034, 0.8991, 0.8931,
         0.8924],
        [0.9389, 0.9222, 0.9156, 0.9087, 0.8999, 0.8968, 0.8933, 0.8810, 0.8808,
         0.8773],
        [0.9649, 0.9562, 0.9552, 0.9535, 0.9532, 0.9495, 0.9480, 0.9475, 0.9472,
         0.9455],
        [0.9587, 0.9464, 0.9455, 0.9435, 0.9428, 0.9386, 0.9370, 0.9366, 0.9362,
         0.9362],
        [0.9588, 0.9439, 0.9335, 0.9332, 0.9188, 0.9122, 0.9116, 0.8960, 0.8951,
         0.8942],
        [0.9532, 0.9163, 0.9049, 0.8964, 0.8919, 0.8905, 0.8879, 0.8826, 0.8785,
         0.8778],
        [0.9573, 0.9263, 0.9226, 0.9199, 0.9188, 0.9086, 0.9011, 0.8890, 0.8824,
         0.8791],
        [0.9569, 0.8797, 0.8795, 0.8782, 0.8767, 0.8639, 0.8613, 0.8598, 0.8521,
         0.8491],
        [0.9345, 0.9299, 0.9282, 0.9238, 0.9234, 0.9211, 0.9161, 0.9146, 0.9145,
         0.9133],
        [0.8738, 0.8639, 0.8527, 0.8497, 0.8449, 0.8447, 0.8406, 0.8399, 0.8366,
         0.8321],
        [0.9438, 0.9276, 0.9190, 0.9168, 0.9136, 0.9077, 0.9055, 0.9049, 0.9040,
         0.9000],
        [0.9319, 0.9281, 0.9127, 0.9105, 0.9082, 0.9029, 0.8996, 0.8938, 0.8928,
         0.8911]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 1, 1, 1, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 1, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 1, 1, 1, 1, 1, 1],
        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 1],
        [0, 0, 1, 0, 1, 0, 1, 1, 1, 1],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 1, 0, 1],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 1, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 1, 1, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 436659.8750,  367131.2188,  190242.6094,  165586.5156,  164337.4062,
          152576.2969,  151556.9375,  136821.8750,  132857.3594,  124964.0859],
        [1136809.3750, 1127923.8750, 1121427.2500, 1115290.5000, 1113748.2500,
         1101102.3750, 1100444.1250, 1091104.8750, 1090605.5000, 1088345.6250],
        [ 857676.7500,  772349.0625,  657441.1875,  648204.1250,  598442.5625,
          593052.6250,  583396.7500,  510148.4062,  460509.4688,  444214.4375],
        [1232274.3750,  946715.2500,  913059.1875,  823668.0625,  700260.4375,
          671057.3125,  621564.1250,  588247.1875,  585971.1875,  579232.1875],
        [ 457118.4062,  374906.4062,  366640.6875,  355917.2500,  347685.0312,
          333099.9688,  332738.6875,  325799.8125,  323350.7500,  281042.6875],
        [ 837201.6875,  752018.8750,  719616.6875,  705322.6875,  650684.0000,
          643098.3750,  625092.8125,  612117.3750,  611547.3125,  607771.6250],
        [1275438.6250, 1201452.2500, 1140955.0000, 1125580.2500, 1052303.6250,
          989484.3750,  930396.6250,  917615.0625,  892771.0625,  875513.3125],
        [1015291.7500,  983230.8750,  865487.4375,  727404.6250,  687140.6250,
          586472.0625,  530150.3125,  484340.6562,  478222.4688,  472317.0938],
        [1092661.6250, 1060267.7500, 1021818.3125,  987202.4375,  967173.3750,
          965113.1250,  962469.7500,  939175.8750,  922607.0000,  919344.9375],
        [1085176.2500,  655306.6250,  644340.9375,  606607.7500,  606222.5625,
          568435.8750,  463011.1562,  441341.7812,  431458.6562,  430241.1875],
        [1115800.0000, 1036204.5625,  977467.9375,  942480.4375,  925513.3750,
          922142.5625,  916333.8750,  906582.8750,  892982.2500,  892899.6875],
        [ 766445.2500,  764867.3750,  737294.8125,  696310.0625,  682516.6875,
          667738.9375,  491818.2812,  480992.8438,  359868.5938,  340589.7188],
        [1256115.1250,  981390.0625,  793959.1875,  772164.9375,  590656.6250,
          568208.7500,  552001.8750,  546720.9375,  497308.6875,  479543.6562],
        [1299100.6250, 1234769.5000, 1233737.2500, 1213285.5000, 1212550.8750,
         1177810.7500, 1175019.3750, 1170954.2500, 1143380.8750, 1134361.7500],
        [1033601.9375, 1013551.3125,  905011.5625,  898283.0625,  885670.5625,
          838341.8125,  837247.1875,  829049.0625,  827328.8125,  818344.6250],
        [ 957191.7500,  810514.0000,  802684.8125,  801629.1250,  778518.7500,
          737773.1250,  735755.1875,  717389.7500,  661466.8125,  653034.5000],
        [1399593.5000, 1328032.1250, 1212547.5000, 1181281.1250, 1171289.2500,
         1155741.6250, 1111752.1250, 1103821.1250, 1053652.2500, 1044131.5000],
        [1291833.7500, 1172803.8750, 1144181.5000, 1105136.7500, 1071091.3750,
         1021440.3125, 1012439.4375, 1005520.9375, 1001068.8750, 1000675.6250],
        [1136350.8750, 1069939.7500,  923763.8750,  903753.1875,  855755.0625,
          826225.7500,  761527.3125,  696859.4375,  696509.3125,  663855.6250],
        [ 984117.4375,  982207.4375,  899763.6875,  880459.3125,  867949.0000,
          827592.3750,  808083.6250,  793017.0625,  784458.6250,  768344.4375],
        [1386047.6250, 1208678.6250, 1192845.1250, 1139506.6250, 1092101.1250,
         1063999.3750, 1060631.8750,  991650.5000,  971068.0625,  959598.3125],
        [1158276.1250,  947150.5625,  931563.3125,  886449.6875,  786755.7500,
          779869.6875,  762167.3750,  757781.8125,  742728.1875,  739199.3125],
        [1306373.5000, 1034064.3750,  979537.7500,  960598.1875,  940076.3750,
          895710.7500,  891024.8750,  878391.9375,  785486.5000,  780715.0625],
        [1045362.9375,  986300.9375,  975218.3125,  950285.5000,  927021.2500,
          910702.5000,  888072.6250,  825328.7500,  823195.3125,  813059.5625],
        [1160971.1250, 1123978.8750,  794455.3125,  744369.8125,  696880.6875,
          676030.3125,  663726.4375,  656068.2500,  641179.1875,  636015.3750],
        [ 846820.7500,  810140.7500,  755995.3125,  752058.2500,  751536.3125,
          733550.3125,  728096.5625,  722413.9375,  720038.8750,  713971.6875],
        [1020901.7500,  989044.7500,  940770.6250,  787510.1875,  771942.6250,
          744445.0625,  738483.4375,  712503.8750,  692965.3750,  685769.7500],
        [ 904333.4375,  870702.2500,  833118.1250,  829041.1250,  823282.4375,
          802037.5000,  737238.6250,  734620.0000,  730705.1250,  727539.9375],
        [1176674.5000, 1064087.6250,  976011.1250,  958141.5625,  923753.3125,
          917225.6875,  896630.3125,  867728.8750,  856118.3125,  855058.4375],
        [ 968128.5625,  952681.9375,  950296.4375,  941317.1250,  918554.5000,
          908622.1875,  886767.5625,  818592.0625,  817980.2500,  813752.2500],
        [1161930.3750, 1090721.0000, 1081210.6250, 1052042.7500, 1038347.1875,
         1027116.6250, 1017578.4375, 1012433.5625, 1010430.2500,  940024.4375],
        [ 919502.8125,  867393.7500,  767193.3750,  736734.6250,  727285.3125,
          712427.0000,  694345.3125,  690303.8750,  686516.3750,  677615.5625],
        [ 400921.8125,  398356.4688,  397961.9062,  364121.7188,  350027.5312,
          348594.4062,  347216.8125,  317302.5938,  315103.9375,  314836.9062],
        [ 906646.8750,  814613.3750,  807035.4375,  706082.5625,  630568.9375,
          613094.7500,  600718.3750,  598907.3125,  595665.6250,  585084.4375],
        [1020365.4375,  861543.6250,  824572.6250,  810466.0625,  748012.5625,
          731793.6875,  694167.8125,  685448.8125,  676191.5000,  667245.5625],
        [ 985380.5000,  864813.3125,  811880.1875,  810200.2500,  728368.1250,
          659054.5000,  654192.6250,  650844.8125,  645974.4375,  627970.3750],
        [ 828486.2500,  630477.5625,  604737.3750,  588488.5000,  520762.1875,
          473580.9062,  448010.6250,  441872.4062,  422817.0000,  413400.7812],
        [1116334.3750,  990778.0000,  825675.9375,  692750.6250,  663419.5000,
          642728.0625,  637318.9375,  627942.2500,  620591.6250,  603876.3750],
        [ 857282.5625,  823197.6875,  711469.0000,  572254.6875,  564052.0625,
          534801.3125,  521877.3125,  519779.7188,  494911.8438,  491817.3438],
        [1272334.6250, 1258562.5000, 1245048.6250, 1184606.7500, 1176415.3750,
         1169888.1250, 1167097.8750, 1132391.3750, 1128835.3750, 1128583.5000],
        [ 804554.0625,  592697.5000,  528024.0625,  446783.9688,  421532.2500,
          377722.5312,  367163.0938,  357078.3125,  341786.7812,  330373.5312],
        [ 707717.3125,  613362.6250,  544453.3750,  535665.9375,  510176.6562,
          504979.3125,  497658.3438,  467167.1562,  462554.3750,  459348.8750],
        [ 500211.5938,  498472.4688,  485295.8750,  436283.1562,  424526.9062,
          377311.3750,  369339.7812,  348378.4062,  337409.1562,  335496.1562],
        [ 708579.0625,  548794.2500,  491939.2500,  458805.0938,  430160.7812,
          367305.9688,  364629.0625,  347557.0625,  333391.0938,  325036.0625],
        [ 544427.9375,  500760.9688,  460043.2812,  431563.1562,  410862.1250,
          389558.5312,  350407.9375,  336858.7188,  326218.5938,  319835.1250],
        [ 262039.7812,  104477.9375,   95097.3203,   89263.7812,   84714.5859,
           82073.5859,   61566.6719,   58630.6328,   57905.7422,   56448.8164],
        [ 976598.5625,  953290.0000,  716236.5625,  709899.3125,  708054.8750,
          699652.2500,  689148.8125,  680261.1250,  657302.0000,  631608.3750],
        [ 308916.1875,  285875.5000,  255527.2031,  245442.0625,  237309.6094,
          218444.1719,  191379.5625,  161207.4062,  153548.7500,  137104.5312],
        [ 565726.9375,  537836.8750,  484944.2812,  436910.6250,  402326.3438,
          401660.0312,  362818.0625,  349416.8438,  319438.8438,  299047.1250],
        [ 831385.5000,  816133.6250,  775288.3750,  752226.8750,  736628.5625,
          702122.0625,  692292.8750,  669470.8125,  663196.8750,  657139.6250],
        [ 518041.2812,  420414.5938,  410540.9375,  408270.1562,  373923.0312,
          370799.8125,  351150.9375,  314705.7188,  300796.1875,  280416.4688],
        [ 613489.0000,  511397.8438,  508850.0938,  504569.1875,  477039.5000,
          450802.8750,  437012.7500,  406411.3438,  388608.9688,  377819.8125],
        [ 958041.1250,  634841.5625,  590600.2500,  577243.7500,  549359.8125,
          542389.7500,  402791.6250,  378564.5625,  347542.8125,  344177.1875],
        [ 668518.8750,  526955.0625,  479252.9062,  434330.3750,  382955.7500,
          366636.1562,  348299.3125,  292516.1250,  291665.9688,  277313.4375],
        [ 969173.3125,  855941.9375,  844063.2500,  823105.0625,  819488.7500,
          778324.2500,  761644.2500,  755518.1875,  752296.4375,  734885.5625],
        [ 887320.8750,  744541.6250,  734661.3750,  713836.1875,  706505.5625,
          665739.8125,  650643.6875,  646860.9375,  643179.3750,  643059.7500],
        [ 888288.6250,  718332.4375,  618905.3750,  615991.1875,  501603.1875,
          456849.5000,  452334.2812,  362086.2812,  357508.9688,  352772.5000],
        [ 820541.3125,  484189.5938,  411085.9062,  364502.5000,  341668.1562,
          334800.3438,  322664.4375,  299088.7812,  281965.9062,  279311.5000],
        [ 869783.5625,  558052.0000,  529334.9375,  509504.6562,  501344.9062,
          433719.0000,  389835.3750,  327609.7188,  298302.2812,  284726.4688],
        [ 864561.0625,  286913.0625,  286085.2188,  280989.3438,  274771.7188,
          229078.6562,  220484.6094,  216094.9219,  193492.9844,  185341.2969],
        [ 627826.1250,  588105.2500,  573995.5625,  539069.3125,  535581.1875,
          518400.0625,  483002.0312,  472295.0312,  471982.0938,  463819.5000],
        [ 263810.7812,  229072.9844,  195262.3594,  186994.9531,  174440.7969,
          174002.1562,  164069.4688,  162481.0781,  155103.5312,  145396.7188],
        [ 717039.5625,  568926.6875,  503084.9375,  487240.8125,  465819.1875,
          428105.0938,  414743.3750,  411458.1250,  405850.1250,  383749.4688],
        [ 604852.1875,  572786.5000,  459640.6875,  445474.4062,  431116.8750,
          399609.8438,  381231.8125,  350799.4688,  346060.4688,  337813.5625]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 436659.8750,       0.0000],
         [ 367131.2188,       0.0000],
         [ 190242.6094,       0.0000],
         ...,
         [ 136821.8750,       0.0000],
         [ 132857.3594,       0.0000],
         [ 124964.0859,       0.0000]],

        [[1136809.3750,       0.0000],
         [1127923.8750,       0.0000],
         [1121427.2500,       0.0000],
         ...,
         [1091104.8750,       0.0000],
         [1090605.5000,       0.0000],
         [1088345.6250,       0.0000]],

        [[ 857676.7500,       0.0000],
         [ 772349.0625,       0.0000],
         [ 657441.1875,       0.0000],
         ...,
         [ 510148.4062,       0.0000],
         [ 460509.4688,       0.0000],
         [      0.0000,  444214.4375]],

        ...,

        [[ 263810.7812,       0.0000],
         [ 229072.9844,       0.0000],
         [ 195262.3594,       0.0000],
         ...,
         [      0.0000,  162481.0781],
         [ 155103.5312,       0.0000],
         [ 145396.7188,       0.0000]],

        [[      0.0000,  717039.5625],
         [ 568926.6875,       0.0000],
         [ 503084.9375,       0.0000],
         ...,
         [ 411458.1250,       0.0000],
         [ 405850.1250,       0.0000],
         [ 383749.4688,       0.0000]],

        [[ 604852.1875,       0.0000],
         [      0.0000,  572786.5000],
         [ 459640.6875,       0.0000],
         ...,
         [ 350799.4688,       0.0000],
         [ 346060.4688,       0.0000],
         [ 337813.5625,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 1871177.2500,   151556.9375],
        [11086802.0000,        0.0000],
        [ 5681221.0000,   444214.4375],
        [ 7662049.0000,        0.0000],
        [ 2486433.2500,  1011866.3750],
        [ 6044855.0000,   719616.6875],
        [ 8271071.0000,  2130439.5000],
        [ 6830058.0000,        0.0000],
        [ 8777566.0000,  1060267.7500],
        [ 5932143.0000,        0.0000],
        [ 9528407.0000,        0.0000],
        [ 5988442.5000,        0.0000],
        [ 7038070.0000,        0.0000],
        [11994971.0000,        0.0000],
        [ 8886430.0000,        0.0000],
        [ 7655958.0000,        0.0000],
        [11761842.0000,        0.0000],
        [10826193.0000,        0.0000],
        [ 8534540.0000,        0.0000],
        [ 8595992.0000,        0.0000],
        [11066127.0000,        0.0000],
        [ 8491942.0000,        0.0000],
        [ 9451980.0000,        0.0000],
        [ 9144548.0000,        0.0000],
        [ 7793675.0000,        0.0000],
        [ 7534623.0000,        0.0000],
        [ 7312394.5000,   771942.6250],
        [ 7992619.0000,        0.0000],
        [ 9491430.0000,        0.0000],
        [ 8976693.0000,        0.0000],
        [10431835.0000,        0.0000],
        [ 7479318.5000,        0.0000],
        [ 3237141.5000,   317302.5938],
        [ 4119065.2500,  2739352.5000],
        [ 5329083.0000,  2390724.7500],
        [ 5588485.0000,  1850193.7500],
        [ 3119441.2500,  2253192.5000],
        [ 4650884.0000,  2770532.0000],
        [ 2860182.0000,  3231261.5000],
        [11863764.0000,        0.0000],
        [ 4567716.0000,        0.0000],
        [ 5303084.0000,        0.0000],
        [ 4112724.5000,        0.0000],
        [ 4376197.5000,        0.0000],
        [ 2579171.7500,  1491364.6250],
        [  431973.2188,   520245.6562],
        [ 7422052.0000,        0.0000],
        [ 1574898.7500,   619856.1875],
        [ 1050671.2500,  3109454.7500],
        [ 5727525.0000,  1568360.5000],
        [ 3328644.5000,   420414.5938],
        [ 3349771.0000,  1326230.3750],
        [ 2712516.2500,  2613036.0000],
        [ 3154861.0000,   913583.2500],
        [ 8094441.0000,        0.0000],
        [ 6291807.5000,   744541.6250],
        [ 2387201.2500,  2937471.0000],
        [ 3044543.0000,   895275.5000],
        [ 2460316.0000,  2241897.0000],
        [ 1442699.1250,  1595113.7500],
        [ 4791074.0000,   483002.0312],
        [ 1339710.7500,   510924.0000],
        [ 3640872.7500,  1145144.6250],
        [ 3325482.5000,  1003903.3750]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 57.8125
Top1 accuracy for validation set is 57.8125 size is torch.Size([64, 1])
Epoch 161/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:43, 57.37s/it]  7%|▋         | 2/30 [00:58<11:13, 24.06s/it] 10%|█         | 3/30 [01:01<06:35, 14.65s/it] 13%|█▎        | 4/30 [01:02<03:58,  9.16s/it] 17%|█▋        | 5/30 [01:03<02:33,  6.13s/it] 20%|██        | 6/30 [01:03<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:04<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:05<00:52,  2.38s/it] 30%|███       | 9/30 [01:06<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 3.330369162559509
Epoch 162/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:53, 57.71s/it]  7%|▋         | 2/30 [00:58<11:17, 24.20s/it] 10%|█         | 3/30 [00:59<06:08, 13.66s/it] 13%|█▎        | 4/30 [01:00<03:44,  8.62s/it] 17%|█▋        | 5/30 [01:02<02:36,  6.27s/it] 20%|██        | 6/30 [01:03<01:45,  4.39s/it] 23%|██▎       | 7/30 [01:04<01:13,  3.20s/it] 27%|██▋       | 8/30 [01:04<00:53,  2.42s/it] 30%|███       | 9/30 [01:05<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.30s/it] 40%|████      | 12/30 [01:07<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.07it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 3.286184557278951
Epoch 163/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:14, 58.43s/it]  7%|▋         | 2/30 [01:01<12:07, 25.99s/it] 10%|█         | 3/30 [01:02<06:30, 14.46s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.05s/it] 17%|█▋        | 5/30 [01:03<02:31,  6.06s/it] 20%|██        | 6/30 [01:04<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 3.3149214506149294
Epoch 164/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:09, 58.27s/it]  7%|▋         | 2/30 [01:00<11:50, 25.39s/it] 10%|█         | 3/30 [01:01<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.93s/it] 20%|██        | 6/30 [01:03<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 3.3114905754725137
Epoch 165/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:54, 61.87s/it]  7%|▋         | 2/30 [01:02<12:05, 25.92s/it] 10%|█         | 3/30 [01:03<06:29, 14.42s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.03s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.04s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 3.288334043820699
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0119,  0.0108, -0.0261,  ..., -0.0042, -0.0230,  0.0338],
        [-0.0200,  0.0182, -0.0030,  ...,  0.0131, -0.0319, -0.0036],
        [-0.0545, -0.0227,  0.0044,  ...,  0.0610, -0.0283, -0.0447],
        ...,
        [ 0.0345, -0.0242, -0.0372,  ..., -0.0287, -0.0350, -0.0134],
        [-0.0350, -0.0012, -0.0089,  ..., -0.0210,  0.0181, -0.0204],
        [-0.0363,  0.0022,  0.0059,  ..., -0.0014,  0.0125, -0.0578]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9124, 0.8918, 0.8535, 0.8481, 0.8403, 0.8393, 0.8393, 0.8325, 0.8317,
         0.8253],
        [0.9763, 0.9758, 0.9741, 0.9737, 0.9736, 0.9735, 0.9730, 0.9725, 0.9714,
         0.9709],
        [0.9542, 0.9436, 0.9350, 0.9335, 0.9277, 0.9264, 0.9261, 0.9164, 0.9103,
         0.9071],
        [0.9811, 0.9612, 0.9593, 0.9521, 0.9380, 0.9376, 0.9332, 0.9290, 0.9271,
         0.9265],
        [0.9043, 0.8877, 0.8873, 0.8870, 0.8870, 0.8863, 0.8846, 0.8808, 0.8772,
         0.8747],
        [0.9492, 0.9419, 0.9410, 0.9358, 0.9336, 0.9282, 0.9271, 0.9270, 0.9244,
         0.9239],
        [0.9813, 0.9773, 0.9739, 0.9704, 0.9658, 0.9637, 0.9585, 0.9573, 0.9564,
         0.9542],
        [0.9663, 0.9646, 0.9552, 0.9402, 0.9382, 0.9271, 0.9175, 0.9142, 0.9091,
         0.9080],
        [0.9723, 0.9702, 0.9657, 0.9626, 0.9624, 0.9608, 0.9604, 0.9595, 0.9588,
         0.9587],
        [0.9719, 0.9371, 0.9346, 0.9305, 0.9297, 0.9263, 0.9140, 0.9083, 0.9074,
         0.9068],
        [0.9734, 0.9659, 0.9647, 0.9629, 0.9613, 0.9596, 0.9593, 0.9592, 0.9574,
         0.9569],
        [0.9462, 0.9461, 0.9424, 0.9403, 0.9392, 0.9386, 0.9135, 0.9121, 0.8953,
         0.8891],
        [0.9817, 0.9623, 0.9498, 0.9485, 0.9291, 0.9261, 0.9244, 0.9229, 0.9168,
         0.9111],
        [0.9836, 0.9804, 0.9803, 0.9794, 0.9779, 0.9779, 0.9771, 0.9770, 0.9760,
         0.9741],
        [0.9671, 0.9667, 0.9588, 0.9579, 0.9570, 0.9544, 0.9523, 0.9523, 0.9518,
         0.9511],
        [0.9623, 0.9505, 0.9498, 0.9492, 0.9464, 0.9445, 0.9437, 0.9402, 0.9370,
         0.9369],
        [0.9900, 0.9861, 0.9802, 0.9782, 0.9776, 0.9773, 0.9725, 0.9717, 0.9697,
         0.9685],
        [0.9846, 0.9768, 0.9757, 0.9731, 0.9700, 0.9671, 0.9660, 0.9659, 0.9659,
         0.9656],
        [0.9745, 0.9702, 0.9623, 0.9584, 0.9574, 0.9524, 0.9496, 0.9391, 0.9391,
         0.9374],
        [0.9636, 0.9630, 0.9571, 0.9545, 0.9528, 0.9485, 0.9485, 0.9484, 0.9469,
         0.9453],
        [0.9902, 0.9783, 0.9776, 0.9738, 0.9724, 0.9708, 0.9692, 0.9662, 0.9630,
         0.9625],
        [0.9755, 0.9615, 0.9602, 0.9589, 0.9482, 0.9480, 0.9473, 0.9459, 0.9458,
         0.9441],
        [0.9851, 0.9689, 0.9628, 0.9616, 0.9608, 0.9590, 0.9569, 0.9545, 0.9509,
         0.9469],
        [0.9697, 0.9649, 0.9647, 0.9620, 0.9616, 0.9605, 0.9584, 0.9525, 0.9524,
         0.9518],
        [0.9775, 0.9730, 0.9486, 0.9438, 0.9374, 0.9365, 0.9348, 0.9334, 0.9320,
         0.9309],
        [0.9519, 0.9503, 0.9472, 0.9466, 0.9457, 0.9449, 0.9447, 0.9444, 0.9442,
         0.9441],
        [0.9671, 0.9647, 0.9611, 0.9506, 0.9467, 0.9454, 0.9449, 0.9430, 0.9400,
         0.9397],
        [0.9594, 0.9574, 0.9538, 0.9532, 0.9520, 0.9519, 0.9461, 0.9440, 0.9436,
         0.9414],
        [0.9776, 0.9700, 0.9645, 0.9638, 0.9613, 0.9603, 0.9589, 0.9579, 0.9565,
         0.9533],
        [0.9647, 0.9619, 0.9599, 0.9592, 0.9587, 0.9574, 0.9573, 0.9529, 0.9500,
         0.9490],
        [0.9768, 0.9720, 0.9710, 0.9698, 0.9693, 0.9682, 0.9682, 0.9669, 0.9649,
         0.9633],
        [0.9593, 0.9553, 0.9468, 0.9452, 0.9431, 0.9409, 0.9390, 0.9389, 0.9386,
         0.9369],
        [0.9032, 0.9019, 0.8985, 0.8984, 0.8919, 0.8908, 0.8883, 0.8846, 0.8821,
         0.8794],
        [0.9579, 0.9519, 0.9495, 0.9390, 0.9337, 0.9334, 0.9295, 0.9292, 0.9275,
         0.9271],
        [0.9657, 0.9550, 0.9509, 0.9494, 0.9434, 0.9431, 0.9400, 0.9390, 0.9381,
         0.9364],
        [0.9637, 0.9537, 0.9527, 0.9467, 0.9408, 0.9371, 0.9325, 0.9316, 0.9316,
         0.9314],
        [0.9538, 0.9280, 0.9242, 0.9229, 0.9144, 0.9092, 0.9036, 0.9028, 0.9010,
         0.8992],
        [0.9741, 0.9646, 0.9515, 0.9380, 0.9365, 0.9321, 0.9314, 0.9314, 0.9309,
         0.9298],
        [0.9549, 0.9507, 0.9421, 0.9265, 0.9231, 0.9186, 0.9171, 0.9163, 0.9161,
         0.9129],
        [0.9815, 0.9814, 0.9810, 0.9767, 0.9760, 0.9758, 0.9753, 0.9743, 0.9742,
         0.9741],
        [0.9484, 0.9306, 0.9207, 0.9110, 0.9021, 0.8992, 0.8978, 0.8931, 0.8927,
         0.8899],
        [0.9425, 0.9300, 0.9252, 0.9226, 0.9169, 0.9163, 0.9129, 0.9110, 0.9104,
         0.9082],
        [0.9142, 0.9135, 0.9117, 0.9019, 0.9019, 0.8923, 0.8908, 0.8896, 0.8877,
         0.8859],
        [0.9432, 0.9212, 0.9189, 0.9127, 0.9073, 0.8938, 0.8933, 0.8892, 0.8877,
         0.8834],
        [0.9199, 0.9149, 0.9094, 0.9076, 0.9033, 0.8979, 0.8878, 0.8867, 0.8858,
         0.8846],
        [0.8701, 0.8132, 0.8069, 0.7964, 0.7893, 0.7887, 0.7731, 0.7635, 0.7632,
         0.7609],
        [0.9644, 0.9605, 0.9412, 0.9409, 0.9403, 0.9396, 0.9391, 0.9377, 0.9357,
         0.9345],
        [0.8797, 0.8709, 0.8692, 0.8671, 0.8589, 0.8540, 0.8500, 0.8333, 0.8292,
         0.8180],
        [0.9227, 0.9220, 0.9108, 0.9000, 0.8999, 0.8972, 0.8938, 0.8867, 0.8820,
         0.8788],
        [0.9501, 0.9492, 0.9465, 0.9446, 0.9442, 0.9368, 0.9352, 0.9341, 0.9333,
         0.9328],
        [0.9146, 0.9012, 0.8957, 0.8949, 0.8939, 0.8933, 0.8890, 0.8827, 0.8784,
         0.8721],
        [0.9308, 0.9204, 0.9174, 0.9157, 0.9133, 0.9095, 0.9046, 0.8996, 0.8981,
         0.8950],
        [0.9630, 0.9342, 0.9295, 0.9251, 0.9249, 0.9221, 0.9019, 0.8994, 0.8921,
         0.8911],
        [0.9388, 0.9197, 0.9147, 0.9046, 0.8985, 0.8963, 0.8929, 0.8817, 0.8785,
         0.8761],
        [0.9653, 0.9553, 0.9519, 0.9517, 0.9517, 0.9477, 0.9457, 0.9453, 0.9452,
         0.9444],
        [0.9585, 0.9454, 0.9420, 0.9414, 0.9408, 0.9363, 0.9347, 0.9344, 0.9343,
         0.9333],
        [0.9588, 0.9417, 0.9313, 0.9304, 0.9157, 0.9095, 0.9064, 0.8953, 0.8934,
         0.8931],
        [0.9528, 0.9163, 0.9042, 0.8941, 0.8912, 0.8882, 0.8862, 0.8813, 0.8785,
         0.8776],
        [0.9558, 0.9222, 0.9197, 0.9192, 0.9154, 0.9074, 0.8990, 0.8885, 0.8797,
         0.8780],
        [0.9559, 0.8760, 0.8757, 0.8757, 0.8736, 0.8635, 0.8604, 0.8578, 0.8486,
         0.8453],
        [0.9328, 0.9292, 0.9276, 0.9224, 0.9220, 0.9196, 0.9131, 0.9130, 0.9118,
         0.9113],
        [0.8714, 0.8595, 0.8520, 0.8466, 0.8441, 0.8405, 0.8389, 0.8361, 0.8335,
         0.8311],
        [0.9412, 0.9219, 0.9163, 0.9121, 0.9115, 0.9064, 0.9027, 0.9015, 0.8999,
         0.8976],
        [0.9261, 0.9252, 0.9087, 0.9077, 0.9041, 0.8994, 0.8935, 0.8889, 0.8889,
         0.8875]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 0, 1],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 1, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 0, 0, 1, 1, 1, 1, 1, 1, 1],
        [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 1, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 1, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 457699.4375,  340931.5938,  197251.5312,  182609.5625,  163414.6875,
          161099.2188,  161037.4688,  146263.0156,  144532.6719,  131852.8906],
        [1139931.6250, 1131826.7500, 1105307.5000, 1099196.0000, 1098271.7500,
         1095985.6250, 1088721.5000, 1079817.3750, 1063096.6250, 1055296.5000],
        [ 831901.8750,  715267.2500,  632245.3125,  618918.3750,  569509.6875,
          559577.1250,  557033.6875,  484561.4688,  444393.6562,  424722.0938],
        [1222396.7500,  919307.2500,  895340.8750,  806913.0625,  660205.6875,
          656149.0000,  615940.0625,  580555.0625,  564788.3750,  560348.8125],
        [ 407619.3125,  321549.9688,  319812.8750,  318693.0625,  318531.0938,
          315190.5000,  307939.3438,  291426.5938,  276840.1875,  267290.6250],
        [ 773996.2500,  698062.0625,  688956.2500,  639499.8125,  619963.9375,
          573605.9375,  564678.0000,  564334.5000,  543733.6875,  539889.9375],
        [1225810.1250, 1156606.0000, 1101765.1250, 1048468.0000,  982280.5000,
          952343.1250,  884831.3750,  869604.4375,  858892.2500,  832078.7500],
        [ 988682.6250,  965572.6250,  843636.7500,  681408.4375,  661550.0625,
          564654.8125,  492667.0000,  469696.1250,  436670.6875,  429796.2188],
        [1077312.7500, 1045263.2500,  980389.0625,  937446.1250,  934915.0000,
          913654.1250,  909347.6875,  897254.8125,  888367.4375,  887346.2500],
        [1070552.1250,  651618.5625,  628977.3125,  592855.8125,  586044.3750,
          558376.1250,  468547.2188,  431885.1562,  426487.7188,  422938.0000],
        [1093851.2500,  983283.3750,  965884.7500,  941878.3750,  920817.3750,
          898206.8125,  894610.3125,  893252.2500,  870164.3125,  865163.1250],
        [ 741759.1250,  740616.1875,  702608.3750,  682344.2500,  671388.2500,
          665546.8125,  464773.2500,  456010.6875,  358607.5000,  328387.3438],
        [1232889.1250,  933350.6875,  780964.5000,  767048.5625,  581377.3125,
          556689.0625,  543277.0625,  532189.6875,  487641.0312,  449346.5312],
        [1265643.3750, 1208747.7500, 1208237.1250, 1192282.1250, 1167556.5000,
         1166312.3750, 1154080.6250, 1151505.8750, 1135290.3750, 1106125.7500],
        [ 999527.2500,  995027.7500,  888606.3750,  876677.2500,  866316.5625,
          834076.8750,  809949.9375,  809556.0625,  803764.9375,  796193.7500],
        [ 933648.8750,  788632.1875,  781173.8125,  774974.9375,  744208.6875,
          724472.0000,  716496.8125,  680951.1250,  651042.1875,  650159.2500],
        [1387951.0000, 1312640.2500, 1206687.3750, 1172235.7500, 1161651.2500,
         1157199.6250, 1079956.5000, 1067922.3750, 1037706.7500, 1020159.1250],
        [1284684.7500, 1149197.6250, 1131310.8750, 1088954.1250, 1042020.6250,
          999807.5625,  984057.3125,  983518.7500,  983462.5625,  979307.0000],
        [1111034.5000, 1045774.8125,  933989.0625,  883054.4375,  870166.0000,
          811326.0625,  779024.5625,  670237.3750,  670218.8125,  654507.1250],
        [ 950794.0625,  943509.1875,  866765.3125,  835339.3750,  815106.0625,
          766873.7500,  766365.5625,  765653.3125,  749111.9375,  732432.5625],
        [1391342.3750, 1173615.0000, 1161656.7500, 1100147.1250, 1078348.8750,
         1054141.7500, 1031052.1250,  987972.0000,  943344.5625,  936708.0000],
        [1128060.5000,  923763.8750,  905873.3125,  889174.3750,  763174.7500,
          761878.8750,  754000.1875,  738382.0000,  738268.6875,  720328.7500],
        [1294018.6250, 1026121.9375,  940996.6875,  925037.7500,  914531.9375,
          890275.6875,  864788.6250,  835061.3750,  793620.0625,  749162.7500],
        [1038466.0625,  969282.3750,  966811.8750,  930444.5000,  924438.9375,
          910797.1250,  883580.8750,  812104.0000,  811186.0000,  803677.5625],
        [1160823.8750, 1087400.5000,  768145.1250,  717077.8750,  654353.0000,
          645600.0000,  630517.2500,  618050.1250,  606093.6250,  596234.0000],
        [ 804806.5625,  786680.6875,  752968.2500,  746601.5000,  736810.5625,
          728281.2500,  726794.3750,  723395.0000,  721404.6875,  719716.2500],
        [1000853.1875,  967130.9375,  918558.0000,  790361.6875,  747817.1875,
          733298.5000,  728041.6875,  708444.5625,  679385.8750,  676457.8750],
        [ 896301.1875,  871100.9375,  827493.7500,  819835.0000,  805980.1875,
          805402.3125,  740713.0000,  718971.2500,  715259.7500,  692800.1250],
        [1162352.7500, 1042820.8750,  963592.9375,  953686.5000,  920510.8750,
          907613.1875,  890080.4375,  876724.0625,  859783.8750,  820854.4375],
        [ 966114.1875,  928424.4375,  902047.4375,  893991.0625,  886607.8125,
          870522.9375,  869033.1875,  816746.3750,  783364.9375,  772654.7500],
        [1148508.5000, 1073395.1250, 1057642.1250, 1039159.5000, 1032379.3750,
         1016576.4375, 1015607.4375,  997635.0000,  969532.0000,  947570.5625],
        [ 895343.5000,  844469.8750,  748329.4375,  731114.3125,  709397.8750,
          687696.5625,  669320.6875,  668990.1875,  665633.1250,  649509.1250],
        [ 401359.4688,  393882.9688,  375159.9688,  374820.2188,  341664.5625,
          336366.9062,  324580.0938,  307565.1250,  296823.7812,  285817.3750],
        [ 876405.5625,  805228.8125,  778358.3750,  669091.6250,  620346.0000,
          618416.2500,  584389.0625,  581922.0000,  567822.5000,  564898.3125],
        [ 980256.3125,  841921.5625,  793813.0625,  777210.8750,  712867.4375,
          709479.6875,  678861.9375,  669765.1250,  661353.8750,  645132.8750],
        [ 952244.1250,  826497.6875,  814587.6875,  747462.0625,  686775.0625,
          652009.5625,  609770.5000,  602427.4375,  602333.8125,  600243.0625],
        [ 827605.7500,  571927.3750,  542032.4375,  531963.4375,  471002.7500,
          437412.5938,  403557.9375,  399333.6250,  389076.2188,  379397.6562],
        [1105208.3750,  965143.5000,  800552.6875,  659660.0000,  645787.8125,
          606607.1250,  600539.0625,  600417.6875,  595920.7500,  586735.0625],
        [ 840093.6875,  791461.4375,  699585.5625,  559916.1250,  533326.3125,
          500516.5312,  489516.0000,  483992.0312,  482404.5312,  461038.0625],
        [1227878.7500, 1227652.7500, 1219984.7500, 1147998.1250, 1135868.7500,
         1132099.8750, 1124621.0000, 1108644.6250, 1107432.5000, 1104623.6250],
        [ 765397.8125,  594129.8750,  515630.5312,  448573.2812,  395451.3438,
          379238.8750,  371424.5000,  347522.2500,  345531.8438,  331904.8438],
        [ 703390.1250,  588379.6250,  549814.7500,  529322.3125,  488374.5312,
          484347.5938,  461185.8125,  449080.5000,  444976.7812,  431285.8750],
        [ 469644.5625,  465392.4062,  453593.5312,  394159.1562,  393948.3125,
          343524.2812,  336104.6250,  330571.1250,  321775.1250,  313701.9062],
        [ 710726.4375,  519283.3125,  502493.2188,  460051.1562,  425881.7500,
          351213.2500,  348389.0312,  328623.2500,  321630.6250,  302662.2500],
        [ 509508.0625,  474552.0000,  438408.6250,  427752.0625,  402242.7188,
          371918.2500,  322391.9062,  317278.3750,  313249.0000,  307838.6250],
        [ 250198.7344,  110997.5938,  101414.4531,   87311.2344,   78889.8125,
           78182.8516,   62607.2734,   54598.6016,   54328.3555,   52599.7305],
        [ 962611.0625,  910674.6250,  691108.1875,  687465.7500,  681595.0000,
          675734.4375,  670179.1875,  657451.2500,  638349.3750,  628204.6250],
        [ 287058.6875,  253086.2031,  247015.3594,  239684.8594,  213312.9219,
          198648.3125,  187731.2969,  147957.7656,  139574.2031,  118879.2422],
        [ 530788.6875,  525249.1875,  447483.7188,  383692.0000,  382721.0000,
          368715.4375,  351025.3750,  317308.0312,  296641.8125,  283322.8438],
        [ 784992.1875,  774029.5000,  745078.6250,  725622.6250,  721102.0000,
          648942.5625,  634564.3125,  623920.7500,  617116.5625,  613010.5625],
        [ 472757.3750,  390157.8750,  360549.1250,  356612.7500,  351498.0625,
          348428.2188,  327714.0938,  299716.9688,  281561.2188,  257518.7812],
        [ 595617.3750,  513068.5625,  491744.1562,  480253.0625,  463816.4062,
          439565.7812,  409621.1250,  381254.0000,  372983.8438,  357084.4062],
        [ 942756.3750,  625470.9375,  584211.3125,  549183.2500,  547124.6250,
          526141.5000,  394197.8750,  380087.2188,  342639.9062,  337793.9062],
        [ 667569.5625,  507896.9688,  473174.6250,  409311.0625,  375539.4375,
          363736.4688,  346632.5625,  295445.3438,  282039.0625,  272511.5312],
        [ 974788.7500,  844717.1875,  805322.5000,  802744.5000,  802315.9375,
          758523.6250,  736700.8750,  732646.3125,  731071.7500,  722766.8125],
        [ 884889.6875,  733875.6875,  698686.1250,  692646.8750,  687144.5625,
          644154.7500,  629451.3750,  626638.1250,  625734.6250,  616783.6250],
        [ 888569.8750,  695940.9375,  600042.7500,  591923.0625,  480022.2812,
          438991.4375,  420224.9375,  358699.1562,  348962.9688,  347448.3750],
        [ 815666.7500,  484324.9375,  407189.9688,  352390.5312,  338086.8438,
          324227.4062,  314898.4688,  293460.2812,  282164.6875,  278320.5000],
        [ 851240.9375,  526408.5625,  508281.1875,  504806.9375,  478174.0938,
          426486.5000,  377881.7812,  325472.8125,  286857.8125,  279946.4375],
        [ 851814.2500,  272211.2500,  271177.6562,  270949.1562,  262857.7500,
          227559.7031,  217808.8750,  209796.0156,  183925.2812,  175679.8906],
        [ 613005.9375,  582043.0000,  568503.6250,  527790.4375,  525143.4375,
          507525.0938,  462429.1250,  462072.4688,  454097.3125,  450681.6875],
        [ 255046.6094,  214904.3594,  193183.9688,  178826.1875,  172566.0938,
          163870.8594,  160181.0938,  153912.1875,  148315.3281,  143335.8594],
        [ 691135.1875,  524765.5000,  484291.2188,  456067.6875,  452035.0000,
          420345.2188,  398349.2500,  391939.3438,  382926.1562,  370345.0000],
        [ 557190.4375,  550056.5000,  434520.9375,  427954.4688,  406366.7812,
          380179.6562,  349439.8438,  327247.7812,  327142.0312,  321026.3125]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 457699.4375,       0.0000],
         [ 340931.5938,       0.0000],
         [ 197251.5312,       0.0000],
         ...,
         [ 146263.0156,       0.0000],
         [ 144532.6719,       0.0000],
         [ 131852.8906,       0.0000]],

        [[1139931.6250,       0.0000],
         [1131826.7500,       0.0000],
         [1105307.5000,       0.0000],
         ...,
         [1079817.3750,       0.0000],
         [1063096.6250,       0.0000],
         [1055296.5000,       0.0000]],

        [[ 831901.8750,       0.0000],
         [ 715267.2500,       0.0000],
         [ 632245.3125,       0.0000],
         ...,
         [ 484561.4688,       0.0000],
         [      0.0000,  444393.6562],
         [ 424722.0938,       0.0000]],

        ...,

        [[ 255046.6094,       0.0000],
         [ 214904.3594,       0.0000],
         [ 193183.9688,       0.0000],
         ...,
         [ 153912.1875,       0.0000],
         [      0.0000,  148315.3281],
         [ 143335.8594,       0.0000]],

        [[      0.0000,  691135.1875],
         [ 524765.5000,       0.0000],
         [ 484291.2188,       0.0000],
         ...,
         [ 391939.3438,       0.0000],
         [ 382926.1562,       0.0000],
         [ 370345.0000,       0.0000]],

        [[      0.0000,  557190.4375],
         [ 550056.5000,       0.0000],
         [ 434520.9375,       0.0000],
         ...,
         [      0.0000,  327247.7812],
         [ 327142.0312,       0.0000],
         [ 321026.3125,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 1925654.5000,   161037.4688],
        [10957452.0000,        0.0000],
        [ 5393737.0000,   444393.6562],
        [ 7481945.0000,        0.0000],
        [ 2237522.0000,   907371.7500],
        [ 4964924.5000,  1241795.7500],
        [ 7858571.5000,  2054108.2500],
        [ 6534335.5000,        0.0000],
        [ 8426033.0000,  1045263.2500],
        [ 5838282.5000,        0.0000],
        [ 9327112.0000,        0.0000],
        [ 5812042.0000,        0.0000],
        [ 6864774.0000,        0.0000],
        [11755782.0000,        0.0000],
        [ 8679696.0000,        0.0000],
        [ 7445759.5000,        0.0000],
        [11604110.0000,        0.0000],
        [10626321.0000,        0.0000],
        [ 8429332.0000,        0.0000],
        [ 8191951.0000,        0.0000],
        [10858329.0000,        0.0000],
        [ 8322905.5000,        0.0000],
        [ 9233615.0000,        0.0000],
        [ 9050790.0000,        0.0000],
        [ 7484295.5000,        0.0000],
        [ 7447459.0000,        0.0000],
        [ 7202532.0000,   747817.1875],
        [ 7893857.0000,        0.0000],
        [ 9398020.0000,        0.0000],
        [ 8689507.0000,        0.0000],
        [10298006.0000,        0.0000],
        [ 7269804.5000,        0.0000],
        [ 3141216.5000,   296823.7812],
        [ 4007313.7500,  2659564.7500],
        [ 5159287.5000,  2311375.5000],
        [ 5315609.0000,  1778741.7500],
        [ 2800716.0000,  2152593.7500],
        [ 4450432.5000,  2716139.7500],
        [ 2705349.0000,  3136501.5000],
        [10429372.0000,  1107432.5000],
        [ 4494805.0000,        0.0000],
        [ 5130158.0000,        0.0000],
        [ 3822414.7500,        0.0000],
        [ 4270954.0000,        0.0000],
        [ 2768191.0000,  1116948.7500],
        [  414189.5000,   516939.1250],
        [ 6575169.0000,   628204.6250],
        [ 1461071.7500,   571877.0000],
        [  972732.8750,  2914215.0000],
        [ 4734388.0000,  2153991.5000],
        [ 3056356.7500,   390157.8750],
        [ 3209335.2500,  1295673.5000],
        [ 2664535.2500,  2565071.7500],
        [ 3111370.7500,   882485.6875],
        [ 7911598.0000,        0.0000],
        [ 6106130.0000,   733875.6875],
        [ 2286729.7500,  2884096.0000],
        [ 2999215.2500,   891514.8750],
        [ 2399680.0000,  2165877.2500],
        [ 1374385.7500,  1569394.1250],
        [ 4699194.5000,   454097.3125],
        [ 1303080.0000,   481062.5312],
        [ 3460719.0000,  1111480.3750],
        [ 2768732.0000,  1312392.7500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 57.8125
Top1 accuracy for validation set is 57.8125 size is torch.Size([64, 1])
Epoch 166/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:33, 63.22s/it]  7%|▋         | 2/30 [01:03<12:21, 26.47s/it] 10%|█         | 3/30 [01:04<06:37, 14.73s/it] 13%|█▎        | 4/30 [01:05<03:59,  9.21s/it] 17%|█▋        | 5/30 [01:06<02:33,  6.16s/it] 20%|██        | 6/30 [01:06<01:43,  4.32s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.15s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.84s/it]
Epoch loss is 3.3090903838475545
Epoch 167/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:36, 57.12s/it]  7%|▋         | 2/30 [01:00<11:49, 25.35s/it] 10%|█         | 3/30 [01:00<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:01<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.92s/it] 20%|██        | 6/30 [01:03<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:03<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 3.2940701643625894
Epoch 168/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:47, 59.55s/it]  7%|▋         | 2/30 [01:00<11:38, 24.96s/it] 10%|█         | 3/30 [01:01<06:15, 13.91s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 3.2814791361490885
Epoch 169/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:23, 62.86s/it]  7%|▋         | 2/30 [01:03<12:17, 26.32s/it] 10%|█         | 3/30 [01:04<06:35, 14.65s/it] 13%|█▎        | 4/30 [01:05<03:58,  9.16s/it] 17%|█▋        | 5/30 [01:05<02:33,  6.13s/it] 20%|██        | 6/30 [01:06<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.38s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.28s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Epoch loss is 3.2599087715148927
Epoch 170/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:55, 59.86s/it]  7%|▋         | 2/30 [01:03<12:28, 26.74s/it] 10%|█         | 3/30 [01:04<06:41, 14.87s/it] 13%|█▎        | 4/30 [01:04<04:01,  9.30s/it] 17%|█▋        | 5/30 [01:05<02:35,  6.21s/it] 20%|██        | 6/30 [01:06<01:44,  4.36s/it] 23%|██▎       | 7/30 [01:07<01:13,  3.18s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.40s/it] 30%|███       | 9/30 [01:08<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 3.2405830224355063
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0129,  0.0106, -0.0236,  ..., -0.0019, -0.0233,  0.0332],
        [-0.0169,  0.0192, -0.0012,  ...,  0.0134, -0.0305, -0.0041],
        [-0.0534, -0.0246,  0.0056,  ...,  0.0625, -0.0304, -0.0425],
        ...,
        [ 0.0352, -0.0234, -0.0341,  ..., -0.0275, -0.0365, -0.0132],
        [-0.0338, -0.0022, -0.0095,  ..., -0.0185,  0.0173, -0.0197],
        [-0.0352,  0.0016,  0.0068,  ...,  0.0003,  0.0128, -0.0582]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9170, 0.8897, 0.8562, 0.8554, 0.8459, 0.8427, 0.8398, 0.8386, 0.8374,
         0.8340],
        [0.9768, 0.9752, 0.9735, 0.9734, 0.9731, 0.9726, 0.9716, 0.9715, 0.9714,
         0.9695],
        [0.9512, 0.9383, 0.9319, 0.9312, 0.9263, 0.9256, 0.9235, 0.9145, 0.9073,
         0.9028],
        [0.9796, 0.9596, 0.9580, 0.9505, 0.9361, 0.9347, 0.9322, 0.9282, 0.9282,
         0.9280],
        [0.8959, 0.8839, 0.8807, 0.8801, 0.8801, 0.8775, 0.8739, 0.8724, 0.8723,
         0.8720],
        [0.9426, 0.9392, 0.9344, 0.9316, 0.9258, 0.9207, 0.9190, 0.9170, 0.9167,
         0.9158],
        [0.9784, 0.9746, 0.9716, 0.9653, 0.9608, 0.9601, 0.9545, 0.9536, 0.9535,
         0.9501],
        [0.9642, 0.9634, 0.9535, 0.9350, 0.9350, 0.9246, 0.9129, 0.9119, 0.9051,
         0.9024],
        [0.9713, 0.9688, 0.9623, 0.9602, 0.9578, 0.9571, 0.9570, 0.9561, 0.9561,
         0.9554],
        [0.9709, 0.9359, 0.9313, 0.9281, 0.9274, 0.9267, 0.9141, 0.9103, 0.9062,
         0.9051],
        [0.9724, 0.9640, 0.9630, 0.9615, 0.9614, 0.9580, 0.9578, 0.9574, 0.9564,
         0.9545],
        [0.9439, 0.9430, 0.9393, 0.9386, 0.9381, 0.9370, 0.9087, 0.9077, 0.8943,
         0.8863],
        [0.9802, 0.9589, 0.9493, 0.9478, 0.9287, 0.9247, 0.9246, 0.9210, 0.9152,
         0.9070],
        [0.9820, 0.9784, 0.9782, 0.9778, 0.9771, 0.9768, 0.9763, 0.9757, 0.9734,
         0.9721],
        [0.9656, 0.9651, 0.9576, 0.9573, 0.9550, 0.9547, 0.9501, 0.9501, 0.9501,
         0.9490],
        [0.9607, 0.9493, 0.9481, 0.9468, 0.9431, 0.9430, 0.9424, 0.9374, 0.9372,
         0.9359],
        [0.9888, 0.9846, 0.9787, 0.9783, 0.9777, 0.9744, 0.9708, 0.9701, 0.9688,
         0.9685],
        [0.9839, 0.9755, 0.9750, 0.9719, 0.9690, 0.9654, 0.9653, 0.9640, 0.9635,
         0.9634],
        [0.9732, 0.9684, 0.9623, 0.9596, 0.9543, 0.9511, 0.9505, 0.9365, 0.9364,
         0.9354],
        [0.9618, 0.9616, 0.9555, 0.9513, 0.9475, 0.9447, 0.9446, 0.9439, 0.9437,
         0.9431],
        [0.9900, 0.9780, 0.9750, 0.9725, 0.9715, 0.9706, 0.9678, 0.9659, 0.9618,
         0.9608],
        [0.9733, 0.9589, 0.9584, 0.9583, 0.9465, 0.9465, 0.9463, 0.9442, 0.9442,
         0.9434],
        [0.9846, 0.9672, 0.9610, 0.9602, 0.9585, 0.9579, 0.9551, 0.9516, 0.9516,
         0.9450],
        [0.9686, 0.9631, 0.9629, 0.9621, 0.9596, 0.9588, 0.9573, 0.9514, 0.9510,
         0.9508],
        [0.9776, 0.9704, 0.9452, 0.9401, 0.9328, 0.9314, 0.9307, 0.9284, 0.9275,
         0.9265],
        [0.9480, 0.9478, 0.9468, 0.9465, 0.9465, 0.9459, 0.9457, 0.9442, 0.9438,
         0.9435],
        [0.9663, 0.9635, 0.9587, 0.9502, 0.9446, 0.9441, 0.9437, 0.9425, 0.9396,
         0.9386],
        [0.9619, 0.9548, 0.9534, 0.9523, 0.9519, 0.9496, 0.9467, 0.9422, 0.9414,
         0.9403],
        [0.9763, 0.9682, 0.9637, 0.9628, 0.9616, 0.9601, 0.9598, 0.9561, 0.9557,
         0.9504],
        [0.9638, 0.9595, 0.9567, 0.9566, 0.9560, 0.9552, 0.9546, 0.9539, 0.9494,
         0.9477],
        [0.9749, 0.9712, 0.9700, 0.9696, 0.9694, 0.9672, 0.9661, 0.9659, 0.9621,
         0.9621],
        [0.9579, 0.9543, 0.9452, 0.9446, 0.9428, 0.9397, 0.9380, 0.9379, 0.9372,
         0.9357],
        [0.9029, 0.9024, 0.8995, 0.8935, 0.8901, 0.8854, 0.8851, 0.8839, 0.8788,
         0.8773],
        [0.9544, 0.9509, 0.9463, 0.9345, 0.9327, 0.9314, 0.9257, 0.9256, 0.9242,
         0.9211],
        [0.9629, 0.9531, 0.9477, 0.9451, 0.9407, 0.9404, 0.9387, 0.9377, 0.9373,
         0.9351],
        [0.9586, 0.9508, 0.9508, 0.9406, 0.9362, 0.9358, 0.9289, 0.9272, 0.9265,
         0.9249],
        [0.9534, 0.9217, 0.9174, 0.9154, 0.9084, 0.9039, 0.9013, 0.8983, 0.8959,
         0.8956],
        [0.9736, 0.9617, 0.9490, 0.9361, 0.9347, 0.9284, 0.9277, 0.9274, 0.9270,
         0.9259],
        [0.9523, 0.9477, 0.9411, 0.9253, 0.9179, 0.9140, 0.9137, 0.9099, 0.9089,
         0.9089],
        [0.9802, 0.9800, 0.9791, 0.9755, 0.9735, 0.9734, 0.9730, 0.9730, 0.9728,
         0.9717],
        [0.9452, 0.9288, 0.9186, 0.9088, 0.8997, 0.8993, 0.8981, 0.8919, 0.8892,
         0.8878],
        [0.9407, 0.9260, 0.9249, 0.9222, 0.9127, 0.9123, 0.9112, 0.9080, 0.9078,
         0.9063],
        [0.9112, 0.9096, 0.9084, 0.8967, 0.8954, 0.8899, 0.8862, 0.8854, 0.8831,
         0.8830],
        [0.9431, 0.9179, 0.9165, 0.9112, 0.9056, 0.8937, 0.8896, 0.8851, 0.8846,
         0.8790],
        [0.9146, 0.9088, 0.9071, 0.9055, 0.9009, 0.8948, 0.8872, 0.8818, 0.8813,
         0.8807],
        [0.8695, 0.8133, 0.8096, 0.7953, 0.7846, 0.7827, 0.7763, 0.7579, 0.7563,
         0.7551],
        [0.9634, 0.9571, 0.9399, 0.9388, 0.9379, 0.9373, 0.9359, 0.9349, 0.9344,
         0.9337],
        [0.8765, 0.8698, 0.8648, 0.8626, 0.8516, 0.8482, 0.8475, 0.8263, 0.8215,
         0.8077],
        [0.9212, 0.9161, 0.9052, 0.8937, 0.8908, 0.8900, 0.8896, 0.8827, 0.8798,
         0.8712],
        [0.9473, 0.9453, 0.9440, 0.9427, 0.9397, 0.9320, 0.9311, 0.9308, 0.9287,
         0.9276],
        [0.9081, 0.8945, 0.8909, 0.8893, 0.8865, 0.8845, 0.8831, 0.8826, 0.8712,
         0.8666],
        [0.9291, 0.9192, 0.9158, 0.9157, 0.9101, 0.9093, 0.9004, 0.8985, 0.8974,
         0.8969],
        [0.9619, 0.9328, 0.9283, 0.9245, 0.9222, 0.9209, 0.9014, 0.8996, 0.8916,
         0.8908],
        [0.9389, 0.9173, 0.9135, 0.9014, 0.8979, 0.8958, 0.8918, 0.8815, 0.8764,
         0.8761],
        [0.9648, 0.9538, 0.9498, 0.9489, 0.9481, 0.9453, 0.9434, 0.9431, 0.9425,
         0.9424],
        [0.9576, 0.9447, 0.9404, 0.9393, 0.9382, 0.9342, 0.9342, 0.9336, 0.9324,
         0.9305],
        [0.9599, 0.9400, 0.9306, 0.9274, 0.9144, 0.9068, 0.9028, 0.8953, 0.8940,
         0.8929],
        [0.9531, 0.9167, 0.9032, 0.8920, 0.8894, 0.8859, 0.8845, 0.8800, 0.8783,
         0.8778],
        [0.9556, 0.9180, 0.9172, 0.9136, 0.9116, 0.9063, 0.8979, 0.8868, 0.8771,
         0.8768],
        [0.9542, 0.8766, 0.8735, 0.8732, 0.8724, 0.8658, 0.8587, 0.8533, 0.8444,
         0.8435],
        [0.9312, 0.9279, 0.9268, 0.9206, 0.9205, 0.9176, 0.9118, 0.9113, 0.9103,
         0.9088],
        [0.8697, 0.8543, 0.8495, 0.8440, 0.8433, 0.8398, 0.8326, 0.8317, 0.8297,
         0.8277],
        [0.9374, 0.9147, 0.9118, 0.9087, 0.9072, 0.9037, 0.9012, 0.8979, 0.8950,
         0.8939],
        [0.9215, 0.9181, 0.9096, 0.9062, 0.8958, 0.8942, 0.8864, 0.8860, 0.8826,
         0.8817]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 1, 1, 0, 1],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 1, 0, 1],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 1, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 1],
        [1, 0, 0, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],
        [0, 0, 1, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 488603.7188,  331205.7188,  205177.2031,  202916.2500,  177027.5625,
          169035.1250,  162202.2344,  159509.1406,  156872.7188,  149446.0781],
        [1148977.3750, 1123074.5000, 1095569.6250, 1093787.6250, 1090036.7500,
         1081918.2500, 1066045.0000, 1065765.3750, 1063566.1250, 1034793.3750],
        [ 796844.7500,  662897.7500,  604772.5625,  598591.5625,  558418.7500,
          553200.3125,  536407.7500,  471622.1250,  425837.4688,  399329.4375],
        [1196504.5000,  899026.1250,  877854.3750,  789155.1250,  642408.8125,
          629835.1250,  607390.3750,  574142.2500,  573563.8125,  572033.7500],
        [ 361674.9062,  304728.1562,  291009.4375,  288667.6875,  288497.6250,
          278125.7500,  264134.2812,  258713.4219,  258039.7188,  257029.0625],
        [ 705326.6875,  671636.0625,  626593.3125,  602061.5625,  554719.1875,
          515713.1562,  503298.5000,  488812.9688,  486770.3125,  480952.9688],
        [1176059.7500, 1112637.7500, 1067330.7500,  974214.4375,  914382.8125,
          904650.8125,  835196.8125,  825133.6250,  823946.9375,  784995.9375],
        [ 959814.3125,  948815.8125,  823079.1250,  632615.6875,  632223.6250,
          545304.5625,  461119.0000,  454369.7812,  412511.1250,  396909.3750],
        [1061384.6250, 1025482.0625,  934462.1875,  906069.4375,  875214.4375,
          866669.4375,  866252.0625,  855061.6875,  854159.4375,  845669.0625],
        [1056526.0000,  640683.4375,  599639.4375,  573170.6250,  567586.5000,
          561243.5625,  469047.9375,  444122.5000,  419334.2188,  412293.2188],
        [1079584.7500,  956740.9375,  943398.5625,  923471.3750,  922132.8750,
          878433.0625,  875403.1250,  870826.8125,  858272.3750,  835021.5625],
        [ 718153.6875,  708534.4375,  672650.1250,  665388.8125,  660823.0000,
          650597.7500,  434497.7188,  428247.6250,  353371.1875,  315428.0625],
        [1206664.3750,  889895.4375,  775610.0625,  759350.8750,  578070.5625,
          545625.5625,  544945.3125,  517551.9062,  476512.9688,  424065.6250],
        [1237832.6250, 1175069.7500, 1172333.0000, 1166084.3750, 1153589.8750,
         1148193.1250, 1140218.7500, 1131224.6250, 1093947.1250, 1074098.7500],
        [ 979180.9375,  972152.1875,  873643.3750,  870145.2500,  841020.3750,
          837956.5000,  784950.3125,  784331.4375,  784196.0625,  772839.0625],
        [ 912526.4375,  775116.0625,  762349.8750,  748230.2500,  709488.5000,
          709297.6875,  702390.0000,  654002.3750,  652939.8750,  640614.4375],
        [1364301.2500, 1283969.3750, 1180708.8750, 1174081.7500, 1163354.1250,
         1110655.2500, 1054825.6250, 1043841.6875, 1024649.2500, 1019738.0000],
        [1271389.7500, 1127874.3750, 1120009.0000, 1071113.8750, 1027824.1250,
          975687.1875,  975221.1875,  956299.4375,  949655.8750,  949374.2500],
        [1091688.7500, 1018308.4375,  933546.5000,  897991.8125,  833129.3125,
          795358.1875,  788845.8750,  646167.3125,  644632.8750,  636116.0625],
        [ 927414.7500,  924860.4375,  847269.0625,  798427.0000,  756199.3750,
          725797.0625,  725328.5625,  718395.5000,  716187.3750,  710008.3125],
        [1387006.3750, 1169125.3750, 1118939.2500, 1079947.1250, 1065208.5000,
         1050926.7500, 1009728.0000,  982978.6250,  927051.2500,  913629.6875],
        [1092781.5000,  890029.5000,  883344.1250,  882287.5000,  744870.4375,
          744847.0000,  742791.3125,  721282.9375,  720820.8125,  712917.0625],
        [1284690.7500, 1001964.7500,  917062.1250,  906577.7500,  884989.1875,
          876948.1250,  842513.5000,  801684.9375,  801009.3125,  729024.1250],
        [1021404.2500,  943983.5625,  941472.4375,  931802.2500,  899071.5000,
          887739.8750,  869796.7500,  799133.1875,  794234.8125,  792046.6250],
        [1162611.0000, 1047799.3125,  731742.0625,  679809.8125,  612470.0625,
          600496.1250,  594524.9375,  575509.5000,  567672.5625,  559785.3125],
        [ 761586.8125,  759438.5625,  748759.8125,  745716.2500,  745542.7500,
          738869.5000,  736986.1875,  721284.2500,  716857.0000,  714278.8125],
        [ 988965.5000,  949497.3750,  887438.5000,  785272.2500,  725084.4375,
          720041.6875,  716284.3750,  703487.4375,  675491.5000,  665253.6875],
        [ 928461.6875,  838536.0625,  822685.1875,  809435.6875,  804433.6250,
          778598.1875,  747438.5625,  700801.5000,  692552.4375,  682086.5625],
        [1141438.2500, 1015709.1875,  953144.5625,  940609.1250,  925086.1875,
          905281.7500,  901368.9375,  855214.1250,  849692.5625,  787634.8750],
        [ 954492.6250,  896983.5625,  861828.8125,  860668.2500,  853028.7500,
          844113.1875,  836887.9375,  828770.7500,  776770.6875,  758446.3125],
        [1117337.6250, 1059841.1250, 1042794.0000, 1036748.2500, 1033033.3750,
         1002010.6250,  985333.5625,  983433.4375,  931579.2500,  930852.8125],
        [ 876454.0000,  833485.2500,  732024.0625,  725110.6875,  706414.5000,
          676708.2500,  660325.3125,  659495.1875,  652916.8125,  638357.9375],
        [ 399586.5938,  396768.5938,  380625.8750,  349680.5312,  332995.5000,
          311481.5938,  310067.8750,  304791.1875,  283115.4062,  277172.7812],
        [ 834795.4375,  793694.2500,  743215.6875,  627399.9375,  611802.8125,
          600997.4375,  553918.3125,  552568.1250,  541938.8750,  518585.0312],
        [ 942414.7500,  819134.0625,  758052.1250,  730932.3125,  685962.7500,
          682926.8750,  666858.8125,  657033.1250,  653226.9375,  633660.8125],
        [ 885387.6875,  792270.2500,  792175.0625,  684540.7500,  643554.8750,
          639757.1875,  579274.2500,  566030.7500,  559623.6250,  547182.5625],
        [ 821977.0000,  523012.8438,  491865.1562,  478009.9688,  432439.0625,
          405685.6562,  390574.4375,  374153.4688,  361761.5000,  360065.9688],
        [1097504.2500,  925499.2500,  772264.3125,  642040.1250,  629403.9375,
          575700.5000,  569373.9375,  567130.3125,  563910.5625,  555328.9375],
        [ 810085.1250,  757653.8750,  690197.8750,  550528.8125,  495486.1250,
          468220.2188,  466182.2500,  441753.5625,  435693.5625,  435501.2500],
        [1205212.8750, 1202966.7500, 1188029.1250, 1127897.0000, 1096066.0000,
         1094010.8750, 1088546.0000, 1087656.7500, 1085076.0000, 1068116.8750],
        [ 731573.1875,  578827.4375,  500020.8438,  434878.3125,  381904.6250,
          379841.8750,  373228.2812,  341589.6562,  328644.9062,  322073.8438],
        [ 685522.6250,  555678.0625,  547633.6250,  526514.0000,  459600.8125,
          456865.1875,  450193.6562,  430070.5312,  428556.8750,  419631.0625],
        [ 450330.2500,  440151.3750,  432216.0312,  365895.9688,  358911.3125,
          332189.5312,  315118.6875,  311158.5625,  301138.0312,  301016.5938],
        [ 710087.5625,  495120.5312,  485170.4688,  450146.4688,  415675.5312,
          350536.2812,  330688.1250,  309782.6562,  307872.9688,  284255.1875],
        [ 472644.6562,  435007.7188,  424729.4062,  414737.0938,  388465.2188,
          356172.2500,  319519.5938,  295620.3438,  293421.0938,  291015.8125],
        [ 248138.5156,  111094.0703,  105440.9688,   85900.2344,   73812.0391,
           71803.4844,   65525.7578,   50369.6289,   49252.5352,   48399.5039],
        [ 948759.7500,  866578.4375,  678575.8750,  667966.3125,  659522.8750,
          653422.5625,  640841.7500,  631212.7500,  626692.5625,  620751.3750],
        [ 274129.9375,  249167.2188,  231788.4688,  224815.4844,  191957.1719,
          183014.0469,  181182.0938,  133745.2188,  125004.4922,  102631.0312],
        [ 519275.3750,  482897.9688,  413250.5938,  350503.8750,  336047.5938,
          332323.2500,  330444.4375,  299391.0000,  287557.0625,  254289.3438],
        [ 753578.8750,  732432.5625,  719342.2500,  705877.1250,  675827.8750,
          605437.3750,  598115.6250,  595594.6250,  577895.3125,  568953.8125],
        [ 430476.3438,  354449.5625,  336987.9062,  329348.3125,  316315.5312,
          307240.0000,  301118.2188,  298921.0938,  254296.3750,  237873.1406],
        [ 581433.8750,  504574.4688,  480423.0000,  480001.2188,  443188.7188,
          437940.1875,  385785.2812,  375246.5625,  369768.6875,  367054.2188],
        [ 928709.5625,  613174.3125,  574441.3125,  544500.6875,  526653.0625,
          516846.5938,  391343.6250,  381194.3750,  340324.4375,  336076.4062],
        [ 668812.1875,  490988.7500,  465191.8750,  391025.4062,  372128.2812,
          360996.4062,  341086.7188,  294615.3125,  273912.5312,  272741.3438],
        [ 968421.3125,  827155.2500,  781734.1875,  771175.8125,  761994.4375,
          732769.3125,  713240.1250,  709550.1250,  704251.3125,  702859.0000],
        [ 872927.1250,  726138.3750,  682985.5000,  672405.8125,  661926.7500,
          625392.1875,  625149.5000,  620110.6250,  609118.3750,  593216.0625],
        [ 902954.6250,  679300.3750,  593711.3125,  566955.1250,  471033.7500,
          422586.8125,  398942.6875,  358500.4688,  352018.6875,  346385.6875],
        [ 818874.7500,  486860.8750,  401641.2500,  342073.0938,  329605.9688,
          313701.9062,  307366.0312,  288356.5000,  281410.3750,  279333.8750],
        [ 848955.5000,  495836.3750,  490049.4375,  465848.9375,  452856.5312,
          419577.0312,  372312.8750,  317715.3125,  276576.5625,  275450.7500],
        [ 832281.9375,  274545.9375,  262775.7812,  261456.4219,  258589.0938,
          235270.5000,  212676.9219,  196863.0781,  173413.4062,  171146.6094],
        [ 598958.7500,  571432.8750,  562710.4375,  515087.9062,  513682.0000,
          493449.9375,  453738.0312,  450405.8125,  444201.2812,  435158.3125],
        [ 248682.6875,  199644.0625,  186514.9531,  172252.8594,  170581.6562,
          162369.8594,  146363.3438,  144572.0938,  140451.7344,  136564.0000],
        [ 654785.0000,  472954.4375,  453855.7188,  434120.0000,  424863.5000,
          404146.0625,  390015.7500,  372192.1875,  356878.7812,  351401.5312],
        [ 521596.6875,  496474.1875,  440080.0312,  418988.0312,  361419.7500,
          352945.4688,  315871.4688,  313820.7188,  298948.1875,  295256.9062]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 488603.7188,       0.0000],
         [ 331205.7188,       0.0000],
         [ 205177.2031,       0.0000],
         ...,
         [ 159509.1406,       0.0000],
         [ 156872.7188,       0.0000],
         [ 149446.0781,       0.0000]],

        [[1148977.3750,       0.0000],
         [1123074.5000,       0.0000],
         [1095569.6250,       0.0000],
         ...,
         [1065765.3750,       0.0000],
         [1063566.1250,       0.0000],
         [1034793.3750,       0.0000]],

        [[ 796844.7500,       0.0000],
         [ 662897.7500,       0.0000],
         [ 604772.5625,       0.0000],
         ...,
         [ 471622.1250,       0.0000],
         [      0.0000,  425837.4688],
         [ 399329.4375,       0.0000]],

        ...,

        [[ 248682.6875,       0.0000],
         [ 199644.0625,       0.0000],
         [ 186514.9531,       0.0000],
         ...,
         [      0.0000,  144572.0938],
         [ 140451.7344,       0.0000],
         [      0.0000,  136564.0000]],

        [[      0.0000,  654785.0000],
         [ 472954.4375,       0.0000],
         [ 453855.7188,       0.0000],
         ...,
         [ 372192.1875,       0.0000],
         [ 356878.7812,       0.0000],
         [ 351401.5312,       0.0000]],

        [[      0.0000,  521596.6875],
         [ 496474.1875,       0.0000],
         [      0.0000,  440080.0312],
         ...,
         [ 313820.7188,       0.0000],
         [ 298948.1875,       0.0000],
         [ 295256.9062,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2032960.6250,   169035.1250],
        [10863533.0000,        0.0000],
        [ 5182085.0000,   425837.4688],
        [ 7361914.5000,        0.0000],
        [ 1782075.5000,  1068544.5000],
        [ 3994482.7500,  1641402.0000],
        [ 7436836.0000,  1981713.5000],
        [ 6266762.5000,        0.0000],
        [ 8064942.0000,  1025482.0625],
        [ 5743647.5000,        0.0000],
        [ 9143285.0000,        0.0000],
        [ 5607692.5000,        0.0000],
        [ 6718293.0000,        0.0000],
        [11492592.0000,        0.0000],
        [ 8500416.0000,        0.0000],
        [ 7266955.5000,        0.0000],
        [11420126.0000,        0.0000],
        [10424449.0000,        0.0000],
        [ 8285785.5000,        0.0000],
        [ 7849887.0000,        0.0000],
        [10704541.0000,        0.0000],
        [ 8135972.0000,        0.0000],
        [ 9046464.0000,        0.0000],
        [ 8880686.0000,        0.0000],
        [ 7132420.0000,        0.0000],
        [ 7389319.5000,        0.0000],
        [ 7096775.0000,   720041.6875],
        [ 7805029.0000,        0.0000],
        [ 9275180.0000,        0.0000],
        [ 8471991.0000,        0.0000],
        [10122964.0000,        0.0000],
        [ 7161292.5000,        0.0000],
        [ 3063170.7500,   283115.4062],
        [ 3827434.0000,  2551482.0000],
        [ 4977528.0000,  2252675.0000],
        [ 5012139.0000,  1677658.0000],
        [ 2200909.0000,  2438636.0000],
        [ 4233112.5000,  2665043.5000],
        [ 2529121.0000,  3022181.7500],
        [10175461.0000,  1068116.8750],
        [ 4372583.0000,        0.0000],
        [ 4960266.0000,        0.0000],
        [ 3608126.2500,        0.0000],
        [ 4139335.5000,        0.0000],
        [ 2614811.0000,  1076522.0000],
        [  394693.5625,   515043.1875],
        [ 6363112.0000,   631212.7500],
        [ 1371058.5000,   526376.6250],
        [  896148.5625,  2709832.0000],
        [ 4448928.5000,  2084127.1250],
        [ 2812577.0000,   354449.5625],
        [ 2798233.5000,  1627182.6250],
        [ 2613037.7500,  2540227.0000],
        [ 3075281.5000,   856217.2500],
        [ 7673151.0000,        0.0000],
        [ 5963232.0000,   726138.3750],
        [ 2232660.2500,  2859729.2500],
        [ 2960722.5000,   888502.1250],
        [ 2329087.7500,  2086091.5000],
        [ 1324244.3750,  1554775.2500],
        [ 4594624.0000,   444201.2812],
        [ 1256279.6250,   451717.7500],
        [ 3256282.0000,  1058931.0000],
        [ 2537853.2500,  1277548.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 59.375
Top1 accuracy for validation set is 59.375 size is torch.Size([64, 1])
Epoch 171/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:28, 56.85s/it]  7%|▋         | 2/30 [00:57<11:07, 23.85s/it] 10%|█         | 3/30 [01:00<06:23, 14.21s/it] 13%|█▎        | 4/30 [01:01<03:51,  8.89s/it] 17%|█▋        | 5/30 [01:01<02:28,  5.96s/it] 20%|██        | 6/30 [01:02<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:03<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.32s/it] 30%|███       | 9/30 [01:04<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.2498543580373127
Epoch 172/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:36, 57.12s/it]  7%|▋         | 2/30 [00:59<11:37, 24.91s/it] 10%|█         | 3/30 [01:00<06:14, 13.88s/it] 13%|█▎        | 4/30 [01:00<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.83s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.23it/s] 60%|██████    | 18/30 [01:11<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.2605331261952717
Epoch 173/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:18, 60.63s/it]  7%|▋         | 2/30 [01:02<12:11, 26.12s/it] 10%|█         | 3/30 [01:03<06:32, 14.53s/it] 13%|█▎        | 4/30 [01:04<03:58,  9.17s/it] 17%|█▋        | 5/30 [01:05<02:33,  6.13s/it] 20%|██        | 6/30 [01:05<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.38s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 3.2171682278315226
Epoch 174/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:33, 57.03s/it]  7%|▋         | 2/30 [00:58<11:17, 24.18s/it] 10%|█         | 3/30 [00:58<06:04, 13.48s/it] 13%|█▎        | 4/30 [00:59<03:39,  8.46s/it] 17%|█▋        | 5/30 [01:00<02:21,  5.68s/it] 20%|██        | 6/30 [01:01<01:36,  4.00s/it] 23%|██▎       | 7/30 [01:01<01:07,  2.94s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.24s/it] 30%|███       | 9/30 [01:03<00:37,  1.77s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.46s/it] 37%|███▋      | 11/30 [01:04<00:23,  1.24s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:07<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:10<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:13<00:05,  1.33it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:16<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.65s/it]
Epoch loss is 3.2200165669123333
Epoch 175/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:02, 60.09s/it]  7%|▋         | 2/30 [01:00<11:45, 25.18s/it] 10%|█         | 3/30 [01:01<06:18, 14.02s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 3.187012958526611
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 1.4353e-02,  1.0383e-02, -2.0725e-02,  ...,  1.7349e-04,
         -2.3744e-02,  3.1552e-02],
        [-1.4447e-02,  2.0756e-02,  8.9282e-05,  ...,  1.3130e-02,
         -2.9128e-02, -5.0397e-03],
        [-5.3007e-02, -2.6085e-02,  6.1210e-03,  ...,  6.3772e-02,
         -3.2250e-02, -4.0456e-02],
        ...,
        [ 3.5800e-02, -2.3280e-02, -3.0499e-02,  ..., -2.6387e-02,
         -3.8068e-02, -1.2575e-02],
        [-3.3182e-02, -3.3383e-03, -1.0161e-02,  ..., -1.6292e-02,
          1.6301e-02, -1.9399e-02],
        [-3.4769e-02,  7.1403e-04,  7.7355e-03,  ...,  2.1911e-03,
          1.3013e-02, -5.8833e-02]], device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9194, 0.8887, 0.8597, 0.8567, 0.8506, 0.8464, 0.8452, 0.8422, 0.8392,
         0.8362],
        [0.9763, 0.9749, 0.9741, 0.9727, 0.9715, 0.9714, 0.9711, 0.9705, 0.9698,
         0.9693],
        [0.9490, 0.9341, 0.9293, 0.9282, 0.9248, 0.9232, 0.9206, 0.9112, 0.9018,
         0.8997],
        [0.9782, 0.9577, 0.9562, 0.9487, 0.9342, 0.9316, 0.9304, 0.9298, 0.9295,
         0.9287],
        [0.8866, 0.8783, 0.8758, 0.8739, 0.8727, 0.8716, 0.8709, 0.8675, 0.8654,
         0.8642],
        [0.9344, 0.9342, 0.9283, 0.9271, 0.9166, 0.9147, 0.9126, 0.9082, 0.9079,
         0.9076],
        [0.9753, 0.9716, 0.9697, 0.9597, 0.9575, 0.9537, 0.9504, 0.9494, 0.9490,
         0.9455],
        [0.9620, 0.9618, 0.9509, 0.9314, 0.9298, 0.9219, 0.9086, 0.9086, 0.8998,
         0.8991],
        [0.9693, 0.9667, 0.9574, 0.9562, 0.9534, 0.9529, 0.9529, 0.9526, 0.9523,
         0.9496],
        [0.9695, 0.9338, 0.9285, 0.9268, 0.9265, 0.9232, 0.9136, 0.9116, 0.9051,
         0.9041],
        [0.9716, 0.9621, 0.9618, 0.9611, 0.9592, 0.9574, 0.9563, 0.9551, 0.9548,
         0.9520],
        [0.9413, 0.9386, 0.9382, 0.9369, 0.9345, 0.9343, 0.9031, 0.9023, 0.8939,
         0.8824],
        [0.9785, 0.9558, 0.9494, 0.9455, 0.9285, 0.9249, 0.9225, 0.9177, 0.9138,
         0.9038],
        [0.9800, 0.9772, 0.9767, 0.9762, 0.9762, 0.9760, 0.9748, 0.9741, 0.9729,
         0.9712],
        [0.9640, 0.9605, 0.9561, 0.9560, 0.9533, 0.9531, 0.9484, 0.9483, 0.9472,
         0.9466],
        [0.9585, 0.9480, 0.9463, 0.9447, 0.9415, 0.9405, 0.9394, 0.9366, 0.9352,
         0.9350],
        [0.9874, 0.9838, 0.9782, 0.9765, 0.9762, 0.9722, 0.9698, 0.9681, 0.9668,
         0.9662],
        [0.9831, 0.9744, 0.9733, 0.9708, 0.9659, 0.9647, 0.9636, 0.9623, 0.9614,
         0.9611],
        [0.9718, 0.9661, 0.9620, 0.9590, 0.9506, 0.9503, 0.9494, 0.9353, 0.9339,
         0.9339],
        [0.9596, 0.9594, 0.9543, 0.9478, 0.9427, 0.9417, 0.9414, 0.9411, 0.9402,
         0.9400],
        [0.9890, 0.9781, 0.9718, 0.9710, 0.9702, 0.9702, 0.9665, 0.9649, 0.9609,
         0.9603],
        [0.9712, 0.9576, 0.9573, 0.9559, 0.9474, 0.9453, 0.9450, 0.9447, 0.9421,
         0.9407],
        [0.9839, 0.9654, 0.9597, 0.9585, 0.9585, 0.9548, 0.9530, 0.9519, 0.9485,
         0.9433],
        [0.9672, 0.9618, 0.9613, 0.9608, 0.9573, 0.9559, 0.9555, 0.9520, 0.9507,
         0.9499],
        [0.9773, 0.9676, 0.9416, 0.9380, 0.9284, 0.9283, 0.9257, 0.9239, 0.9234,
         0.9228],
        [0.9476, 0.9474, 0.9471, 0.9463, 0.9454, 0.9453, 0.9451, 0.9448, 0.9430,
         0.9424],
        [0.9650, 0.9619, 0.9561, 0.9491, 0.9435, 0.9430, 0.9420, 0.9419, 0.9385,
         0.9372],
        [0.9628, 0.9537, 0.9522, 0.9510, 0.9509, 0.9478, 0.9464, 0.9404, 0.9400,
         0.9395],
        [0.9740, 0.9660, 0.9625, 0.9602, 0.9601, 0.9600, 0.9596, 0.9548, 0.9546,
         0.9485],
        [0.9600, 0.9569, 0.9559, 0.9542, 0.9542, 0.9531, 0.9531, 0.9508, 0.9498,
         0.9460],
        [0.9731, 0.9703, 0.9696, 0.9687, 0.9686, 0.9665, 0.9656, 0.9648, 0.9606,
         0.9601],
        [0.9561, 0.9545, 0.9440, 0.9425, 0.9421, 0.9391, 0.9387, 0.9367, 0.9356,
         0.9336],
        [0.9032, 0.9018, 0.8995, 0.8874, 0.8873, 0.8854, 0.8794, 0.8790, 0.8784,
         0.8765],
        [0.9518, 0.9486, 0.9429, 0.9319, 0.9303, 0.9295, 0.9228, 0.9216, 0.9198,
         0.9150],
        [0.9587, 0.9498, 0.9419, 0.9389, 0.9380, 0.9369, 0.9356, 0.9355, 0.9348,
         0.9337],
        [0.9532, 0.9483, 0.9466, 0.9355, 0.9344, 0.9309, 0.9267, 0.9237, 0.9215,
         0.9200],
        [0.9531, 0.9147, 0.9103, 0.9096, 0.9027, 0.8997, 0.8993, 0.8964, 0.8909,
         0.8887],
        [0.9729, 0.9588, 0.9453, 0.9354, 0.9303, 0.9242, 0.9239, 0.9236, 0.9229,
         0.9210],
        [0.9498, 0.9445, 0.9403, 0.9238, 0.9163, 0.9131, 0.9058, 0.9048, 0.9036,
         0.9020],
        [0.9790, 0.9786, 0.9759, 0.9739, 0.9725, 0.9722, 0.9712, 0.9705, 0.9703,
         0.9702],
        [0.9412, 0.9271, 0.9174, 0.9061, 0.8999, 0.8988, 0.8979, 0.8908, 0.8868,
         0.8854],
        [0.9380, 0.9254, 0.9196, 0.9194, 0.9149, 0.9086, 0.9055, 0.9046, 0.9044,
         0.9037],
        [0.9108, 0.9068, 0.9064, 0.8928, 0.8899, 0.8891, 0.8860, 0.8835, 0.8830,
         0.8803],
        [0.9433, 0.9162, 0.9134, 0.9103, 0.9064, 0.8928, 0.8862, 0.8833, 0.8820,
         0.8790],
        [0.9096, 0.9058, 0.9020, 0.9011, 0.8988, 0.8921, 0.8859, 0.8830, 0.8777,
         0.8776],
        [0.8669, 0.8137, 0.8120, 0.7940, 0.7809, 0.7786, 0.7774, 0.7590, 0.7561,
         0.7524],
        [0.9620, 0.9534, 0.9383, 0.9363, 0.9359, 0.9354, 0.9328, 0.9318, 0.9310,
         0.9306],
        [0.8746, 0.8669, 0.8615, 0.8585, 0.8457, 0.8449, 0.8430, 0.8211, 0.8148,
         0.8068],
        [0.9187, 0.9088, 0.9003, 0.8887, 0.8879, 0.8826, 0.8819, 0.8818, 0.8743,
         0.8671],
        [0.9443, 0.9439, 0.9409, 0.9387, 0.9344, 0.9274, 0.9269, 0.9261, 0.9242,
         0.9236],
        [0.9027, 0.8873, 0.8868, 0.8838, 0.8808, 0.8802, 0.8774, 0.8715, 0.8635,
         0.8628],
        [0.9272, 0.9176, 0.9135, 0.9128, 0.9084, 0.9049, 0.8990, 0.8962, 0.8945,
         0.8939],
        [0.9616, 0.9317, 0.9275, 0.9239, 0.9190, 0.9178, 0.9008, 0.9007, 0.8922,
         0.8913],
        [0.9400, 0.9145, 0.9125, 0.8979, 0.8976, 0.8947, 0.8904, 0.8802, 0.8769,
         0.8761],
        [0.9650, 0.9539, 0.9502, 0.9457, 0.9453, 0.9439, 0.9431, 0.9431, 0.9421,
         0.9406],
        [0.9566, 0.9427, 0.9393, 0.9370, 0.9359, 0.9340, 0.9330, 0.9324, 0.9307,
         0.9301],
        [0.9604, 0.9389, 0.9302, 0.9234, 0.9142, 0.9040, 0.9003, 0.8955, 0.8954,
         0.8934],
        [0.9537, 0.9167, 0.9027, 0.8904, 0.8880, 0.8848, 0.8831, 0.8793, 0.8790,
         0.8778],
        [0.9552, 0.9148, 0.9138, 0.9077, 0.9062, 0.9049, 0.8966, 0.8845, 0.8752,
         0.8742],
        [0.9515, 0.8769, 0.8738, 0.8713, 0.8688, 0.8671, 0.8571, 0.8501, 0.8476,
         0.8373],
        [0.9296, 0.9260, 0.9251, 0.9193, 0.9179, 0.9146, 0.9106, 0.9094, 0.9089,
         0.9065],
        [0.8670, 0.8471, 0.8450, 0.8419, 0.8393, 0.8382, 0.8274, 0.8262, 0.8223,
         0.8220],
        [0.9333, 0.9079, 0.9067, 0.9052, 0.9032, 0.9003, 0.9002, 0.8948, 0.8924,
         0.8875],
        [0.9160, 0.9105, 0.9092, 0.9012, 0.8898, 0.8840, 0.8833, 0.8796, 0.8762,
         0.8750]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 1, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 1, 1, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 1, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],
        [0, 0, 1, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 505679.0312,  326242.5312,  215606.8594,  206491.4375,  189272.2812,
          178461.2500,  175227.6250,  167914.1406,  160847.9062,  154115.4844],
        [1139958.7500, 1118061.3750, 1105192.6250, 1082965.0000, 1065021.6250,
         1063527.6250, 1058379.6250, 1050498.7500, 1039883.2500, 1032774.2500],
        [ 772062.5625,  624198.6875,  583259.8750,  573789.1875,  546360.7500,
          533866.1875,  514769.2188,  450202.7188,  393351.0312,  381640.6875],
        [1172609.2500,  874826.5000,  856132.1875,  769192.6875,  624802.0000,
          602209.1875,  592340.9375,  587164.9375,  584333.3125,  577750.3750],
        [ 316526.7500,  281417.0938,  271380.5000,  264276.6562,  259584.8281,
          255791.2500,  252976.4062,  241092.6875,  233912.7188,  229942.5781],
        [ 626845.5000,  624970.0625,  574939.5000,  564958.0625,  486008.6875,
          472801.5625,  458865.9062,  431141.1250,  429341.4688,  427728.8125],
        [1123753.7500, 1066063.2500, 1037504.8750,  899331.3125,  872347.1250,
          825809.8125,  787758.7500,  777035.2500,  771788.0000,  734158.5000],
        [ 930352.2500,  927436.0000,  793776.7500,  600882.2500,  587142.5000,
          524148.8125,  433922.5312,  433380.7812,  382245.6875,  378807.2500],
        [1032610.7500,  994519.1875,  870298.8125,  855438.5000,  822069.5000,
          817030.7500,  816553.2500,  813630.3750,  809241.1875,  778646.4375],
        [1035661.1875,  621167.1250,  576158.0625,  562632.1250,  559754.8750,
          534253.8125,  465729.8750,  452935.1562,  412240.9062,  406755.2812],
        [1067110.8750,  930866.0625,  927442.1875,  917992.2500,  893360.4375,
          870205.0625,  856907.3750,  842584.2500,  839028.8125,  806619.1875],
        [ 691518.8750,  665892.8750,  661621.3125,  649844.3750,  627927.3125,
          626131.0000,  400901.5312,  396142.4688,  351270.8438,  298087.8750],
        [1177279.5000,  850960.0625,  776267.9375,  734973.9375,  576181.0625,
          547613.2500,  528653.3750,  493619.8125,  467416.2812,  404822.2500],
        [1202342.7500, 1154877.7500, 1146702.7500, 1138726.6250, 1138315.1250,
         1136107.0000, 1117268.3750, 1104934.3750, 1086734.8750, 1061290.5000],
        [ 956356.8750,  910112.9375,  855326.7500,  853373.7500,  821741.8750,
          819378.5625,  766166.0625,  764718.5000,  752682.5625,  745943.8750],
        [ 884628.0625,  760933.4375,  743605.6250,  726662.0625,  694337.3125,
          683908.4375,  673505.1250,  647111.4375,  634202.5000,  632297.7500],
        [1336204.6250, 1269172.7500, 1171482.5000, 1143986.2500, 1139240.3750,
         1076243.7500, 1039688.9375, 1014294.8750,  996448.3125,  987031.1875],
        [1256559.6250, 1110229.5000, 1093230.7500, 1054717.0000,  983721.3750,
          967031.3750,  952000.8125,  934067.5000,  922458.3125,  917651.7500],
        [1070369.3750,  985360.8125,  930008.8750,  890796.2500,  790277.3125,
          786463.1250,  776940.3750,  635165.5625,  622665.3125,  622649.3125],
        [ 898038.9375,  895798.6875,  833083.1875,  759130.8125,  705871.7500,
          696302.7500,  692762.5000,  689783.3125,  680797.2500,  679129.3750],
        [1367130.1250, 1170347.8750, 1069417.5000, 1057879.1250, 1045535.4375,
         1045131.7500,  991502.0625,  969086.4375,  914926.2500,  907299.9375],
        [1060328.3750,  873130.3125,  869660.7500,  852559.4375,  754556.9375,
          732291.4375,  729927.1250,  726184.7500,  699467.4375,  686377.0000],
        [1272400.1250,  976102.3125,  899775.7500,  885065.1875,  883951.6875,
          839103.2500,  817849.1875,  805479.9375,  766513.2500,  711650.1875],
        [1001149.0000,  926769.3125,  920337.9375,  914422.0625,  869841.6250,
          852214.8125,  847866.3750,  805689.6875,  791595.0625,  781860.2500],
        [1157727.2500, 1007723.1250,  694355.1875,  659555.0000,  575577.5000,
          574986.0625,  553785.1875,  539466.8125,  535621.0625,  530871.1875],
        [ 757123.0000,  754407.2500,  752051.1250,  743479.3750,  733525.1250,
          732770.0000,  730813.1250,  727677.9375,  709015.0625,  702331.6875],
        [ 971288.5000,  929195.0625,  855058.4375,  773887.0000,  714077.9375,
          708595.3125,  699021.3750,  697939.6250,  664488.3750,  652468.0000],
        [ 939945.5000,  825932.6250,  807975.7500,  795084.3750,  793632.1250,
          759208.3125,  744624.7500,  683299.5000,  679474.0625,  674359.3125],
        [1103165.6250,  983981.3125,  936833.9375,  906953.8750,  904920.9375,
          903406.7500,  899039.0000,  838968.8125,  836646.1250,  766832.7500],
        [ 903668.7500,  864483.5625,  851893.8125,  832354.1875,  831751.8750,
          819155.9375,  819027.8125,  792056.4375,  781518.0625,  739504.5625],
        [1089950.5000, 1046784.5625, 1036330.0625, 1024002.5625, 1021258.1875,
          991924.8125,  978352.0625,  967909.6875,  911192.4375,  904414.5000],
        [ 855289.1875,  835812.7500,  718755.2500,  703708.1875,  699334.7500,
          670929.9375,  666927.5000,  648168.1875,  637636.8750,  619778.9375],
        [ 401419.5625,  393741.0000,  380541.6562,  320352.8750,  320029.1875,
          311173.4062,  285764.5312,  284077.9688,  281566.3438,  274322.4375],
        [ 803543.4375,  767840.4375,  707938.0625,  604675.1250,  590874.6250,
          584680.5625,  531378.3125,  522455.4688,  509018.0625,  475378.2188],
        [ 887367.3750,  781557.5625,  698059.3750,  668954.5000,  660287.5000,
          649766.8750,  637304.3750,  637030.3750,  630347.6875,  621033.8750],
        [ 819548.1875,  764741.1250,  745849.3125,  636488.6875,  626809.0625,
          595922.5000,  561603.9375,  538085.1875,  521369.9062,  510550.4688],
        [ 818378.1875,  473165.5938,  444486.9062,  439685.7188,  398497.4375,
          381876.6250,  379557.2500,  364475.3750,  336880.5625,  326259.3750],
        [1086831.3750,  888697.0625,  732459.0625,  635736.3750,  591124.3125,
          541547.2500,  539386.0625,  536940.5000,  531980.1250,  517716.7812],
        [ 780649.5000,  723749.6250,  681793.9375,  538874.5000,  484293.0625,
          462627.6250,  416531.8750,  410815.8750,  403964.5625,  394603.3125],
        [1185816.2500, 1178135.3750, 1133715.1250, 1102982.5000, 1080563.2500,
         1076082.6250, 1060350.6250, 1049738.6250, 1047679.3750, 1045360.9375],
        [ 690586.3750,  564499.2500,  491619.4062,  418307.7188,  382814.0625,
          376879.8125,  372265.6562,  336354.0625,  317742.8750,  311501.7812],
        [ 660104.9375,  551417.3750,  507393.4375,  506254.2188,  474568.7500,
          433687.9688,  414591.5312,  409835.6250,  408256.5312,  404513.1250],
        [ 447662.9688,  422558.5938,  420486.7500,  346270.4375,  331955.4688,
          328434.9688,  313809.6250,  302782.6562,  300806.2500,  289286.6875],
        [ 712390.3750,  483448.1250,  464440.9375,  444486.0625,  419956.5625,
          345833.1562,  314766.6562,  302125.5625,  296583.2812,  284240.5625],
        [ 439875.2812,  416733.3125,  394727.5312,  389336.0312,  376979.4062,
          342359.9688,  313490.1875,  300819.4375,  278983.7812,  278621.6875],
        [ 238998.0312,  111757.2578,  109126.2500,   84405.1641,   69985.3438,
           67674.6562,   66590.8359,   51151.0273,   49104.1875,   46556.7539],
        [ 929538.1250,  822138.5625,  663068.5000,  644094.0000,  640910.1875,
          635877.1250,  613061.5000,  604266.4375,  597366.6250,  593906.0625],
        [ 266642.4062,  238888.4375,  221311.6719,  211971.8594,  176682.6406,
          174548.4688,  169989.0781,  124316.6250,  113615.4219,  101262.6172],
        [ 501303.3125,  435159.1562,  384916.1250,  326263.0938,  322474.3125,
          299275.6562,  296070.3438,  295662.0938,  265584.8750,  239651.9375],
        [ 721794.1875,  717635.4375,  688378.3125,  666398.5625,  627316.8125,
          566858.3750,  563201.6875,  556967.8125,  541764.7500,  536908.2500],
        [ 398551.7500,  319976.4062,  317753.7812,  304227.2812,  291686.8438,
          288891.3125,  277724.4688,  255244.6719,  227601.5938,  225410.8125],
        [ 565872.0625,  492811.7500,  465404.4375,  460289.4375,  432387.9062,
          411289.0625,  378162.6250,  363041.2812,  354612.5312,  351358.2812],
        [ 925005.1250,  603096.0000,  568078.7500,  539320.7500,  503351.2812,
          494813.2188,  387762.6875,  387462.9062,  342996.5938,  338775.6250],
        [ 678927.3750,  471720.1562,  458748.6562,  372338.4375,  370534.3750,
          355763.1562,  334466.5312,  288934.5625,  275542.9688,  272531.7812],
        [ 970515.3750,  828006.8750,  786160.9375,  736386.9375,  732935.6875,
          717958.5000,  710155.9375,  710155.9375,  700043.4375,  684628.8750],
        [ 860642.0000,  705727.0625,  672654.6250,  650183.4375,  640197.8750,
          623427.0625,  614635.6250,  608841.3750,  594513.5625,  589614.3125],
        [ 908743.5000,  668690.3750,  590353.0625,  535999.6875,  469773.5938,
          406227.6562,  384982.1875,  359556.4062,  358997.9062,  348956.6250],
        [ 825611.3125,  486603.7188,  398861.2812,  334165.8750,  322991.3750,
          308567.0000,  301104.1562,  285385.1562,  284147.3125,  279426.8750],
        [ 844386.9375,  473988.0000,  467203.2500,  428329.2812,  418948.8750,
          411528.7812,  365300.1250,  307371.0312,  269082.0000,  265154.6562],
        [ 800343.5000,  275680.6875,  263840.2188,  254487.5312,  245763.6562,
          239675.4844,  207906.4531,  187906.2969,  181409.9844,  156538.7031],
        [ 585110.6250,  555824.8750,  549013.0625,  505092.0312,  494989.2500,
          472203.1562,  446506.6875,  438910.2188,  435332.6562,  421103.5312],
        [ 239372.5938,  180089.0625,  174920.7344,  167262.6875,  161095.0625,
          158514.9531,  135862.9219,  133542.9375,  126460.0391,  125891.4844],
        [ 617055.4375,  429613.4688,  421764.2812,  413212.7812,  401204.4375,
          385003.8750,  384667.6875,  356057.4688,  344213.9375,  320725.8125],
        [ 482083.5000,  445425.5625,  437591.1562,  389965.9062,  331371.6250,
          304982.5312,  302275.1562,  286657.9062,  272847.2500,  268148.2500]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 505679.0312,       0.0000],
         [ 326242.5312,       0.0000],
         [ 215606.8594,       0.0000],
         ...,
         [ 167914.1406,       0.0000],
         [ 160847.9062,       0.0000],
         [ 154115.4844,       0.0000]],

        [[1139958.7500,       0.0000],
         [1118061.3750,       0.0000],
         [1105192.6250,       0.0000],
         ...,
         [1050498.7500,       0.0000],
         [1039883.2500,       0.0000],
         [1032774.2500,       0.0000]],

        [[ 772062.5625,       0.0000],
         [ 624198.6875,       0.0000],
         [ 583259.8750,       0.0000],
         ...,
         [ 450202.7188,       0.0000],
         [      0.0000,  393351.0312],
         [ 381640.6875,       0.0000]],

        ...,

        [[ 239372.5938,       0.0000],
         [ 180089.0625,       0.0000],
         [ 174920.7344,       0.0000],
         ...,
         [ 133542.9375,       0.0000],
         [      0.0000,  126460.0391],
         [      0.0000,  125891.4844]],

        [[      0.0000,  617055.4375],
         [ 429613.4688,       0.0000],
         [ 421764.2812,       0.0000],
         ...,
         [ 356057.4688,       0.0000],
         [ 344213.9375,       0.0000],
         [ 320725.8125,       0.0000]],

        [[      0.0000,  482083.5000],
         [ 445425.5625,       0.0000],
         [      0.0000,  437591.1562],
         ...,
         [ 286657.9062,       0.0000],
         [ 272847.2500,       0.0000],
         [ 268148.2500,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2104631.0000,   175227.6250],
        [10756263.0000,        0.0000],
        [ 4980150.0000,   393351.0312],
        [ 7241361.0000,        0.0000],
        [ 1596151.0000,  1010750.6250],
        [ 3615560.2500,  1482040.3750],
        [ 6985698.5000,  1909852.0000],
        [ 5992095.0000,        0.0000],
        [ 7615520.0000,   994519.1875],
        [ 5627288.0000,        0.0000],
        [ 8952116.0000,        0.0000],
        [ 5369339.0000,        0.0000],
        [ 6557787.0000,        0.0000],
        [11287300.0000,        0.0000],
        [ 8245802.0000,        0.0000],
        [ 7081191.5000,        0.0000],
        [11173793.0000,        0.0000],
        [10191668.0000,        0.0000],
        [ 8110696.0000,        0.0000],
        [ 7530699.0000,        0.0000],
        [10538256.0000,        0.0000],
        [ 7984484.0000,        0.0000],
        [ 8857891.0000,        0.0000],
        [ 8711746.0000,        0.0000],
        [ 6829668.0000,        0.0000],
        [ 7343193.0000,        0.0000],
        [ 6968080.0000,   697939.6250],
        [ 7703536.5000,        0.0000],
        [ 9080749.0000,        0.0000],
        [ 8235415.0000,        0.0000],
        [ 9972119.0000,        0.0000],
        [ 7056342.5000,        0.0000],
        [ 2978666.5000,   274322.4375],
        [ 3659014.0000,  2438768.5000],
        [ 4072673.0000,  2799036.5000],
        [ 4736679.0000,  1584289.2500],
        [ 2019484.7500,  2343778.2500],
        [ 3991154.0000,  2611265.0000],
        [ 2381186.7500,  2916717.0000],
        [10960425.0000,        0.0000],
        [ 4262571.0000,        0.0000],
        [ 4770623.5000,        0.0000],
        [ 3504054.5000,        0.0000],
        [ 4068271.0000,        0.0000],
        [ 2193030.2500,  1338896.5000],
        [  388911.2188,   506438.3125],
        [ 6108350.0000,   635877.1250],
        [ 1403369.1250,   395860.1250],
        [  820075.2500,  2546285.7500],
        [ 4180936.0000,  2006288.0000],
        [ 2587092.5000,   319976.4062],
        [ 2698284.2500,  1576945.1250],
        [ 2570773.0000,  2519890.0000],
        [ 3050225.0000,   829283.0000],
        [ 7576949.0000,        0.0000],
        [ 5265095.5000,  1295341.3750],
        [ 2200293.0000,  2831988.0000],
        [ 2941399.0000,   885465.0000],
        [ 2251671.7500,  1999621.0000],
        [ 1289946.2500,  1523606.2500],
        [ 4468753.5000,   435332.6562],
        [ 1183398.2500,   419614.2188],
        [ 3071796.2500,  1001723.1250],
        [ 2299399.0000,  1221949.7500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 59.375
Top1 accuracy for validation set is 59.375 size is torch.Size([64, 1])
Epoch 176/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:44, 57.38s/it]  7%|▋         | 2/30 [00:59<11:38, 24.96s/it] 10%|█         | 3/30 [01:00<06:15, 13.91s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.84s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.2144452492396036
Epoch 177/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:17, 58.52s/it]  7%|▋         | 2/30 [00:59<11:36, 24.88s/it] 10%|█         | 3/30 [01:00<06:14, 13.86s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.69s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.82s/it] 20%|██        | 6/30 [01:02<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 3.1988728761672975
Epoch 178/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:42, 57.34s/it]  7%|▋         | 2/30 [01:01<12:01, 25.77s/it] 10%|█         | 3/30 [01:01<06:27, 14.35s/it] 13%|█▎        | 4/30 [01:02<03:53,  8.98s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.01s/it] 20%|██        | 6/30 [01:04<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 3.2332921981811524
Epoch 179/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:37, 59.21s/it]  7%|▋         | 2/30 [01:00<11:37, 24.90s/it] 10%|█         | 3/30 [01:00<06:14, 13.87s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.69s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.83s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 3.207351501782735
Epoch 180/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:05<31:30, 65.18s/it]  7%|▋         | 2/30 [01:05<12:43, 27.28s/it] 10%|█         | 3/30 [01:06<06:49, 15.16s/it] 13%|█▎        | 4/30 [01:07<04:06,  9.47s/it] 17%|█▋        | 5/30 [01:08<02:38,  6.33s/it] 20%|██        | 6/30 [01:08<01:46,  4.43s/it] 23%|██▎       | 7/30 [01:09<01:14,  3.23s/it] 27%|██▋       | 8/30 [01:10<00:53,  2.44s/it] 30%|███       | 9/30 [01:11<00:40,  1.91s/it] 33%|███▎      | 10/30 [01:11<00:31,  1.55s/it] 37%|███▋      | 11/30 [01:12<00:24,  1.30s/it] 40%|████      | 12/30 [01:13<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:14<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:14,  1.07it/s] 50%|█████     | 15/30 [01:15<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:16<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:17<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:18<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:19<00:07,  1.30it/s] 70%|███████   | 21/30 [01:20<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:21<00:05,  1.32it/s] 80%|████████  | 24/30 [01:22<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:23<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:24<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:25<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:26<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  1.34it/s]100%|██████████| 30/30 [01:27<00:00,  2.90s/it]
Epoch loss is 3.1763548374176027
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0165,  0.0107, -0.0176,  ...,  0.0020, -0.0232,  0.0289],
        [-0.0124,  0.0218,  0.0011,  ...,  0.0126, -0.0283, -0.0057],
        [-0.0526, -0.0277,  0.0063,  ...,  0.0649, -0.0336, -0.0385],
        ...,
        [ 0.0359, -0.0229, -0.0277,  ..., -0.0254, -0.0388, -0.0124],
        [-0.0324, -0.0043, -0.0106,  ..., -0.0142,  0.0155, -0.0188],
        [-0.0344, -0.0001,  0.0091,  ...,  0.0041,  0.0133, -0.0587]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9201, 0.8885, 0.8611, 0.8537, 0.8519, 0.8512, 0.8494, 0.8473, 0.8410,
         0.8342],
        [0.9745, 0.9738, 0.9738, 0.9722, 0.9696, 0.9690, 0.9689, 0.9688, 0.9686,
         0.9682],
        [0.9475, 0.9307, 0.9281, 0.9243, 0.9222, 0.9208, 0.9182, 0.9072, 0.8976,
         0.8960],
        [0.9767, 0.9555, 0.9524, 0.9456, 0.9319, 0.9303, 0.9299, 0.9295, 0.9285,
         0.9284],
        [0.8782, 0.8753, 0.8726, 0.8715, 0.8701, 0.8673, 0.8661, 0.8629, 0.8591,
         0.8565],
        [0.9293, 0.9258, 0.9240, 0.9217, 0.9100, 0.9095, 0.9084, 0.9009, 0.8993,
         0.8960],
        [0.9717, 0.9682, 0.9669, 0.9544, 0.9537, 0.9478, 0.9468, 0.9450, 0.9438,
         0.9408],
        [0.9597, 0.9593, 0.9484, 0.9278, 0.9247, 0.9180, 0.9056, 0.9043, 0.8957,
         0.8938],
        [0.9667, 0.9647, 0.9523, 0.9520, 0.9485, 0.9482, 0.9478, 0.9475, 0.9468,
         0.9444],
        [0.9689, 0.9317, 0.9265, 0.9247, 0.9245, 0.9194, 0.9131, 0.9125, 0.9039,
         0.9036],
        [0.9708, 0.9596, 0.9593, 0.9586, 0.9569, 0.9564, 0.9556, 0.9517, 0.9502,
         0.9499],
        [0.9378, 0.9363, 0.9354, 0.9322, 0.9319, 0.9282, 0.8965, 0.8959, 0.8934,
         0.8760],
        [0.9767, 0.9529, 0.9485, 0.9422, 0.9289, 0.9234, 0.9218, 0.9134, 0.9134,
         0.9017],
        [0.9779, 0.9773, 0.9761, 0.9761, 0.9750, 0.9744, 0.9734, 0.9730, 0.9730,
         0.9710],
        [0.9621, 0.9562, 0.9551, 0.9544, 0.9514, 0.9498, 0.9467, 0.9465, 0.9456,
         0.9450],
        [0.9558, 0.9461, 0.9443, 0.9423, 0.9399, 0.9383, 0.9351, 0.9348, 0.9345,
         0.9330],
        [0.9863, 0.9826, 0.9773, 0.9747, 0.9739, 0.9714, 0.9688, 0.9662, 0.9647,
         0.9647],
        [0.9820, 0.9733, 0.9710, 0.9689, 0.9637, 0.9621, 0.9618, 0.9606, 0.9597,
         0.9596],
        [0.9702, 0.9636, 0.9616, 0.9578, 0.9499, 0.9477, 0.9460, 0.9341, 0.9330,
         0.9312],
        [0.9571, 0.9561, 0.9531, 0.9439, 0.9409, 0.9403, 0.9403, 0.9374, 0.9368,
         0.9366],
        [0.9886, 0.9776, 0.9700, 0.9698, 0.9694, 0.9686, 0.9645, 0.9623, 0.9606,
         0.9586],
        [0.9693, 0.9569, 0.9565, 0.9512, 0.9479, 0.9449, 0.9435, 0.9432, 0.9415,
         0.9401],
        [0.9830, 0.9642, 0.9588, 0.9584, 0.9573, 0.9527, 0.9525, 0.9515, 0.9455,
         0.9420],
        [0.9653, 0.9609, 0.9591, 0.9585, 0.9553, 0.9548, 0.9526, 0.9510, 0.9504,
         0.9480],
        [0.9770, 0.9644, 0.9383, 0.9371, 0.9278, 0.9245, 0.9220, 0.9196, 0.9193,
         0.9181],
        [0.9491, 0.9487, 0.9486, 0.9465, 0.9459, 0.9443, 0.9431, 0.9429, 0.9424,
         0.9412],
        [0.9635, 0.9604, 0.9534, 0.9480, 0.9423, 0.9417, 0.9416, 0.9388, 0.9370,
         0.9361],
        [0.9632, 0.9536, 0.9496, 0.9493, 0.9492, 0.9489, 0.9438, 0.9409, 0.9405,
         0.9389],
        [0.9709, 0.9626, 0.9598, 0.9594, 0.9590, 0.9586, 0.9553, 0.9538, 0.9530,
         0.9464],
        [0.9578, 0.9568, 0.9540, 0.9524, 0.9516, 0.9506, 0.9503, 0.9500, 0.9475,
         0.9446],
        [0.9718, 0.9688, 0.9688, 0.9682, 0.9673, 0.9660, 0.9659, 0.9641, 0.9592,
         0.9590],
        [0.9548, 0.9547, 0.9430, 0.9408, 0.9399, 0.9392, 0.9391, 0.9351, 0.9331,
         0.9317],
        [0.9046, 0.9016, 0.8987, 0.8864, 0.8844, 0.8812, 0.8803, 0.8735, 0.8734,
         0.8730],
        [0.9508, 0.9461, 0.9404, 0.9307, 0.9284, 0.9272, 0.9213, 0.9200, 0.9161,
         0.9112],
        [0.9548, 0.9476, 0.9383, 0.9374, 0.9342, 0.9338, 0.9329, 0.9324, 0.9321,
         0.9299],
        [0.9493, 0.9447, 0.9424, 0.9347, 0.9287, 0.9261, 0.9252, 0.9210, 0.9176,
         0.9165],
        [0.9522, 0.9084, 0.9045, 0.9034, 0.8998, 0.8981, 0.8977, 0.8931, 0.8846,
         0.8834],
        [0.9724, 0.9565, 0.9421, 0.9346, 0.9252, 0.9220, 0.9220, 0.9208, 0.9194,
         0.9165],
        [0.9466, 0.9404, 0.9392, 0.9220, 0.9154, 0.9129, 0.9002, 0.8988, 0.8987,
         0.8971],
        [0.9774, 0.9763, 0.9728, 0.9718, 0.9710, 0.9708, 0.9707, 0.9702, 0.9686,
         0.9685],
        [0.9357, 0.9259, 0.9153, 0.9037, 0.9019, 0.9011, 0.8937, 0.8903, 0.8849,
         0.8849],
        [0.9342, 0.9225, 0.9172, 0.9158, 0.9148, 0.9032, 0.9016, 0.9013, 0.9001,
         0.8998],
        [0.9118, 0.9059, 0.9041, 0.8901, 0.8881, 0.8870, 0.8850, 0.8847, 0.8834,
         0.8806],
        [0.9432, 0.9138, 0.9105, 0.9097, 0.9075, 0.8909, 0.8835, 0.8816, 0.8805,
         0.8795],
        [0.9058, 0.9035, 0.8965, 0.8961, 0.8959, 0.8902, 0.8843, 0.8839, 0.8766,
         0.8743],
        [0.8653, 0.8165, 0.8148, 0.7935, 0.7790, 0.7786, 0.7769, 0.7642, 0.7619,
         0.7516],
        [0.9605, 0.9497, 0.9363, 0.9356, 0.9337, 0.9333, 0.9288, 0.9285, 0.9281,
         0.9280],
        [0.8741, 0.8656, 0.8594, 0.8542, 0.8428, 0.8405, 0.8380, 0.8167, 0.8093,
         0.8076],
        [0.9156, 0.8994, 0.8948, 0.8861, 0.8839, 0.8789, 0.8753, 0.8747, 0.8693,
         0.8630],
        [0.9426, 0.9425, 0.9386, 0.9336, 0.9294, 0.9241, 0.9221, 0.9215, 0.9203,
         0.9200],
        [0.8963, 0.8849, 0.8821, 0.8805, 0.8750, 0.8721, 0.8689, 0.8617, 0.8612,
         0.8597],
        [0.9248, 0.9161, 0.9109, 0.9086, 0.9065, 0.9002, 0.9001, 0.8938, 0.8897,
         0.8883],
        [0.9619, 0.9302, 0.9271, 0.9229, 0.9161, 0.9147, 0.9027, 0.8999, 0.8923,
         0.8919],
        [0.9412, 0.9120, 0.9116, 0.8979, 0.8940, 0.8933, 0.8895, 0.8792, 0.8771,
         0.8768],
        [0.9643, 0.9537, 0.9502, 0.9439, 0.9439, 0.9428, 0.9421, 0.9420, 0.9402,
         0.9383],
        [0.9557, 0.9399, 0.9368, 0.9333, 0.9329, 0.9326, 0.9322, 0.9299, 0.9296,
         0.9281],
        [0.9595, 0.9373, 0.9294, 0.9201, 0.9132, 0.9002, 0.8973, 0.8960, 0.8948,
         0.8928],
        [0.9543, 0.9169, 0.9018, 0.8882, 0.8862, 0.8827, 0.8812, 0.8806, 0.8771,
         0.8771],
        [0.9547, 0.9116, 0.9106, 0.9046, 0.9033, 0.9007, 0.8965, 0.8838, 0.8744,
         0.8713],
        [0.9488, 0.8759, 0.8717, 0.8693, 0.8659, 0.8655, 0.8569, 0.8492, 0.8486,
         0.8348],
        [0.9277, 0.9248, 0.9220, 0.9183, 0.9149, 0.9112, 0.9096, 0.9079, 0.9073,
         0.9046],
        [0.8628, 0.8419, 0.8408, 0.8392, 0.8351, 0.8338, 0.8245, 0.8213, 0.8179,
         0.8176],
        [0.9286, 0.9043, 0.9018, 0.8993, 0.8992, 0.8992, 0.8956, 0.8899, 0.8892,
         0.8866],
        [0.9106, 0.9069, 0.9022, 0.8943, 0.8828, 0.8791, 0.8752, 0.8750, 0.8711,
         0.8699]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 1, 1, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 1, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 1, 1],
        [0, 0, 1, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 511372.5000,  325226.7500,  219949.4844,  197909.7031,  193008.4688,
          190937.1250,  186093.8750,  180562.1406,  165156.1250,  149805.1094],
        [1111153.2500, 1101204.1250, 1099995.0000, 1075588.1250, 1036046.5000,
         1027978.9375, 1026287.3125, 1025321.7500, 1021322.4375, 1015624.9375],
        [ 756375.3750,  594330.5000,  573031.2500,  542926.9375,  526354.3125,
          516133.3438,  497672.1250,  425163.8750,  370730.5312,  362391.6562],
        [1146687.3750,  847075.9375,  810414.3125,  735349.7500,  604795.0625,
          591039.7500,  587560.3750,  584293.1875,  576560.3750,  575162.6875],
        [ 280782.0000,  269384.4375,  259314.6406,  255389.2969,  250248.6094,
          240463.5312,  236343.1875,  225688.5000,  213766.0312,  206001.4688],
        [ 583133.6250,  554434.1250,  540203.0625,  522934.5312,  442595.7188,
          439200.4062,  432210.6250,  388318.1562,  379621.7188,  362405.8125],
        [1068733.3750, 1016508.5625,  997867.1875,  834556.6875,  826126.4375,
          758805.8750,  748547.1250,  729822.7500,  716597.3125,  686511.1875],
        [ 900241.8125,  894342.5000,  765386.1250,  570571.3750,  546086.7500,
          495939.9375,  415592.2812,  408065.4062,  360599.6562,  351246.0625],
        [ 994396.9375,  966986.1250,  809386.2500,  805963.3125,  766862.0000,
          764075.6250,  759198.8750,  755547.7500,  748253.7500,  723472.2500],
        [1026332.3125,  602949.3750,  559694.0625,  546177.8750,  544531.3125,
          505698.3438,  462415.4375,  458217.0000,  405760.3125,  403764.2812],
        [1053932.7500,  898527.2500,  894805.7500,  886167.3750,  865111.9375,
          858160.2500,  848997.5625,  803084.5625,  786014.0000,  782339.8125],
        [ 657708.9375,  644047.2500,  635528.5000,  607202.7500,  604626.6875,
          574091.8750,  364894.1250,  361928.5000,  349202.6562,  272235.9062],
        [1147919.3750,  816603.8125,  767100.4375,  701211.3125,  579344.9375,
          535530.6250,  523603.2188,  464751.5312,  464355.4688,  392721.6875],
        [1166482.6250, 1157817.7500, 1138028.6250, 1137991.7500, 1120103.0000,
         1110907.3750, 1094719.5000, 1088857.5000, 1087725.1250, 1057991.1250],
        [ 930919.3750,  856172.1875,  842421.1250,  834581.3750,  799140.7500,
          781356.3750,  747151.3750,  745448.1875,  735278.8750,  729535.3125],
        [ 850879.7500,  741469.1875,  722267.1875,  701325.6875,  678370.1250,
          662734.0000,  632871.5000,  630763.8125,  627922.5000,  614395.9375],
        [1315976.8750, 1247544.6250, 1157735.0000, 1114943.7500, 1102766.8750,
         1063301.3750, 1025282.6250,  987763.7500,  966678.2500,  965805.6250],
        [1237917.6250, 1093235.8750, 1057660.1250, 1026351.9375,  952860.1250,
          930845.6875,  927386.4375,  911922.6875,  900388.6875,  898098.8750],
        [1045378.9375,  950824.8750,  924015.8125,  875652.8125,  782663.0000,
          758279.1875,  740370.4375,  624141.5000,  614604.0000,  599324.3750],
        [ 866541.3125,  854879.8125,  819405.9375,  717648.4375,  687461.1250,
          682483.5000,  681680.1250,  654457.2500,  648857.2500,  646751.1875],
        [1360686.3750, 1162218.5000, 1042690.6250, 1039206.1250, 1034254.7500,
         1022164.3125,  964050.6875,  934262.6250,  911430.5625,  885586.1250],
        [1031992.5000,  864459.6250,  859316.6875,  797018.0625,  760466.2500,
          728396.5625,  713858.6250,  711283.8125,  694302.1875,  680425.2500],
        [1255446.8750,  960132.0000,  888650.4375,  883658.4375,  869509.8750,
          813658.3750,  812201.6250,  800602.3125,  734186.5000,  698692.7500],
        [ 975183.0625,  915666.4375,  892540.3750,  884479.5625,  845210.3125,
          838549.6250,  812846.3125,  794718.2500,  787858.0000,  761889.0000],
        [1152879.3750,  962296.3125,  663298.0625,  651705.5625,  570708.0000,
          543920.3750,  525444.5625,  507302.9688,  505021.1875,  496703.9062],
        [ 772874.4375,  769494.9375,  767865.3125,  744741.1250,  738393.2500,
          722183.1875,  709875.6250,  708114.3125,  703176.1250,  691006.6250],
        [ 949940.3125,  908655.9375,  822877.4375,  760935.6250,  701974.7500,
          695579.9375,  695131.0625,  667138.0625,  651098.6250,  642225.0625],
        [ 945402.5000,  825119.4375,  779197.6250,  775688.5000,  774090.0000,
          771562.8125,  717235.8750,  687591.0000,  683697.8125,  668133.2500],
        [1055973.1250,  937442.5625,  900671.1875,  896280.6875,  890669.7500,
          885710.3125,  845220.7500,  826768.8125,  818215.0625,  743997.1875],
        [ 875908.3125,  862809.1250,  829355.8750,  810961.6875,  801372.3125,
          789606.8125,  786554.6250,  783437.3750,  756228.1875,  724765.0625],
        [1069106.5000, 1024704.9375, 1024184.1875, 1015604.5000, 1002448.3750,
          984031.9375,  982896.1875,  957621.8125,  893331.4375,  891281.5625],
        [ 838769.5625,  837401.2500,  708947.4375,  687361.5000,  677991.0625,
          671214.0625,  670578.8125,  633380.5000,  615145.8125,  602775.6875],
        [ 409688.3125,  392375.8125,  376445.5312,  316017.6250,  306836.8125,
          293293.2188,  289359.5000,  262490.5000,  262387.1250,  260963.4219],
        [ 792654.1250,  740530.0000,  683062.3750,  594995.6875,  575468.8750,
          565754.9375,  520250.8750,  510680.0000,  482487.7812,  450216.8750],
        [ 838787.1875,  757138.1875,  663228.5000,  653953.1250,  624992.6875,
          621579.5625,  613724.8750,  609189.8750,  606349.1875,  587882.1250],
        [ 775119.0625,  726051.0625,  702687.4375,  629288.1875,  577927.3125,
          556524.0000,  549749.1875,  517339.6875,  493266.4062,  485148.7188],
        [ 808232.3125,  432592.0625,  409178.7812,  402734.4062,  382303.3125,
          373017.6562,  371276.8125,  347745.3750,  307838.9062,  302338.8750],
        [1078755.1250,  859294.5000,  699676.3125,  629111.1250,  549613.4375,
          525233.1250,  524956.1875,  515989.1562,  505784.6875,  485306.9688],
        [ 746148.0625,  682800.5000,  671718.0625,  525135.9375,  478044.1562,
          461303.7188,  384857.4062,  377163.8750,  376708.0625,  367766.1875],
        [1158792.1250, 1141425.1250, 1084526.5000, 1070057.1250, 1057610.7500,
         1054194.0000, 1052856.8750, 1045176.5625, 1021472.5000, 1020170.8750],
        [ 638946.8750,  554956.7500,  477181.9375,  404478.4375,  394318.5625,
          389581.9375,  350282.0000,  333764.2500,  309225.9688,  309184.6875],
        [ 625324.1250,  528653.8750,  490578.2812,  480487.5938,  473511.3750,
          401586.5000,  392466.7188,  390541.6562,  384148.5625,  382324.0938],
        [ 453820.6562,  417419.0312,  406401.6562,  333090.4688,  323427.2500,
          318376.5312,  309633.5000,  308421.6562,  302382.3750,  290734.2500],
        [ 711058.6875,  467024.1875,  445726.3750,  440274.8125,  426593.0625,
          336528.9688,  302877.0938,  294758.9062,  290079.8125,  286050.8438],
        [ 416915.3750,  403042.5625,  364636.3438,  362575.5625,  361768.7188,
          333606.4375,  306609.5312,  304735.1250,  274607.2188,  265688.0000],
        [ 233471.2188,  116344.4531,  113506.1406,   83752.7891,   68090.0156,
           67720.6250,   66042.5781,   55125.3086,   53329.2969,   46016.7227],
        [ 910323.8125,  780354.8125,  644525.8750,  637505.0000,  620898.2500,
          617539.3125,  579074.3125,  575864.1250,  572612.8750,  572365.0000],
        [ 265062.1250,  234480.9219,  214795.7656,  199457.3750,  169287.1250,
          163945.7500,  158270.8438,  116735.0000,  104999.8516,  102501.4219],
        [ 479086.0938,  380379.8438,  356196.3438,  314515.8125,  304780.1875,
          283716.0000,  269401.9375,  267218.2500,  247206.2656,  226001.6562],
        [ 705333.4375,  703436.3750,  665927.7500,  620124.1875,  583965.1250,
          541247.8125,  525926.8750,  521503.1875,  512296.0312,  510447.6875],
        [ 363740.9688,  309188.2188,  297032.5000,  290406.1562,  268502.3750,
          257488.8281,  246064.0781,  221966.0781,  220295.2344,  215609.3125],
        [ 546452.5000,  482636.4375,  448428.2500,  433635.4375,  420655.2188,
          384661.4688,  384195.5000,  351181.4062,  331081.5938,  324352.3438],
        [ 928320.0000,  590540.5625,  565085.7500,  532021.7500,  482710.5625,
          473075.3125,  398874.2500,  382926.1562,  343664.5000,  341588.6562],
        [ 690891.3750,  455067.5312,  452411.0938,  372440.3750,  352185.9062,
          348308.2812,  329980.2188,  285057.1250,  276415.9688,  275333.5938],
        [ 960302.3125,  825740.5000,  785302.1875,  717940.6875,  717940.6875,
          706644.3125,  699852.5000,  698349.0625,  681337.0000,  663213.3125],
        [ 849647.1250,  677709.2500,  648966.7500,  617432.1875,  613711.3750,
          610787.3125,  607754.8125,  588276.4375,  585177.0625,  573304.5625],
        [ 897035.7500,  653083.6875,  583821.9375,  511042.9375,  463357.9375,
          384587.7188,  369060.9375,  362181.9688,  356060.5312,  346272.0938],
        [ 832640.0000,  488383.3750,  393500.3750,  323796.9375,  314940.2188,
          299513.1875,  292988.8125,  290758.0938,  276691.0625,  276312.1250],
        [ 838198.6875,  452907.9688,  446012.1250,  409675.0312,  402051.6875,
          387220.9375,  364614.0938,  304136.7500,  265863.1250,  254570.5625],
        [ 770431.1875,  271967.3438,  256053.6250,  247213.1094,  235793.1875,
          234373.1719,  207082.8594,  185705.2656,  183952.6562,  151033.7500],
        [ 569440.7500,  546521.2500,  525194.5625,  498127.0312,  474423.5000,
          449816.4375,  439642.5000,  429584.3438,  425614.9688,  409452.0312],
        [ 225473.5781,  167293.4844,  164568.1094,  160950.5469,  151789.6719,
          148975.3594,  130342.3594,  124578.7891,  118643.4375,  118117.0000],
        [ 576949.2500,  407882.5625,  393528.1562,  379513.4688,  379297.0938,
          379046.8750,  360218.8125,  332179.0938,  328526.4375,  316483.5625],
        [ 446049.5625,  423399.6562,  395569.0312,  353701.9375,  300137.4062,
          284528.3125,  268979.0938,  268293.2812,  253776.9688,  249341.2031]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 511372.5000,       0.0000],
         [ 325226.7500,       0.0000],
         [ 219949.4844,       0.0000],
         ...,
         [ 180562.1406,       0.0000],
         [ 165156.1250,       0.0000],
         [ 149805.1094,       0.0000]],

        [[1111153.2500,       0.0000],
         [1101204.1250,       0.0000],
         [1099995.0000,       0.0000],
         ...,
         [1025321.7500,       0.0000],
         [1021322.4375,       0.0000],
         [1015624.9375,       0.0000]],

        [[ 756375.3750,       0.0000],
         [ 594330.5000,       0.0000],
         [ 573031.2500,       0.0000],
         ...,
         [ 425163.8750,       0.0000],
         [ 370730.5312,       0.0000],
         [      0.0000,  362391.6562]],

        ...,

        [[ 225473.5781,       0.0000],
         [      0.0000,  167293.4844],
         [ 164568.1094,       0.0000],
         ...,
         [ 124578.7891,       0.0000],
         [ 118643.4375,       0.0000],
         [ 118117.0000,       0.0000]],

        [[      0.0000,  576949.2500],
         [ 407882.5625,       0.0000],
         [ 393528.1562,       0.0000],
         ...,
         [ 332179.0938,       0.0000],
         [ 328526.4375,       0.0000],
         [ 316483.5625,       0.0000]],

        [[      0.0000,  446049.5625],
         [      0.0000,  423399.6562],
         [ 395569.0312,       0.0000],
         ...,
         [ 268293.2812,       0.0000],
         [      0.0000,  253776.9688],
         [ 249341.2031,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2133927.5000,   186093.8750],
        [10540522.0000,        0.0000],
        [ 4802718.5000,   362391.6562],
        [ 7058939.0000,        0.0000],
        [ 1440942.0000,   996439.7500],
        [ 3293984.2500,  1351073.5000],
        [ 6560083.0000,  1823993.6250],
        [ 5708072.0000,        0.0000],
        [ 7127156.5000,   966986.1250],
        [ 5515540.5000,        0.0000],
        [ 8677141.0000,        0.0000],
        [ 5071467.0000,        0.0000],
        [ 6393142.5000,        0.0000],
        [11160623.0000,        0.0000],
        [ 8002005.0000,        0.0000],
        [ 6863000.0000,        0.0000],
        [10947798.0000,        0.0000],
        [ 9936668.0000,        0.0000],
        [ 7915255.0000,        0.0000],
        [ 7260166.0000,        0.0000],
        [10356551.0000,        0.0000],
        [ 7841520.0000,        0.0000],
        [ 8716739.0000,        0.0000],
        [ 8508941.0000,        0.0000],
        [ 6579280.0000,        0.0000],
        [ 7327725.5000,        0.0000],
        [ 6828418.5000,   667138.0625],
        [ 7627719.0000,        0.0000],
        [ 8800950.0000,        0.0000],
        [ 8020999.5000,        0.0000],
        [ 9845212.0000,        0.0000],
        [ 6943566.0000,        0.0000],
        [ 2907367.5000,   262490.5000],
        [ 3564604.0000,  2351497.5000],
        [ 4461085.0000,  2115740.5000],
        [ 4511931.0000,  1501170.1250],
        [ 1897689.8750,  2239568.5000],
        [ 3806560.0000,  2567160.7500],
        [ 2271400.5000,  2800245.7500],
        [10706283.0000,        0.0000],
        [ 4161921.5000,        0.0000],
        [ 4549623.0000,        0.0000],
        [ 3463707.0000,        0.0000],
        [ 4000972.7500,        0.0000],
        [ 2078193.5000,  1315991.3750],
        [  394060.6250,   509338.5000],
        [ 5873558.0000,   637505.0000],
        [ 1356469.6250,   373066.6250],
        [  736576.1875,  2391926.0000],
        [ 3940191.0000,  1950017.7500],
        [ 2393261.5000,   297032.5000],
        [ 2584548.5000,  1522731.8750],
        [ 2533592.7500,  2505214.7500],
        [ 3033494.5000,   804597.0000],
        [ 7456623.0000,        0.0000],
        [ 5109880.5000,  1262886.2500],
        [ 2147100.5000,  2779405.0000],
        [ 2907640.2500,   881883.7500],
        [ 2187956.0000,  1937294.8750],
        [ 1259751.6250,  1483854.6250],
        [ 4342202.5000,   425614.9688],
        [ 1343438.8750,   167293.4844],
        [ 2916457.2500,   937168.0625],
        [ 1836022.0000,  1407754.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 59.375
Top1 accuracy for validation set is 59.375 size is torch.Size([64, 1])
Epoch 181/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<31:09, 64.48s/it]  7%|▋         | 2/30 [01:05<12:35, 26.99s/it] 10%|█         | 3/30 [01:05<06:45, 15.01s/it] 13%|█▎        | 4/30 [01:06<04:03,  9.38s/it] 17%|█▋        | 5/30 [01:07<02:36,  6.27s/it] 20%|██        | 6/30 [01:08<01:45,  4.39s/it] 23%|██▎       | 7/30 [01:08<01:13,  3.20s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.42s/it] 30%|███       | 9/30 [01:10<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:11<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:14<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.33it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.88s/it]
Epoch loss is 3.1546799103418985
Epoch 182/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:19, 56.53s/it]  7%|▋         | 2/30 [01:00<11:56, 25.58s/it] 10%|█         | 3/30 [01:01<06:24, 14.24s/it] 13%|█▎        | 4/30 [01:01<03:51,  8.92s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.97s/it] 20%|██        | 6/30 [01:03<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.33s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 3.1371211051940917
Epoch 183/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:42, 61.46s/it]  7%|▋         | 2/30 [01:02<12:00, 25.75s/it] 10%|█         | 3/30 [01:02<06:26, 14.33s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.97s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.00s/it] 20%|██        | 6/30 [01:05<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 3.1054099877675374
Epoch 184/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:48, 61.68s/it]  7%|▋         | 2/30 [01:02<12:03, 25.84s/it] 10%|█         | 3/30 [01:03<06:28, 14.38s/it] 13%|█▎        | 4/30 [01:03<03:53,  9.00s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.02s/it] 20%|██        | 6/30 [01:05<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 3.1577124277750652
Epoch 185/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:09<33:23, 69.08s/it]  7%|▋         | 2/30 [01:09<13:28, 28.88s/it] 10%|█         | 3/30 [01:10<07:12, 16.04s/it] 13%|█▎        | 4/30 [01:11<04:20, 10.00s/it] 17%|█▋        | 5/30 [01:12<02:46,  6.66s/it] 20%|██        | 6/30 [01:12<01:51,  4.65s/it] 23%|██▎       | 7/30 [01:13<01:17,  3.38s/it] 27%|██▋       | 8/30 [01:14<00:55,  2.54s/it] 30%|███       | 9/30 [01:15<00:41,  1.98s/it] 33%|███▎      | 10/30 [01:15<00:31,  1.60s/it] 37%|███▋      | 11/30 [01:16<00:25,  1.34s/it] 40%|████      | 12/30 [01:17<00:20,  1.16s/it] 43%|████▎     | 13/30 [01:18<00:17,  1.03s/it] 47%|████▋     | 14/30 [01:18<00:15,  1.06it/s] 50%|█████     | 15/30 [01:19<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:20<00:11,  1.18it/s] 57%|█████▋    | 17/30 [01:21<00:10,  1.23it/s] 60%|██████    | 18/30 [01:21<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:22<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:23<00:07,  1.30it/s] 70%|███████   | 21/30 [01:24<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:24<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:25<00:05,  1.32it/s] 80%|████████  | 24/30 [01:26<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:27<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:27<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:28<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:29<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:30<00:00,  1.34it/s]100%|██████████| 30/30 [01:30<00:00,  1.34it/s]100%|██████████| 30/30 [01:30<00:00,  3.03s/it]
Epoch loss is 3.1488007307052612
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0188,  0.0107, -0.0144,  ...,  0.0034, -0.0224,  0.0265],
        [-0.0112,  0.0221,  0.0025,  ...,  0.0126, -0.0276, -0.0062],
        [-0.0521, -0.0290,  0.0069,  ...,  0.0657, -0.0346, -0.0366],
        ...,
        [ 0.0356, -0.0228, -0.0251,  ..., -0.0252, -0.0392, -0.0125],
        [-0.0320, -0.0049, -0.0108,  ..., -0.0127,  0.0143, -0.0179],
        [-0.0345, -0.0014,  0.0109,  ...,  0.0056,  0.0136, -0.0584]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9197, 0.8873, 0.8618, 0.8543, 0.8523, 0.8515, 0.8492, 0.8485, 0.8415,
         0.8340],
        [0.9728, 0.9723, 0.9723, 0.9709, 0.9685, 0.9685, 0.9670, 0.9669, 0.9663,
         0.9663],
        [0.9452, 0.9271, 0.9262, 0.9198, 0.9191, 0.9183, 0.9181, 0.9044, 0.8963,
         0.8918],
        [0.9752, 0.9518, 0.9465, 0.9404, 0.9304, 0.9297, 0.9296, 0.9295, 0.9254,
         0.9244],
        [0.8760, 0.8701, 0.8696, 0.8694, 0.8684, 0.8635, 0.8614, 0.8536, 0.8504,
         0.8474],
        [0.9244, 0.9180, 0.9161, 0.9161, 0.9058, 0.9048, 0.8999, 0.8935, 0.8899,
         0.8866],
        [0.9678, 0.9642, 0.9632, 0.9495, 0.9483, 0.9425, 0.9403, 0.9402, 0.9380,
         0.9350],
        [0.9572, 0.9557, 0.9457, 0.9237, 0.9191, 0.9148, 0.9031, 0.9002, 0.8915,
         0.8907],
        [0.9639, 0.9630, 0.9467, 0.9466, 0.9427, 0.9426, 0.9417, 0.9393, 0.9393,
         0.9381],
        [0.9682, 0.9299, 0.9263, 0.9225, 0.9210, 0.9166, 0.9125, 0.9124, 0.9031,
         0.9027],
        [0.9697, 0.9573, 0.9568, 0.9554, 0.9547, 0.9542, 0.9536, 0.9483, 0.9477,
         0.9454],
        [0.9332, 0.9329, 0.9319, 0.9293, 0.9234, 0.9204, 0.8926, 0.8896, 0.8892,
         0.8667],
        [0.9745, 0.9492, 0.9467, 0.9394, 0.9292, 0.9221, 0.9212, 0.9131, 0.9115,
         0.9021],
        [0.9780, 0.9753, 0.9753, 0.9742, 0.9739, 0.9730, 0.9725, 0.9717, 0.9711,
         0.9696],
        [0.9603, 0.9546, 0.9532, 0.9531, 0.9519, 0.9474, 0.9453, 0.9450, 0.9448,
         0.9444],
        [0.9534, 0.9444, 0.9421, 0.9400, 0.9384, 0.9363, 0.9340, 0.9331, 0.9326,
         0.9310],
        [0.9861, 0.9808, 0.9765, 0.9727, 0.9725, 0.9713, 0.9681, 0.9646, 0.9629,
         0.9627],
        [0.9808, 0.9720, 0.9693, 0.9662, 0.9622, 0.9604, 0.9594, 0.9593, 0.9584,
         0.9579],
        [0.9681, 0.9614, 0.9604, 0.9571, 0.9499, 0.9456, 0.9399, 0.9329, 0.9315,
         0.9295],
        [0.9546, 0.9529, 0.9519, 0.9390, 0.9389, 0.9387, 0.9387, 0.9361, 0.9346,
         0.9346],
        [0.9884, 0.9770, 0.9699, 0.9689, 0.9672, 0.9671, 0.9626, 0.9603, 0.9601,
         0.9571],
        [0.9672, 0.9563, 0.9555, 0.9480, 0.9437, 0.9434, 0.9422, 0.9418, 0.9407,
         0.9402],
        [0.9823, 0.9635, 0.9586, 0.9583, 0.9566, 0.9535, 0.9508, 0.9496, 0.9422,
         0.9416],
        [0.9633, 0.9599, 0.9568, 0.9564, 0.9534, 0.9533, 0.9498, 0.9496, 0.9495,
         0.9470],
        [0.9766, 0.9602, 0.9365, 0.9354, 0.9264, 0.9203, 0.9179, 0.9160, 0.9153,
         0.9151],
        [0.9504, 0.9500, 0.9490, 0.9461, 0.9455, 0.9430, 0.9416, 0.9404, 0.9399,
         0.9394],
        [0.9622, 0.9586, 0.9501, 0.9465, 0.9414, 0.9408, 0.9405, 0.9356, 0.9349,
         0.9340],
        [0.9634, 0.9528, 0.9493, 0.9485, 0.9481, 0.9470, 0.9419, 0.9402, 0.9401,
         0.9378],
        [0.9691, 0.9595, 0.9590, 0.9571, 0.9568, 0.9566, 0.9525, 0.9522, 0.9510,
         0.9441],
        [0.9568, 0.9565, 0.9513, 0.9506, 0.9499, 0.9496, 0.9489, 0.9479, 0.9447,
         0.9438],
        [0.9710, 0.9684, 0.9673, 0.9667, 0.9662, 0.9656, 0.9655, 0.9627, 0.9584,
         0.9583],
        [0.9544, 0.9536, 0.9415, 0.9399, 0.9399, 0.9385, 0.9381, 0.9321, 0.9304,
         0.9295],
        [0.9062, 0.9011, 0.8984, 0.8868, 0.8824, 0.8813, 0.8749, 0.8741, 0.8709,
         0.8701],
        [0.9493, 0.9437, 0.9372, 0.9283, 0.9280, 0.9246, 0.9209, 0.9190, 0.9136,
         0.9085],
        [0.9525, 0.9467, 0.9401, 0.9341, 0.9329, 0.9313, 0.9311, 0.9305, 0.9292,
         0.9290],
        [0.9468, 0.9411, 0.9387, 0.9342, 0.9254, 0.9247, 0.9217, 0.9189, 0.9181,
         0.9151],
        [0.9510, 0.9020, 0.9003, 0.8979, 0.8976, 0.8964, 0.8955, 0.8889, 0.8806,
         0.8799],
        [0.9714, 0.9544, 0.9395, 0.9339, 0.9218, 0.9215, 0.9195, 0.9189, 0.9173,
         0.9127],
        [0.9436, 0.9393, 0.9372, 0.9198, 0.9137, 0.9127, 0.8959, 0.8950, 0.8949,
         0.8945],
        [0.9755, 0.9754, 0.9704, 0.9702, 0.9702, 0.9701, 0.9698, 0.9683, 0.9682,
         0.9679],
        [0.9312, 0.9239, 0.9119, 0.9029, 0.9025, 0.9007, 0.8891, 0.8888, 0.8846,
         0.8844],
        [0.9308, 0.9187, 0.9186, 0.9123, 0.9117, 0.8997, 0.8997, 0.8993, 0.8987,
         0.8984],
        [0.9131, 0.9055, 0.9025, 0.8924, 0.8904, 0.8867, 0.8837, 0.8835, 0.8814,
         0.8804],
        [0.9428, 0.9112, 0.9101, 0.9089, 0.9077, 0.8884, 0.8825, 0.8814, 0.8800,
         0.8796],
        [0.9016, 0.9003, 0.8932, 0.8916, 0.8894, 0.8884, 0.8843, 0.8817, 0.8755,
         0.8704],
        [0.8629, 0.8182, 0.8168, 0.7938, 0.7801, 0.7764, 0.7730, 0.7687, 0.7675,
         0.7510],
        [0.9591, 0.9470, 0.9351, 0.9337, 0.9313, 0.9307, 0.9263, 0.9260, 0.9257,
         0.9256],
        [0.8717, 0.8634, 0.8570, 0.8496, 0.8397, 0.8374, 0.8294, 0.8117, 0.8073,
         0.8069],
        [0.9127, 0.8920, 0.8890, 0.8823, 0.8794, 0.8753, 0.8683, 0.8676, 0.8639,
         0.8591],
        [0.9414, 0.9412, 0.9362, 0.9287, 0.9247, 0.9211, 0.9174, 0.9171, 0.9169,
         0.9158],
        [0.8868, 0.8863, 0.8803, 0.8724, 0.8689, 0.8623, 0.8612, 0.8595, 0.8541,
         0.8506],
        [0.9225, 0.9155, 0.9078, 0.9037, 0.9028, 0.9021, 0.8951, 0.8920, 0.8861,
         0.8853],
        [0.9619, 0.9287, 0.9272, 0.9222, 0.9132, 0.9110, 0.9046, 0.8993, 0.8923,
         0.8922],
        [0.9419, 0.9107, 0.9085, 0.8975, 0.8913, 0.8901, 0.8882, 0.8797, 0.8781,
         0.8774],
        [0.9627, 0.9526, 0.9496, 0.9435, 0.9435, 0.9400, 0.9395, 0.9389, 0.9381,
         0.9374],
        [0.9555, 0.9369, 0.9348, 0.9318, 0.9303, 0.9296, 0.9290, 0.9276, 0.9269,
         0.9265],
        [0.9585, 0.9354, 0.9277, 0.9163, 0.9122, 0.8963, 0.8953, 0.8944, 0.8940,
         0.8924],
        [0.9546, 0.9173, 0.9006, 0.8860, 0.8842, 0.8813, 0.8799, 0.8792, 0.8767,
         0.8743],
        [0.9535, 0.9084, 0.9079, 0.9020, 0.9009, 0.8969, 0.8959, 0.8836, 0.8728,
         0.8691],
        [0.9459, 0.8745, 0.8687, 0.8678, 0.8642, 0.8627, 0.8556, 0.8497, 0.8473,
         0.8327],
        [0.9261, 0.9236, 0.9193, 0.9174, 0.9119, 0.9082, 0.9080, 0.9070, 0.9064,
         0.9033],
        [0.8587, 0.8429, 0.8368, 0.8347, 0.8337, 0.8300, 0.8228, 0.8190, 0.8170,
         0.8149],
        [0.9228, 0.9001, 0.8988, 0.8987, 0.8948, 0.8926, 0.8911, 0.8864, 0.8854,
         0.8845],
        [0.9057, 0.9041, 0.8937, 0.8864, 0.8777, 0.8753, 0.8695, 0.8685, 0.8669,
         0.8640]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 1, 0, 1, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 1, 1],
        [0, 0, 1, 0, 0, 1, 1, 1, 1, 1],
        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 0, 0, 1, 0, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 0, 1, 0],
        [1, 1, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 507835.9062,  319758.2812,  222338.7500,  199715.6719,  194102.1562,
          191700.5156,  185491.7812,  183781.3438,  166301.0469,  149336.5312],
        [1084816.2500, 1077202.8750, 1076929.6250, 1056606.6250, 1020822.9375,
         1020316.8125,  998170.8125,  997603.6250,  989123.0625,  988632.6250],
        [ 731409.2500,  565174.1875,  557648.1250,  509247.1875,  503519.7812,
          497906.1562,  496750.8125,  408140.9062,  364011.6562,  341222.3750],
        [1122583.0000,  803627.7500,  744918.7500,  682809.0000,  592143.2500,
          586576.6875,  585529.8750,  584255.8750,  550965.8125,  543722.2500],
        [ 272194.1250,  250023.4219,  248334.5469,  247743.1719,  244253.9844,
          227692.5469,  221013.2188,  197768.2031,  188899.9062,  180839.9375],
        [ 543789.6875,  495964.5312,  482810.9062,  482690.2812,  416460.0000,
          410579.7188,  382866.3125,  349352.5312,  331983.6875,  316922.1562],
        [1009557.5625,  959139.9375,  946027.5000,  777560.8125,  764992.8125,
          703440.4375,  681940.8750,  681176.5000,  660360.5625,  632490.1875],
        [ 868696.7500,  850441.6250,  737124.0000,  538115.0000,  503809.4375,
          474149.4062,  400729.5312,  384481.3750,  339597.9062,  335916.2188],
        [ 955126.4375,  942962.3125,  747629.6875,  746041.3750,  705995.6875,
          704944.0625,  695664.9375,  672167.9375,  672079.4375,  660687.5000],
        [1016541.5625,  587840.0625,  558345.2500,  529025.0625,  517485.2812,
          486003.5625,  458517.2812,  457883.2500,  400851.8438,  398400.5312],
        [1038529.4375,  869555.5000,  863059.2500,  846740.0000,  837446.0625,
          831429.8750,  824289.6250,  764386.8125,  757560.0000,  733182.4375],
        [ 616153.3125,  613985.3750,  605352.5000,  583200.3750,  535385.0625,
          513275.5938,  345148.1562,  330625.6875,  328497.9375,  238203.8906],
        [1111755.2500,  774761.3750,  746893.4375,  673337.5000,  581770.0000,
          525748.3125,  519362.0625,  462714.9688,  451874.2188,  395124.5312],
        [1168565.7500, 1125255.1250, 1123716.2500, 1106277.7500, 1102603.8750,
         1088513.8750, 1079885.3750, 1068276.7500, 1058849.0000, 1036945.9375],
        [ 907421.0625,  836823.2500,  819629.4375,  818663.9375,  805294.0625,
          755164.5000,  732167.8125,  729076.3125,  727618.3125,  723132.1875],
        [ 822052.2500,  723320.5000,  699863.1250,  678906.6250,  664155.7500,
          644390.1250,  623682.8125,  615373.5000,  610697.0000,  597381.3750],
        [1311412.7500, 1215963.5000, 1144500.1250, 1083316.1250, 1081016.7500,
         1061757.1250, 1014736.0625,  965232.8750,  941433.8750,  939695.4375],
        [1215834.8750, 1072313.7500, 1031916.8125,  987624.3750,  932930.6250,
          909388.5000,  895726.9375,  894298.0625,  883036.6875,  876994.1250],
        [1015234.6250,  921631.7500,  909171.6875,  866757.0000,  781821.5000,
          735649.9375,  678282.1250,  613596.1250,  601515.1875,  584702.3125],
        [ 836544.8125,  815990.3750,  804706.8125,  669689.1875,  668240.9375,
          666519.9375,  666500.8750,  642105.6250,  628816.0000,  628810.0000],
        [1355490.2500, 1151601.5000, 1040804.9375, 1026608.4375, 1001485.1875,
         1000269.1250,  938151.8125,  907008.3125,  904397.2500,  866545.4375],
        [1001901.6875,  856962.0625,  847458.1250,  761735.0625,  715851.3750,
          713349.6250,  701143.8125,  697167.8750,  686180.0000,  680773.8125],
        [1242442.7500,  949468.4375,  885858.9375,  881897.1875,  861470.5625,
          823066.5625,  792743.3750,  778599.6875,  700805.5625,  695006.4375],
        [ 947932.1250,  902373.5000,  863096.3125,  858607.2500,  822924.5000,
          821513.0625,  781287.1250,  778848.4375,  778176.5625,  750439.8125],
        [1146289.3750,  906566.5000,  645716.3750,  635935.3125,  559471.5000,
          512364.4375,  495582.0625,  481697.9375,  477532.9062,  475519.2500],
        [ 788474.3125,  783401.5000,  772823.5000,  740818.9375,  734204.6875,
          709164.5000,  695040.8750,  683252.5625,  677819.8125,  673273.9375],
        [ 932613.0625,  885510.1250,  784229.0000,  745070.8125,  692852.3125,
          687410.6875,  683876.4375,  637652.6875,  631560.7500,  623148.8750],
        [ 948789.6250,  815347.0625,  775548.6250,  767113.6250,  762372.4375,
          750499.2500,  697955.5625,  680914.7500,  680420.1250,  657783.6250],
        [1028634.0625,  897195.6875,  890600.0625,  867100.1250,  863768.2500,
          861365.3750,  811630.1250,  808884.6875,  795176.8750,  720064.3750],
        [ 863041.9375,  860169.3750,  797697.0625,  790159.6875,  781908.6875,
          779403.5000,  770747.9375,  760549.6250,  726381.4375,  717024.5625],
        [1057238.6250, 1018356.0625, 1003580.9375,  995165.3125,  987452.0000,
          979293.0000,  977886.5625,  939936.5625,  883707.2500,  882246.3125],
        [ 833831.1250,  825339.7500,  694015.5625,  678544.8125,  678324.8125,
          664694.3750,  660684.3125,  606860.6250,  591722.7500,  584291.5000],
        [ 419105.1250,  389741.7188,  374765.5625,  317509.9375,  298421.7812,
          293794.0625,  267816.0000,  264778.9062,  253058.9375,  250107.1250],
        [ 776150.9375,  715660.3125,  652924.2500,  574897.8125,  572338.8125,
          544952.6250,  516884.5625,  503286.0000,  466004.8750,  433148.9688],
        [ 811653.4375,  747149.2500,  679857.0625,  623923.7500,  613194.1875,
          599897.3750,  598350.6875,  593218.8750,  582401.1875,  580269.4375],
        [ 748714.1875,  689743.1875,  666361.0625,  624693.5000,  550974.7500,
          546054.5000,  522936.0000,  502489.4062,  496554.6875,  476032.4062],
        [ 795124.5625,  394411.4375,  385020.4062,  372443.2188,  370701.5312,
          364517.0938,  359678.8438,  327219.4062,  290886.7812,  287652.2500],
        [1064269.2500,  834211.3125,  674408.8125,  622329.3125,  523282.2188,
          521244.1250,  507003.5938,  502571.8125,  491345.2188,  459746.8125],
        [ 714711.5000,  672394.8750,  652872.5625,  508871.9688,  466299.2188,
          459520.6250,  361568.0000,  357022.4375,  356746.4375,  354489.4062],
        [1127974.5000, 1125798.2500, 1048263.0625, 1046069.0625, 1045253.3125,
         1044636.4375, 1039065.4375, 1018032.6875, 1015453.5000, 1011068.3125],
        [ 598516.1875,  539289.3750,  454865.3438,  399476.4688,  397668.6875,
          387414.8438,  328391.7500,  326875.3750,  307570.1250,  307051.0938],
        [ 595389.0625,  500710.3750,  500080.9375,  457481.6875,  453438.2188,
          381729.8438,  381721.8750,  379622.0625,  376253.1562,  374921.0625],
        [ 462150.9062,  414903.2188,  397452.9062,  344179.7812,  334485.3750,
          317053.0312,  304008.8750,  303091.1875,  294009.8750,  289744.7188],
        [ 706999.5625,  449988.9375,  442895.4688,  435233.8750,  428338.2812,
          325156.3438,  298498.6562,  294207.3750,  288245.1250,  286478.0625],
        [ 392233.2500,  384950.6562,  347982.2500,  340208.2812,  329747.4375,
          324889.4688,  306531.7500,  295309.5625,  270284.5938,  251314.5625],
        [ 225767.9375,  119152.6797,  116776.3125,   84132.4766,   69169.6641,
           65613.0469,   62525.1133,   58760.2109,   57798.4336,   45669.6758],
        [ 892133.5625,  750694.6250,  633494.0625,  620488.6250,  599968.3125,
          594759.6875,  558132.8750,  556261.3125,  553347.5625,  552481.1875],
        [ 256096.6094,  227496.7656,  207627.0781,  186754.8906,  161951.0625,
          156868.6875,  139942.7188,  108621.4297,  102047.3047,  101382.1484],
        [ 459832.7500,  342093.0000,  327789.4062,  297907.9688,  285876.2812,
          269466.1562,  243769.5000,  241289.8125,  228933.6562,  213745.6406],
        [ 693324.9375,  690718.7500,  643471.3750,  577850.1250,  546067.0000,
          518600.3125,  491403.8125,  489311.5625,  488050.0000,  480337.7812],
        [ 317383.4062,  315213.0625,  289360.0625,  258416.0156,  245788.2656,
          223889.0312,  220264.5625,  214910.7188,  199035.3438,  189389.8125],
        [ 529178.4375,  478818.8750,  428695.0625,  404317.9688,  399273.8438,
          395178.0312,  357535.2500,  342224.1562,  314373.0938,  311003.7188],
        [ 929149.0000,  577612.6250,  565565.0625,  526419.6250,  462852.2188,
          448980.6875,  409444.1875,  379589.8438,  343716.6562,  343302.2500],
        [ 698220.5000,  446666.7812,  433004.4062,  370088.6562,  338841.8438,
          332820.8750,  324197.0938,  286965.9062,  280506.0625,  277551.5625],
        [ 939265.3750,  812968.0625,  779494.9375,  713765.3750,  713765.3750,
          678995.9375,  674496.3125,  668955.7500,  661225.8750,  654450.3750],
        [ 846955.6250,  649365.4375,  630080.8125,  604237.6250,  591231.4375,
          585754.3750,  580500.8125,  569059.6250,  563342.9375,  560158.0625],
        [ 884883.7500,  635999.5625,  570011.2500,  484312.9062,  456316.0938,
          363693.8125,  358419.4375,  353945.2188,  351907.5938,  343835.3438],
        [ 837121.8125,  490875.9375,  386615.4062,  314124.6250,  305921.4062,
          293804.1562,  287715.9062,  285018.7812,  275022.0938,  265773.6562],
        [ 823913.1875,  432631.6875,  429383.2500,  394583.4062,  388538.5625,
          366926.1250,  361491.7812,  303630.7500,  259902.8750,  246728.6094],
        [ 738894.8125,  266416.9688,  245148.7188,  242020.5938,  230007.0469,
          225020.7500,  203264.2969,  186989.5938,  180581.7812,  146588.7969],
        [ 557214.3750,  536919.5625,  504948.4688,  491731.9688,  454605.5625,
          431376.3438,  430216.1562,  423640.0000,  419953.7500,  402092.7188],
        [ 212447.2500,  169549.3438,  155583.5156,  150870.7969,  148690.2031,
          141117.2656,  127350.5391,  120518.8047,  117113.0156,  113703.1094],
        [ 531058.6250,  383843.5312,  377255.5938,  376699.0938,  356172.2500,
          344798.4375,  337967.5938,  316011.5938,  311416.8438,  307535.8125],
        [ 415852.3750,  406919.3750,  350486.8125,  315711.8750,  278782.2188,
          269313.3125,  248107.0469,  244372.8125,  239168.3594,  229224.8750]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 507835.9062,       0.0000],
         [ 319758.2812,       0.0000],
         [ 222338.7500,       0.0000],
         ...,
         [ 183781.3438,       0.0000],
         [ 166301.0469,       0.0000],
         [ 149336.5312,       0.0000]],

        [[1084816.2500,       0.0000],
         [1077202.8750,       0.0000],
         [1076929.6250,       0.0000],
         ...,
         [ 997603.6250,       0.0000],
         [ 989123.0625,       0.0000],
         [ 988632.6250,       0.0000]],

        [[ 731409.2500,       0.0000],
         [ 565174.1875,       0.0000],
         [ 557648.1250,       0.0000],
         ...,
         [ 408140.9062,       0.0000],
         [ 364011.6562,       0.0000],
         [      0.0000,  341222.3750]],

        ...,

        [[ 212447.2500,       0.0000],
         [      0.0000,  169549.3438],
         [ 155583.5156,       0.0000],
         ...,
         [ 120518.8047,       0.0000],
         [ 117113.0156,       0.0000],
         [ 113703.1094,       0.0000]],

        [[      0.0000,  531058.6250],
         [ 383843.5312,       0.0000],
         [ 377255.5938,       0.0000],
         ...,
         [ 316011.5938,       0.0000],
         [ 311416.8438,       0.0000],
         [ 307535.8125,       0.0000]],

        [[      0.0000,  415852.3750],
         [      0.0000,  406919.3750],
         [ 350486.8125,       0.0000],
         ...,
         [      0.0000,  244372.8125],
         [ 239168.3594,       0.0000],
         [ 229224.8750,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2126259.7500,   194102.1562],
        [10310226.0000,        0.0000],
        [ 4633808.0000,   341222.3750],
        [ 6797132.5000,        0.0000],
        [ 1292967.1250,   985795.8750],
        [ 2988293.7500,  1225125.8750],
        [ 6093099.0000,  1723588.2500],
        [ 5433061.5000,        0.0000],
        [ 6560337.0000,   942962.3125],
        [ 5410894.0000,        0.0000],
        [ 8366179.0000,        0.0000],
        [ 4709827.5000,        0.0000],
        [ 6243341.5000,        0.0000],
        [10958890.0000,        0.0000],
        [ 7854991.0000,        0.0000],
        [ 6679823.0000,        0.0000],
        [10759065.0000,        0.0000],
        [ 9700066.0000,        0.0000],
        [ 7708362.0000,        0.0000],
        [ 7027925.0000,        0.0000],
        [10192363.0000,        0.0000],
        [ 7662523.5000,        0.0000],
        [ 8611359.0000,        0.0000],
        [ 8305199.0000,        0.0000],
        [ 6336676.0000,        0.0000],
        [ 7258275.0000,        0.0000],
        [ 6680775.5000,   623148.8750],
        [ 7536744.5000,        0.0000],
        [ 8544420.0000,        0.0000],
        [ 7847084.0000,        0.0000],
        [ 9724862.0000,        0.0000],
        [ 6818310.0000,        0.0000],
        [ 2876040.0000,   253058.9375],
        [ 3487589.5000,  2268659.7500],
        [ 3764916.5000,  2664999.0000],
        [ 4386096.0000,  1438457.3750],
        [ 2090093.0000,  1857562.6250],
        [ 3679602.5000,  2520809.7500],
        [ 2206952.2500,  2697544.7500],
        [10521614.0000,        0.0000],
        [ 4047119.5000,        0.0000],
        [ 4401348.0000,        0.0000],
        [ 3461079.7500,        0.0000],
        [ 3956041.7500,        0.0000],
        [ 1956795.3750,  1286656.5000],
        [  397998.9375,   507366.6250],
        [ 5678268.0000,   633494.0625],
        [ 1301219.0000,   347569.8125],
        [  669882.3750,  2240821.7500],
        [ 3716491.7500,  1902644.0000],
        [ 2184290.2500,   289360.0625],
        [ 2457129.0000,  1503469.5000],
        [ 2496033.5000,  2490598.7500],
        [ 3009376.0000,   779487.6250],
        [ 7297383.0000,        0.0000],
        [ 4967978.0000,  1212708.3750],
        [ 2087801.7500,  2715523.5000],
        [ 2864502.5000,   877491.3750],
        [ 2129790.5000,  1877939.5000],
        [ 1226350.2500,  1438583.1250],
        [ 4232745.0000,   419953.7500],
        [ 1287394.5000,   169549.3438],
        [ 2773733.2500,   869026.2500],
        [ 1661481.1250,  1336457.7500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 57.8125
Top1 accuracy for validation set is 57.8125 size is torch.Size([64, 1])
Epoch 186/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:58, 57.89s/it]  7%|▋         | 2/30 [00:58<11:19, 24.28s/it] 10%|█         | 3/30 [01:00<06:15, 13.89s/it] 13%|█▎        | 4/30 [01:00<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.83s/it] 20%|██        | 6/30 [01:02<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.121639529863993
Epoch 187/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:37, 59.24s/it]  7%|▋         | 2/30 [00:59<11:35, 24.83s/it] 10%|█         | 3/30 [01:01<06:24, 14.23s/it] 13%|█▎        | 4/30 [01:02<03:51,  8.91s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.97s/it] 20%|██        | 6/30 [01:03<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 3.106002863248189
Epoch 188/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:16, 56.43s/it]  7%|▋         | 2/30 [01:00<11:49, 25.34s/it] 10%|█         | 3/30 [01:00<06:20, 14.11s/it] 13%|█▎        | 4/30 [01:01<03:49,  8.83s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.92s/it] 20%|██        | 6/30 [01:02<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:03<01:10,  3.04s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.70s/it]
Epoch loss is 3.1179621537526447
Epoch 189/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:21, 58.67s/it]  7%|▋         | 2/30 [01:00<11:41, 25.06s/it] 10%|█         | 3/30 [01:00<06:16, 13.96s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 3.1005399306615193
Epoch 190/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:34, 59.12s/it]  7%|▋         | 2/30 [00:59<11:33, 24.78s/it] 10%|█         | 3/30 [01:00<06:12, 13.81s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.65s/it] 17%|█▋        | 5/30 [01:02<02:31,  6.06s/it] 20%|██        | 6/30 [01:03<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.36s/it] 30%|███       | 9/30 [01:05<00:38,  1.86s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.07it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 3.1027561028798423
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0209,  0.0107, -0.0116,  ...,  0.0041, -0.0206,  0.0250],
        [-0.0101,  0.0225,  0.0038,  ...,  0.0124, -0.0266, -0.0065],
        [-0.0517, -0.0300,  0.0074,  ...,  0.0661, -0.0349, -0.0352],
        ...,
        [ 0.0347, -0.0227, -0.0230,  ..., -0.0251, -0.0391, -0.0126],
        [-0.0314, -0.0050, -0.0114,  ..., -0.0115,  0.0133, -0.0173],
        [-0.0341, -0.0025,  0.0114,  ...,  0.0063,  0.0145, -0.0578]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9188, 0.8858, 0.8617, 0.8567, 0.8521, 0.8503, 0.8496, 0.8447, 0.8418,
         0.8339],
        [0.9718, 0.9704, 0.9701, 0.9692, 0.9684, 0.9676, 0.9674, 0.9657, 0.9651,
         0.9641],
        [0.9429, 0.9268, 0.9217, 0.9211, 0.9155, 0.9149, 0.9142, 0.9029, 0.8954,
         0.8887],
        [0.9740, 0.9474, 0.9428, 0.9355, 0.9307, 0.9294, 0.9289, 0.9268, 0.9214,
         0.9195],
        [0.8763, 0.8689, 0.8685, 0.8680, 0.8617, 0.8565, 0.8555, 0.8436, 0.8424,
         0.8393],
        [0.9201, 0.9108, 0.9101, 0.9068, 0.9017, 0.9004, 0.8933, 0.8859, 0.8809,
         0.8788],
        [0.9633, 0.9597, 0.9590, 0.9444, 0.9422, 0.9372, 0.9354, 0.9322, 0.9319,
         0.9291],
        [0.9546, 0.9524, 0.9430, 0.9193, 0.9134, 0.9113, 0.9008, 0.8959, 0.8877,
         0.8876],
        [0.9610, 0.9604, 0.9414, 0.9414, 0.9383, 0.9373, 0.9370, 0.9330, 0.9330,
         0.9327],
        [0.9668, 0.9283, 0.9257, 0.9204, 0.9171, 0.9141, 0.9123, 0.9119, 0.9021,
         0.9009],
        [0.9691, 0.9551, 0.9549, 0.9530, 0.9529, 0.9522, 0.9509, 0.9455, 0.9452,
         0.9420],
        [0.9308, 0.9286, 0.9283, 0.9269, 0.9153, 0.9133, 0.8918, 0.8840, 0.8836,
         0.8585],
        [0.9725, 0.9459, 0.9439, 0.9369, 0.9291, 0.9210, 0.9205, 0.9126, 0.9102,
         0.9031],
        [0.9777, 0.9744, 0.9742, 0.9720, 0.9711, 0.9710, 0.9709, 0.9706, 0.9699,
         0.9694],
        [0.9585, 0.9535, 0.9526, 0.9522, 0.9521, 0.9469, 0.9441, 0.9438, 0.9438,
         0.9434],
        [0.9513, 0.9428, 0.9397, 0.9379, 0.9365, 0.9342, 0.9334, 0.9311, 0.9304,
         0.9282],
        [0.9857, 0.9788, 0.9759, 0.9706, 0.9706, 0.9705, 0.9674, 0.9632, 0.9620,
         0.9611],
        [0.9792, 0.9701, 0.9680, 0.9628, 0.9603, 0.9600, 0.9589, 0.9583, 0.9581,
         0.9578],
        [0.9663, 0.9593, 0.9592, 0.9565, 0.9499, 0.9431, 0.9336, 0.9319, 0.9300,
         0.9283],
        [0.9505, 0.9503, 0.9500, 0.9380, 0.9379, 0.9371, 0.9351, 0.9334, 0.9334,
         0.9320],
        [0.9872, 0.9761, 0.9695, 0.9687, 0.9659, 0.9658, 0.9613, 0.9592, 0.9588,
         0.9561],
        [0.9658, 0.9559, 0.9542, 0.9476, 0.9429, 0.9423, 0.9395, 0.9391, 0.9385,
         0.9359],
        [0.9816, 0.9627, 0.9587, 0.9574, 0.9560, 0.9538, 0.9490, 0.9477, 0.9415,
         0.9397],
        [0.9621, 0.9593, 0.9546, 0.9544, 0.9532, 0.9515, 0.9488, 0.9487, 0.9472,
         0.9461],
        [0.9763, 0.9564, 0.9363, 0.9332, 0.9252, 0.9168, 0.9142, 0.9135, 0.9129,
         0.9128],
        [0.9508, 0.9506, 0.9491, 0.9469, 0.9442, 0.9413, 0.9400, 0.9380, 0.9374,
         0.9360],
        [0.9614, 0.9567, 0.9469, 0.9454, 0.9406, 0.9406, 0.9391, 0.9344, 0.9338,
         0.9315],
        [0.9641, 0.9526, 0.9497, 0.9486, 0.9476, 0.9453, 0.9401, 0.9396, 0.9394,
         0.9367],
        [0.9682, 0.9587, 0.9565, 0.9552, 0.9551, 0.9537, 0.9514, 0.9512, 0.9471,
         0.9421],
        [0.9564, 0.9557, 0.9498, 0.9493, 0.9487, 0.9484, 0.9479, 0.9459, 0.9436,
         0.9429],
        [0.9707, 0.9676, 0.9663, 0.9659, 0.9646, 0.9640, 0.9635, 0.9615, 0.9577,
         0.9569],
        [0.9543, 0.9524, 0.9413, 0.9396, 0.9390, 0.9383, 0.9365, 0.9286, 0.9277,
         0.9274],
        [0.9081, 0.9019, 0.8979, 0.8866, 0.8852, 0.8779, 0.8755, 0.8690, 0.8686,
         0.8682],
        [0.9482, 0.9426, 0.9347, 0.9277, 0.9258, 0.9225, 0.9197, 0.9179, 0.9120,
         0.9061],
        [0.9514, 0.9456, 0.9410, 0.9325, 0.9310, 0.9304, 0.9298, 0.9291, 0.9287,
         0.9270],
        [0.9450, 0.9386, 0.9343, 0.9336, 0.9256, 0.9224, 0.9222, 0.9176, 0.9167,
         0.9154],
        [0.9499, 0.8972, 0.8967, 0.8960, 0.8933, 0.8932, 0.8911, 0.8843, 0.8803,
         0.8792],
        [0.9706, 0.9527, 0.9365, 0.9332, 0.9197, 0.9183, 0.9180, 0.9166, 0.9150,
         0.9107],
        [0.9400, 0.9379, 0.9347, 0.9181, 0.9116, 0.9107, 0.8923, 0.8920, 0.8914,
         0.8914],
        [0.9750, 0.9742, 0.9705, 0.9697, 0.9695, 0.9687, 0.9686, 0.9681, 0.9670,
         0.9655],
        [0.9271, 0.9217, 0.9082, 0.9040, 0.9039, 0.8970, 0.8878, 0.8849, 0.8845,
         0.8825],
        [0.9273, 0.9204, 0.9159, 0.9100, 0.9095, 0.8984, 0.8980, 0.8977, 0.8963,
         0.8953],
        [0.9124, 0.9058, 0.9029, 0.8944, 0.8922, 0.8873, 0.8822, 0.8801, 0.8799,
         0.8784],
        [0.9423, 0.9102, 0.9097, 0.9095, 0.9067, 0.8863, 0.8822, 0.8795, 0.8788,
         0.8770],
        [0.8973, 0.8965, 0.8906, 0.8866, 0.8863, 0.8841, 0.8827, 0.8778, 0.8744,
         0.8681],
        [0.8605, 0.8187, 0.8184, 0.7943, 0.7810, 0.7749, 0.7724, 0.7724, 0.7689,
         0.7508],
        [0.9569, 0.9439, 0.9340, 0.9303, 0.9285, 0.9277, 0.9231, 0.9230, 0.9226,
         0.9223],
        [0.8682, 0.8605, 0.8538, 0.8456, 0.8366, 0.8335, 0.8217, 0.8061, 0.8061,
         0.8058],
        [0.9119, 0.8852, 0.8835, 0.8782, 0.8758, 0.8715, 0.8612, 0.8610, 0.8573,
         0.8544],
        [0.9401, 0.9392, 0.9343, 0.9236, 0.9199, 0.9182, 0.9135, 0.9132, 0.9131,
         0.9118],
        [0.8889, 0.8774, 0.8762, 0.8640, 0.8628, 0.8592, 0.8511, 0.8506, 0.8479,
         0.8470],
        [0.9199, 0.9151, 0.9059, 0.9026, 0.9008, 0.8960, 0.8902, 0.8900, 0.8850,
         0.8807],
        [0.9619, 0.9276, 0.9269, 0.9228, 0.9107, 0.9074, 0.9059, 0.8992, 0.8921,
         0.8920],
        [0.9416, 0.9105, 0.9050, 0.8968, 0.8893, 0.8862, 0.8861, 0.8820, 0.8776,
         0.8773],
        [0.9609, 0.9515, 0.9477, 0.9424, 0.9424, 0.9371, 0.9369, 0.9366, 0.9365,
         0.9349],
        [0.9542, 0.9336, 0.9330, 0.9307, 0.9278, 0.9262, 0.9255, 0.9254, 0.9230,
         0.9229],
        [0.9570, 0.9331, 0.9254, 0.9122, 0.9106, 0.8942, 0.8930, 0.8927, 0.8917,
         0.8917],
        [0.9544, 0.9164, 0.8998, 0.8838, 0.8822, 0.8818, 0.8775, 0.8772, 0.8756,
         0.8718],
        [0.9520, 0.9063, 0.9047, 0.8991, 0.8982, 0.8975, 0.8913, 0.8834, 0.8717,
         0.8699],
        [0.9428, 0.8724, 0.8663, 0.8653, 0.8624, 0.8594, 0.8541, 0.8480, 0.8462,
         0.8318],
        [0.9253, 0.9228, 0.9171, 0.9169, 0.9099, 0.9071, 0.9069, 0.9062, 0.9053,
         0.9026],
        [0.8536, 0.8442, 0.8327, 0.8320, 0.8306, 0.8267, 0.8199, 0.8169, 0.8153,
         0.8119],
        [0.9162, 0.8983, 0.8961, 0.8956, 0.8897, 0.8872, 0.8863, 0.8850, 0.8814,
         0.8812],
        [0.9009, 0.9005, 0.8855, 0.8775, 0.8732, 0.8716, 0.8647, 0.8633, 0.8605,
         0.8603]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 1, 1],
        [0, 0, 1, 0, 0, 1, 1, 1, 1, 1],
        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 0, 1, 1, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 501425.2500,  312888.9375,  222000.1719,  206656.1406,  193402.2188,
          188669.4531,  186664.6250,  173947.7500,  167045.0938,  149074.2812],
        [1069445.0000, 1047853.2500, 1043459.5000, 1031351.0625, 1019327.6875,
         1007874.0625, 1004299.0000,  980084.3750,  972192.0625,  958669.0000],
        [ 707653.1875,  562303.8125,  522642.8438,  518070.9062,  478448.2812,
          474359.7188,  469929.5312,  399884.3125,  359346.6250,  326235.6875],
        [1104344.5000,  754652.6250,  706424.0000,  637239.3125,  594454.5625,
          583440.6875,  579870.6250,  562694.9375,  520537.7500,  506630.4688],
        [ 273357.7188,  245983.5938,  244380.5000,  242969.7031,  221841.8594,
          205953.9375,  203103.4844,  171316.5938,  168503.6719,  161138.7031],
        [ 511258.8750,  447269.9688,  443361.6250,  422404.6875,  392910.9062,
          385925.4688,  348301.9688,  313328.4688,  291751.9375,  283163.1875],
        [ 947403.4375,  900303.6250,  890416.6250,  723385.3750,  700482.8125,
          652877.5625,  635661.2500,  607890.4375,  604732.1875,  581577.5000],
        [ 836699.5625,  810306.8750,  708590.5625,  505497.2812,  464377.6250,
          450814.9375,  387836.2812,  361947.1250,  321603.6250,  321353.1562],
        [ 917078.7500,  908353.5625,  693257.5000,  692436.1875,  662612.6875,
          653334.7500,  650552.4375,  614543.6250,  614250.0625,  611987.2500],
        [ 995894.4375,  574756.3750,  554005.9375,  512968.2812,  489865.3125,
          469025.5625,  456956.6875,  454439.5625,  395080.4375,  388263.7188],
        [1029517.3750,  842765.8125,  839746.0625,  817194.3125,  816227.0000,
          808011.1875,  793219.0625,  734512.8125,  731164.5000,  699222.0625],
        [ 595699.1875,  576841.4375,  574747.6250,  563205.5000,  477295.2188,
          463453.3750,  340890.5938,  305218.8125,  303396.8750,  212074.5781],
        [1080979.6250,  739301.5000,  717998.2500,  649657.8125,  580941.6875,
          517644.7188,  513880.9375,  458991.0938,  443394.1562,  400648.5312],
        [1163074.5000, 1109892.8750, 1106475.1250, 1071978.3750, 1059764.3750,
         1057769.1250, 1055508.8750, 1052074.8750, 1040688.8125, 1033279.6875],
        [ 884167.5625,  824095.5000,  812809.8750,  808196.1250,  807112.4375,
          748987.0000,  719777.3125,  717459.6250,  717356.2500,  712871.5625],
        [ 798636.4375,  706852.6250,  676247.5625,  658667.4375,  645794.5625,
          625401.6875,  618448.0625,  597749.0000,  592397.4375,  573846.6875],
        [1304611.7500, 1182899.8750, 1134671.2500, 1052227.3750, 1051051.0000,
         1050520.8750, 1004495.3750,  945557.5625,  929704.6875,  918144.5625],
        [1188231.8750, 1043440.6250, 1013505.9375,  941190.5625,  907964.6875,
          903125.9375,  890114.4375,  881613.7500,  879447.2500,  875629.4375],
        [ 988958.0000,  895087.3125,  893665.5000,  859704.3750,  782068.3125,
          709799.1875,  619844.0000,  604941.6250,  588815.7500,  574432.5000],
        [ 789472.0625,  787158.7500,  783498.6875,  660474.0000,  658987.2500,
          651141.5000,  632929.4375,  618345.5000,  617585.2500,  605381.9375],
        [1332771.0000, 1137819.2500, 1034352.3750, 1023888.2500,  983102.4375,
          982436.0625,  920846.3750,  893531.7500,  888433.5000,  854610.0000],
        [ 981410.6250,  851736.2500,  831276.9375,  757425.6250,  707927.2500,
          701531.0625,  674158.6875,  670432.9375,  664346.4375,  640238.8125],
        [1230221.7500,  939097.9375,  887079.6875,  871143.3125,  853179.2500,
          827492.1250,  772762.3750,  758545.3750,  693657.6250,  675969.0000],
        [ 931059.6250,  894727.1875,  837191.3125,  834187.5000,  820301.1875,
          800284.8125,  770041.1250,  768819.4375,  752804.5625,  741262.0000],
        [1140351.3750,  857998.2500,  644599.0625,  616044.6250,  549707.2500,
          487523.4062,  469642.3750,  465240.6562,  461046.8750,  460680.3125],
        [ 792223.3750,  789977.3750,  772962.1250,  749920.4375,  720895.7500,
          691505.0625,  678850.2500,  659818.5625,  654432.2500,  641810.5625],
        [ 921632.5625,  861962.0000,  749234.1875,  733256.5000,  685020.7500,
          684666.1250,  670098.6875,  626873.0000,  621222.8125,  601523.2500],
        [ 958710.1250,  812716.1250,  780417.3125,  767674.2500,  757383.0000,
          732306.1250,  680239.0625,  675191.3750,  672907.3750,  647654.7500],
        [1016159.6250,  887276.8125,  860148.0625,  843799.2500,  842085.4375,
          825548.3750,  799508.9375,  797223.2500,  752127.8750,  699784.4375],
        [ 858514.6875,  849920.3125,  780737.3750,  775510.9375,  768966.7500,
          765992.1875,  760446.6250,  738868.7500,  715432.3750,  708094.7500],
        [1053544.7500, 1008054.7500,  989394.7500,  983844.3125,  965046.0000,
          956970.8750,  950264.6875,  922844.6250,  874196.8125,  865056.6875],
        [ 832899.6875,  810659.3750,  692104.7500,  675009.8125,  669148.4375,
          662779.5000,  645920.8750,  576772.0625,  569636.7500,  567410.5625],
        [ 430683.3125,  393812.7188,  372084.6562,  316562.0625,  310584.0312,
          279850.3438,  270211.4062,  246278.6562,  244798.2656,  243638.4062],
        [ 763556.2500,  704822.3750,  629987.6875,  569928.5625,  554789.5625,
          529211.7500,  508480.9375,  495549.4375,  455416.1562,  418143.4062],
        [ 799274.1250,  735454.8750,  689323.0000,  609874.0000,  597431.5625,
          591897.6875,  587097.1875,  580878.5000,  578035.3125,  563867.0000],
        [ 729442.8125,  665584.2500,  625911.8750,  619979.3750,  552713.0625,
          528519.2500,  526408.5625,  493276.7500,  486947.6875,  478054.6250],
        [ 782302.5000,  368493.6250,  365839.0938,  362102.1562,  348539.5625,
          347803.4375,  337755.5938,  306669.4688,  289336.3125,  285047.3125],
        [1051751.8750,  814393.5625,  645790.9375,  616295.0000,  507969.6250,
          497769.4375,  496107.4062,  486062.9062,  475041.0312,  446957.4375],
        [ 679370.3125,  659449.3125,  629450.1875,  496519.6250,  452527.5938,
          446718.3438,  343672.7188,  342110.2812,  339201.0625,  339185.5312],
        [1120272.8750, 1106463.5000, 1049665.6250, 1038378.9375, 1035054.9375,
         1023891.2500, 1021279.6250, 1014270.7500,  998767.8125,  977711.2500],
        [ 565018.9375,  523211.8750,  431094.6562,  406337.3125,  405666.3125,
          367286.7188,  322060.9688,  309211.5312,  307299.5000,  298855.5312],
        [ 566635.6875,  513127.2812,  481075.4062,  442106.3125,  439486.5625,
          374926.0625,  372544.8125,  370938.4688,  363587.6875,  358801.1250],
        [ 458088.0938,  416390.8438,  399704.3438,  353930.0312,  342926.5938,
          319991.0312,  297215.5312,  288522.6562,  287686.0000,  281573.3125],
        [ 701416.0000,  443603.5312,  440318.0625,  439312.2500,  422311.6562,
          315407.9062,  297574.9062,  285988.9062,  283427.1562,  276129.8438],
        [ 368997.5938,  364599.1562,  335145.3438,  316809.0938,  315286.1250,
          305575.2812,  299481.5000,  279269.6875,  266161.7188,  242990.7812],
        [ 218059.7500,  120003.2812,  119499.1953,   84664.4297,   70030.0781,
           64245.6250,   61998.8516,   61949.7383,   58927.2188,   45508.8555],
        [ 864382.1250,  718412.6250,  622965.8750,  591570.9375,  576098.6875,
          569701.9375,  533429.0625,  532475.0000,  529589.4375,  527347.6875],
        [ 243379.9531,  218051.4219,  198197.1875,  176260.8906,  155056.3438,
          148315.3281,  125378.3125,  100288.8750,  100288.2969,   99862.8359],
        [ 454350.7188,  310285.0000,  302832.0312,  280836.6250,  271399.6562,
          255373.7031,  220222.5469,  219790.5469,  208524.9844,  199984.0156],
        [ 679793.5625,  671317.1875,  626151.8125,  537343.1875,  509949.9375,
          497414.9375,  464957.6875,  463167.0938,  462274.3438,  453819.3750],
        [ 327195.3750,  277603.9688,  273084.4062,  229359.3594,  225502.6094,
          214252.7969,  190753.4688,  189462.4531,  182161.5000,  179847.4062],
        [ 509526.0625,  475604.9688,  416953.1562,  397891.3438,  387912.1250,
          361967.1875,  333308.1250,  332466.8438,  309373.1562,  291180.1562],
        [ 928665.3125,  568644.5625,  563622.9375,  530889.4375,  446782.6875,
          426115.3125,  417070.0625,  379028.7812,  342442.2500,  341889.1562],
        [ 694462.4375,  445510.0938,  412169.7812,  366285.2500,  329379.7188,
          314706.6250,  314620.8125,  296537.1875,  278329.2500,  277389.3438],
        [ 915785.2500,  800584.7500,  757898.9375,  702478.3750,  702478.3750,
          651369.4375,  649970.7500,  647196.0000,  646420.6250,  631634.2500],
        [ 831866.1250,  620169.7500,  614214.3125,  595061.5000,  570738.5000,
          557639.6250,  552134.6250,  551459.4375,  532978.5000,  532147.0625],
        [ 865608.7500,  615419.2500,  551398.4375,  456637.3750,  446458.5312,
          352779.5625,  347059.2500,  345401.6875,  340572.8125,  340461.4375],
        [ 834370.4375,  484534.6875,  382640.7188,  304125.7188,  297505.3750,
          295873.6250,  278268.4688,  277074.4375,  270496.3125,  256422.6094],
        [ 805714.2500,  419513.4062,  410306.0938,  378870.1250,  373610.7812,
          370321.6875,  338626.0625,  302626.1875,  255931.7969,  249320.2812],
        [ 706634.1875,  258451.2656,  236915.7031,  233549.3906,  224263.6406,
          214592.4531,  199175.2812,  182455.5000,  177892.0156,  144716.2500],
        [ 550297.8750,  531190.8125,  489710.7188,  488326.0625,  441548.4375,
          424404.6875,  423136.0938,  418765.5312,  413631.4688,  397941.0312],
        [ 197776.5000,  172719.2188,  146554.8281,  145163.1250,  142213.7656,
          134619.8594,  122168.8750,  116977.1719,  114341.8438,  108946.7656],
        [ 483632.5938,  374484.3750,  362901.0938,  360153.2188,  331220.5938,
          319445.8750,  315422.6562,  309505.9375,  294053.3438,  293020.3750],
        [ 388757.2188,  386306.2188,  311543.9688,  278043.8125,  261500.0469,
          255595.2031,  231559.3594,  227150.7656,  218151.2656,  217583.4219]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 501425.2500,       0.0000],
         [ 312888.9375,       0.0000],
         [ 222000.1719,       0.0000],
         ...,
         [ 173947.7500,       0.0000],
         [ 167045.0938,       0.0000],
         [ 149074.2812,       0.0000]],

        [[1069445.0000,       0.0000],
         [1047853.2500,       0.0000],
         [1043459.5000,       0.0000],
         ...,
         [ 980084.3750,       0.0000],
         [ 972192.0625,       0.0000],
         [ 958669.0000,       0.0000]],

        [[ 707653.1875,       0.0000],
         [ 562303.8125,       0.0000],
         [ 522642.8438,       0.0000],
         ...,
         [ 399884.3125,       0.0000],
         [ 359346.6250,       0.0000],
         [      0.0000,  326235.6875]],

        ...,

        [[ 197776.5000,       0.0000],
         [      0.0000,  172719.2188],
         [ 146554.8281,       0.0000],
         ...,
         [ 116977.1719,       0.0000],
         [ 114341.8438,       0.0000],
         [ 108946.7656,       0.0000]],

        [[      0.0000,  483632.5938],
         [ 374484.3750,       0.0000],
         [ 362901.0938,       0.0000],
         ...,
         [ 309505.9375,       0.0000],
         [ 294053.3438,       0.0000],
         [ 293020.3750,       0.0000]],

        [[      0.0000,  388757.2188],
         [      0.0000,  386306.2188],
         [ 311543.9688,       0.0000],
         ...,
         [ 227150.7656,       0.0000],
         [ 218151.2656,       0.0000],
         [ 217583.4219,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2108371.5000,   193402.2188],
        [10134555.0000,        0.0000],
        [ 4492639.0000,   326235.6875],
        [ 6550289.0000,        0.0000],
        [ 1170284.8750,   968264.9375],
        [ 2723337.7500,  1116339.2500],
        [ 5630928.5000,  1613802.0000],
        [ 5169027.0000,        0.0000],
        [ 6110053.0000,   908353.5625],
        [ 5291256.5000,        0.0000],
        [ 8111580.0000,        0.0000],
        [ 4412823.0000,        0.0000],
        [ 6103438.5000,        0.0000],
        [10750507.0000,        0.0000],
        [ 7752833.0000,        0.0000],
        [ 6494041.5000,        0.0000],
        [10573885.0000,        0.0000],
        [ 9524265.0000,        0.0000],
        [ 7517316.5000,        0.0000],
        [ 6804974.5000,        0.0000],
        [10051791.0000,        0.0000],
        [ 7480484.0000,        0.0000],
        [ 8509148.0000,        0.0000],
        [ 8150678.5000,        0.0000],
        [ 6152834.0000,        0.0000],
        [ 7152396.0000,        0.0000],
        [ 7155490.0000,        0.0000],
        [ 7485199.5000,        0.0000],
        [ 8323662.5000,        0.0000],
        [ 7722485.0000,        0.0000],
        [ 9569218.0000,        0.0000],
        [ 6702342.0000,        0.0000],
        [ 2863705.7500,   244798.2656],
        [ 3422919.0000,  2206967.0000],
        [ 3679403.5000,  2653729.7500],
        [ 4311811.0000,  1395027.0000],
        [ 2019358.0000,  1774531.1250],
        [ 3555699.0000,  2482440.5000],
        [ 2126664.5000,  2601540.5000],
        [10385757.0000,        0.0000],
        [ 3936043.5000,        0.0000],
        [ 4283229.5000,        0.0000],
        [ 3446028.7500,        0.0000],
        [ 3905490.0000,        0.0000],
        [ 1598180.2500,  1496136.1250],
        [  401815.9375,   503071.0625],
        [ 5443007.5000,   622965.8750],
        [ 1241504.0000,   323575.5000],
        [  613117.0000,  2110482.7500],
        [ 3517663.2500,  1848525.7500],
        [ 2016139.0000,   273084.4062],
        [ 2323071.7500,  1493111.3750],
        [ 2474982.0000,  2470168.5000],
        [ 2969259.7500,   760130.8750],
        [ 7105817.5000,        0.0000],
        [ 4806093.0000,  1152316.7500],
        [ 2024293.0000,  2637504.0000],
        [ 2814137.0000,   867175.3750],
        [ 2091666.7500,  1813174.0000],
        [ 1045405.0625,  1533240.5000],
        [ 4165321.0000,   413631.4688],
        [ 1228762.7500,   172719.2188],
        [ 2644784.7500,   799055.2500],
        [ 1513973.2500,  1262218.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 57.8125
Top1 accuracy for validation set is 57.8125 size is torch.Size([64, 1])
Epoch 191/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:08, 58.23s/it]  7%|▋         | 2/30 [00:59<11:24, 24.45s/it] 10%|█         | 3/30 [00:59<06:08, 13.63s/it] 13%|█▎        | 4/30 [01:02<04:06,  9.48s/it] 17%|█▋        | 5/30 [01:03<02:38,  6.33s/it] 20%|██        | 6/30 [01:04<01:46,  4.43s/it] 23%|██▎       | 7/30 [01:05<01:14,  3.23s/it] 27%|██▋       | 8/30 [01:05<00:53,  2.44s/it] 30%|███       | 9/30 [01:06<00:40,  1.91s/it] 33%|███▎      | 10/30 [01:07<00:31,  1.55s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.31s/it] 40%|████      | 12/30 [01:08<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.07it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 3.0859494686126707
Epoch 192/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:10, 58.28s/it]  7%|▋         | 2/30 [00:59<11:33, 24.78s/it] 10%|█         | 3/30 [01:02<06:38, 14.76s/it] 13%|█▎        | 4/30 [01:03<03:59,  9.23s/it] 17%|█▋        | 5/30 [01:03<02:34,  6.17s/it] 20%|██        | 6/30 [01:04<01:43,  4.33s/it] 23%|██▎       | 7/30 [01:05<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.39s/it] 30%|███       | 9/30 [01:06<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.29s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.07it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 3.077675199508667
Epoch 193/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:08, 60.28s/it]  7%|▋         | 2/30 [01:01<11:49, 25.36s/it] 10%|█         | 3/30 [01:01<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 3.047455676396688
Epoch 194/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<29:59, 62.04s/it]  7%|▋         | 2/30 [01:02<12:07, 25.98s/it] 10%|█         | 3/30 [01:03<06:30, 14.46s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.05s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.05s/it] 20%|██        | 6/30 [01:05<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 3.0457278887430825
Epoch 195/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:45, 59.51s/it]  7%|▋         | 2/30 [01:00<11:38, 24.95s/it] 10%|█         | 3/30 [01:01<06:15, 13.90s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 3.058574144045512
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0229,  0.0100, -0.0095,  ...,  0.0052, -0.0186,  0.0238],
        [-0.0089,  0.0228,  0.0057,  ...,  0.0120, -0.0249, -0.0070],
        [-0.0510, -0.0308,  0.0077,  ...,  0.0671, -0.0349, -0.0340],
        ...,
        [ 0.0333, -0.0227, -0.0208,  ..., -0.0249, -0.0388, -0.0130],
        [-0.0311, -0.0050, -0.0117,  ..., -0.0108,  0.0123, -0.0171],
        [-0.0343, -0.0038,  0.0120,  ...,  0.0077,  0.0153, -0.0571]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9161, 0.8838, 0.8611, 0.8580, 0.8512, 0.8498, 0.8493, 0.8425, 0.8418,
         0.8367],
        [0.9707, 0.9683, 0.9683, 0.9679, 0.9676, 0.9667, 0.9663, 0.9647, 0.9633,
         0.9620],
        [0.9404, 0.9268, 0.9213, 0.9176, 0.9134, 0.9126, 0.9116, 0.9029, 0.8951,
         0.8868],
        [0.9726, 0.9423, 0.9394, 0.9314, 0.9307, 0.9292, 0.9277, 0.9237, 0.9180,
         0.9165],
        [0.8753, 0.8674, 0.8664, 0.8651, 0.8551, 0.8493, 0.8475, 0.8335, 0.8335,
         0.8310],
        [0.9156, 0.9039, 0.9032, 0.8979, 0.8976, 0.8945, 0.8869, 0.8763, 0.8714,
         0.8702],
        [0.9584, 0.9551, 0.9540, 0.9378, 0.9352, 0.9312, 0.9297, 0.9263, 0.9240,
         0.9238],
        [0.9518, 0.9490, 0.9401, 0.9152, 0.9082, 0.9074, 0.8987, 0.8913, 0.8838,
         0.8834],
        [0.9587, 0.9579, 0.9376, 0.9359, 0.9352, 0.9347, 0.9335, 0.9304, 0.9289,
         0.9284],
        [0.9654, 0.9269, 0.9247, 0.9184, 0.9132, 0.9119, 0.9112, 0.9111, 0.9008,
         0.8989],
        [0.9692, 0.9535, 0.9520, 0.9518, 0.9510, 0.9490, 0.9479, 0.9430, 0.9425,
         0.9389],
        [0.9293, 0.9276, 0.9237, 0.9226, 0.9085, 0.9070, 0.8895, 0.8783, 0.8781,
         0.8517],
        [0.9706, 0.9428, 0.9409, 0.9347, 0.9284, 0.9208, 0.9184, 0.9117, 0.9093,
         0.9037],
        [0.9776, 0.9734, 0.9733, 0.9703, 0.9702, 0.9701, 0.9696, 0.9692, 0.9689,
         0.9685],
        [0.9567, 0.9526, 0.9520, 0.9518, 0.9507, 0.9461, 0.9443, 0.9434, 0.9423,
         0.9422],
        [0.9498, 0.9416, 0.9376, 0.9358, 0.9341, 0.9335, 0.9323, 0.9294, 0.9279,
         0.9277],
        [0.9850, 0.9775, 0.9753, 0.9696, 0.9686, 0.9680, 0.9665, 0.9620, 0.9612,
         0.9599],
        [0.9778, 0.9677, 0.9671, 0.9604, 0.9604, 0.9589, 0.9587, 0.9583, 0.9575,
         0.9574],
        [0.9647, 0.9578, 0.9572, 0.9558, 0.9498, 0.9395, 0.9307, 0.9287, 0.9274,
         0.9273],
        [0.9483, 0.9473, 0.9458, 0.9369, 0.9368, 0.9354, 0.9334, 0.9318, 0.9297,
         0.9296],
        [0.9856, 0.9750, 0.9686, 0.9686, 0.9645, 0.9641, 0.9597, 0.9577, 0.9575,
         0.9556],
        [0.9641, 0.9546, 0.9520, 0.9473, 0.9425, 0.9412, 0.9382, 0.9363, 0.9359,
         0.9359],
        [0.9806, 0.9617, 0.9584, 0.9564, 0.9555, 0.9539, 0.9475, 0.9460, 0.9413,
         0.9385],
        [0.9615, 0.9576, 0.9538, 0.9537, 0.9527, 0.9514, 0.9483, 0.9473, 0.9464,
         0.9455],
        [0.9759, 0.9529, 0.9359, 0.9310, 0.9242, 0.9136, 0.9124, 0.9118, 0.9112,
         0.9103],
        [0.9512, 0.9502, 0.9496, 0.9460, 0.9430, 0.9392, 0.9382, 0.9376, 0.9368,
         0.9358],
        [0.9608, 0.9548, 0.9446, 0.9436, 0.9401, 0.9400, 0.9379, 0.9342, 0.9325,
         0.9307],
        [0.9648, 0.9529, 0.9505, 0.9489, 0.9473, 0.9450, 0.9395, 0.9388, 0.9387,
         0.9364],
        [0.9674, 0.9584, 0.9546, 0.9541, 0.9537, 0.9512, 0.9507, 0.9498, 0.9435,
         0.9408],
        [0.9566, 0.9546, 0.9488, 0.9480, 0.9476, 0.9475, 0.9464, 0.9442, 0.9425,
         0.9417],
        [0.9712, 0.9668, 0.9652, 0.9649, 0.9633, 0.9615, 0.9614, 0.9606, 0.9568,
         0.9564],
        [0.9539, 0.9510, 0.9426, 0.9382, 0.9381, 0.9378, 0.9347, 0.9267, 0.9253,
         0.9250],
        [0.9092, 0.9018, 0.8963, 0.8879, 0.8861, 0.8761, 0.8740, 0.8701, 0.8680,
         0.8661],
        [0.9475, 0.9415, 0.9322, 0.9279, 0.9232, 0.9208, 0.9181, 0.9168, 0.9105,
         0.9055],
        [0.9508, 0.9443, 0.9410, 0.9324, 0.9302, 0.9295, 0.9287, 0.9279, 0.9270,
         0.9262],
        [0.9426, 0.9359, 0.9333, 0.9303, 0.9249, 0.9219, 0.9207, 0.9144, 0.9143,
         0.9137],
        [0.9497, 0.8964, 0.8928, 0.8905, 0.8905, 0.8903, 0.8841, 0.8810, 0.8803,
         0.8790],
        [0.9696, 0.9518, 0.9339, 0.9327, 0.9172, 0.9157, 0.9153, 0.9146, 0.9123,
         0.9101],
        [0.9358, 0.9353, 0.9323, 0.9169, 0.9107, 0.9080, 0.8904, 0.8885, 0.8885,
         0.8884],
        [0.9742, 0.9733, 0.9699, 0.9689, 0.9685, 0.9679, 0.9672, 0.9668, 0.9646,
         0.9645],
        [0.9236, 0.9197, 0.9046, 0.9045, 0.9042, 0.8928, 0.8857, 0.8855, 0.8811,
         0.8809],
        [0.9238, 0.9214, 0.9136, 0.9086, 0.9072, 0.8966, 0.8958, 0.8956, 0.8942,
         0.8939],
        [0.9112, 0.9064, 0.9042, 0.8959, 0.8903, 0.8875, 0.8802, 0.8800, 0.8770,
         0.8765],
        [0.9406, 0.9114, 0.9081, 0.9081, 0.9062, 0.8837, 0.8809, 0.8788, 0.8769,
         0.8759],
        [0.8932, 0.8921, 0.8881, 0.8850, 0.8838, 0.8817, 0.8760, 0.8736, 0.8727,
         0.8668],
        [0.8600, 0.8214, 0.8186, 0.7958, 0.7820, 0.7762, 0.7744, 0.7707, 0.7665,
         0.7519],
        [0.9546, 0.9413, 0.9327, 0.9276, 0.9260, 0.9249, 0.9213, 0.9207, 0.9205,
         0.9205],
        [0.8646, 0.8573, 0.8496, 0.8422, 0.8331, 0.8294, 0.8154, 0.8052, 0.8050,
         0.8008],
        [0.9109, 0.8789, 0.8782, 0.8746, 0.8729, 0.8661, 0.8553, 0.8546, 0.8507,
         0.8491],
        [0.9384, 0.9359, 0.9325, 0.9177, 0.9146, 0.9145, 0.9098, 0.9087, 0.9086,
         0.9078],
        [0.8917, 0.8682, 0.8679, 0.8573, 0.8558, 0.8548, 0.8433, 0.8428, 0.8416,
         0.8409],
        [0.9170, 0.9147, 0.9088, 0.8976, 0.8964, 0.8911, 0.8884, 0.8839, 0.8837,
         0.8757],
        [0.9617, 0.9266, 0.9257, 0.9236, 0.9087, 0.9065, 0.9027, 0.8993, 0.8913,
         0.8911],
        [0.9403, 0.9097, 0.9025, 0.8954, 0.8863, 0.8838, 0.8829, 0.8819, 0.8779,
         0.8775],
        [0.9605, 0.9505, 0.9454, 0.9416, 0.9416, 0.9351, 0.9351, 0.9350, 0.9344,
         0.9342],
        [0.9531, 0.9312, 0.9296, 0.9288, 0.9252, 0.9242, 0.9230, 0.9218, 0.9201,
         0.9196],
        [0.9553, 0.9309, 0.9229, 0.9088, 0.9078, 0.8931, 0.8915, 0.8910, 0.8904,
         0.8892],
        [0.9540, 0.9153, 0.9003, 0.8823, 0.8821, 0.8808, 0.8762, 0.8756, 0.8745,
         0.8700],
        [0.9508, 0.9047, 0.9036, 0.8984, 0.8974, 0.8950, 0.8862, 0.8823, 0.8741,
         0.8675],
        [0.9394, 0.8702, 0.8652, 0.8623, 0.8597, 0.8566, 0.8533, 0.8456, 0.8449,
         0.8316],
        [0.9245, 0.9218, 0.9169, 0.9138, 0.9076, 0.9068, 0.9057, 0.9044, 0.9036,
         0.9017],
        [0.8483, 0.8458, 0.8298, 0.8295, 0.8268, 0.8239, 0.8175, 0.8152, 0.8139,
         0.8095],
        [0.9094, 0.8976, 0.8934, 0.8912, 0.8845, 0.8837, 0.8820, 0.8818, 0.8789,
         0.8787],
        [0.8977, 0.8957, 0.8775, 0.8693, 0.8690, 0.8678, 0.8612, 0.8568, 0.8564,
         0.8563]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 1, 0, 1, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 1, 0, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 1, 1],
        [0, 1, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [1, 1, 0, 0, 0, 1, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 482662.2188,  304202.5938,  220031.3125,  210523.9688,  190897.7969,
          187191.0469,  185811.0156,  168675.2188,  166980.1094,  155159.8906],
        [1052682.0000, 1017401.8125, 1016925.5000, 1012368.9375, 1007651.1250,
          994851.2500,  988328.1250,  966155.6250,  947638.3750,  929685.3125],
        [ 682886.5000,  562399.8125,  520158.5938,  493390.1562,  464464.8438,
          459060.2812,  452810.7812,  399777.1875,  357486.1562,  317761.3438],
        [1082456.8750,  701686.9375,  672918.3750,  600369.0000,  594550.4375,
          581632.9375,  569602.5625,  537742.0000,  495967.8438,  485751.9688],
        [ 269531.1875,  240683.5312,  237468.7656,  232918.5781,  201864.9062,
          185847.8750,  181201.2812,  148374.6094,  148291.1406,  143057.1250],
        [ 479259.2812,  405537.8750,  401182.2812,  371992.7500,  370752.0938,
          354465.7812,  318009.3438,  273455.5000,  254720.6406,  250440.7969],
        [ 883489.9375,  842492.6250,  829855.8750,  657739.6875,  634106.3750,
          599088.4375,  585985.1875,  558434.1875,  540682.8750,  538805.6250],
        [ 803614.6875,  772176.7500,  680088.5625,  476593.4062,  431108.2188,
          426129.9688,  376461.0000,  338518.5312,  304324.1562,  302528.3750],
        [ 887334.3750,  877532.0625,  656040.1250,  640924.8750,  633885.6875,
          629334.9375,  618980.3750,  592280.5000,  579283.0625,  575679.6250],
        [ 976561.3750,  563230.6875,  545416.3750,  498530.9688,  463390.6562,
          454424.8438,  450027.1250,  449686.9062,  388163.0312,  377487.3750],
        [1031017.6875,  823891.9375,  806190.8125,  803889.0625,  794493.1875,
          772606.1875,  760319.8125,  708856.8125,  703874.6250,  668427.6875],
        [ 583242.1250,  569227.8750,  538299.7500,  529828.3750,  433007.3125,
          423876.3750,  329881.7500,  281343.5625,  280637.1875,  192440.3594],
        [1051082.0000,  706963.1250,  687549.6250,  629533.6250,  575161.6250,
          516132.8438,  498791.5938,  453599.5625,  437709.2812,  404491.1250],
        [1162664.2500, 1094987.8750, 1093469.5000, 1046380.3750, 1045475.5625,
         1043712.3750, 1037055.8125, 1030640.1875, 1026523.2500, 1020235.0000],
        [ 862144.5000,  813092.8750,  806216.1875,  804136.7500,  790970.1875,
          740887.4375,  722273.4375,  712431.8125,  701775.3125,  700845.0000],
        [ 781164.1250,  694671.7500,  655941.2500,  639934.1875,  624116.5625,
          619032.2500,  608652.6875,  583920.5000,  571475.9375,  569935.6250],
        [1291713.0000, 1160474.1250, 1125215.3750, 1036260.8750, 1021981.0625,
         1013502.0000,  992187.8750,  929858.1250,  919922.9375,  902326.1250],
        [1166132.1250, 1009086.8125, 1000252.9375,  909485.6250,  908732.2500,
          889440.6875,  887538.3750,  881930.8750,  872043.5000,  870641.6875],
        [ 966111.3750,  875319.6875,  868603.1250,  850833.5000,  781448.7500,
          674500.8125,  594954.3125,  578169.8750,  567627.6250,  566834.5625],
        [ 764470.6250,  753402.8750,  738246.0625,  649895.8125,  648616.5625,
          635513.3750,  618462.2500,  604064.7500,  586062.8125,  585532.6250],
        [1303276.2500, 1119043.8750, 1022016.1875, 1021983.0625,  964310.0000,
          958445.9375,  899990.2500,  874562.0000,  871467.3750,  849118.2500],
        [ 958749.5000,  836274.4375,  806178.5000,  753420.1250,  703658.5000,
          690397.3750,  661962.1875,  643834.7500,  640423.8750,  640415.9375],
        [1213270.3750,  925721.6250,  883290.1875,  858677.6250,  847006.5000,
          828844.3125,  755575.8125,  740345.0000,  691487.2500,  664359.7500],
        [ 923645.8750,  873652.5625,  826648.1875,  826324.1875,  814749.3750,
          799255.0625,  764146.2500,  753282.8125,  744646.6875,  734849.1250],
        [1133479.5000,  816322.0000,  640276.6250,  597151.8750,  541930.0625,
          466033.7812,  457758.3750,  453691.7188,  450182.9375,  444404.2500],
        [ 797041.6250,  786104.6875,  779377.5000,  739956.8125,  709293.0000,
          671379.2500,  661553.8125,  655908.7500,  648470.5625,  639846.9375],
        [ 913980.0625,  839357.0000,  725065.7500,  714672.0000,  679712.5000,
          678922.1875,  658916.2500,  625013.0000,  610345.9375,  594733.0000],
        [ 968150.6875,  816518.1250,  788964.0000,  770758.2500,  754287.8125,
          729140.9375,  674371.5625,  667942.7500,  666563.8125,  645370.3750],
        [1004450.3750,  883557.3125,  836157.1875,  830979.6875,  826302.1875,
          797209.5625,  791662.9375,  781043.4375,  714268.6250,  687298.5625],
        [ 860486.8750,  836780.1875,  769900.1250,  761455.3750,  756656.0625,
          756057.3125,  744398.8750,  721377.1250,  703932.3750,  695881.8750],
        [1060128.2500,  995852.6250,  973611.6875,  969469.1875,  948001.8125,
          923799.1250,  921876.0625,  911382.8125,  863079.0000,  858226.5625],
        [ 828358.3125,  794150.0625,  704720.2500,  662095.3750,  660716.5000,
          657783.6250,  629868.1250,  561573.3750,  550550.3125,  548205.8125],
        [ 437393.0000,  393670.4062,  363910.2812,  322775.8438,  314542.5312,
          272443.1875,  264509.3750,  250121.9062,  242740.3906,  236440.1250],
        [ 755700.5000,  693887.8125,  607183.6250,  571245.4375,  534069.9375,
          516235.7188,  496468.0312,  487891.2812,  445400.9375,  415059.9062],
        [ 792180.3125,  721964.8750,  688558.2500,  609269.4375,  590596.3125,
          584418.5625,  578117.4375,  571483.5000,  563956.8750,  557947.1250],
        [ 704558.2500,  640737.8125,  616991.8750,  590890.9375,  547515.1250,
          524364.8125,  515762.3125,  470947.0625,  470546.5938,  466111.5625],
        [ 780555.0000,  364421.5000,  346102.7188,  335043.0938,  334895.5312,
          333897.3438,  305558.4062,  292179.3438,  289350.1250,  284148.6875],
        [1036948.9375,  804334.6875,  622860.1250,  611850.6875,  490607.2812,
          480078.1250,  477255.1875,  472181.9688,  457140.6250,  442852.4062],
        [ 639690.1250,  634845.1875,  608025.0000,  488232.9375,  446740.0625,
          429756.8750,  334477.7188,  325302.7500,  325260.8750,  324833.7188],
        [1106503.5000, 1092529.2500, 1041437.4375, 1025678.6875, 1020303.1250,
         1012193.2500, 1001071.7500,  995360.8750,  964865.6250,  963947.6875],
        [ 537411.8125,  508107.6875,  409583.2500,  409104.6250,  407252.0938,
          346207.6875,  312509.3125,  311706.8438,  292732.4062,  292124.4375],
        [ 538850.8750,  520859.0000,  465904.9062,  433926.2812,  425113.9688,
          365131.1875,  361382.5312,  360128.8125,  352932.0000,  351479.5938],
        [ 450190.6875,  420425.3750,  407408.2812,  361753.2188,  334075.3750,
          320820.9688,  289154.5312,  288242.6562,  276022.4062,  273958.0000],
        [ 684600.8125,  451590.3125,  430630.3125,  430467.7500,  419117.1250,
          303671.5625,  291956.5000,  283481.7500,  275694.0938,  271884.6250],
        [ 348215.9688,  342473.5938,  323390.8438,  309435.7188,  304132.6875,
          295181.7188,  272005.2188,  263181.0625,  259791.6250,  238674.5938],
        [ 216548.7812,  124775.6953,  119811.9688,   86512.3984,   71117.5000,
           65427.7188,   63766.0312,   60451.7422,   56971.4219,   46263.4453],
        [ 836463.4375,  691860.6250,  611849.5000,  568703.1875,  555847.6875,
          547191.4375,  519697.5000,  515649.2188,  513824.5938,  513788.3438],
        [ 231225.4688,  208350.0781,  186602.6719,  168041.0156,  147470.2188,
          139852.1406,  114458.3594,   99014.9453,   98650.7422,   93004.0391],
        [ 447970.4688,  283607.7812,  280928.2500,  266879.5312,  260457.2031,
          236450.9531,  202533.8438,  200495.6094,  189507.0781,  185259.8438],
        [ 663504.3125,  640670.6250,  609876.3750,  494052.6562,  472322.9688,
          471606.3750,  440938.2812,  434553.2500,  433635.4375,  428585.4688],
        [ 340877.9375,  243380.6250,  242431.3281,  208453.0156,  204028.5312,
          201029.5938,  170583.6250,  169348.1562,  166563.2500,  164967.5469],
        [ 488975.6875,  472913.8750,  434698.3438,  370584.8750,  364266.5625,
          337881.8750,  324805.2188,  304549.1875,  303862.4688,  270863.9062],
        [ 926100.5000,  560787.7500,  553330.1250,  537653.2500,  434271.5625,
          421085.4688,  398799.2812,  379789.3438,  338625.7500,  337791.6562],
        [ 681804.3125,  440562.1250,  397532.9375,  358893.1875,  315444.6250,
          304232.1875,  300189.5000,  296141.5312,  279791.3750,  277997.6562],
        [ 909745.0000,  789005.3750,  733228.6250,  695203.9375,  695203.9375,
          633549.6250,  632818.3750,  632709.1250,  626855.1250,  625122.6250],
        [ 819476.2500,  599249.5625,  585719.7500,  578715.4375,  549598.1875,
          541539.5000,  532484.6875,  523756.0312,  510891.4062,  507157.8750],
        [ 844721.1250,  596673.1250,  531894.3750,  434985.7188,  428713.8125,
          347708.5625,  339903.4062,  337158.9062,  334411.3438,  328616.0625],
        [ 829551.2500,  477236.5625,  385153.3438,  297715.1250,  296809.6250,
          291606.4375,  272931.0312,  270644.4375,  266379.3438,  249868.9531],
        [ 792290.6250,  410325.6562,  403678.4062,  374684.0625,  369390.1562,
          357298.6875,  314838.4375,  297828.7188,  264814.0312,  241207.6562],
        [ 673504.5000,  250505.2969,  233285.6094,  223730.4375,  215693.4375,
          206420.3750,  196865.3438,  176419.3125,  174472.9062,  144344.7812],
        [ 544452.3750,  523378.0625,  488436.0000,  467098.1250,  427223.3125,
          422492.1250,  416230.8750,  408284.2188,  404020.4062,  392761.0312],
        [ 183289.6562,  176789.0000,  140639.2500,  140066.3750,  134785.3281,
          129350.7188,  118035.1328,  114237.9766,  112056.2969,  105238.3438],
        [ 438796.3750,  370598.6562,  348937.0000,  338085.2500,  307533.1562,
          303911.4688,  296528.1250,  295599.5000,  283642.6562,  282846.0625],
        [ 371368.1875,  360666.3750,  278271.4062,  247346.5625,  246119.2188,
          242059.1406,  220201.7656,  206873.6250,  205619.9062,  205303.0781]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 482662.2188,       0.0000],
         [ 304202.5938,       0.0000],
         [ 220031.3125,       0.0000],
         ...,
         [ 168675.2188,       0.0000],
         [ 166980.1094,       0.0000],
         [ 155159.8906,       0.0000]],

        [[1052682.0000,       0.0000],
         [1017401.8125,       0.0000],
         [1016925.5000,       0.0000],
         ...,
         [ 966155.6250,       0.0000],
         [ 947638.3750,       0.0000],
         [ 929685.3125,       0.0000]],

        [[ 682886.5000,       0.0000],
         [ 562399.8125,       0.0000],
         [ 520158.5938,       0.0000],
         ...,
         [ 399777.1875,       0.0000],
         [ 357486.1562,       0.0000],
         [      0.0000,  317761.3438]],

        ...,

        [[ 183289.6562,       0.0000],
         [      0.0000,  176789.0000],
         [ 140639.2500,       0.0000],
         ...,
         [ 114237.9766,       0.0000],
         [ 112056.2969,       0.0000],
         [ 105238.3438,       0.0000]],

        [[      0.0000,  438796.3750],
         [ 370598.6562,       0.0000],
         [ 348937.0000,       0.0000],
         ...,
         [      0.0000,  295599.5000],
         [      0.0000,  283642.6562],
         [ 282846.0625,       0.0000]],

        [[      0.0000,  371368.1875],
         [      0.0000,  360666.3750],
         [ 278271.4062,       0.0000],
         ...,
         [ 206873.6250,       0.0000],
         [ 205619.9062,       0.0000],
         [ 205303.0781,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2081237.2500,   190897.7969],
        [ 9933688.0000,        0.0000],
        [ 4392434.5000,   317761.3438],
        [ 6322679.0000,        0.0000],
        [  912650.5625,  1076588.5000],
        [ 2472380.7500,  1007435.4375],
        [ 4644279.5000,  2026401.2500],
        [ 4911543.5000,        0.0000],
        [ 5813743.5000,   877532.0625],
        [ 5166919.5000,        0.0000],
        [ 7873567.5000,        0.0000],
        [ 4161784.7500,        0.0000],
        [ 5961014.0000,        0.0000],
        [10601144.0000,        0.0000],
        [ 7654773.0000,        0.0000],
        [ 6348845.0000,        0.0000],
        [10393441.0000,        0.0000],
        [ 9395284.0000,        0.0000],
        [ 7324404.0000,        0.0000],
        [ 6584267.5000,        0.0000],
        [ 9884213.0000,        0.0000],
        [ 7335316.0000,        0.0000],
        [ 8408579.0000,        0.0000],
        [ 8061200.5000,        0.0000],
        [ 6001231.0000,        0.0000],
        [ 7088932.5000,        0.0000],
        [ 7040718.0000,        0.0000],
        [ 7482068.5000,        0.0000],
        [ 8152929.5000,        0.0000],
        [ 7606926.5000,        0.0000],
        [ 9425427.0000,        0.0000],
        [ 6598022.0000,        0.0000],
        [ 2862106.7500,   236440.1250],
        [ 3778949.7500,  1744193.5000],
        [ 3623201.0000,  2635291.7500],
        [ 4203130.5000,  1345296.0000],
        [ 1955645.2500,  1710506.6250],
        [ 3442975.5000,  2453134.2500],
        [ 2049174.3750,  2507991.0000],
        [10223890.0000,        0.0000],
        [ 3826740.0000,        0.0000],
        [ 4175709.0000,        0.0000],
        [ 3422051.5000,        0.0000],
        [ 3843094.7500,        0.0000],
        [ 1501974.7500,  1454508.2500],
        [  450510.2500,   461136.4375],
        [ 5263026.0000,   611849.5000],
        [ 1185608.6250,   301061.0312],
        [  564536.0000,  1989554.5000],
        [ 3313248.0000,  1776497.8750],
        [ 1701719.7500,   409943.8750],
        [ 2191063.5000,  1482338.6250],
        [ 2451355.5000,  2436879.2500],
        [ 2915885.7500,   736703.6250],
        [ 6973441.5000,        0.0000],
        [ 5162869.0000,   585719.7500],
        [ 1967066.3750,  2557720.2500],
        [ 2775506.2500,   862389.8750],
        [ 2067694.0000,  1758662.5000],
        [ 1158673.5000,  1336568.6250],
        [ 4086092.5000,   408284.2188],
        [ 1177699.1250,   176789.0000],
        [ 2248439.7500,  1018038.5000],
        [ 1389533.7500,  1194295.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False]], device='cuda:0')
Top 1 acc 56.25
Top1 accuracy for validation set is 56.25 size is torch.Size([64, 1])
Epoch 196/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:23, 60.80s/it]  7%|▋         | 2/30 [01:01<11:53, 25.48s/it] 10%|█         | 3/30 [01:02<06:22, 14.18s/it] 13%|█▎        | 4/30 [01:03<03:50,  8.88s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.95s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 3.0422844648361207
Epoch 197/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:20, 60.72s/it]  7%|▋         | 2/30 [01:01<11:52, 25.44s/it] 10%|█         | 3/30 [01:02<06:25, 14.29s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.99s/it] 20%|██        | 6/30 [01:04<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 3.061447747548421
Epoch 198/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:55<27:03, 55.97s/it]  7%|▋         | 2/30 [00:58<11:19, 24.27s/it] 10%|█         | 3/30 [00:58<06:05, 13.53s/it] 13%|█▎        | 4/30 [01:00<03:48,  8.77s/it] 17%|█▋        | 5/30 [01:01<02:33,  6.13s/it] 20%|██        | 6/30 [01:02<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:03<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:03<00:52,  2.38s/it] 30%|███       | 9/30 [01:04<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:05<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.28s/it] 40%|████      | 12/30 [01:06<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:07<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 3.0009103298187254
Epoch 199/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:30, 58.99s/it]  7%|▋         | 2/30 [00:59<11:32, 24.73s/it] 10%|█         | 3/30 [01:00<06:12, 13.78s/it] 13%|█▎        | 4/30 [01:01<03:44,  8.64s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.79s/it] 20%|██        | 6/30 [01:02<01:37,  4.08s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 3.0062809864679974
Epoch 200/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:09, 60.32s/it]  7%|▋         | 2/30 [01:01<11:47, 25.28s/it] 10%|█         | 3/30 [01:01<06:20, 14.08s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.81s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.91s/it] 20%|██        | 6/30 [01:04<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 3.000643006960551
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0249,  0.0093, -0.0081,  ...,  0.0059, -0.0167,  0.0227],
        [-0.0074,  0.0232,  0.0073,  ...,  0.0109, -0.0226, -0.0078],
        [-0.0495, -0.0315,  0.0076,  ...,  0.0681, -0.0348, -0.0332],
        ...,
        [ 0.0322, -0.0220, -0.0195,  ..., -0.0246, -0.0381, -0.0134],
        [-0.0312, -0.0049, -0.0121,  ..., -0.0099,  0.0114, -0.0167],
        [-0.0349, -0.0045,  0.0126,  ...,  0.0096,  0.0156, -0.0565]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9134, 0.8809, 0.8604, 0.8583, 0.8503, 0.8496, 0.8483, 0.8435, 0.8408,
         0.8389],
        [0.9687, 0.9679, 0.9663, 0.9660, 0.9657, 0.9647, 0.9646, 0.9633, 0.9616,
         0.9609],
        [0.9390, 0.9268, 0.9203, 0.9158, 0.9119, 0.9116, 0.9081, 0.9026, 0.8942,
         0.8863],
        [0.9708, 0.9382, 0.9356, 0.9313, 0.9286, 0.9270, 0.9267, 0.9215, 0.9158,
         0.9136],
        [0.8741, 0.8658, 0.8640, 0.8616, 0.8499, 0.8422, 0.8390, 0.8308, 0.8271,
         0.8243],
        [0.9118, 0.8977, 0.8951, 0.8928, 0.8895, 0.8885, 0.8815, 0.8673, 0.8635,
         0.8624],
        [0.9538, 0.9509, 0.9505, 0.9316, 0.9271, 0.9256, 0.9244, 0.9204, 0.9203,
         0.9195],
        [0.9491, 0.9463, 0.9368, 0.9109, 0.9035, 0.9029, 0.8968, 0.8865, 0.8806,
         0.8800],
        [0.9566, 0.9555, 0.9346, 0.9329, 0.9328, 0.9313, 0.9303, 0.9282, 0.9266,
         0.9263],
        [0.9637, 0.9256, 0.9236, 0.9160, 0.9115, 0.9104, 0.9101, 0.9079, 0.8995,
         0.8966],
        [0.9695, 0.9516, 0.9503, 0.9487, 0.9481, 0.9446, 0.9445, 0.9406, 0.9390,
         0.9360],
        [0.9284, 0.9270, 0.9189, 0.9181, 0.9022, 0.9011, 0.8872, 0.8731, 0.8731,
         0.8463],
        [0.9692, 0.9405, 0.9376, 0.9332, 0.9278, 0.9213, 0.9162, 0.9108, 0.9086,
         0.9043],
        [0.9777, 0.9724, 0.9719, 0.9708, 0.9694, 0.9685, 0.9684, 0.9680, 0.9674,
         0.9670],
        [0.9557, 0.9522, 0.9521, 0.9513, 0.9498, 0.9457, 0.9448, 0.9431, 0.9417,
         0.9417],
        [0.9485, 0.9408, 0.9358, 0.9338, 0.9337, 0.9323, 0.9307, 0.9283, 0.9281,
         0.9280],
        [0.9845, 0.9763, 0.9750, 0.9687, 0.9663, 0.9660, 0.9651, 0.9629, 0.9589,
         0.9589],
        [0.9773, 0.9664, 0.9651, 0.9607, 0.9589, 0.9585, 0.9584, 0.9573, 0.9567,
         0.9560],
        [0.9628, 0.9555, 0.9549, 0.9548, 0.9495, 0.9362, 0.9291, 0.9279, 0.9261,
         0.9212],
        [0.9458, 0.9450, 0.9403, 0.9358, 0.9350, 0.9335, 0.9312, 0.9301, 0.9271,
         0.9264],
        [0.9841, 0.9736, 0.9683, 0.9676, 0.9631, 0.9625, 0.9578, 0.9570, 0.9553,
         0.9550],
        [0.9622, 0.9532, 0.9498, 0.9466, 0.9409, 0.9399, 0.9372, 0.9354, 0.9328,
         0.9323],
        [0.9797, 0.9609, 0.9578, 0.9559, 0.9553, 0.9542, 0.9458, 0.9448, 0.9407,
         0.9375],
        [0.9610, 0.9564, 0.9544, 0.9531, 0.9516, 0.9515, 0.9473, 0.9458, 0.9447,
         0.9439],
        [0.9753, 0.9492, 0.9348, 0.9291, 0.9237, 0.9115, 0.9109, 0.9106, 0.9087,
         0.9084],
        [0.9509, 0.9503, 0.9496, 0.9445, 0.9419, 0.9372, 0.9370, 0.9370, 0.9361,
         0.9353],
        [0.9599, 0.9538, 0.9434, 0.9410, 0.9394, 0.9393, 0.9364, 0.9336, 0.9316,
         0.9296],
        [0.9658, 0.9534, 0.9519, 0.9499, 0.9476, 0.9443, 0.9401, 0.9390, 0.9385,
         0.9372],
        [0.9663, 0.9580, 0.9532, 0.9529, 0.9522, 0.9502, 0.9493, 0.9486, 0.9403,
         0.9403],
        [0.9568, 0.9544, 0.9474, 0.9473, 0.9471, 0.9465, 0.9445, 0.9432, 0.9415,
         0.9410],
        [0.9716, 0.9659, 0.9643, 0.9635, 0.9624, 0.9601, 0.9597, 0.9588, 0.9564,
         0.9562],
        [0.9534, 0.9495, 0.9439, 0.9385, 0.9375, 0.9372, 0.9327, 0.9245, 0.9232,
         0.9231],
        [0.9099, 0.9019, 0.8944, 0.8894, 0.8851, 0.8763, 0.8712, 0.8705, 0.8682,
         0.8652],
        [0.9465, 0.9399, 0.9291, 0.9279, 0.9206, 0.9192, 0.9161, 0.9154, 0.9087,
         0.9037],
        [0.9503, 0.9433, 0.9405, 0.9325, 0.9306, 0.9281, 0.9270, 0.9269, 0.9268,
         0.9254],
        [0.9402, 0.9346, 0.9336, 0.9258, 0.9232, 0.9200, 0.9191, 0.9125, 0.9124,
         0.9110],
        [0.9500, 0.8956, 0.8894, 0.8882, 0.8864, 0.8847, 0.8812, 0.8781, 0.8774,
         0.8771],
        [0.9687, 0.9512, 0.9320, 0.9307, 0.9140, 0.9139, 0.9122, 0.9115, 0.9096,
         0.9082],
        [0.9326, 0.9313, 0.9302, 0.9166, 0.9096, 0.9058, 0.8895, 0.8869, 0.8862,
         0.8858],
        [0.9733, 0.9719, 0.9694, 0.9682, 0.9671, 0.9671, 0.9653, 0.9648, 0.9641,
         0.9627],
        [0.9215, 0.9181, 0.9049, 0.9048, 0.9011, 0.8886, 0.8859, 0.8832, 0.8814,
         0.8806],
        [0.9227, 0.9202, 0.9123, 0.9071, 0.9051, 0.8948, 0.8939, 0.8939, 0.8931,
         0.8926],
        [0.9100, 0.9080, 0.9051, 0.8969, 0.8879, 0.8862, 0.8803, 0.8783, 0.8772,
         0.8742],
        [0.9387, 0.9131, 0.9065, 0.9059, 0.9055, 0.8810, 0.8801, 0.8792, 0.8775,
         0.8757],
        [0.8898, 0.8878, 0.8854, 0.8840, 0.8838, 0.8766, 0.8711, 0.8693, 0.8691,
         0.8655],
        [0.8600, 0.8243, 0.8195, 0.7980, 0.7847, 0.7801, 0.7752, 0.7660, 0.7657,
         0.7571],
        [0.9526, 0.9392, 0.9318, 0.9255, 0.9239, 0.9232, 0.9204, 0.9203, 0.9189,
         0.9181],
        [0.8621, 0.8555, 0.8454, 0.8389, 0.8308, 0.8273, 0.8084, 0.8049, 0.8046,
         0.7997],
        [0.9097, 0.8728, 0.8726, 0.8708, 0.8704, 0.8595, 0.8497, 0.8487, 0.8446,
         0.8440],
        [0.9362, 0.9320, 0.9301, 0.9117, 0.9104, 0.9092, 0.9058, 0.9036, 0.9036,
         0.9034],
        [0.8941, 0.8621, 0.8605, 0.8507, 0.8500, 0.8486, 0.8410, 0.8391, 0.8385,
         0.8380],
        [0.9143, 0.9139, 0.9106, 0.8938, 0.8912, 0.8866, 0.8855, 0.8819, 0.8777,
         0.8724],
        [0.9617, 0.9268, 0.9245, 0.9237, 0.9070, 0.9065, 0.8993, 0.8984, 0.8903,
         0.8897],
        [0.9390, 0.9089, 0.9003, 0.8932, 0.8833, 0.8828, 0.8808, 0.8787, 0.8784,
         0.8776],
        [0.9602, 0.9493, 0.9431, 0.9408, 0.9408, 0.9353, 0.9343, 0.9342, 0.9338,
         0.9320],
        [0.9520, 0.9293, 0.9260, 0.9257, 0.9233, 0.9228, 0.9198, 0.9181, 0.9174,
         0.9173],
        [0.9538, 0.9292, 0.9205, 0.9073, 0.9024, 0.8919, 0.8916, 0.8897, 0.8893,
         0.8864],
        [0.9537, 0.9142, 0.9004, 0.8824, 0.8805, 0.8794, 0.8753, 0.8740, 0.8737,
         0.8701],
        [0.9496, 0.9037, 0.9028, 0.8988, 0.8955, 0.8916, 0.8809, 0.8808, 0.8758,
         0.8662],
        [0.9365, 0.8677, 0.8639, 0.8594, 0.8570, 0.8538, 0.8529, 0.8450, 0.8418,
         0.8304],
        [0.9231, 0.9203, 0.9164, 0.9099, 0.9065, 0.9048, 0.9041, 0.9029, 0.9021,
         0.9011],
        [0.8480, 0.8427, 0.8264, 0.8261, 0.8229, 0.8202, 0.8151, 0.8132, 0.8121,
         0.8070],
        [0.9045, 0.8974, 0.8911, 0.8879, 0.8845, 0.8809, 0.8801, 0.8780, 0.8772,
         0.8761],
        [0.8947, 0.8917, 0.8691, 0.8642, 0.8637, 0.8610, 0.8585, 0.8538, 0.8537,
         0.8510]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 1, 0, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 1, 0, 0, 1],
        [1, 0, 0, 1, 1, 1, 1, 1, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 1, 0, 1, 1, 1, 1],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 1, 0, 1, 1, 0, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 1, 1, 0, 0],
        [1, 1, 0, 0, 1, 0, 1, 1, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 464265.5938,  291808.9688,  217882.8438,  211368.4688,  188497.1562,
          186696.3125,  183188.2969,  171027.9844,  164574.7031,  160275.8438],
        [1023719.3125, 1012313.8750,  988617.6250,  985218.8750,  980985.8125,
          967024.8750,  965428.0625,  946893.0625,  924614.3750,  915160.1250],
        [ 669331.6250,  562689.5000,  512829.8125,  480494.4688,  454402.7188,
          452465.4375,  430590.9062,  398087.1875,  353088.5625,  315331.2188],
        [1054099.5000,  661762.0625,  637633.2500,  599469.0000,  577023.5625,
          564046.1250,  561347.9375,  521110.9375,  480705.2812,  466085.7812],
        [ 264939.3125,  235305.0469,  229245.8594,  221625.3125,  187395.3750,
          167966.8438,  160495.7969,  142757.1562,  135333.3750,  130123.5078],
        [ 454235.9375,  370905.5625,  357460.2500,  346176.0000,  330160.2812,
          325280.4062,  294359.1875,  240448.3906,  227647.6094,  224170.8438],
        [ 827010.8750,  793494.4375,  789341.8125,  602640.0625,  565181.7500,
          553161.3125,  543125.2500,  512942.7812,  512225.6875,  506406.3438],
        [ 773408.9375,  743466.6875,  648719.8750,  447917.5000,  403194.4062,
          399745.1250,  366296.7812,  316297.1250,  290780.2812,  288078.5938],
        [ 861252.0625,  847048.5000,  628584.0000,  613352.6875,  612758.6875,
          599872.2500,  591576.0000,  573546.3125,  560958.3125,  558797.5625],
        [ 952107.9375,  553099.0625,  537079.8125,  482052.2500,  452060.4375,
          445230.1875,  443036.5625,  429207.2188,  380854.2500,  365155.5625],
        [1034787.5000,  802041.3125,  786607.9375,  768824.5000,  762725.0625,
          725125.2500,  724334.5000,  684639.3125,  669236.5000,  641318.5625],
        [ 575153.9375,  564367.3750,  502493.7188,  496408.3750,  395634.3125,
          389391.7500,  319338.3438,  261292.3906,  261159.8594,  178163.1562],
        [1031183.8750,  684039.5625,  656225.9375,  616318.4375,  570633.4375,
          520171.5000,  483195.5312,  447400.9375,  433670.5938,  408106.6875],
        [1164176.5000, 1078127.8750, 1071870.0000, 1054548.0000, 1032899.3750,
         1020359.6250, 1018405.5625, 1013593.8750, 1004029.0000,  999086.0000],
        [ 850435.9375,  807960.3125,  807456.5625,  797570.7500,  780938.4375,
          737162.6875,  727346.3750,  709485.8125,  695720.6250,  695538.8750],
        [ 766886.8750,  686464.0625,  639238.1875,  621752.7500,  620958.6875,
          608333.0000,  594595.7500,  574894.0000,  572983.1875,  571924.6875],
        [1282229.2500, 1141417.6250, 1120095.5000, 1023263.5625,  988684.5000,
          984383.9375,  971655.4375,  941682.6250,  889186.1875,  889084.4375],
        [1157036.2500,  989968.6250,  972161.5625,  912709.1875,  889450.0000,
          884748.6875,  882929.7500,  868942.8125,  861606.0625,  853805.1875],
        [ 940490.7500,  847648.8750,  839765.3125,  839529.8750,  778332.4375,
          643355.4375,  580991.5625,  571326.6250,  556808.5000,  518954.5625],
        [ 737625.4375,  729134.6875,  681962.3750,  639227.2500,  632698.8750,
          618713.5625,  598653.8125,  589432.6875,  564662.3750,  558884.9375],
        [1275244.0000, 1098241.3750, 1017919.1250, 1007649.1250,  944832.8125,
          936786.6250,  875954.3125,  865209.3125,  844522.2500,  841039.6250],
        [ 932939.5625,  819659.9375,  780987.6250,  745946.7500,  688231.2500,
          678174.1250,  652570.6250,  635918.3125,  613124.0000,  608400.8125],
        [1196875.5000,  914777.9375,  875255.3750,  852392.0000,  844569.7500,
          831333.1875,  737379.9375,  727265.8750,  685886.8750,  655025.4375],
        [ 917212.5000,  858049.8125,  833751.6250,  818699.0625,  801876.1250,
          800393.8750,  753949.8125,  738077.1250,  726633.6250,  718244.8125],
        [1124615.6250,  775005.1875,  630644.7500,  580922.2500,  537917.3750,
          451917.3438,  447954.2500,  446071.6875,  434462.9375,  432251.8750],
        [ 793480.8125,  786722.7500,  778943.5625,  723844.9375,  698259.1250,
          652063.0625,  651088.6875,  650795.0625,  642548.5000,  635282.4375],
        [ 902149.7500,  827646.8750,  713359.1250,  688680.3750,  672932.4375,
          672439.7500,  645402.3750,  620093.4375,  602631.4375,  585656.0625],
        [ 981835.6875,  822184.0000,  805344.0000,  782644.3125,  757386.6250,
          721644.1250,  680520.6875,  669026.5625,  664508.6250,  652944.8125],
        [ 988272.5000,  878885.5000,  819698.2500,  816574.1875,  808248.5625,
          786032.7500,  775468.8125,  768374.5000,  682481.5625,  682316.1875],
        [ 863860.4375,  834813.0000,  754759.1250,  753869.2500,  751418.0625,
          745118.3750,  724504.4375,  711372.6875,  694120.8125,  689307.2500],
        [1066539.1250,  983336.8125,  961154.3750,  950731.5000,  934914.0625,
          905237.7500,  899993.7500,  888743.6250,  858407.4375,  855613.0625],
        [ 822995.1250,  777968.8125,  717836.6875,  664892.1875,  655208.5000,
          652041.8750,  611887.4375,  544060.4375,  534448.4375,  533320.6875],
        [ 442049.4062,  394124.1875,  353963.4375,  329473.6562,  309957.0000,
          273245.1250,  254239.1406,  251490.7812,  243525.0469,  233122.1250],
        [ 744891.0625,  677735.1250,  581331.8125,  571552.2500,  514979.4062,
          504805.5000,  483039.3750,  477879.1562,  434131.5938,  404505.8125],
        [ 786232.9375,  711544.3125,  683637.8125,  609872.3125,  593564.0625,
          572776.6875,  564194.1250,  563392.3750,  562566.1250,  551589.3750],
        [ 680957.6250,  628991.1875,  619690.8750,  554428.8125,  534176.3750,
          510613.7500,  503852.1875,  458674.7500,  457669.3438,  448642.5938],
        [ 783496.3750,  360133.2812,  329786.7500,  324232.3438,  315763.3750,
          308027.7500,  293242.5938,  280599.1875,  277788.0312,  276603.2188],
        [1023776.9375,  796791.5000,  606106.3750,  595065.5000,  468729.5625,
          467814.5000,  456586.4062,  452190.6562,  439938.2188,  431431.8750],
        [ 611068.1250,  599541.6250,  590428.4375,  486079.1250,  440075.4062,
          416911.0000,  329922.3438,  317943.2188,  314885.2500,  313267.2188],
        [1093096.2500, 1070446.0000, 1032970.3125, 1016034.6875, 1000809.2500,
          999940.0625,  975434.1875,  967508.3125,  957775.2500,  938823.0625],
        [ 521330.1562,  496669.3125,  411501.3125,  410677.2188,  389834.6562,
          325760.9688,  313768.3438,  301878.4375,  294175.6250,  290506.4375],
        [ 530059.3125,  511515.9375,  457297.1875,  424495.7812,  412578.0312,
          356137.6250,  351695.5625,  351552.0312,  347574.6250,  344946.7500],
        [ 442493.1250,  429680.2500,  412795.6562,  366981.4062,  322840.1875,
          314927.9062,  289306.8125,  281369.5938,  277080.5312,  265345.9062],
        [ 666821.3125,  462550.8750,  420945.7500,  417458.0625,  414987.1250,
          292486.0000,  288698.5312,  284774.8125,  278274.3125,  270994.1250],
        [ 331424.3750,  322003.8125,  311129.8125,  304941.5000,  304323.8750,
          274705.7188,  253968.9531,  247419.4688,  246733.3125,  234416.5312],
        [ 216447.8125,  130076.8672,  121401.4062,   89331.4844,   73885.6406,
           69129.2422,   64448.1328,   56585.7812,   56267.5820,   49830.3516],
        [ 812602.9375,  671548.3125,  604490.0625,  551988.7500,  539519.3125,
          534291.0000,  513445.9375,  512839.5938,  502218.7188,  496995.7812],
        [ 223027.8594,  203060.0781,  175885.4219,  160117.2656,  142724.2188,
          135826.3906,  103609.7344,   98514.6953,   98171.1641,   91478.0078],
        [ 440773.0312,  260045.6875,  259323.7969,  252720.7969,  251278.1250,
          215175.8750,  186957.3281,  184171.0156,  173838.9531,  172207.8594],
        [ 643524.1875,  605665.5000,  589949.5000,  452976.1875,  444972.9688,
          437377.9688,  416752.3750,  403735.3750,  403713.0625,  402571.2188],
        [ 352504.4688,  223037.2188,  218157.0781,  189536.3438,  187875.6719,
          184118.6875,  165007.0312,  160671.7500,  159385.9688,  158126.4688],
        [ 470468.5312,  467856.0312,  446184.0000,  350969.8125,  338081.7188,
          316491.1250,  311749.3750,  296159.3125,  278952.4062,  258528.4219],
        [ 925446.2500,  562315.0625,  544284.6875,  538119.5625,  423945.9688,
          420834.5625,  379544.2500,  374657.6250,  334069.6250,  330931.0312],
        [ 669385.8750,  435587.6250,  385067.4062,  347779.8750,  302084.3750,
          300094.7812,  291555.2812,  282716.8750,  281893.3438,  278409.1562],
        [ 905710.0625,  776024.4375,  709590.0000,  687215.3125,  687215.3125,
          635144.9375,  625908.3125,  624973.6250,  621488.3125,  605965.9375],
        [ 806624.5000,  582560.0625,  556341.9375,  553465.2500,  535287.5625,
          531287.5625,  508723.4688,  496724.2500,  491555.6562,  490900.2812],
        [ 827115.8125,  582051.3750,  513818.6875,  425580.0625,  396885.1562,
          341654.1562,  339980.2500,  331037.0938,  328987.3125,  315741.0625],
        [ 825437.3750,  469495.0000,  385627.8438,  298059.7188,  290180.8125,
          285696.9375,  269677.2188,  264674.9062,  263429.6562,  250139.0781],
        [ 779427.3125,  404206.1562,  399062.9375,  377166.0312,  359698.4062,
          339919.3125,  291994.3750,  291594.2188,  271387.5000,  236795.0938],
        [ 646085.3750,  241812.4844,  229145.5312,  214793.3125,  207466.3438,
          198171.4844,  195605.4844,  174746.1719,  166942.0625,  141867.5156],
        [ 533655.4375,  512299.9375,  484914.2188,  441898.5312,  421072.6250,
          410603.9688,  406408.2188,  399964.3750,  395178.4062,  389532.5312],
        [ 182480.5625,  169105.9219,  133988.6719,  133415.5156,  127501.4766,
          122628.5469,  114018.9844,  110920.0312,  109282.7812,  101646.6484],
        [ 409270.0625,  369706.6562,  337528.8750,  322447.8438,  307548.4062,
          291962.9062,  288797.6562,  279913.0938,  276852.3125,  272523.7188],
        [ 355492.1875,  340592.6250,  246546.3281,  229883.8125,  228289.4062,
          219608.0625,  211927.2031,  198175.2656,  197972.5781,  190442.1094]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 464265.5938,       0.0000],
         [ 291808.9688,       0.0000],
         [ 217882.8438,       0.0000],
         ...,
         [ 171027.9844,       0.0000],
         [ 164574.7031,       0.0000],
         [ 160275.8438,       0.0000]],

        [[1023719.3125,       0.0000],
         [1012313.8750,       0.0000],
         [ 988617.6250,       0.0000],
         ...,
         [ 946893.0625,       0.0000],
         [ 924614.3750,       0.0000],
         [ 915160.1250,       0.0000]],

        [[ 669331.6250,       0.0000],
         [ 562689.5000,       0.0000],
         [ 512829.8125,       0.0000],
         ...,
         [ 398087.1875,       0.0000],
         [ 353088.5625,       0.0000],
         [      0.0000,  315331.2188]],

        ...,

        [[      0.0000,  182480.5625],
         [ 169105.9219,       0.0000],
         [ 133988.6719,       0.0000],
         ...,
         [ 110920.0312,       0.0000],
         [ 109282.7812,       0.0000],
         [ 101646.6484,       0.0000]],

        [[      0.0000,  409270.0625],
         [ 369706.6562,       0.0000],
         [ 337528.8750,       0.0000],
         ...,
         [      0.0000,  279913.0938],
         [ 276852.3125,       0.0000],
         [ 272523.7188,       0.0000]],

        [[      0.0000,  355492.1875],
         [      0.0000,  340592.6250],
         [ 246546.3281,       0.0000],
         ...,
         [      0.0000,  198175.2656],
         [ 197972.5781,       0.0000],
         [ 190442.1094,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2051089.0000,   188497.1562],
        [ 9709976.0000,        0.0000],
        [ 4313980.5000,   315331.2188],
        [ 6123283.5000,        0.0000],
        [  834973.3750,  1040214.2500],
        [ 2248512.5000,   922331.9375],
        [ 4301323.0000,  1904207.5000],
        [ 4677905.0000,        0.0000],
        [ 5600698.0000,   847048.5000],
        [ 5039883.5000,        0.0000],
        [ 7599640.0000,        0.0000],
        [ 3943403.2500,        0.0000],
        [ 5850946.5000,        0.0000],
        [10457096.0000,        0.0000],
        [ 7609616.5000,        0.0000],
        [ 6258031.0000,        0.0000],
        [10231683.0000,        0.0000],
        [ 9273358.0000,        0.0000],
        [ 7117204.0000,        0.0000],
        [ 6350996.0000,        0.0000],
        [ 9707398.0000,        0.0000],
        [ 7155953.0000,        0.0000],
        [ 8320762.0000,        0.0000],
        [ 7966889.0000,        0.0000],
        [ 5861763.5000,        0.0000],
        [ 7013029.0000,        0.0000],
        [ 6930992.0000,        0.0000],
        [ 7538040.0000,        0.0000],
        [ 8006353.0000,        0.0000],
        [ 7523143.5000,        0.0000],
        [ 9304671.0000,        0.0000],
        [ 6514660.0000,        0.0000],
        [ 3085190.0000,        0.0000],
        [ 3697331.0000,  1697520.0000],
        [ 4135935.2500,  2063434.7500],
        [ 3629073.7500,  1768623.5000],
        [ 1887553.0000,  1662120.0000],
        [ 3311756.5000,  2426675.0000],
        [ 1979203.5000,  2440918.5000],
        [10052838.0000,        0.0000],
        [ 3756102.5000,        0.0000],
        [ 4087852.7500,        0.0000],
        [ 3402821.5000,        0.0000],
        [ 3797991.0000,        0.0000],
        [ 1411412.7500,  1419654.7500],
        [  459478.1875,   467926.0938],
        [ 5135450.5000,   604490.0625],
        [ 1061441.6250,   370973.1562],
        [  693208.4375,  1703284.0000],
        [ 3107075.5000,  1694162.6250],
        [ 1617257.1250,   381163.6875],
        [ 2322629.0000,  1212811.7500],
        [ 2428685.0000,  2405463.5000],
        [ 2856270.0000,   718304.5000],
        [ 6879236.0000,        0.0000],
        [ 5000005.0000,   553465.2500],
        [ 1925781.2500,  2477069.5000],
        [ 2497156.5000,  1105261.8750],
        [ 2043971.6250,  1707279.7500],
        [ 1125666.1250,  1290969.7500],
        [ 3600385.5000,   795142.7500],
        [ 1122508.5000,   182480.5625],
        [ 2178570.7500,   977980.8750],
        [ 1084452.8750,  1334476.7500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 59.375
Top1 accuracy for validation set is 59.375 size is torch.Size([64, 1])
Epoch 201/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<31:13, 64.61s/it]  7%|▋         | 2/30 [01:05<12:37, 27.04s/it] 10%|█         | 3/30 [01:06<06:45, 15.04s/it] 13%|█▎        | 4/30 [01:06<04:04,  9.39s/it] 17%|█▋        | 5/30 [01:07<02:36,  6.28s/it] 20%|██        | 6/30 [01:08<01:45,  4.40s/it] 23%|██▎       | 7/30 [01:09<01:13,  3.20s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.42s/it] 30%|███       | 9/30 [01:10<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:11<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:12<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:14,  1.07it/s] 50%|█████     | 15/30 [01:15<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:18<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:21<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:24<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.88s/it]
Epoch loss is 2.998575242360433
Epoch 202/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:55, 57.78s/it]  7%|▋         | 2/30 [01:00<11:43, 25.12s/it] 10%|█         | 3/30 [01:00<06:17, 13.99s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.76s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 3.0101431449254354
Epoch 203/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:19, 56.53s/it]  7%|▋         | 2/30 [01:01<12:05, 25.92s/it] 10%|█         | 3/30 [01:01<06:29, 14.43s/it] 13%|█▎        | 4/30 [01:02<03:54,  9.03s/it] 17%|█▋        | 5/30 [01:03<02:31,  6.04s/it] 20%|██        | 6/30 [01:04<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.35s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.9962338050206503
Epoch 204/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:47, 59.58s/it]  7%|▋         | 2/30 [01:00<11:39, 24.97s/it] 10%|█         | 3/30 [01:01<06:15, 13.91s/it] 13%|█▎        | 4/30 [01:03<03:59,  9.20s/it] 17%|█▋        | 5/30 [01:03<02:33,  6.15s/it] 20%|██        | 6/30 [01:04<01:43,  4.32s/it] 23%|██▎       | 7/30 [01:05<01:12,  3.15s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.38s/it] 30%|███       | 9/30 [01:06<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.29s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.9717804114023845
Epoch 205/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:53, 59.79s/it]  7%|▋         | 2/30 [01:00<11:41, 25.06s/it] 10%|█         | 3/30 [01:01<06:16, 13.96s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.989750377337138
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0247,  0.0088, -0.0084,  ...,  0.0068, -0.0155,  0.0225],
        [-0.0062,  0.0239,  0.0086,  ...,  0.0093, -0.0206, -0.0085],
        [-0.0472, -0.0319,  0.0075,  ...,  0.0692, -0.0340, -0.0322],
        ...,
        [ 0.0311, -0.0211, -0.0185,  ..., -0.0252, -0.0372, -0.0135],
        [-0.0307, -0.0048, -0.0124,  ..., -0.0092,  0.0112, -0.0160],
        [-0.0353, -0.0051,  0.0132,  ...,  0.0115,  0.0163, -0.0558]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9132, 0.8765, 0.8616, 0.8595, 0.8508, 0.8505, 0.8497, 0.8453, 0.8419,
         0.8410],
        [0.9673, 0.9661, 0.9641, 0.9634, 0.9629, 0.9628, 0.9627, 0.9613, 0.9610,
         0.9590],
        [0.9370, 0.9264, 0.9185, 0.9139, 0.9112, 0.9078, 0.9038, 0.9018, 0.8914,
         0.8857],
        [0.9697, 0.9351, 0.9324, 0.9315, 0.9276, 0.9263, 0.9227, 0.9191, 0.9137,
         0.9123],
        [0.8730, 0.8643, 0.8611, 0.8572, 0.8470, 0.8350, 0.8306, 0.8302, 0.8284,
         0.8189],
        [0.9072, 0.8915, 0.8887, 0.8870, 0.8824, 0.8813, 0.8770, 0.8593, 0.8562,
         0.8561],
        [0.9495, 0.9480, 0.9471, 0.9266, 0.9201, 0.9194, 0.9191, 0.9168, 0.9165,
         0.9145],
        [0.9471, 0.9433, 0.9343, 0.9074, 0.8998, 0.8979, 0.8951, 0.8820, 0.8778,
         0.8769],
        [0.9544, 0.9530, 0.9315, 0.9311, 0.9303, 0.9293, 0.9258, 0.9252, 0.9250,
         0.9247],
        [0.9622, 0.9239, 0.9227, 0.9140, 0.9105, 0.9098, 0.9083, 0.9053, 0.8989,
         0.8957],
        [0.9698, 0.9496, 0.9485, 0.9459, 0.9445, 0.9412, 0.9387, 0.9377, 0.9355,
         0.9336],
        [0.9276, 0.9259, 0.9141, 0.9134, 0.8962, 0.8954, 0.8852, 0.8679, 0.8678,
         0.8422],
        [0.9679, 0.9387, 0.9346, 0.9330, 0.9279, 0.9219, 0.9142, 0.9101, 0.9080,
         0.9050],
        [0.9777, 0.9712, 0.9704, 0.9700, 0.9685, 0.9683, 0.9666, 0.9664, 0.9662,
         0.9662],
        [0.9551, 0.9513, 0.9509, 0.9505, 0.9489, 0.9453, 0.9447, 0.9434, 0.9418,
         0.9414],
        [0.9468, 0.9395, 0.9341, 0.9339, 0.9314, 0.9313, 0.9288, 0.9284, 0.9280,
         0.9272],
        [0.9842, 0.9751, 0.9747, 0.9681, 0.9645, 0.9642, 0.9639, 0.9634, 0.9580,
         0.9564],
        [0.9767, 0.9656, 0.9620, 0.9604, 0.9585, 0.9575, 0.9568, 0.9567, 0.9553,
         0.9551],
        [0.9608, 0.9539, 0.9526, 0.9524, 0.9490, 0.9335, 0.9271, 0.9270, 0.9241,
         0.9194],
        [0.9437, 0.9418, 0.9349, 0.9345, 0.9329, 0.9316, 0.9285, 0.9282, 0.9257,
         0.9238],
        [0.9827, 0.9723, 0.9678, 0.9668, 0.9620, 0.9612, 0.9563, 0.9559, 0.9544,
         0.9538],
        [0.9600, 0.9513, 0.9481, 0.9458, 0.9390, 0.9382, 0.9361, 0.9349, 0.9292,
         0.9289],
        [0.9789, 0.9600, 0.9574, 0.9553, 0.9552, 0.9537, 0.9440, 0.9439, 0.9399,
         0.9365],
        [0.9599, 0.9550, 0.9546, 0.9515, 0.9506, 0.9500, 0.9456, 0.9445, 0.9417,
         0.9410],
        [0.9747, 0.9454, 0.9335, 0.9276, 0.9235, 0.9101, 0.9095, 0.9082, 0.9069,
         0.9065],
        [0.9504, 0.9502, 0.9484, 0.9427, 0.9407, 0.9370, 0.9369, 0.9351, 0.9342,
         0.9338],
        [0.9590, 0.9531, 0.9427, 0.9392, 0.9385, 0.9384, 0.9345, 0.9329, 0.9307,
         0.9288],
        [0.9667, 0.9532, 0.9531, 0.9514, 0.9479, 0.9427, 0.9408, 0.9393, 0.9380,
         0.9376],
        [0.9652, 0.9576, 0.9523, 0.9510, 0.9504, 0.9496, 0.9476, 0.9474, 0.9399,
         0.9384],
        [0.9566, 0.9548, 0.9471, 0.9456, 0.9454, 0.9451, 0.9424, 0.9423, 0.9407,
         0.9398],
        [0.9719, 0.9654, 0.9640, 0.9625, 0.9615, 0.9599, 0.9574, 0.9569, 0.9566,
         0.9564],
        [0.9528, 0.9477, 0.9445, 0.9381, 0.9365, 0.9357, 0.9307, 0.9224, 0.9220,
         0.9205],
        [0.9105, 0.9021, 0.8925, 0.8896, 0.8841, 0.8759, 0.8718, 0.8682, 0.8675,
         0.8649],
        [0.9447, 0.9380, 0.9276, 0.9262, 0.9181, 0.9176, 0.9142, 0.9140, 0.9071,
         0.9023],
        [0.9494, 0.9424, 0.9398, 0.9324, 0.9308, 0.9268, 0.9263, 0.9260, 0.9256,
         0.9244],
        [0.9370, 0.9343, 0.9336, 0.9210, 0.9210, 0.9181, 0.9175, 0.9121, 0.9109,
         0.9081],
        [0.9495, 0.8937, 0.8858, 0.8839, 0.8811, 0.8809, 0.8784, 0.8762, 0.8756,
         0.8735],
        [0.9677, 0.9503, 0.9311, 0.9274, 0.9111, 0.9099, 0.9099, 0.9087, 0.9080,
         0.9036],
        [0.9304, 0.9287, 0.9272, 0.9162, 0.9097, 0.9054, 0.8886, 0.8860, 0.8853,
         0.8844],
        [0.9723, 0.9698, 0.9683, 0.9672, 0.9660, 0.9656, 0.9636, 0.9633, 0.9627,
         0.9626],
        [0.9192, 0.9160, 0.9048, 0.9048, 0.8989, 0.8857, 0.8842, 0.8814, 0.8809,
         0.8803],
        [0.9231, 0.9166, 0.9108, 0.9052, 0.9030, 0.8942, 0.8922, 0.8921, 0.8914,
         0.8913],
        [0.9087, 0.9086, 0.9055, 0.8979, 0.8863, 0.8849, 0.8809, 0.8782, 0.8766,
         0.8741],
        [0.9365, 0.9142, 0.9043, 0.9042, 0.9033, 0.8789, 0.8788, 0.8785, 0.8777,
         0.8765],
        [0.8860, 0.8839, 0.8837, 0.8826, 0.8824, 0.8717, 0.8703, 0.8647, 0.8643,
         0.8632],
        [0.8585, 0.8261, 0.8196, 0.7992, 0.7857, 0.7828, 0.7769, 0.7641, 0.7627,
         0.7586],
        [0.9509, 0.9375, 0.9311, 0.9241, 0.9227, 0.9222, 0.9202, 0.9201, 0.9178,
         0.9159],
        [0.8594, 0.8538, 0.8413, 0.8355, 0.8284, 0.8264, 0.8044, 0.8041, 0.8015,
         0.7983],
        [0.9090, 0.8693, 0.8675, 0.8668, 0.8662, 0.8529, 0.8459, 0.8436, 0.8404,
         0.8377],
        [0.9341, 0.9282, 0.9280, 0.9065, 0.9055, 0.9037, 0.9017, 0.8991, 0.8988,
         0.8987],
        [0.8956, 0.8572, 0.8562, 0.8479, 0.8440, 0.8420, 0.8418, 0.8408, 0.8359,
         0.8358],
        [0.9136, 0.9110, 0.9105, 0.8907, 0.8859, 0.8825, 0.8815, 0.8796, 0.8712,
         0.8709],
        [0.9611, 0.9270, 0.9258, 0.9217, 0.9072, 0.9038, 0.8987, 0.8931, 0.8893,
         0.8885],
        [0.9383, 0.9074, 0.8971, 0.8909, 0.8836, 0.8799, 0.8788, 0.8784, 0.8774,
         0.8749],
        [0.9597, 0.9476, 0.9410, 0.9392, 0.9392, 0.9360, 0.9343, 0.9333, 0.9326,
         0.9298],
        [0.9504, 0.9270, 0.9226, 0.9223, 0.9218, 0.9206, 0.9167, 0.9150, 0.9145,
         0.9145],
        [0.9520, 0.9272, 0.9177, 0.9059, 0.8959, 0.8923, 0.8892, 0.8880, 0.8877,
         0.8832],
        [0.9532, 0.9126, 0.9001, 0.8825, 0.8788, 0.8781, 0.8746, 0.8729, 0.8724,
         0.8709],
        [0.9484, 0.9029, 0.9001, 0.8984, 0.8939, 0.8871, 0.8797, 0.8764, 0.8757,
         0.8659],
        [0.9339, 0.8654, 0.8628, 0.8575, 0.8537, 0.8514, 0.8512, 0.8443, 0.8395,
         0.8290],
        [0.9218, 0.9190, 0.9157, 0.9063, 0.9062, 0.9037, 0.9026, 0.9024, 0.9011,
         0.9000],
        [0.8502, 0.8375, 0.8238, 0.8225, 0.8186, 0.8160, 0.8126, 0.8113, 0.8101,
         0.8043],
        [0.9004, 0.8977, 0.8885, 0.8859, 0.8848, 0.8804, 0.8770, 0.8744, 0.8740,
         0.8726],
        [0.8910, 0.8880, 0.8610, 0.8599, 0.8596, 0.8556, 0.8536, 0.8520, 0.8507,
         0.8444]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 1, 0, 1, 0, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 1, 1, 1, 1, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 1, 0, 1, 1, 1, 1],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 1, 0, 1, 0, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 463260.7188,  274118.9688,  221599.5312,  215168.2812,  190019.0469,
          189209.1094,  186947.8906,  175445.8281,  167153.7812,  164997.4375],
        [1003216.3750,  985428.4375,  958793.3750,  949121.6875,  941656.5000,
          940452.1250,  939115.8750,  920611.8125,  916251.6875,  891324.8750],
        [ 651096.7500,  558823.6250,  499211.7812,  467530.8750,  450052.0312,
          428744.1250,  404879.7812,  393657.2500,  339069.1250,  312475.0312],
        [1037624.6250,  633485.0000,  609501.3125,  601099.5000,  568709.6875,
          558693.6250,  530654.6250,  503858.0000,  466361.0000,  457134.5312],
        [ 260897.7344,  230458.0469,  219988.9375,  208077.4375,  179963.2188,
          151521.6875,  142307.2500,  141466.9375,  137832.3906,  120372.4688],
        [ 425194.2500,  339567.7500,  326390.6875,  318665.0938,  298204.7188,
          293472.8750,  276051.6250,  214466.2188,  204998.8438,  204825.8906],
        [ 777359.8750,  761212.1875,  751440.3125,  561064.8125,  511334.9688,
          506206.4062,  504074.2500,  487318.4062,  485533.3438,  471677.0000],
        [ 751456.7500,  712173.0000,  625735.1875,  426394.2188,  382597.2812,
          372058.0312,  357659.0312,  296730.6562,  279220.1562,  275603.3750],
        [ 834137.3750,  817612.1875,  601415.4375,  597955.9375,  590916.8750,
          583222.1250,  554375.4375,  549655.3125,  548246.5625,  546030.5000],
        [ 932711.7500,  539936.2500,  530223.1250,  468724.1875,  445701.7500,
          441106.5312,  431565.2188,  413492.2500,  377394.8750,  360784.0312],
        [1039855.5000,  779219.9375,  766666.0625,  738760.2500,  724023.7500,
          690958.5625,  666347.0625,  657353.3750,  636588.8125,  620195.8125],
        [ 568602.3125,  555258.0000,  469369.6875,  464147.8438,  363368.9688,
          359246.5625,  310637.0312,  242502.3125,  241970.2812,  167920.0781],
        [1011032.6250,  666512.9375,  629016.3125,  614753.5000,  571490.0625,
          524498.3125,  469755.6875,  442962.2188,  430218.2188,  412031.0312],
        [1163809.1250, 1060793.6250, 1048149.1250, 1043211.7500, 1019919.8750,
         1017322.2500,  993769.3125,  990301.0000,  987784.5000,  986903.1250],
        [ 842529.6250,  797572.3750,  793656.4375,  789237.8750,  771192.7500,
          732798.6250,  726545.6250,  712925.9375,  696645.5000,  692443.4375],
        [ 748566.3750,  673888.6875,  624664.3750,  622688.5000,  600904.0000,
          599473.6250,  578507.3750,  575219.1875,  572043.0000,  565500.3750],
        [1276201.5000, 1120960.1250, 1115491.5000, 1014624.8125,  963092.2500,
          960051.3750,  955728.6875,  948242.3125,  877911.2500,  858119.3750],
        [1147215.6250,  979147.3750,  929480.4375,  909515.1250,  884935.2500,
          871863.8750,  862999.9375,  862722.6875,  845021.6875,  842540.8750],
        [ 914140.4375,  828521.0625,  812816.8750,  810759.1250,  772624.5625,
          619269.6250,  564686.6250,  564122.5625,  541469.2500,  505674.2188],
        [ 715737.4375,  696666.7500,  631484.8750,  628259.7500,  613855.3750,
          602709.6250,  576344.3125,  574072.1875,  553941.5000,  539188.5625],
        [1250356.7500, 1076920.3750, 1009590.2500,  996558.5625,  929592.1875,
          919715.8750,  856907.3750,  852040.8750,  834026.7500,  827323.3125],
        [ 904282.5625,  798608.9375,  762732.3750,  737550.1875,  669065.5000,
          662284.1875,  642449.8125,  631752.9375,  581854.3125,  579337.7500],
        [1184621.3750,  903964.3750,  870420.8125,  845537.6250,  843302.1250,
          825387.7500,  718920.5000,  718061.9375,  678540.9375,  646312.1250],
        [ 902203.1250,  841568.3750,  836339.0000,  800218.3125,  790595.3750,
          783512.8125,  735283.8125,  724446.4375,  695576.0000,  688730.3125],
        [1115558.6250,  733148.8750,  618691.1250,  568761.2500,  536486.5625,
          443283.3750,  439084.4062,  431332.7500,  423102.9688,  421024.4375],
        [ 788287.1250,  785627.2500,  766155.1250,  705427.6250,  686028.1875,
          651028.5000,  650066.8750,  633134.1250,  625516.8125,  621673.8125],
        [ 890617.0625,  819334.0000,  706160.6250,  671072.0000,  664387.6250,
          663682.8125,  627879.3750,  613857.7500,  594638.3125,  579051.6250],
        [ 994336.1875,  819850.6875,  818663.1250,  799194.1250,  760068.9375,
          705445.7500,  686971.6250,  672107.6875,  660064.0625,  656315.4375],
        [ 972944.3125,  873375.1250,  809253.5000,  794861.5000,  787709.9375,
          778838.0625,  756609.0625,  754948.5000,  678183.1875,  664023.3750],
        [ 861183.0625,  839509.0625,  751781.4375,  736065.3750,  734058.3750,
          730097.6875,  702936.8125,  701714.4375,  685669.7500,  677275.7500],
        [1071769.8750,  975781.1875,  957294.8750,  935966.8750,  923895.1250,
          903082.8750,  870854.1875,  865182.9375,  860600.1250,  857974.5000],
        [ 815763.9375,  757719.0000,  724011.3125,  660501.0625,  645818.6250,
          638440.6875,  594678.5625,  527928.3750,  524774.5000,  514374.6562],
        [ 445765.0938,  395470.2188,  344360.7188,  330679.2812,  305708.1875,
          271663.0000,  256287.1719,  243517.1562,  240983.4844,  232346.8594],
        [ 725815.0000,  659628.5625,  569254.5000,  557498.1875,  496487.0000,
          493376.0625,  469862.2812,  468444.0000,  424538.6562,  396136.8125],
        [ 777042.6250,  702868.4375,  677669.1875,  609646.6875,  595106.9375,
          562029.8750,  558367.0625,  556212.0000,  553005.6875,  543285.8750],
        [ 651050.8750,  625742.3750,  619540.8125,  518025.4688,  517398.4375,
          496873.9688,  492524.1875,  455999.8125,  448023.9062,  430356.9062],
        [ 778203.2500,  350531.9375,  312962.3438,  304637.5000,  292698.9062,
          291892.1875,  281804.9062,  272784.2812,  270485.7500,  262829.4062],
        [1008596.1875,  786424.1250,  597718.2500,  567040.5625,  449602.8750,
          442015.6875,  441819.7188,  434334.0938,  430112.3750,  403920.2500],
        [ 592145.0000,  578280.1250,  565832.6875,  483639.5000,  440809.6250,
          414569.0000,  325861.3125,  313938.0312,  310681.7500,  306857.0000],
        [1077170.0000, 1039741.4375, 1016941.0625, 1001192.0625,  983908.1250,
          979147.3750,  951104.1875,  947301.3750,  939320.0625,  937570.4375],
        [ 504779.0000,  482099.5938,  410712.4688,  410655.6875,  377532.4062,
          312732.0312,  305975.3750,  294050.0000,  291948.7188,  289287.2188],
        [ 533446.3125,  486451.0625,  447370.6250,  412961.8125,  400425.0625,
          352805.4688,  342873.9375,  342417.1250,  339289.7188,  338556.9688],
        [ 434138.6250,  433557.7188,  415160.5000,  372443.5625,  315271.6875,
          309289.9688,  292050.6250,  280695.5312,  274369.5312,  264851.9062],
        [ 645622.1250,  469834.5312,  407682.6562,  407388.0625,  401777.2812,
          283560.9688,  283447.9688,  282052.7812,  278760.9375,  274066.4375],
        [ 313930.5625,  304827.5312,  303772.9688,  298940.1875,  298307.4062,
          255824.6719,  250899.0781,  231597.1094,  230228.2656,  226722.2656],
        [ 212098.6562,  133455.6094,  121675.2969,   90807.8516,   74908.8047,
           71927.4688,   66067.9688,   55014.2266,   53935.9062,   50905.9570],
        [ 793066.1875,  655711.1250,  597997.0000,  540958.8125,  530676.3750,
          526477.8125,  511772.0938,  510879.6875,  494564.0938,  481096.0938],
        [ 214614.7656,  198147.0938,  165744.9844,  152708.0469,  137907.3281,
          133991.4844,   97909.6406,   97508.8672,   93911.1797,   89759.2344],
        [ 436249.4688,  247446.1406,  240917.7656,  238569.2500,  236767.0938,
          195666.8594,  177000.5469,  171417.1094,  163670.0156,  157404.2656],
        [ 624110.5625,  574136.2500,  572018.5000,  420800.8438,  414724.4062,
          404186.9062,  392676.7500,  378680.5000,  376897.0938,  376384.5000],
        [ 360386.8438,  208192.9531,  204993.9531,  182116.6875,  172299.5312,
          167498.7812,  166921.5312,  164508.6406,  153469.8438,  153257.4688],
        [ 465442.1562,  448522.3438,  445434.0625,  335870.0938,  313689.0625,
          298863.8125,  294346.8438,  286498.8438,  254207.8750,  253115.8750],
        [ 917675.3750,  563912.1875,  554101.1250,  522778.4375,  425035.7188,
          405069.0312,  376327.4375,  347676.7500,  329357.4062,  325294.3750],
        [ 662870.5625,  426571.5312,  368197.8438,  336846.8438,  303308.0625,
          287668.4375,  283245.3125,  281733.6875,  277710.9375,  268063.0938],
        [ 900372.3125,  756716.6250,  688448.5625,  671591.8750,  671591.8750,
          641693.0000,  626088.0000,  617060.6875,  611398.6250,  587167.1875],
        [ 788466.7500,  563840.6875,  529845.5000,  527686.2500,  523928.8750,
          514839.9375,  486784.7188,  475006.6250,  471876.7812,  471807.5000],
        [ 805960.9375,  566009.1875,  493835.4688,  417413.4375,  361573.1562,
          343707.4375,  328903.8750,  323156.8438,  321760.0938,  301601.6250],
        [ 820147.8750,  459069.0312,  383851.5938,  298701.6875,  283338.5000,
          280637.9688,  266956.6562,  260450.0156,  258649.0156,  253218.0156],
        [ 765674.5000,  399545.0625,  384273.1562,  375007.2500,  351658.6562,
          318904.0625,  286904.3125,  273905.7500,  271178.9688,  235492.7344],
        [ 622424.3125,  233884.3906,  225442.3906,  208994.2500,  197840.4688,
          191586.1094,  190936.9375,  173027.3438,  161566.0156,  139091.5938],
        [ 523615.7188,  503071.0312,  479708.3438,  419802.7812,  419055.1562,
          404077.4375,  398244.4062,  397119.1250,  389353.5000,  383279.0938],
        [ 188325.0156,  156979.8750,  129171.1094,  126824.4141,  119967.3516,
          115581.9297,  109979.5156,  108013.4062,  106227.3281,   97765.4844],
        [ 385441.4062,  371270.7812,  325525.2500,  313655.2500,  308785.6875,
          289651.8750,  276066.1250,  265920.1875,  264497.2812,  259256.5156],
        [ 337184.3125,  323311.5938,  219624.8125,  216146.4375,  215351.0000,
          203327.7031,  197781.2188,  193113.7812,  189540.6875,  173419.5312]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[ 463260.7188,       0.0000],
         [ 274118.9688,       0.0000],
         [ 221599.5312,       0.0000],
         ...,
         [ 175445.8281,       0.0000],
         [ 167153.7812,       0.0000],
         [ 164997.4375,       0.0000]],

        [[1003216.3750,       0.0000],
         [ 985428.4375,       0.0000],
         [ 958793.3750,       0.0000],
         ...,
         [ 920611.8125,       0.0000],
         [ 916251.6875,       0.0000],
         [ 891324.8750,       0.0000]],

        [[ 651096.7500,       0.0000],
         [ 558823.6250,       0.0000],
         [ 499211.7812,       0.0000],
         ...,
         [ 393657.2500,       0.0000],
         [ 339069.1250,       0.0000],
         [      0.0000,  312475.0312]],

        ...,

        [[      0.0000,  188325.0156],
         [ 156979.8750,       0.0000],
         [ 129171.1094,       0.0000],
         ...,
         [ 108013.4062,       0.0000],
         [ 106227.3281,       0.0000],
         [  97765.4844,       0.0000]],

        [[      0.0000,  385441.4062],
         [ 371270.7812,       0.0000],
         [ 325525.2500,       0.0000],
         ...,
         [ 265920.1875,       0.0000],
         [      0.0000,  264497.2812],
         [ 259256.5156,       0.0000]],

        [[      0.0000,  337184.3125],
         [      0.0000,  323311.5938],
         [ 219624.8125,       0.0000],
         ...,
         [      0.0000,  193113.7812],
         [ 189540.6875,       0.0000],
         [ 173419.5312,       0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2058711.3750,   189209.1094],
        [ 9445972.0000,        0.0000],
        [ 4193065.0000,   312475.0312],
        [ 5967122.0000,        0.0000],
        [  787712.5000,  1005173.6875],
        [ 2057178.6250,   844659.3750],
        [ 4009411.2500,  1807810.2500],
        [ 4479627.5000,        0.0000],
        [ 5405956.0000,   817612.1875],
        [ 4941640.0000,        0.0000],
        [ 7319968.5000,        0.0000],
        [ 3743023.0000,        0.0000],
        [ 5772271.0000,        0.0000],
        [10311964.0000,        0.0000],
        [ 7555548.0000,        0.0000],
        [ 6161456.0000,        0.0000],
        [10090423.0000,        0.0000],
        [ 9135443.0000,        0.0000],
        [ 6934085.0000,        0.0000],
        [ 6132260.5000,        0.0000],
        [ 9553032.0000,        0.0000],
        [ 6969918.0000,        0.0000],
        [ 8235069.5000,        0.0000],
        [ 7798474.0000,        0.0000],
        [ 5730474.5000,        0.0000],
        [ 6912945.0000,        0.0000],
        [ 6830681.0000,        0.0000],
        [ 7573018.0000,        0.0000],
        [ 7870746.5000,        0.0000],
        [ 7420292.0000,        0.0000],
        [ 9222402.0000,        0.0000],
        [ 6404011.0000,        0.0000],
        [ 3066781.2500,        0.0000],
        [ 3611549.5000,  1649491.6250],
        [ 4085415.5000,  2049818.7500],
        [ 3522743.7500,  1732793.0000],
        [ 2064835.3750,  1353995.0000],
        [ 3168845.5000,  2392738.5000],
        [ 1923801.7500,  2408812.2500],
        [ 9873396.0000,        0.0000],
        [ 3679772.7500,        0.0000],
        [ 3996598.0000,        0.0000],
        [ 3391829.5000,        0.0000],
        [ 3734194.0000,        0.0000],
        [ 1327014.7500,  1388035.2500],
        [  463568.1875,   467229.5625],
        [ 5045202.0000,   597997.0000],
        [ 1032787.1250,   349415.3750],
        [  800561.3125,  1464547.1250],
        [ 2915568.5000,  1619047.6250],
        [ 1571983.5000,   361662.8125],
        [ 2195527.5000,  1200463.3750],
        [ 2399624.0000,  2367604.0000],
        [ 2801581.5000,   694634.6250],
        [ 6772128.5000,        0.0000],
        [ 4826397.5000,   527686.2500],
        [ 1879714.7500,  2384207.5000],
        [ 2468881.7500,  1096138.6250],
        [ 2005925.0000,  1656619.3750],
        [ 1099058.5000,  1245735.2500],
        [ 3523895.5000,   793430.9375],
        [ 1070510.5000,   188325.0156],
        [ 2120479.7500,   939590.5625],
        [  996512.6875,  1272288.3750]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 59.375
Top1 accuracy for validation set is 59.375 size is torch.Size([64, 1])
Epoch 206/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:59, 57.93s/it]  7%|▋         | 2/30 [00:58<11:20, 24.29s/it] 10%|█         | 3/30 [00:59<06:05, 13.54s/it] 13%|█▎        | 4/30 [01:00<03:40,  8.49s/it] 17%|█▋        | 5/30 [01:00<02:22,  5.70s/it] 20%|██        | 6/30 [01:01<01:36,  4.02s/it] 23%|██▎       | 7/30 [01:02<01:07,  2.95s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.25s/it] 30%|███       | 9/30 [01:03<00:37,  1.78s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.46s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.24s/it] 40%|████      | 12/30 [01:06<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.66s/it]
Epoch loss is 2.977369435628255
Epoch 207/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:29, 56.89s/it]  7%|▋         | 2/30 [00:59<11:35, 24.82s/it] 10%|█         | 3/30 [01:00<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:01<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.93s/it] 20%|██        | 6/30 [01:02<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:03<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.70s/it]
Epoch loss is 2.9404724915822347
Epoch 208/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:14, 62.58s/it]  7%|▋         | 2/30 [01:03<12:13, 26.21s/it] 10%|█         | 3/30 [01:04<06:33, 14.58s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.12s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.10s/it] 20%|██        | 6/30 [01:06<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:07<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.32it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 2.955597766240438
Epoch 209/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:14, 56.36s/it]  7%|▋         | 2/30 [00:57<11:10, 23.94s/it] 10%|█         | 3/30 [01:00<06:32, 14.53s/it] 13%|█▎        | 4/30 [01:01<03:56,  9.09s/it] 17%|█▋        | 5/30 [01:02<02:32,  6.08s/it] 20%|██        | 6/30 [01:03<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:03<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:04<00:52,  2.36s/it] 30%|███       | 9/30 [01:05<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.28s/it] 40%|████      | 12/30 [01:07<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.07it/s] 50%|█████     | 15/30 [01:09<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.9683407068252565
Epoch 210/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:27, 56.79s/it]  7%|▋         | 2/30 [01:00<11:52, 25.45s/it] 10%|█         | 3/30 [01:01<06:22, 14.17s/it] 13%|█▎        | 4/30 [01:01<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.94s/it] 20%|██        | 6/30 [01:03<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.917013931274414
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0233,  0.0089, -0.0091,  ...,  0.0077, -0.0144,  0.0223],
        [-0.0055,  0.0252,  0.0096,  ...,  0.0085, -0.0193, -0.0102],
        [-0.0450, -0.0316,  0.0078,  ...,  0.0705, -0.0327, -0.0313],
        ...,
        [ 0.0302, -0.0203, -0.0172,  ..., -0.0255, -0.0364, -0.0138],
        [-0.0301, -0.0046, -0.0125,  ..., -0.0086,  0.0112, -0.0153],
        [-0.0357, -0.0056,  0.0135,  ...,  0.0129,  0.0172, -0.0551]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9143, 0.8713, 0.8628, 0.8616, 0.8524, 0.8514, 0.8513, 0.8454, 0.8425,
         0.8416],
        [0.9662, 0.9628, 0.9617, 0.9608, 0.9603, 0.9599, 0.9597, 0.9588, 0.9587,
         0.9552],
        [0.9356, 0.9252, 0.9172, 0.9100, 0.9087, 0.9018, 0.9007, 0.8988, 0.8881,
         0.8835],
        [0.9679, 0.9320, 0.9304, 0.9296, 0.9265, 0.9262, 0.9194, 0.9160, 0.9121,
         0.9107],
        [0.8728, 0.8629, 0.8587, 0.8528, 0.8444, 0.8297, 0.8297, 0.8281, 0.8202,
         0.8155],
        [0.9019, 0.8852, 0.8828, 0.8813, 0.8784, 0.8725, 0.8720, 0.8524, 0.8506,
         0.8500],
        [0.9461, 0.9455, 0.9437, 0.9227, 0.9144, 0.9144, 0.9136, 0.9129, 0.9119,
         0.9102],
        [0.9452, 0.9398, 0.9318, 0.9040, 0.8958, 0.8933, 0.8931, 0.8776, 0.8756,
         0.8740],
        [0.9515, 0.9506, 0.9293, 0.9282, 0.9272, 0.9270, 0.9238, 0.9229, 0.9229,
         0.9227],
        [0.9611, 0.9224, 0.9215, 0.9124, 0.9097, 0.9094, 0.9070, 0.9029, 0.8982,
         0.8956],
        [0.9696, 0.9477, 0.9464, 0.9427, 0.9416, 0.9380, 0.9353, 0.9332, 0.9329,
         0.9321],
        [0.9268, 0.9242, 0.9102, 0.9095, 0.8914, 0.8909, 0.8832, 0.8636, 0.8632,
         0.8389],
        [0.9665, 0.9362, 0.9334, 0.9315, 0.9282, 0.9218, 0.9124, 0.9087, 0.9075,
         0.9055],
        [0.9774, 0.9702, 0.9695, 0.9683, 0.9679, 0.9668, 0.9660, 0.9658, 0.9649,
         0.9649],
        [0.9545, 0.9492, 0.9492, 0.9491, 0.9477, 0.9450, 0.9444, 0.9432, 0.9414,
         0.9407],
        [0.9445, 0.9379, 0.9336, 0.9320, 0.9302, 0.9294, 0.9280, 0.9274, 0.9269,
         0.9268],
        [0.9835, 0.9742, 0.9737, 0.9682, 0.9639, 0.9629, 0.9628, 0.9619, 0.9566,
         0.9556],
        [0.9753, 0.9648, 0.9599, 0.9592, 0.9580, 0.9566, 0.9562, 0.9551, 0.9549,
         0.9540],
        [0.9587, 0.9532, 0.9498, 0.9491, 0.9485, 0.9312, 0.9258, 0.9253, 0.9222,
         0.9190],
        [0.9418, 0.9373, 0.9342, 0.9305, 0.9296, 0.9294, 0.9260, 0.9258, 0.9250,
         0.9223],
        [0.9814, 0.9707, 0.9671, 0.9662, 0.9605, 0.9601, 0.9549, 0.9547, 0.9543,
         0.9535],
        [0.9579, 0.9494, 0.9468, 0.9446, 0.9376, 0.9364, 0.9349, 0.9345, 0.9280,
         0.9278],
        [0.9784, 0.9596, 0.9570, 0.9553, 0.9549, 0.9534, 0.9434, 0.9424, 0.9389,
         0.9359],
        [0.9584, 0.9549, 0.9540, 0.9501, 0.9497, 0.9480, 0.9440, 0.9433, 0.9398,
         0.9393],
        [0.9741, 0.9414, 0.9306, 0.9258, 0.9231, 0.9080, 0.9078, 0.9055, 0.9052,
         0.9044],
        [0.9503, 0.9501, 0.9471, 0.9410, 0.9393, 0.9373, 0.9370, 0.9334, 0.9333,
         0.9318],
        [0.9582, 0.9528, 0.9423, 0.9397, 0.9376, 0.9359, 0.9322, 0.9319, 0.9296,
         0.9281],
        [0.9664, 0.9542, 0.9528, 0.9521, 0.9476, 0.9415, 0.9407, 0.9390, 0.9374,
         0.9374],
        [0.9643, 0.9572, 0.9511, 0.9492, 0.9489, 0.9477, 0.9466, 0.9461, 0.9395,
         0.9373],
        [0.9559, 0.9548, 0.9463, 0.9441, 0.9437, 0.9423, 0.9411, 0.9399, 0.9398,
         0.9394],
        [0.9720, 0.9645, 0.9637, 0.9610, 0.9604, 0.9593, 0.9572, 0.9568, 0.9552,
         0.9545],
        [0.9522, 0.9458, 0.9442, 0.9364, 0.9350, 0.9338, 0.9289, 0.9218, 0.9197,
         0.9178],
        [0.9106, 0.9016, 0.8905, 0.8893, 0.8832, 0.8759, 0.8726, 0.8683, 0.8655,
         0.8642],
        [0.9429, 0.9359, 0.9267, 0.9229, 0.9168, 0.9159, 0.9127, 0.9125, 0.9062,
         0.9009],
        [0.9481, 0.9411, 0.9392, 0.9323, 0.9305, 0.9258, 0.9251, 0.9241, 0.9236,
         0.9229],
        [0.9343, 0.9336, 0.9333, 0.9191, 0.9171, 0.9163, 0.9159, 0.9124, 0.9091,
         0.9055],
        [0.9494, 0.8916, 0.8843, 0.8802, 0.8780, 0.8764, 0.8751, 0.8746, 0.8723,
         0.8710],
        [0.9661, 0.9485, 0.9296, 0.9243, 0.9097, 0.9081, 0.9061, 0.9058, 0.9056,
         0.9011],
        [0.9279, 0.9264, 0.9242, 0.9158, 0.9106, 0.9069, 0.8878, 0.8863, 0.8842,
         0.8832],
        [0.9715, 0.9675, 0.9669, 0.9660, 0.9648, 0.9643, 0.9623, 0.9622, 0.9621,
         0.9611],
        [0.9167, 0.9130, 0.9046, 0.9045, 0.8969, 0.8852, 0.8812, 0.8799, 0.8795,
         0.8785],
        [0.9232, 0.9127, 0.9086, 0.9034, 0.9005, 0.8932, 0.8906, 0.8901, 0.8900,
         0.8887],
        [0.9078, 0.9075, 0.9057, 0.8981, 0.8842, 0.8840, 0.8809, 0.8789, 0.8737,
         0.8735],
        [0.9344, 0.9144, 0.9032, 0.9016, 0.9000, 0.8778, 0.8774, 0.8771, 0.8766,
         0.8760],
        [0.8839, 0.8821, 0.8813, 0.8806, 0.8793, 0.8696, 0.8666, 0.8633, 0.8604,
         0.8597],
        [0.8557, 0.8277, 0.8199, 0.7999, 0.7855, 0.7843, 0.7799, 0.7621, 0.7598,
         0.7597],
        [0.9495, 0.9364, 0.9307, 0.9228, 0.9214, 0.9212, 0.9200, 0.9199, 0.9166,
         0.9137],
        [0.8577, 0.8509, 0.8378, 0.8323, 0.8271, 0.8258, 0.8034, 0.8031, 0.7968,
         0.7955],
        [0.9085, 0.8679, 0.8625, 0.8623, 0.8606, 0.8470, 0.8426, 0.8389, 0.8368,
         0.8341],
        [0.9316, 0.9264, 0.9246, 0.9037, 0.8994, 0.8980, 0.8975, 0.8950, 0.8943,
         0.8943],
        [0.8969, 0.8533, 0.8533, 0.8463, 0.8433, 0.8427, 0.8364, 0.8363, 0.8348,
         0.8333],
        [0.9130, 0.9102, 0.9062, 0.8885, 0.8801, 0.8799, 0.8784, 0.8771, 0.8693,
         0.8640],
        [0.9600, 0.9270, 0.9269, 0.9196, 0.9077, 0.9006, 0.8982, 0.8883, 0.8876,
         0.8870],
        [0.9380, 0.9052, 0.8935, 0.8891, 0.8838, 0.8791, 0.8770, 0.8767, 0.8761,
         0.8740],
        [0.9592, 0.9452, 0.9391, 0.9369, 0.9369, 0.9361, 0.9341, 0.9317, 0.9315,
         0.9277],
        [0.9481, 0.9248, 0.9206, 0.9201, 0.9189, 0.9175, 0.9140, 0.9127, 0.9116,
         0.9115],
        [0.9508, 0.9260, 0.9150, 0.9049, 0.8927, 0.8893, 0.8870, 0.8866, 0.8851,
         0.8809],
        [0.9526, 0.9116, 0.9000, 0.8825, 0.8771, 0.8770, 0.8737, 0.8725, 0.8717,
         0.8706],
        [0.9469, 0.9020, 0.8978, 0.8967, 0.8923, 0.8823, 0.8786, 0.8764, 0.8701,
         0.8653],
        [0.9323, 0.8633, 0.8618, 0.8557, 0.8502, 0.8498, 0.8484, 0.8434, 0.8370,
         0.8267],
        [0.9202, 0.9176, 0.9142, 0.9053, 0.9048, 0.9034, 0.9011, 0.9005, 0.8991,
         0.8986],
        [0.8515, 0.8320, 0.8214, 0.8186, 0.8135, 0.8118, 0.8092, 0.8090, 0.8070,
         0.8015],
        [0.8983, 0.8969, 0.8871, 0.8864, 0.8817, 0.8798, 0.8733, 0.8730, 0.8704,
         0.8677],
        [0.8869, 0.8847, 0.8560, 0.8551, 0.8537, 0.8528, 0.8493, 0.8472, 0.8467,
         0.8405]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 1, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 1, 0, 1, 1, 1, 0, 0],
        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 1, 1, 1, 1],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 1, 0, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 0, 1, 0, 1, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 470213.3125,  254389.0312,  225569.5000,  221731.4531,  194291.9844,
          191442.9062,  191256.7812,  175776.0938,  168744.5625,  166516.4062],
        [ 986954.9375,  940654.8750,  925806.4375,  913985.3125,  907837.3750,
          902232.3750,  899307.3125,  888672.5000,  887176.1875,  843242.6250],
        [ 637865.0000,  549731.3750,  490150.4062,  442471.5938,  434311.3125,
          393312.7812,  387532.0000,  377076.1250,  323512.0625,  303189.4688],
        [1012407.5625,  605598.5000,  592427.3750,  585352.8750,  560222.1875,
          557902.9375,  506295.7812,  482019.6250,  455749.8438,  446891.3750],
        [ 260092.8125,  225690.4375,  212501.5469,  195361.0781,  173398.3594,
          140551.8281,  140528.6406,  137300.5312,  122696.7500,  114734.3203],
        [ 393933.2812,  310231.1562,  299941.9688,  293688.1875,  281749.2812,
          258859.5000,  257194.3281,  194321.0781,  189370.4844,  187761.7344],
        [ 741110.0625,  734879.3125,  715982.5000,  530597.9375,  471364.0312,
          471305.5938,  465812.5000,  460804.2188,  454746.0625,  443904.4062],
        [ 731822.3125,  677645.9375,  603648.3750,  405954.6250,  361430.0625,
          348266.7812,  347755.6562,  278658.3125,  270557.4375,  264558.8125],
        [ 800116.1250,  789897.5625,  582994.6250,  573796.3125,  565417.8750,
          564177.9375,  538806.6250,  532303.9375,  532207.4375,  530669.2500],
        [ 917796.1875,  528524.8125,  521381.8438,  457761.4375,  440744.5000,
          438833.6250,  424116.5938,  399832.8438,  374037.5000,  360077.6562],
        [1036503.0625,  757901.0625,  744318.6875,  706100.0625,  694596.3125,
          659784.0000,  635478.8125,  616502.5000,  613655.1875,  606893.5625],
        [ 562227.6875,  541587.0625,  443973.8438,  439413.2500,  339051.9688,
          336790.9688,  301751.8125,  227941.9844,  226610.9375,  160177.8906],
        [ 991798.0000,  643615.0000,  618143.2500,  601204.3750,  573603.6875,
          523910.9062,  457913.8125,  434361.4062,  427166.6562,  414712.9688],
        [1159520.5000, 1044835.7500, 1035148.6875, 1017259.2500, 1012155.5625,
          996133.8750,  984314.4375,  981639.0625,  969290.6875,  969135.4375],
        [ 835526.6875,  774994.8750,  774220.6875,  773561.6250,  757620.6250,
          729907.6250,  722991.5000,  710937.9375,  693172.8750,  685611.5000],
        [ 724305.5000,  659089.6875,  619783.0625,  606084.9375,  590208.3750,
          583328.3125,  572478.5000,  567615.1250,  562957.8750,  562289.3125],
        [1264497.2500, 1107559.2500, 1099682.5000, 1015916.5000,  955287.6250,
          941529.0000,  940764.3125,  928094.2500,  861406.5000,  849238.0625],
        [1125010.5000,  968050.9375,  902797.0000,  893726.0000,  877850.9375,
          861497.6250,  856465.3750,  842531.1875,  839689.2500,  829926.3125],
        [ 887557.0000,  819681.8125,  780989.0625,  773248.9375,  767178.7500,
          598861.0625,  554692.1875,  550871.2500,  526401.0000,  503467.9375],
        [ 696991.0625,  653541.0000,  624800.1875,  593289.5625,  585040.3125,
          583734.5625,  556279.3750,  554801.6875,  548248.6875,  527776.3125],
        [1227031.2500, 1053261.5000, 1000813.0625,  988021.8750,  910469.6875,
          904778.5625,  839667.6250,  837969.2500,  832513.0000,  823400.1875],
        [ 876691.4375,  776850.0000,  748479.3125,  725469.0000,  656289.8125,
          645269.4375,  631696.3125,  627672.2500,  572420.6875,  570361.3750],
        [1174911.8750,  899103.2500,  866131.5000,  845127.2500,  840369.3125,
          821947.1875,  713366.6250,  702600.3125,  668701.8750,  640332.1875],
        [ 882934.8125,  840142.5625,  829533.8750,  785019.8750,  780478.3125,
          760836.1875,  718580.5000,  712132.9375,  676936.6875,  672747.6250],
        [1105363.3750,  692918.3750,  594143.4375,  554253.8125,  533171.6875,
          429837.2188,  428822.2188,  414615.6562,  413053.9688,  408170.5312],
        [ 786619.1875,  784272.3750,  751906.2500,  689115.3125,  672105.6875,
          653847.0625,  650772.1250,  617633.5625,  616908.8750,  603849.8750],
        [ 880657.5000,  814855.8125,  701826.8750,  676171.5000,  655946.2500,
          640050.1250,  607395.5625,  605336.3125,  585706.3125,  573083.7500],
        [ 990179.1875,  831290.3750,  815576.5000,  806730.0000,  757232.7500,
          694044.0625,  685469.0000,  669521.1875,  654826.1875,  654818.6875],
        [ 960873.0000,  868182.4375,  796026.6875,  774254.6875,  771355.3750,
          758087.5625,  746442.6875,  740527.1875,  674480.2500,  653589.0000],
        [ 851762.1875,  838880.8125,  742794.8125,  720133.6875,  715487.6250,
          701969.4375,  689693.8750,  678590.7500,  677324.1250,  673528.2500],
        [1072618.5000,  964370.6875,  952814.6250,  916807.6875,  909189.9375,
          894520.7500,  868865.0000,  863872.8125,  843317.4375,  835930.7500],
        [ 808253.8750,  738189.0625,  721366.1250,  645184.5625,  632702.5625,
          621262.5625,  579595.2500,  523653.6562,  507865.0000,  494714.1562],
        [ 446162.7500,  392354.8125,  334909.2500,  329233.3750,  301778.8750,
          271790.7812,  259428.9219,  243793.6719,  234179.2344,  230026.3594],
        [ 707949.5625,  640879.0000,  561252.1250,  531776.2500,  487665.7188,
          481003.3750,  459905.9688,  458194.2812,  419021.1875,  388418.9062],
        [ 762410.8750,  690205.8125,  671803.3125,  608805.3750,  593209.8125,
          554047.1875,  549208.3750,  541450.1875,  537654.3125,  531979.1250],
        [ 625800.2500,  620230.6875,  617362.6875,  503644.1875,  489777.0312,
          483965.7188,  481145.6250,  457537.0938,  436592.8438,  415038.5625],
        [ 777036.7500,  339928.3750,  306252.1250,  289037.0938,  280044.4375,
          273885.3438,  268742.9375,  266884.5938,  258324.3594,  253571.7969],
        [ 985424.7500,  766723.8125,  585706.3125,  542728.6875,  440483.9688,
          430431.5938,  418703.6562,  416564.0625,  415274.5312,  389371.3125],
        [ 570935.0000,  559507.7500,  541819.5000,  480520.5938,  445944.0938,
          423161.8750,  322160.7812,  315386.8750,  306202.5000,  301728.2188],
        [1065560.1250, 1006313.3125,  997807.2500,  984518.2500,  967311.8125,
          960735.6250,  934568.1875,  931970.2500,  931176.8750,  918257.5625],
        [ 486772.1562,  461750.9062,  409507.0625,  408778.1875,  366839.3438,
          310404.8750,  293311.1250,  287632.2500,  286063.9375,  282287.9688],
        [ 534173.8125,  459936.6875,  433387.3750,  402581.9688,  386270.1250,
          347942.4375,  335130.0000,  333120.0000,  332443.0625,  326507.4375],
        [ 428459.6250,  426663.0625,  416276.9062,  373337.5938,  305882.9062,
          305027.9062,  292129.7188,  283775.7812,  263385.6875,  262710.3750],
        [ 626902.9375,  471199.0938,  401390.8438,  392230.2500,  383727.8750,
          279267.3125,  277746.1562,  276644.0938,  274394.1250,  272267.8750],
        [ 304607.8750,  296790.6562,  293550.9688,  290691.2812,  285425.1562,
          248427.3906,  238009.0781,  227068.0312,  217675.1406,  215655.5781],
        [ 203680.5312,  136468.5781,  122216.7656,   91737.0391,   74667.8047,
           73473.7344,   68964.8828,   53472.7070,   51774.1719,   51712.3438],
        [ 777452.5625,  644823.4375,  594530.0000,  531491.2500,  520486.5938,
          519455.1875,  510315.8125,  509425.4688,  485909.0000,  466437.9688],
        [ 209528.4844,  190190.1875,  157670.0469,  145804.6875,  135421.1562,
          132849.8906,   96483.3906,   96050.4297,   87840.8281,   86169.1953],
        [ 432943.7188,  242450.0625,  224557.2656,  223722.3438,  218347.7344,
          179775.7188,  168841.4688,  160205.5469,  155412.3750,  149507.5312],
        [ 602369.4375,  558824.6875,  545100.2500,  404242.4062,  380242.7500,
          372705.4375,  369862.5000,  357300.4062,  353578.5000,  353572.4375],
        [ 366940.8125,  196804.3281,  196779.3750,  178126.6406,  170727.6562,
          169079.6406,  154661.4375,  154429.2969,  151041.6719,  147831.2500],
        [ 461946.4688,  443410.2500,  419061.9688,  325405.7812,  288750.0000,
          287860.8125,  281556.9375,  276437.5938,  247266.6094,  229274.7188],
        [ 903647.1875,  563843.9375,  563600.8750,  507775.8750,  427832.8438,
          386950.7188,  373796.4375,  324561.5312,  321343.0312,  318377.7500],
        [ 660371.2500,  413352.6875,  349356.8750,  328263.6562,  304159.6562,
          284366.3750,  276310.2812,  274981.4375,  272463.1875,  264363.8438],
        [ 892998.4375,  731269.0625,  670587.0625,  650163.6250,  650163.6250,
          641998.4375,  624131.4375,  603525.2500,  601369.5625,  569690.0625],
        [ 762362.8750,  546750.1250,  514561.0938,  511353.4688,  502483.1562,
          492762.8438,  468478.3750,  459786.7188,  452364.4688,  451673.9062],
        [ 791876.6875,  555830.7500,  475194.1875,  411557.8438,  345673.5625,
          329071.4062,  318419.9375,  316688.2812,  309898.7500,  291821.5000],
        [ 813502.3750,  452510.3125,  383512.3750,  298869.5312,  276482.9375,
          276257.3125,  263443.4688,  258954.0781,  256142.5156,  251901.7188],
        [ 749606.5000,  394544.2500,  371550.6250,  365717.6875,  343567.2188,
          297664.8438,  282498.3125,  273693.7188,  250178.6875,  233649.8594],
        [ 608144.4375,  226955.2500,  222117.2656,  203791.8594,  188166.6875,
          187191.5625,  183475.2188,  170809.8906,  155977.8125,  134613.4375],
        [ 512176.3438,  493051.9375,  469886.5312,  413624.7812,  410639.2500,
          402687.1562,  389786.3438,  386410.8750,  378457.0312,  375725.3438],
        [ 191812.6250,  145221.1562,  124852.5859,  119913.3594,  111466.3594,
          108813.6562,  104871.7500,  104515.7109,  101628.0391,   93896.8594],
        [ 374382.6250,  366776.3750,  318964.2812,  315957.3438,  295375.4688,
          287196.4062,  262062.5312,  260743.0156,  251342.3594,  241735.0156],
        [ 318224.7500,  308320.1562,  204493.5156,  201998.5469,  197839.6875,
          195493.7812,  185770.2656,  180503.9531,  179005.8750,  163981.7031]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[470213.3125,      0.0000],
         [254389.0312,      0.0000],
         [225569.5000,      0.0000],
         ...,
         [175776.0938,      0.0000],
         [168744.5625,      0.0000],
         [166516.4062,      0.0000]],

        [[986954.9375,      0.0000],
         [940654.8750,      0.0000],
         [925806.4375,      0.0000],
         ...,
         [888672.5000,      0.0000],
         [887176.1875,      0.0000],
         [843242.6250,      0.0000]],

        [[637865.0000,      0.0000],
         [549731.3750,      0.0000],
         [490150.4062,      0.0000],
         ...,
         [377076.1250,      0.0000],
         [323512.0625,      0.0000],
         [     0.0000, 303189.4688]],

        ...,

        [[     0.0000, 191812.6250],
         [145221.1562,      0.0000],
         [124852.5859,      0.0000],
         ...,
         [104515.7109,      0.0000],
         [101628.0391,      0.0000],
         [ 93896.8594,      0.0000]],

        [[374382.6250,      0.0000],
         [     0.0000, 366776.3750],
         [318964.2812,      0.0000],
         ...,
         [260743.0156,      0.0000],
         [     0.0000, 251342.3594],
         [241735.0156,      0.0000]],

        [[     0.0000, 318224.7500],
         [     0.0000, 308320.1562],
         [204493.5156,      0.0000],
         ...,
         [180503.9531,      0.0000],
         [179005.8750,      0.0000],
         [163981.7031,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2068675.2500,   191256.7812],
        [ 9095870.0000,        0.0000],
        [ 4035962.7500,   303189.4688],
        [ 5804868.0000,        0.0000],
        [  746742.3125,   976114.0000],
        [ 1889426.2500,   777624.8125],
        [ 3764052.5000,  1726454.0000],
        [ 4290298.0000,        0.0000],
        [ 5220490.0000,   789897.5625],
        [ 4863107.0000,        0.0000],
        [ 7071733.0000,        0.0000],
        [ 3579527.2500,        0.0000],
        [ 5686430.0000,        0.0000],
        [10169433.0000,        0.0000],
        [ 7458545.5000,        0.0000],
        [ 6048140.5000,        0.0000],
        [ 9963976.0000,        0.0000],
        [ 8997545.0000,        0.0000],
        [ 6762949.0000,        0.0000],
        [ 5924502.0000,        0.0000],
        [ 9417926.0000,        0.0000],
        [ 6831200.0000,        0.0000],
        [ 8172591.5000,        0.0000],
        [ 7659343.0000,        0.0000],
        [ 5574350.0000,        0.0000],
        [ 6827030.5000,        0.0000],
        [ 6741030.0000,        0.0000],
        [ 7559688.0000,        0.0000],
        [ 7743819.0000,        0.0000],
        [ 7290165.5000,        0.0000],
        [ 9122308.0000,        0.0000],
        [ 6272787.0000,        0.0000],
        [ 3043658.0000,        0.0000],
        [ 3526518.2500,  1609548.1250],
        [ 4013350.5000,  2027424.0000],
        [ 3427526.7500,  1703568.1250],
        [ 1976847.2500,  1336860.7500],
        [ 3053557.7500,  2337854.7500],
        [ 1895072.8750,  2372294.5000],
        [ 9698220.0000,        0.0000],
        [ 3593348.0000,        0.0000],
        [ 3891493.0000,        0.0000],
        [ 3357649.5000,        0.0000],
        [ 3655770.5000,        0.0000],
        [ 1253555.6250,  1364345.5000],
        [  465802.6875,   462365.8750],
        [ 4965797.0000,   594530.0000],
        [ 1006328.1875,   331680.0625],
        [  747824.9375,  1407938.8750],
        [ 2746086.7500,  1551712.1250],
        [ 1541786.5000,   344635.5625],
        [ 2074057.5000,  1186913.7500],
        [ 2362217.5000,  2329512.5000],
        [ 3014636.5000,   413352.6875],
        [ 6635896.5000,        0.0000],
        [ 4651223.5000,   511353.4688],
        [ 1840935.3750,  2305097.5000],
        [ 2439411.5000,  1092165.2500],
        [ 1957686.1250,  1604985.6250],
        [ 1070785.5000,  1210458.0000],
        [ 3443349.5000,   789096.2500],
        [ 1015179.5625,   191812.6250],
        [ 2069220.3750,   905315.1250],
        [  925824.6875,  1209807.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 57.8125
Top1 accuracy for validation set is 57.8125 size is torch.Size([64, 1])
Epoch 211/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:28, 58.90s/it]  7%|▋         | 2/30 [01:00<11:40, 25.00s/it] 10%|█         | 3/30 [01:03<06:45, 15.02s/it] 13%|█▎        | 4/30 [01:04<04:04,  9.39s/it] 17%|█▋        | 5/30 [01:04<02:36,  6.27s/it] 20%|██        | 6/30 [01:05<01:45,  4.39s/it] 23%|██▎       | 7/30 [01:06<01:13,  3.20s/it] 27%|██▋       | 8/30 [01:07<00:53,  2.42s/it] 30%|███       | 9/30 [01:07<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.30s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.9006619294484457
Epoch 212/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:54, 59.80s/it]  7%|▋         | 2/30 [01:00<11:41, 25.06s/it] 10%|█         | 3/30 [01:01<06:17, 13.96s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.75s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.917776107788086
Epoch 213/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:36, 57.12s/it]  7%|▋         | 2/30 [00:58<11:17, 24.21s/it] 10%|█         | 3/30 [01:00<06:16, 13.93s/it] 13%|█▎        | 4/30 [01:00<03:46,  8.73s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.85s/it] 20%|██        | 6/30 [01:02<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.30s/it] 30%|███       | 9/30 [01:04<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.29it/s] 70%|███████   | 21/30 [01:13<00:06,  1.30it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 2.9123213291168213
Epoch 214/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:31, 59.01s/it]  7%|▋         | 2/30 [01:00<11:51, 25.42s/it] 10%|█         | 3/30 [01:01<06:22, 14.15s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.86s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:03<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.913146742184957
Epoch 215/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:08, 60.29s/it]  7%|▋         | 2/30 [01:01<11:47, 25.26s/it] 10%|█         | 3/30 [01:02<06:25, 14.28s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.94s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.99s/it] 20%|██        | 6/30 [01:04<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.920994448661804
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0232,  0.0086, -0.0081,  ...,  0.0081, -0.0127,  0.0216],
        [-0.0048,  0.0257,  0.0104,  ...,  0.0084, -0.0188, -0.0114],
        [-0.0431, -0.0314,  0.0084,  ...,  0.0721, -0.0316, -0.0305],
        ...,
        [ 0.0294, -0.0196, -0.0162,  ..., -0.0256, -0.0356, -0.0141],
        [-0.0295, -0.0043, -0.0128,  ..., -0.0081,  0.0115, -0.0145],
        [-0.0363, -0.0060,  0.0135,  ...,  0.0146,  0.0179, -0.0546]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9138, 0.8684, 0.8626, 0.8620, 0.8527, 0.8515, 0.8504, 0.8445, 0.8439,
         0.8402],
        [0.9648, 0.9605, 0.9596, 0.9595, 0.9589, 0.9580, 0.9573, 0.9570, 0.9559,
         0.9530],
        [0.9340, 0.9234, 0.9168, 0.9057, 0.9050, 0.8995, 0.8951, 0.8938, 0.8849,
         0.8804],
        [0.9656, 0.9287, 0.9283, 0.9267, 0.9261, 0.9244, 0.9171, 0.9119, 0.9108,
         0.9087],
        [0.8728, 0.8614, 0.8571, 0.8480, 0.8422, 0.8317, 0.8285, 0.8210, 0.8127,
         0.8112],
        [0.8971, 0.8788, 0.8779, 0.8760, 0.8759, 0.8668, 0.8652, 0.8469, 0.8465,
         0.8455],
        [0.9445, 0.9423, 0.9411, 0.9193, 0.9122, 0.9091, 0.9084, 0.9083, 0.9078,
         0.9076],
        [0.9432, 0.9359, 0.9291, 0.9004, 0.8914, 0.8907, 0.8890, 0.8738, 0.8737,
         0.8713],
        [0.9483, 0.9478, 0.9276, 0.9258, 0.9254, 0.9230, 0.9224, 0.9218, 0.9207,
         0.9203],
        [0.9603, 0.9211, 0.9202, 0.9107, 0.9097, 0.9082, 0.9058, 0.9011, 0.8979,
         0.8949],
        [0.9691, 0.9457, 0.9446, 0.9397, 0.9393, 0.9350, 0.9332, 0.9320, 0.9294,
         0.9291],
        [0.9260, 0.9226, 0.9067, 0.9060, 0.8876, 0.8874, 0.8816, 0.8608, 0.8601,
         0.8370],
        [0.9656, 0.9340, 0.9330, 0.9286, 0.9276, 0.9219, 0.9108, 0.9074, 0.9069,
         0.9059],
        [0.9772, 0.9690, 0.9689, 0.9672, 0.9667, 0.9659, 0.9654, 0.9653, 0.9639,
         0.9637],
        [0.9538, 0.9476, 0.9468, 0.9468, 0.9464, 0.9450, 0.9442, 0.9433, 0.9412,
         0.9401],
        [0.9421, 0.9366, 0.9335, 0.9300, 0.9294, 0.9276, 0.9274, 0.9272, 0.9262,
         0.9252],
        [0.9826, 0.9737, 0.9720, 0.9680, 0.9634, 0.9611, 0.9608, 0.9605, 0.9550,
         0.9544],
        [0.9741, 0.9647, 0.9596, 0.9573, 0.9567, 0.9556, 0.9553, 0.9544, 0.9540,
         0.9531],
        [0.9565, 0.9524, 0.9483, 0.9470, 0.9458, 0.9291, 0.9244, 0.9233, 0.9198,
         0.9182],
        [0.9398, 0.9333, 0.9332, 0.9281, 0.9272, 0.9258, 0.9244, 0.9240, 0.9221,
         0.9215],
        [0.9803, 0.9692, 0.9660, 0.9659, 0.9589, 0.9588, 0.9542, 0.9535, 0.9531,
         0.9530],
        [0.9554, 0.9476, 0.9454, 0.9433, 0.9361, 0.9341, 0.9340, 0.9335, 0.9274,
         0.9271],
        [0.9778, 0.9593, 0.9565, 0.9552, 0.9543, 0.9528, 0.9434, 0.9408, 0.9378,
         0.9350],
        [0.9572, 0.9552, 0.9531, 0.9497, 0.9485, 0.9461, 0.9425, 0.9425, 0.9389,
         0.9373],
        [0.9735, 0.9378, 0.9280, 0.9241, 0.9230, 0.9058, 0.9049, 0.9035, 0.9029,
         0.9026],
        [0.9500, 0.9496, 0.9454, 0.9390, 0.9378, 0.9377, 0.9371, 0.9326, 0.9319,
         0.9315],
        [0.9574, 0.9525, 0.9417, 0.9403, 0.9368, 0.9331, 0.9313, 0.9288, 0.9285,
         0.9275],
        [0.9661, 0.9551, 0.9545, 0.9507, 0.9474, 0.9406, 0.9396, 0.9383, 0.9373,
         0.9371],
        [0.9637, 0.9560, 0.9498, 0.9496, 0.9458, 0.9458, 0.9454, 0.9442, 0.9395,
         0.9374],
        [0.9545, 0.9544, 0.9450, 0.9424, 0.9418, 0.9397, 0.9395, 0.9393, 0.9381,
         0.9380],
        [0.9719, 0.9638, 0.9634, 0.9599, 0.9595, 0.9583, 0.9574, 0.9568, 0.9540,
         0.9531],
        [0.9518, 0.9442, 0.9433, 0.9345, 0.9336, 0.9324, 0.9277, 0.9215, 0.9175,
         0.9163],
        [0.9096, 0.9010, 0.8884, 0.8877, 0.8821, 0.8764, 0.8741, 0.8684, 0.8664,
         0.8618],
        [0.9410, 0.9340, 0.9254, 0.9192, 0.9161, 0.9147, 0.9118, 0.9106, 0.9057,
         0.9004],
        [0.9476, 0.9397, 0.9389, 0.9325, 0.9303, 0.9251, 0.9227, 0.9226, 0.9213,
         0.9209],
        [0.9334, 0.9328, 0.9319, 0.9168, 0.9144, 0.9137, 0.9132, 0.9124, 0.9069,
         0.9029],
        [0.9495, 0.8895, 0.8828, 0.8796, 0.8751, 0.8735, 0.8724, 0.8714, 0.8692,
         0.8679],
        [0.9636, 0.9464, 0.9281, 0.9221, 0.9091, 0.9051, 0.9031, 0.9025, 0.9024,
         0.8995],
        [0.9272, 0.9210, 0.9206, 0.9153, 0.9106, 0.9068, 0.8869, 0.8858, 0.8825,
         0.8817],
        [0.9709, 0.9653, 0.9652, 0.9648, 0.9631, 0.9628, 0.9618, 0.9614, 0.9606,
         0.9595],
        [0.9144, 0.9106, 0.9043, 0.9041, 0.8955, 0.8848, 0.8810, 0.8786, 0.8765,
         0.8758],
        [0.9233, 0.9098, 0.9067, 0.9021, 0.8982, 0.8925, 0.8899, 0.8886, 0.8882,
         0.8867],
        [0.9070, 0.9064, 0.9061, 0.8980, 0.8841, 0.8821, 0.8807, 0.8795, 0.8728,
         0.8720],
        [0.9322, 0.9140, 0.9016, 0.8988, 0.8968, 0.8772, 0.8761, 0.8754, 0.8749,
         0.8738],
        [0.8840, 0.8801, 0.8785, 0.8772, 0.8762, 0.8692, 0.8618, 0.8615, 0.8577,
         0.8559],
        [0.8533, 0.8295, 0.8213, 0.8008, 0.7861, 0.7857, 0.7841, 0.7618, 0.7611,
         0.7583],
        [0.9486, 0.9359, 0.9305, 0.9217, 0.9208, 0.9203, 0.9202, 0.9198, 0.9159,
         0.9131],
        [0.8575, 0.8472, 0.8351, 0.8299, 0.8268, 0.8250, 0.8016, 0.8014, 0.7960,
         0.7902],
        [0.9075, 0.8665, 0.8579, 0.8575, 0.8555, 0.8415, 0.8390, 0.8341, 0.8335,
         0.8320],
        [0.9289, 0.9256, 0.9207, 0.9015, 0.8935, 0.8931, 0.8926, 0.8908, 0.8903,
         0.8902],
        [0.8979, 0.8507, 0.8492, 0.8463, 0.8446, 0.8421, 0.8359, 0.8307, 0.8307,
         0.8304],
        [0.9121, 0.9092, 0.9022, 0.8866, 0.8773, 0.8769, 0.8751, 0.8729, 0.8673,
         0.8597],
        [0.9590, 0.9283, 0.9264, 0.9177, 0.9082, 0.8987, 0.8985, 0.8875, 0.8859,
         0.8824],
        [0.9378, 0.9032, 0.8908, 0.8876, 0.8844, 0.8794, 0.8761, 0.8747, 0.8743,
         0.8730],
        [0.9587, 0.9432, 0.9379, 0.9355, 0.9353, 0.9353, 0.9340, 0.9304, 0.9303,
         0.9260],
        [0.9457, 0.9233, 0.9193, 0.9171, 0.9151, 0.9146, 0.9112, 0.9112, 0.9099,
         0.9094],
        [0.9496, 0.9248, 0.9127, 0.9038, 0.8927, 0.8851, 0.8849, 0.8832, 0.8827,
         0.8786],
        [0.9523, 0.9104, 0.9000, 0.8824, 0.8762, 0.8751, 0.8730, 0.8723, 0.8717,
         0.8681],
        [0.9454, 0.9006, 0.8953, 0.8947, 0.8919, 0.8778, 0.8774, 0.8772, 0.8647,
         0.8644],
        [0.9308, 0.8611, 0.8607, 0.8545, 0.8480, 0.8468, 0.8458, 0.8423, 0.8350,
         0.8247],
        [0.9183, 0.9158, 0.9123, 0.9052, 0.9038, 0.9011, 0.8998, 0.8983, 0.8976,
         0.8970],
        [0.8533, 0.8261, 0.8193, 0.8139, 0.8084, 0.8080, 0.8070, 0.8058, 0.8033,
         0.7988],
        [0.8990, 0.8930, 0.8881, 0.8843, 0.8789, 0.8785, 0.8718, 0.8691, 0.8674,
         0.8662],
        [0.8827, 0.8811, 0.8525, 0.8506, 0.8503, 0.8477, 0.8463, 0.8436, 0.8405,
         0.8377]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 1],
        [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 1, 1, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 1, 1, 1],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 1, 0],
        [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 467103.4375,  244236.2812,  224916.9219,  223006.1719,  194998.6719,
          191768.9062,  188734.4219,  173619.5938,  172103.2812,  163334.1406],
        [ 967730.6250,  910244.8750,  898847.7500,  897452.5000,  889923.4375,
          878714.5625,  869464.2500,  865799.5625,  852254.5625,  817488.1250],
        [ 623080.5625,  535984.3750,  487848.9688,  415797.6250,  412006.2812,
          380766.0000,  357577.5312,  350840.9688,  309037.0000,  289829.0000],
        [ 979389.1875,  577933.3750,  574652.8125,  561304.0625,  556473.0625,
          543645.0625,  489764.9062,  454410.5312,  447625.0000,  434480.3438],
        [ 260119.6250,  220950.8438,  207849.5625,  182542.0000,  168062.0156,
          144499.7344,  138031.7969,  123980.3750,  110136.7422,  107910.6562],
        [ 368187.3125,  283472.5625,  279825.5312,  272093.9375,  271732.4375,
          238693.2656,  233152.3750,  179599.0469,  178484.0781,  176071.8906],
        [ 724255.1250,  701921.8750,  690156.4375,  505533.4375,  456479.3125,
          436964.3750,  432585.5000,  432067.2188,  428849.5938,  427216.3750],
        [ 711230.2500,  640287.6250,  581107.8750,  385898.5938,  339205.2812,
          335622.5625,  327519.1250,  263736.8438,  263574.4062,  254516.4375],
        [ 764893.5625,  758689.3125,  569099.2500,  554166.6250,  551600.9375,
          532679.1875,  528238.1250,  523635.6875,  515315.9375,  512255.5000],
        [ 907126.9375,  518712.1250,  511454.4375,  447021.3750,  440625.9688,
          431166.5938,  416406.3750,  389553.3125,  372349.0938,  356558.3125],
        [1029372.0000,  736258.4375,  725526.4375,  676043.8125,  672239.0625,
          631899.9375,  616041.1250,  606071.1250,  583613.1875,  581248.6875],
        [ 555841.3125,  529338.4375,  422238.3750,  417765.1250,  321267.0312,
          320304.3125,  294880.1250,  219159.4688,  216930.7656,  156005.6250],
        [ 978649.7500,  623260.0625,  614830.8750,  577214.5000,  569220.2500,
          524038.8438,  447678.7812,  425982.4688,  423016.6250,  417175.0938],
        [1155762.5000, 1027674.1250, 1025922.3125, 1001372.5000,  994600.8125,
          983601.3125,  975841.6875,  975258.3750,  955637.5000,  953039.0625],
        [ 826706.5000,  756508.1250,  748800.5625,  748394.3750,  744320.8125,
          729168.7500,  721220.2500,  712307.5000,  691112.8125,  680263.7500],
        [ 700078.7500,  647057.1250,  619189.3125,  588721.4375,  583292.6875,
          568776.3750,  567353.7500,  565308.9375,  557530.1250,  550053.3125],
        [1248634.8750, 1098877.2500, 1073064.6250, 1012960.9375,  948551.6250,
          918244.4375,  913542.6250,  910345.5625,  841004.3125,  834050.6250],
        [1104755.2500,  966152.8750,  898203.3750,  869289.3125,  862128.8750,
          849243.7500,  845568.3125,  834173.1875,  829931.1250,  818743.5625],
        [ 859415.0000,  810715.7500,  764435.6250,  750165.8125,  737911.7500,
          580934.4375,  543142.3750,  535024.7500,  509227.8125,  497303.9688],
        [ 677341.6250,  617075.3750,  616588.3125,  572845.0000,  565510.0625,
          554746.6875,  543411.2500,  540605.5625,  526154.0625,  521566.3438],
        [1207622.0000, 1030991.1250,  984130.5000,  983695.1250,  889466.9375,
          887749.1875,  832165.2500,  824016.9375,  819085.6250,  818262.6875],
        [ 846752.9375,  756962.0000,  733109.0000,  712259.9375,  642698.0625,
          624449.3125,  623353.3750,  618484.0625,  567068.1250,  564694.1875],
        [1165889.7500,  894944.8125,  860141.4375,  843813.0000,  832504.2500,
          815719.6250,  712732.8750,  686860.2500,  658556.8750,  632719.4375],
        [ 868874.0625,  844394.9375,  819342.6250,  780050.4375,  766505.9375,
          740589.3750,  704170.6875,  703859.8125,  668964.0625,  653284.2500],
        [1095393.1250,  658267.4375,  572142.8750,  540845.8750,  532901.2500,
          416697.9375,  411522.5000,  403297.0938,  399983.0938,  397866.3125],
        [ 783323.0625,  779512.0625,  733604.1875,  669532.6875,  658169.5000,
          656912.8125,  651754.0625,  611377.6250,  605134.3125,  601265.7500],
        [ 870237.3750,  812447.9375,  696006.0000,  682104.7500,  648591.8125,
          615449.1875,  599407.3125,  579109.6250,  576576.8750,  568426.0625],
        [ 986409.1875,  842581.0625,  836008.8750,  791548.2500,  755033.4375,
          685400.3750,  674799.3750,  662900.8750,  653737.9375,  651335.8750],
        [ 953183.6250,  853442.0625,  781737.9375,  779428.0625,  738204.5625,
          738048.3125,  733401.3125,  721456.2500,  674567.0625,  654698.8125],
        [ 835188.0000,  834831.3125,  729319.0000,  702682.7500,  696555.1250,
          675996.7500,  673839.8750,  672788.0625,  661290.1875,  659741.8125],
        [1071156.7500,  954305.1250,  949192.3125,  902400.1875,  896878.3125,
          882136.1250,  871366.7500,  863871.1250,  829302.0625,  819043.4375],
        [ 803904.4375,  721478.9375,  711981.5000,  627509.4375,  619949.1875,
          609107.9375,  569442.8750,  521610.6250,  492160.3125,  484035.8750],
        [ 440107.7500,  388798.7500,  324918.3125,  321585.2188,  297137.3125,
          273934.2188,  264724.3750,  244116.3594,  237430.4844,  222218.3281],
        [ 688800.6250,  623026.5000,  551398.4375,  504300.2500,  482890.5938,
          472973.8438,  454167.4688,  446539.0312,  415824.1875,  385745.9062],
        [ 757236.3750,  676610.1250,  668804.5000,  609695.5000,  591307.0000,
          548761.3125,  530494.7500,  529621.2500,  520167.5312,  516863.3125],
        [ 617904.5625,  612818.3125,  604787.5625,  487277.5312,  471410.3125,
          466174.6875,  462920.6562,  457806.8125,  423312.0625,  399913.6562],
        [ 778331.6250,  330020.5312,  299848.4688,  286750.5938,  268822.6562,
          262541.8125,  258484.0469,  254781.3906,  247155.3438,  242559.2031],
        [ 951927.2500,  743962.4375,  572692.5625,  525918.8125,  436912.2812,
          412789.3438,  401077.8438,  397705.4375,  396919.2188,  380542.3750],
        [ 565684.3125,  517955.8125,  514735.8438,  476918.0625,  446112.9688,
          422889.9688,  317870.4688,  313276.7812,  298824.7812,  295220.5625],
        [1055409.2500,  975373.7500,  973048.1875,  967337.6250,  944892.3125,
          940030.7500,  926634.1250,  922412.5625,  911768.7500,  897924.1250],
        [ 471046.3438,  446006.5938,  408103.5625,  406797.9688,  359627.4062,
          308447.2188,  292439.9688,  282555.9688,  274036.6562,  271511.2188],
        [ 535297.2500,  441004.3125,  421877.6875,  395347.6562,  373590.1250,
          344762.5938,  332064.0938,  325687.3438,  323806.8438,  317004.0312],
        [ 423993.6250,  420408.1562,  418415.8125,  372569.6562,  305679.0625,
          297018.0312,  291252.0938,  286343.6875,  259983.9688,  257113.4062],
        [ 607586.1875,  468114.8750,  392306.5938,  377165.3125,  366431.6562,
          276798.1875,  272707.8125,  269821.8125,  268059.5312,  263898.8750],
        [ 305328.5625,  288587.8438,  282099.5938,  276932.5938,  272813.4062,
          247053.2969,  222088.0312,  221171.1562,  209654.2188,  204186.2031],
        [ 196726.8281,  140154.4219,  124513.2266,   92974.7812,   75349.9453,
           74974.4766,   73265.2188,   53240.7773,   52723.7773,   50672.9766],
        [ 768432.3750,  640380.4375,  592869.3750,  522769.4688,  516095.4375,
          512222.7812,  511734.5000,  508800.1562,  481610.6562,  462654.9375],
        [ 208871.9062,  180459.2188,  151818.0469,  140966.3594,  134824.1406,
          131292.3750,   94073.0703,   93784.2656,   86763.4062,   79926.3828],
        [ 427029.8125,  237684.0156,  210303.6250,  209111.2812,  203220.2969,
          166169.1406,  160413.7656,  149635.0469,  148423.2812,  145114.9688],
        [ 579301.2500,  552668.8125,  515837.1250,  391884.0312,  349636.5312,
          347724.1562,  344985.2500,  336065.2188,  333984.2500,  333214.0312],
        [ 372096.0000,  189535.8125,  185638.5000,  178152.7969,  173733.5469,
          167801.7656,  153393.0156,  142469.6406,  142448.5781,  141920.3125],
        [ 456118.5625,  437674.2188,  395534.3438,  316611.5938,  277466.3438,
          275750.8750,  268690.6562,  260273.7031,  240263.6094,  215688.5000],
        [ 891223.7500,  574680.7500,  559417.6250,  494206.7500,  431415.0312,
          376458.4688,  375503.2500,  320583.6250,  313729.4375,  298238.5625],
        [ 658488.4375,  401416.4688,  336433.6562,  321053.2500,  306763.6562,
          285636.1875,  272446.0312,  267364.5625,  265771.8750,  260694.0156],
        [ 887248.1250,  710956.2500,  658741.5625,  636558.4375,  634891.1875,
          634891.1875,  623158.3750,  591878.5000,  590936.0000,  556311.1875],
        [ 736539.3750,  535028.3125,  505251.0312,  489813.0312,  476168.6250,
          472204.9688,  449962.7500,  449803.5625,  441802.4375,  438899.3438],
        [ 778915.3125,  546773.5625,  459874.4062,  404630.0312,  345635.6562,
          309831.4062,  309167.5938,  301875.0000,  299418.4062,  282654.3438],
        [ 809495.1250,  445177.9688,  383340.5000,  298339.2500,  272806.6562,
          268891.6250,  260906.9375,  258206.1406,  256156.2031,  243180.4219],
        [ 733469.1875,  387027.1250,  358616.7500,  355689.8750,  341395.2188,
          279264.9062,  277807.6250,  276960.3125,  231525.1250,  230720.1719],
        [ 595574.7500,  220024.1562,  218813.0156,  200285.2031,  182572.6562,
          179249.1250,  176896.2656,  168143.4531,  151485.2656,  130786.8828],
        [ 498389.7812,  480761.2188,  457448.0938,  412995.6875,  404885.1875,
          389829.8125,  382259.1875,  374070.3438,  370514.5625,  367430.7188],
        [ 196737.8906,  133510.3438,  121127.4375,  112170.3828,  103653.9141,
          103006.5859,  101532.1328,   99902.3594,   96280.6172,   90299.4609],
        [ 378217.4375,  346985.7812,  323410.5625,  306641.9688,  283883.5000,
          282000.8750,  256263.7188,  246528.9219,  240661.7344,  236588.5469],
        [ 299430.0938,  292748.8750,  194564.3594,  189432.0938,  188435.6875,
          181664.4688,  178057.0000,  171402.2344,  163859.4688,  157584.6719]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[467103.4375,      0.0000],
         [244236.2812,      0.0000],
         [224916.9219,      0.0000],
         ...,
         [173619.5938,      0.0000],
         [172103.2812,      0.0000],
         [163334.1406,      0.0000]],

        [[967730.6250,      0.0000],
         [910244.8750,      0.0000],
         [898847.7500,      0.0000],
         ...,
         [865799.5625,      0.0000],
         [852254.5625,      0.0000],
         [817488.1250,      0.0000]],

        [[623080.5625,      0.0000],
         [535984.3750,      0.0000],
         [487848.9688,      0.0000],
         ...,
         [350840.9688,      0.0000],
         [309037.0000,      0.0000],
         [     0.0000, 289829.0000]],

        ...,

        [[     0.0000, 196737.8906],
         [133510.3438,      0.0000],
         [121127.4375,      0.0000],
         ...,
         [ 99902.3594,      0.0000],
         [ 96280.6172,      0.0000],
         [ 90299.4609,      0.0000]],

        [[378217.4375,      0.0000],
         [     0.0000, 346985.7812],
         [323410.5625,      0.0000],
         ...,
         [246528.9219,      0.0000],
         [     0.0000, 240661.7344],
         [     0.0000, 236588.5469]],

        [[     0.0000, 299430.0938],
         [     0.0000, 292748.8750],
         [194564.3594,      0.0000],
         ...,
         [171402.2344,      0.0000],
         [163859.4688,      0.0000],
         [157584.6719,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[ 2052052.8750,   191768.9062],
        [ 8847920.0000,        0.0000],
        [ 3872939.5000,   289829.0000],
        [ 5619678.5000,        0.0000],
        [  713151.1250,   950932.2500],
        [ 1755042.0000,   726270.4375],
        [ 3150174.7500,  2085854.5000],
        [ 4102699.0000,        0.0000],
        [ 5045681.0000,   764893.5625],
        [ 4790974.5000,        0.0000],
        [ 6858313.5000,        0.0000],
        [ 3453730.5000,        0.0000],
        [ 5601067.0000,        0.0000],
        [10048710.0000,        0.0000],
        [ 7358803.0000,        0.0000],
        [ 5947362.0000,        0.0000],
        [ 9799276.0000,        0.0000],
        [ 8878190.0000,        0.0000],
        [ 6588277.5000,        0.0000],
        [ 5735844.0000,        0.0000],
        [ 9277185.0000,        0.0000],
        [ 6689831.0000,        0.0000],
        [ 8103882.0000,        0.0000],
        [ 7550036.0000,        0.0000],
        [ 5428917.5000,        0.0000],
        [ 6750586.0000,        0.0000],
        [ 6648357.5000,        0.0000],
        [ 7539755.0000,        0.0000],
        [ 7628168.0000,        0.0000],
        [ 7142233.0000,        0.0000],
        [ 9039652.0000,        0.0000],
        [ 6161181.0000,        0.0000],
        [ 3014971.0000,        0.0000],
        [ 3061029.7500,  1964636.8750],
        [ 3932213.7500,  2017347.8750],
        [ 3323827.2500,  1680499.0000],
        [ 1903960.2500,  1325335.5000],
        [ 2951865.5000,  2268582.2500],
        [ 1849541.5000,  2319948.0000],
        [ 9514831.0000,        0.0000],
        [ 3520573.0000,        0.0000],
        [ 3810441.7500,        0.0000],
        [ 3332777.5000,        0.0000],
        [ 3562890.5000,        0.0000],
        [ 1189924.5000,  1339990.3750],
        [  473201.9375,   461394.4688],
        [ 4924700.5000,   592869.3750],
        [  984271.3125,   318507.8125],
        [  705869.8125,  1351235.3750],
        [ 2598278.2500,  1487022.3750],
        [ 1377161.5000,   470028.4375],
        [ 1972813.2500,  1171259.1250],
        [ 2335614.5000,  2299842.7500],
        [ 2974651.7500,   401416.4688],
        [ 6525571.0000,        0.0000],
        [ 4505660.5000,   489813.0312],
        [ 1802625.7500,  2236150.0000],
        [ 2409776.2500,  1086724.6250],
        [ 1918145.0000,  1554331.2500],
        [ 1046410.1250,  1177420.6250],
        [ 3358158.0000,   780426.3750],
        [  961483.2500,   196737.8906],
        [ 1793063.3750,  1108119.5000],
        [  865467.7500,  1151711.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 57.8125
Top1 accuracy for validation set is 57.8125 size is torch.Size([64, 1])
Epoch 216/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:10, 56.22s/it]  7%|▋         | 2/30 [00:56<11:00, 23.59s/it] 10%|█         | 3/30 [00:59<06:16, 13.96s/it] 13%|█▎        | 4/30 [01:00<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:01<02:31,  6.06s/it] 20%|██        | 6/30 [01:02<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:03<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:03<00:51,  2.36s/it] 30%|███       | 9/30 [01:04<00:38,  1.86s/it] 33%|███▎      | 10/30 [01:05<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.28s/it] 40%|████      | 12/30 [01:06<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:07<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 2.8835794289906818
Epoch 217/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:40, 59.34s/it]  7%|▋         | 2/30 [01:01<12:04, 25.87s/it] 10%|█         | 3/30 [01:02<06:28, 14.40s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.03s/it] 20%|██        | 6/30 [01:04<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.9249692440032957
Epoch 218/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:52, 59.75s/it]  7%|▋         | 2/30 [01:00<11:41, 25.04s/it] 10%|█         | 3/30 [01:01<06:16, 13.95s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.8954004128774007
Epoch 219/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:07, 60.27s/it]  7%|▋         | 2/30 [01:01<11:47, 25.26s/it] 10%|█         | 3/30 [01:02<06:34, 14.61s/it] 13%|█▎        | 4/30 [01:03<03:57,  9.14s/it] 17%|█▋        | 5/30 [01:04<02:32,  6.11s/it] 20%|██        | 6/30 [01:05<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:05<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.37s/it] 30%|███       | 9/30 [01:07<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.29s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 2.8838926394780477
Epoch 220/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:47, 59.56s/it]  7%|▋         | 2/30 [01:00<11:39, 24.97s/it] 10%|█         | 3/30 [01:01<06:15, 13.91s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.839617292086283
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0233,  0.0085, -0.0067,  ...,  0.0087, -0.0113,  0.0203],
        [-0.0044,  0.0259,  0.0116,  ...,  0.0085, -0.0185, -0.0124],
        [-0.0414, -0.0311,  0.0089,  ...,  0.0738, -0.0306, -0.0296],
        ...,
        [ 0.0287, -0.0189, -0.0147,  ..., -0.0254, -0.0348, -0.0142],
        [-0.0291, -0.0041, -0.0127,  ..., -0.0071,  0.0115, -0.0138],
        [-0.0368, -0.0063,  0.0138,  ...,  0.0166,  0.0186, -0.0542]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9130, 0.8660, 0.8625, 0.8616, 0.8526, 0.8525, 0.8496, 0.8469, 0.8432,
         0.8390],
        [0.9631, 0.9595, 0.9587, 0.9586, 0.9575, 0.9561, 0.9558, 0.9552, 0.9538,
         0.9513],
        [0.9315, 0.9215, 0.9168, 0.9020, 0.9000, 0.8976, 0.8887, 0.8882, 0.8800,
         0.8766],
        [0.9638, 0.9264, 0.9255, 0.9252, 0.9239, 0.9227, 0.9151, 0.9098, 0.9086,
         0.9064],
        [0.8736, 0.8597, 0.8569, 0.8426, 0.8404, 0.8331, 0.8269, 0.8141, 0.8103,
         0.8092],
        [0.8931, 0.8752, 0.8730, 0.8729, 0.8714, 0.8621, 0.8602, 0.8434, 0.8428,
         0.8420],
        [0.9425, 0.9393, 0.9387, 0.9161, 0.9103, 0.9066, 0.9064, 0.9057, 0.9042,
         0.9037],
        [0.9412, 0.9320, 0.9264, 0.8972, 0.8879, 0.8864, 0.8850, 0.8719, 0.8690,
         0.8681],
        [0.9462, 0.9442, 0.9263, 0.9242, 0.9224, 0.9218, 0.9201, 0.9191, 0.9181,
         0.9172],
        [0.9596, 0.9196, 0.9190, 0.9092, 0.9089, 0.9073, 0.9048, 0.8994, 0.8975,
         0.8940],
        [0.9686, 0.9436, 0.9425, 0.9375, 0.9359, 0.9325, 0.9312, 0.9308, 0.9262,
         0.9262],
        [0.9249, 0.9210, 0.9025, 0.9019, 0.8834, 0.8834, 0.8802, 0.8581, 0.8569,
         0.8347],
        [0.9644, 0.9326, 0.9316, 0.9265, 0.9262, 0.9209, 0.9099, 0.9063, 0.9060,
         0.9052],
        [0.9768, 0.9685, 0.9677, 0.9665, 0.9661, 0.9652, 0.9648, 0.9636, 0.9628,
         0.9619],
        [0.9527, 0.9463, 0.9458, 0.9452, 0.9447, 0.9441, 0.9436, 0.9435, 0.9408,
         0.9398],
        [0.9400, 0.9357, 0.9336, 0.9290, 0.9280, 0.9273, 0.9268, 0.9259, 0.9252,
         0.9242],
        [0.9818, 0.9731, 0.9702, 0.9676, 0.9626, 0.9593, 0.9591, 0.9587, 0.9535,
         0.9530],
        [0.9733, 0.9648, 0.9598, 0.9563, 0.9546, 0.9542, 0.9541, 0.9540, 0.9533,
         0.9520],
        [0.9541, 0.9517, 0.9478, 0.9435, 0.9424, 0.9271, 0.9233, 0.9211, 0.9177,
         0.9169],
        [0.9385, 0.9329, 0.9297, 0.9259, 0.9256, 0.9248, 0.9230, 0.9204, 0.9203,
         0.9197],
        [0.9792, 0.9678, 0.9658, 0.9645, 0.9578, 0.9568, 0.9546, 0.9524, 0.9521,
         0.9508],
        [0.9535, 0.9458, 0.9440, 0.9416, 0.9343, 0.9334, 0.9324, 0.9315, 0.9267,
         0.9258],
        [0.9772, 0.9588, 0.9561, 0.9545, 0.9528, 0.9519, 0.9428, 0.9390, 0.9367,
         0.9335],
        [0.9564, 0.9559, 0.9521, 0.9492, 0.9477, 0.9444, 0.9420, 0.9414, 0.9380,
         0.9365],
        [0.9727, 0.9333, 0.9264, 0.9231, 0.9221, 0.9030, 0.9025, 0.9016, 0.9016,
         0.8997],
        [0.9502, 0.9491, 0.9440, 0.9375, 0.9374, 0.9372, 0.9364, 0.9319, 0.9314,
         0.9303],
        [0.9568, 0.9525, 0.9415, 0.9408, 0.9361, 0.9307, 0.9305, 0.9274, 0.9274,
         0.9259],
        [0.9662, 0.9557, 0.9555, 0.9493, 0.9472, 0.9398, 0.9379, 0.9376, 0.9372,
         0.9368],
        [0.9628, 0.9548, 0.9502, 0.9480, 0.9446, 0.9440, 0.9428, 0.9412, 0.9387,
         0.9376],
        [0.9540, 0.9528, 0.9435, 0.9408, 0.9398, 0.9390, 0.9377, 0.9377, 0.9377,
         0.9373],
        [0.9714, 0.9632, 0.9631, 0.9589, 0.9588, 0.9573, 0.9569, 0.9565, 0.9534,
         0.9518],
        [0.9517, 0.9426, 0.9421, 0.9329, 0.9328, 0.9312, 0.9264, 0.9213, 0.9154,
         0.9154],
        [0.9083, 0.8994, 0.8867, 0.8839, 0.8804, 0.8762, 0.8749, 0.8680, 0.8670,
         0.8623],
        [0.9393, 0.9320, 0.9240, 0.9159, 0.9150, 0.9131, 0.9111, 0.9088, 0.9055,
         0.9004],
        [0.9478, 0.9393, 0.9382, 0.9327, 0.9298, 0.9239, 0.9211, 0.9205, 0.9197,
         0.9189],
        [0.9324, 0.9318, 0.9294, 0.9148, 0.9128, 0.9127, 0.9111, 0.9092, 0.9046,
         0.9005],
        [0.9498, 0.8876, 0.8818, 0.8787, 0.8742, 0.8723, 0.8685, 0.8684, 0.8680,
         0.8672],
        [0.9610, 0.9444, 0.9265, 0.9205, 0.9078, 0.9025, 0.9007, 0.8995, 0.8992,
         0.8981],
        [0.9260, 0.9170, 0.9167, 0.9146, 0.9102, 0.9053, 0.8863, 0.8848, 0.8805,
         0.8799],
        [0.9698, 0.9637, 0.9630, 0.9626, 0.9613, 0.9612, 0.9612, 0.9599, 0.9588,
         0.9585],
        [0.9118, 0.9089, 0.9043, 0.9037, 0.8942, 0.8841, 0.8806, 0.8769, 0.8744,
         0.8716],
        [0.9232, 0.9072, 0.9048, 0.9006, 0.8958, 0.8919, 0.8891, 0.8873, 0.8858,
         0.8857],
        [0.9073, 0.9064, 0.9050, 0.8972, 0.8841, 0.8806, 0.8804, 0.8798, 0.8718,
         0.8709],
        [0.9300, 0.9130, 0.9004, 0.8957, 0.8939, 0.8770, 0.8748, 0.8737, 0.8731,
         0.8712],
        [0.8845, 0.8786, 0.8753, 0.8739, 0.8728, 0.8693, 0.8601, 0.8565, 0.8557,
         0.8516],
        [0.8511, 0.8319, 0.8225, 0.8016, 0.7878, 0.7875, 0.7854, 0.7638, 0.7609,
         0.7598],
        [0.9480, 0.9354, 0.9301, 0.9207, 0.9204, 0.9201, 0.9193, 0.9193, 0.9155,
         0.9128],
        [0.8569, 0.8428, 0.8332, 0.8282, 0.8262, 0.8236, 0.7990, 0.7988, 0.7950,
         0.7906],
        [0.9067, 0.8652, 0.8540, 0.8521, 0.8511, 0.8374, 0.8347, 0.8323, 0.8305,
         0.8291],
        [0.9258, 0.9241, 0.9171, 0.8995, 0.8886, 0.8875, 0.8870, 0.8869, 0.8866,
         0.8857],
        [0.8986, 0.8495, 0.8473, 0.8462, 0.8450, 0.8421, 0.8358, 0.8313, 0.8310,
         0.8286],
        [0.9110, 0.9085, 0.8985, 0.8838, 0.8763, 0.8736, 0.8702, 0.8692, 0.8651,
         0.8567],
        [0.9577, 0.9290, 0.9256, 0.9156, 0.9091, 0.8992, 0.8963, 0.8871, 0.8852,
         0.8772],
        [0.9379, 0.9007, 0.8885, 0.8865, 0.8854, 0.8798, 0.8751, 0.8730, 0.8730,
         0.8720],
        [0.9581, 0.9416, 0.9368, 0.9349, 0.9338, 0.9338, 0.9334, 0.9292, 0.9283,
         0.9241],
        [0.9433, 0.9220, 0.9172, 0.9141, 0.9116, 0.9114, 0.9102, 0.9080, 0.9076,
         0.9067],
        [0.9485, 0.9235, 0.9105, 0.9022, 0.8931, 0.8836, 0.8833, 0.8810, 0.8783,
         0.8762],
        [0.9518, 0.9092, 0.9004, 0.8830, 0.8752, 0.8735, 0.8730, 0.8723, 0.8708,
         0.8658],
        [0.9442, 0.8997, 0.8932, 0.8929, 0.8917, 0.8777, 0.8771, 0.8736, 0.8642,
         0.8619],
        [0.9290, 0.8594, 0.8593, 0.8534, 0.8467, 0.8437, 0.8436, 0.8413, 0.8337,
         0.8242],
        [0.9162, 0.9138, 0.9105, 0.9053, 0.9019, 0.8987, 0.8984, 0.8964, 0.8963,
         0.8949],
        [0.8544, 0.8196, 0.8169, 0.8090, 0.8046, 0.8035, 0.8022, 0.8021, 0.7989,
         0.7953],
        [0.8996, 0.8895, 0.8882, 0.8815, 0.8791, 0.8749, 0.8702, 0.8649, 0.8636,
         0.8633],
        [0.8790, 0.8773, 0.8491, 0.8475, 0.8465, 0.8465, 0.8397, 0.8392, 0.8353,
         0.8342]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [1, 0, 0, 1, 0, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 1, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 1, 1, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0],
        [1, 0, 1, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 1, 1, 1],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 1, 0],
        [0, 1, 1, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 1, 1, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 461995.8125,  236033.7031,  224341.9219,  221690.0156,  194764.3125,
          194487.3750,  186675.8281,  179565.1406,  170460.8438,  160546.7656],
        [ 944851.7500,  897076.8125,  886923.2500,  886227.3750,  872640.8125,
          854969.5625,  851327.7500,  843726.8750,  827273.5625,  798456.6875],
        [ 601440.1250,  521266.4688,  487371.8438,  394663.1875,  383369.7812,
          370741.8438,  326513.6562,  324179.4688,  288037.0938,  274363.7500],
        [ 953977.5625,  559441.0625,  551807.1875,  549704.6250,  539462.2500,
          530777.5625,  475834.0625,  440858.0000,  433932.4688,  420292.6875],
        [ 262871.2812,  215679.8594,  207096.5000,  168848.0781,  163687.3438,
          147383.7500,  134922.9219,  112444.7891,  106453.4766,  104859.1484],
        [ 347524.9062,  269144.5938,  260653.0156,  260559.8125,  254820.2500,
          223169.1406,  217306.5781,  170734.6406,  169300.0312,  167501.1719],
        [ 703906.1250,  671990.3750,  666328.6250,  482668.1875,  444161.4688,
          421600.2188,  419954.5312,  416183.2500,  407177.5312,  404395.1250],
        [ 691265.0625,  605898.8750,  559568.0625,  368672.5625,  322714.9062,
          315851.3125,  309434.2500,  256566.7031,  246378.7344,  243161.6250],
        [ 741537.0625,  720828.3750,  558465.6250,  541627.8125,  528472.4375,
          523436.4375,  511022.9688,  503733.5312,  496702.4688,  490144.7812],
        [ 899072.3750,  507784.5938,  502940.0625,  437486.8750,  435495.0000,
          425604.8125,  410765.7500,  380280.0938,  369893.1875,  352198.6875],
        [1021181.2500,  714882.0000,  703794.0625,  655300.9375,  640636.3750,
          610162.0000,  598700.6250,  595887.2500,  557804.0000,  557676.8750],
        [ 547165.8125,  517905.4375,  397673.2188,  393906.6250,  302444.9688,
          302350.9688,  288897.9375,  210654.9062,  207156.7344,  150837.7188],
        [ 962132.0625,  611260.4375,  602533.1875,  559918.8125,  557280.2500,
          517254.8438,  442019.5000,  419356.6250,  417852.0000,  413202.9062],
        [1148959.8750, 1020324.5625, 1008799.1250,  991384.8125,  986272.7500,
          974023.0625,  967450.1875,  951685.8125,  940567.0000,  928192.5000],
        [ 814096.1250,  742874.1250,  737649.3125,  731526.4375,  725923.6875,
          720161.8125,  715127.4375,  714075.2500,  687056.1250,  676912.8125],
        [ 678793.9375,  638338.4375,  620090.5000,  580613.7500,  572275.5000,
          566197.6250,  562271.1250,  555487.8125,  549987.2500,  542116.1250],
        [1234074.8750, 1089972.2500, 1045308.0625, 1007709.7500,  938448.8125,
          894902.1250,  892623.7500,  887002.7500,  823136.3750,  818179.1875],
        [1092945.1250,  968406.5000,  900715.8750,  857423.1875,  837183.2500,
          832131.1250,  830185.1875,  829499.0625,  821214.6250,  806143.9375],
        [ 830470.3125,  802400.1250,  759604.4375,  714230.5000,  703237.8750,
          564431.9375,  534619.2500,  518705.7188,  493675.3750,  488126.7812],
        [ 665169.9375,  613334.0000,  586006.3750,  554864.1250,  553227.2500,
          546679.7500,  532517.6875,  513438.0938,  512186.1250,  507986.5938],
        [1189586.8750, 1010761.7500,  981374.1875,  963656.3750,  876269.2500,
          862976.9375,  836678.8125,  811029.0000,  807000.8125,  792694.9375],
        [ 823987.8750,  738179.9375,  719539.8750,  694871.8750,  626301.1875,
          617995.8750,  609069.0000,  601813.6250,  561545.0000,  554151.2500],
        [1154915.1250,  887997.2500,  854839.8125,  835769.7500,  815805.1875,
          805302.5000,  707233.5625,  669975.3125,  648013.7500,  618560.1875],
        [ 858209.3125,  852367.6250,  806844.5625,  773977.8125,  757834.5625,
          723666.1875,  698359.6875,  692839.8125,  660163.5000,  645962.1250],
        [1083330.5000,  616933.6250,  558979.2500,  533269.8125,  525968.9375,
          400338.0000,  397720.6250,  392435.6562,  392168.5625,  381673.0938],
        [ 785289.4375,  773060.9375,  719565.2500,  655184.1250,  654093.4375,
          652682.6875,  644904.0000,  605150.5000,  601021.5000,  591391.5625],
        [ 863716.3125,  812472.8125,  693813.0625,  687330.0625,  642257.5000,
          594652.5000,  592760.8125,  567459.8125,  567026.0000,  555326.3125],
        [ 987998.3750,  849960.8125,  847709.5625,  776173.1250,  752452.8125,
          677653.0000,  659249.3750,  656316.6875,  652587.5000,  648376.5625],
        [ 941239.9375,  839083.2500,  785379.3125,  761327.6250,  724926.7500,
          719297.0000,  707134.4375,  691068.0000,  666900.1250,  656502.0000],
        [ 829687.3125,  815723.5000,  713659.8750,  686835.3125,  676869.5625,
          669726.1875,  657549.0000,  657010.5625,  656757.5000,  653646.3750],
        [1064101.7500,  946214.2500,  944849.0625,  889864.8125,  888350.4375,
          869533.0625,  864059.8125,  859376.4375,  821956.6250,  803818.6250],
        [ 802403.9375,  704457.5000,  700090.1250,  613744.7500,  612400.0000,
          599314.6875,  559006.4375,  520006.3438,  478117.5625,  477956.6562],
        [ 431591.9688,  380396.5000,  317040.0312,  304853.4062,  289980.2188,
          272914.1250,  268007.6250,  242632.5469,  239276.0312,  223967.6250],
        [ 672147.4375,  605572.5000,  540389.0625,  481110.7500,  475374.1250,
          462744.1250,  449674.0625,  434899.0312,  414595.0938,  385533.6875],
        [ 758883.2500,  672784.1875,  662179.3750,  611830.8125,  587056.3125,
          539953.2500,  518734.3750,  513887.8125,  508288.4688,  502305.9062],
        [ 609312.4375,  603904.0000,  583744.5625,  473961.8125,  460609.5938,
          460112.5938,  449697.2188,  437273.2812,  409420.3750,  386237.6875],
        [ 781659.6875,  321154.0000,  295507.6250,  282915.1250,  265424.3438,
          258178.5469,  244688.7969,  244239.0938,  242958.5781,  240104.6719],
        [ 917042.8125,  723177.0000,  559639.0625,  513937.7812,  428825.0625,
          397692.5625,  387106.0938,  380631.3125,  379402.7188,  372989.9062],
        [ 555972.8125,  488862.8750,  486965.7812,  472498.6562,  443954.8125,
          413642.9062,  315145.4375,  308698.5625,  290127.9688,  287950.3125],
        [1039934.8125,  953122.7500,  943793.5625,  937479.1875,  921325.8750,
          919508.0625,  919330.8750,  903014.8750,  887830.5000,  884444.1875],
        [ 454147.0938,  435624.5938,  407604.9062,  404525.5000,  352930.3125,
          305410.6875,  290511.4375,  275857.9375,  266154.8750,  255795.6406],
        [ 534263.5000,  425071.8125,  410612.5938,  387025.2812,  361432.5000,
          341409.2188,  328156.9375,  319861.9688,  313268.1250,  312615.4375],
        [ 425397.0625,  419948.9375,  411772.5625,  368342.9062,  305666.8125,
          290767.2500,  289682.0000,  287240.2188,  256511.4062,  253195.5625],
        [ 589096.0625,  461582.2812,  385525.2500,  360509.9062,  351417.2812,
          276149.8438,  267392.3750,  263364.3438,  260987.8125,  253988.0781],
        [ 307361.9375,  282566.7500,  269566.1562,  264083.1562,  260212.6562,
          247257.1875,  216806.4531,  205939.6094,  203634.5156,  192058.0781],
        [ 190753.8438,  144935.8438,  126678.2812,   94074.6016,   77234.1328,
           76902.3594,   74591.2891,   54802.9922,   52562.9219,   51739.1289],
        [ 760863.8125,  635930.4375,  589593.5000,  515752.0000,  513305.9062,
          511241.3438,  505404.7188,  505384.5000,  478311.8750,  460244.6875],
        [ 207062.9219,  169276.9531,  147719.9219,  137470.2031,  133664.7500,
          128779.2266,   90562.7578,   90285.5156,   85541.6797,   80374.1562],
        [ 422237.9375,  233202.6406,  198824.9375,  193399.4531,  190632.7344,
          156800.4844,  150988.9688,  145814.0000,  142027.4062,  139273.1719],
        [ 554488.0000,  541500.7500,  489462.3125,  380721.3438,  325763.7500,
          320880.0312,  318457.3125,  317944.4688,  316642.0938,  312796.4375],
        [ 376179.5938,  186332.7344,  180631.5625,  177913.9062,  174842.8594,
          167753.1250,  153196.1094,  143669.0938,  143074.3125,  138249.8438],
        [ 448877.9375,  432935.4688,  375616.4062,  304264.9688,  273225.0625,
          263186.0938,  250439.5938,  246970.6094,  232920.5781,  206674.6719],
        [ 874834.8125,  580596.0625,  553236.1875,  479266.1875,  436818.5625,
          379081.5625,  363756.2500,  318779.6875,  310234.1250,  276863.4062],
        [ 659102.8750,  387368.3125,  325292.8438,  316395.1562,  311449.5312,
          287550.7812,  268577.1562,  260751.4844,  260662.4688,  257123.9375],
        [ 879020.5000,  694589.6250,  648916.0000,  631041.7500,  621628.1875,
          621628.1875,  618066.6250,  582354.5000,  574966.8750,  541452.2500],
        [ 711548.4375,  524797.0000,  490093.3750,  469114.5938,  452650.1875,
          451251.0938,  443801.5625,  430226.0000,  427237.9688,  421799.2812],
        [ 767360.2500,  536199.5625,  445701.7500,  396005.7500,  347742.4062,
          303234.5938,  301902.9062,  292243.1562,  281227.4062,  272987.5312],
        [ 803929.7500,  437118.1875,  385964.1250,  300726.2188,  269279.9062,
          262533.8125,  260737.2969,  258117.5000,  252865.7031,  235257.0312],
        [ 721487.8750,  381848.5625,  348092.4375,  346313.0312,  340436.7500,
          278995.2188,  276356.4062,  263021.2188,  230096.5625,  222597.1719],
        [ 580145.5000,  214673.0938,  214348.2344,  197076.8594,  179188.4531,
          171542.2188,  171336.8750,  165859.7812,  148686.5156,  129881.0078],
        [ 483473.5000,  467180.5312,  445592.9375,  413541.9375,  394186.2188,
          376368.0312,  374686.2188,  364361.4062,  363823.1875,  356795.0625],
        [ 199896.6875,  121679.3594,  116955.7500,  104541.8281,   98181.3672,
           96584.2031,   94854.3906,   94693.8750,   90454.2578,   85929.4922],
        [ 381394.0312,  330023.6875,  324040.0938,  294432.1562,  284418.6875,
          267941.6875,  250664.4531,  232322.7031,  227898.5000,  227063.9219],
        [ 284230.5312,  277305.2188,  185435.0156,  181148.2344,  178675.3281,
          178560.6875,  162031.0938,  160837.9375,  152261.8750,  149726.8281]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[461995.8125,      0.0000],
         [236033.7031,      0.0000],
         [224341.9219,      0.0000],
         ...,
         [179565.1406,      0.0000],
         [170460.8438,      0.0000],
         [160546.7656,      0.0000]],

        [[944851.7500,      0.0000],
         [897076.8125,      0.0000],
         [886923.2500,      0.0000],
         ...,
         [843726.8750,      0.0000],
         [827273.5625,      0.0000],
         [798456.6875,      0.0000]],

        [[601440.1250,      0.0000],
         [521266.4688,      0.0000],
         [487371.8438,      0.0000],
         ...,
         [324179.4688,      0.0000],
         [288037.0938,      0.0000],
         [     0.0000, 274363.7500]],

        ...,

        [[     0.0000, 199896.6875],
         [121679.3594,      0.0000],
         [116955.7500,      0.0000],
         ...,
         [ 94693.8750,      0.0000],
         [ 90454.2578,      0.0000],
         [ 85929.4922,      0.0000]],

        [[381394.0312,      0.0000],
         [     0.0000, 330023.6875],
         [324040.0938,      0.0000],
         ...,
         [     0.0000, 232322.7031],
         [     0.0000, 227898.5000],
         [227063.9219,      0.0000]],

        [[     0.0000, 284230.5312],
         [     0.0000, 277305.2188],
         [185435.0156,      0.0000],
         ...,
         [160837.9375,      0.0000],
         [152261.8750,      0.0000],
         [149726.8281,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[2036074.3750,  194487.3750],
        [8663474.0000,       0.0000],
        [3697583.5000,  274363.7500],
        [5456087.5000,       0.0000],
        [ 584778.3125, 1039468.8750],
        [1653154.5000,  687559.5625],
        [3027441.5000, 2010924.0000],
        [3919512.0000,       0.0000],
        [4874434.5000,  741537.0625],
        [4721521.0000,       0.0000],
        [6656025.0000,       0.0000],
        [3318994.2500,       0.0000],
        [5502810.5000,       0.0000],
        [9917660.0000,       0.0000],
        [7265404.0000,       0.0000],
        [5866172.0000,       0.0000],
        [9631358.0000,       0.0000],
        [8775848.0000,       0.0000],
        [6409502.0000,       0.0000],
        [5585410.0000,       0.0000],
        [9132029.0000,       0.0000],
        [6547455.5000,       0.0000],
        [7998412.5000,       0.0000],
        [7470225.5000,       0.0000],
        [5282818.0000,       0.0000],
        [6682343.5000,       0.0000],
        [6576815.0000,       0.0000],
        [7508478.0000,       0.0000],
        [7492858.5000,       0.0000],
        [7017465.5000,       0.0000],
        [8952125.0000,       0.0000],
        [6067498.0000,       0.0000],
        [2746692.5000,  223967.6250],
        [2992815.5000, 1929224.3750],
        [3354874.0000, 2521029.5000],
        [2834369.5000, 2039904.2500],
        [1859558.5000, 1317272.0000],
        [2860585.5000, 2199858.7500],
        [1795729.5000, 2268090.5000],
        [9309784.0000,       0.0000],
        [3448563.0000,       0.0000],
        [3733717.2500,       0.0000],
        [3308524.5000,       0.0000],
        [3470013.2500,       0.0000],
        [1131411.0000, 1318075.5000],
        [ 481907.4062,  462367.9688],
        [4886439.0000,  589593.5000],
        [1037476.5000,  233261.5938],
        [ 811146.7500, 1162055.0000],
        [2136040.5000, 1742616.1250],
        [1520854.8750,  320988.2188],
        [1880072.7500, 1155038.5000],
        [2298453.2500, 2275013.7500],
        [2946906.0000,  387368.3125],
        [6413664.0000,       0.0000],
        [4353405.0000,  469114.5938],
        [1765672.0000, 2178933.2500],
        [2380913.5000, 1085616.1250],
        [2109568.5000, 1299676.8750],
        [ 897513.6250, 1275224.8750],
        [3626467.0000,  413541.9375],
        [ 903874.5000,  199896.6875],
        [1745536.3750, 1074663.6250],
        [ 810292.7500, 1099920.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 57.8125
Top1 accuracy for validation set is 57.8125 size is torch.Size([64, 1])
Epoch 221/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:03, 58.04s/it]  7%|▋         | 2/30 [00:59<11:36, 24.87s/it] 10%|█         | 3/30 [01:00<06:14, 13.86s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.68s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.82s/it] 20%|██        | 6/30 [01:02<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.8712920268376667
Epoch 222/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:00, 60.03s/it]  7%|▋         | 2/30 [01:02<12:10, 26.11s/it] 10%|█         | 3/30 [01:03<06:32, 14.53s/it] 13%|█▎        | 4/30 [01:04<04:06,  9.50s/it] 17%|█▋        | 5/30 [01:05<02:38,  6.34s/it] 20%|██        | 6/30 [01:06<01:46,  4.44s/it] 23%|██▎       | 7/30 [01:07<01:14,  3.23s/it] 27%|██▋       | 8/30 [01:07<00:53,  2.44s/it] 30%|███       | 9/30 [01:08<00:40,  1.91s/it] 33%|███▎      | 10/30 [01:09<00:31,  1.55s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.31s/it] 40%|████      | 12/30 [01:10<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 2.853870670000712
Epoch 223/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:02, 60.07s/it]  7%|▋         | 2/30 [01:00<11:44, 25.18s/it] 10%|█         | 3/30 [01:01<06:18, 14.02s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.863340393702189
Epoch 224/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:13, 60.45s/it]  7%|▋         | 2/30 [01:03<12:20, 26.45s/it] 10%|█         | 3/30 [01:03<06:39, 14.78s/it] 13%|█▎        | 4/30 [01:04<04:00,  9.24s/it] 17%|█▋        | 5/30 [01:05<02:34,  6.18s/it] 20%|██        | 6/30 [01:06<01:44,  4.33s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.39s/it] 30%|███       | 9/30 [01:08<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 2.813223441441854
Epoch 225/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:14, 56.35s/it]  7%|▋         | 2/30 [00:58<11:19, 24.26s/it] 10%|█         | 3/30 [01:00<06:29, 14.44s/it] 13%|█▎        | 4/30 [01:01<03:54,  9.03s/it] 17%|█▋        | 5/30 [01:02<02:31,  6.05s/it] 20%|██        | 6/30 [01:03<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:03<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.35s/it] 30%|███       | 9/30 [01:05<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.28s/it] 40%|████      | 12/30 [01:07<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.862348429361979
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0240,  0.0082, -0.0050,  ...,  0.0089, -0.0101,  0.0188],
        [-0.0038,  0.0259,  0.0126,  ...,  0.0086, -0.0181, -0.0129],
        [-0.0399, -0.0309,  0.0093,  ...,  0.0753, -0.0296, -0.0287],
        ...,
        [ 0.0276, -0.0182, -0.0137,  ..., -0.0251, -0.0340, -0.0147],
        [-0.0289, -0.0042, -0.0126,  ..., -0.0062,  0.0113, -0.0131],
        [-0.0372, -0.0068,  0.0138,  ...,  0.0184,  0.0192, -0.0538]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9118, 0.8640, 0.8627, 0.8606, 0.8537, 0.8512, 0.8492, 0.8486, 0.8425,
         0.8398],
        [0.9612, 0.9586, 0.9583, 0.9579, 0.9555, 0.9549, 0.9537, 0.9533, 0.9517,
         0.9503],
        [0.9292, 0.9203, 0.9166, 0.8992, 0.8959, 0.8959, 0.8844, 0.8832, 0.8764,
         0.8725],
        [0.9622, 0.9245, 0.9241, 0.9223, 0.9209, 0.9203, 0.9135, 0.9077, 0.9048,
         0.9036],
        [0.8739, 0.8576, 0.8567, 0.8393, 0.8368, 0.8348, 0.8238, 0.8124, 0.8093,
         0.8061],
        [0.8899, 0.8749, 0.8669, 0.8669, 0.8669, 0.8566, 0.8562, 0.8409, 0.8382,
         0.8378],
        [0.9401, 0.9365, 0.9364, 0.9123, 0.9087, 0.9055, 0.9052, 0.9051, 0.9000,
         0.8994],
        [0.9394, 0.9281, 0.9244, 0.8945, 0.8857, 0.8820, 0.8817, 0.8698, 0.8652,
         0.8650],
        [0.9440, 0.9407, 0.9248, 0.9226, 0.9212, 0.9194, 0.9183, 0.9160, 0.9152,
         0.9146],
        [0.9591, 0.9183, 0.9182, 0.9080, 0.9078, 0.9068, 0.9041, 0.8981, 0.8972,
         0.8925],
        [0.9676, 0.9415, 0.9413, 0.9357, 0.9326, 0.9304, 0.9303, 0.9291, 0.9248,
         0.9241],
        [0.9235, 0.9193, 0.8985, 0.8978, 0.8792, 0.8791, 0.8790, 0.8555, 0.8538,
         0.8327],
        [0.9634, 0.9319, 0.9299, 0.9256, 0.9238, 0.9196, 0.9094, 0.9059, 0.9056,
         0.9033],
        [0.9762, 0.9682, 0.9665, 0.9663, 0.9658, 0.9646, 0.9639, 0.9616, 0.9615,
         0.9609],
        [0.9512, 0.9456, 0.9450, 0.9443, 0.9438, 0.9435, 0.9429, 0.9407, 0.9399,
         0.9391],
        [0.9378, 0.9346, 0.9336, 0.9289, 0.9272, 0.9264, 0.9262, 0.9244, 0.9240,
         0.9234],
        [0.9811, 0.9722, 0.9682, 0.9671, 0.9616, 0.9576, 0.9574, 0.9564, 0.9519,
         0.9518],
        [0.9727, 0.9647, 0.9596, 0.9556, 0.9536, 0.9535, 0.9529, 0.9526, 0.9517,
         0.9507],
        [0.9515, 0.9510, 0.9476, 0.9392, 0.9392, 0.9253, 0.9222, 0.9190, 0.9159,
         0.9158],
        [0.9377, 0.9322, 0.9264, 0.9257, 0.9236, 0.9224, 0.9223, 0.9192, 0.9191,
         0.9188],
        [0.9781, 0.9664, 0.9655, 0.9635, 0.9571, 0.9550, 0.9546, 0.9515, 0.9509,
         0.9491],
        [0.9523, 0.9440, 0.9426, 0.9396, 0.9323, 0.9319, 0.9311, 0.9288, 0.9259,
         0.9245],
        [0.9766, 0.9580, 0.9557, 0.9535, 0.9513, 0.9508, 0.9426, 0.9373, 0.9355,
         0.9317],
        [0.9562, 0.9553, 0.9508, 0.9484, 0.9470, 0.9432, 0.9416, 0.9404, 0.9367,
         0.9359],
        [0.9719, 0.9288, 0.9254, 0.9222, 0.9200, 0.9007, 0.9005, 0.9004, 0.8993,
         0.8985],
        [0.9502, 0.9481, 0.9427, 0.9381, 0.9374, 0.9350, 0.9346, 0.9319, 0.9309,
         0.9286],
        [0.9560, 0.9516, 0.9412, 0.9406, 0.9355, 0.9295, 0.9272, 0.9268, 0.9258,
         0.9228],
        [0.9668, 0.9564, 0.9560, 0.9480, 0.9470, 0.9390, 0.9368, 0.9366, 0.9364,
         0.9362],
        [0.9621, 0.9545, 0.9500, 0.9470, 0.9435, 0.9425, 0.9407, 0.9398, 0.9378,
         0.9374],
        [0.9536, 0.9511, 0.9422, 0.9394, 0.9386, 0.9381, 0.9376, 0.9376, 0.9359,
         0.9353],
        [0.9707, 0.9630, 0.9624, 0.9583, 0.9577, 0.9572, 0.9561, 0.9557, 0.9531,
         0.9507],
        [0.9518, 0.9410, 0.9408, 0.9321, 0.9310, 0.9301, 0.9250, 0.9214, 0.9145,
         0.9142],
        [0.9072, 0.8979, 0.8859, 0.8813, 0.8791, 0.8758, 0.8754, 0.8683, 0.8672,
         0.8633],
        [0.9379, 0.9303, 0.9228, 0.9135, 0.9133, 0.9122, 0.9106, 0.9071, 0.9052,
         0.9007],
        [0.9481, 0.9396, 0.9360, 0.9328, 0.9295, 0.9226, 0.9198, 0.9185, 0.9179,
         0.9178],
        [0.9312, 0.9304, 0.9267, 0.9128, 0.9125, 0.9111, 0.9083, 0.9053, 0.9024,
         0.8990],
        [0.9501, 0.8860, 0.8803, 0.8778, 0.8731, 0.8715, 0.8697, 0.8690, 0.8651,
         0.8641],
        [0.9583, 0.9425, 0.9247, 0.9191, 0.9069, 0.9004, 0.8989, 0.8970, 0.8963,
         0.8961],
        [0.9250, 0.9147, 0.9132, 0.9130, 0.9103, 0.9037, 0.8858, 0.8843, 0.8797,
         0.8778],
        [0.9685, 0.9619, 0.9612, 0.9612, 0.9600, 0.9596, 0.9591, 0.9586, 0.9580,
         0.9569],
        [0.9095, 0.9071, 0.9043, 0.9035, 0.8927, 0.8832, 0.8799, 0.8748, 0.8724,
         0.8675],
        [0.9229, 0.9051, 0.9031, 0.8990, 0.8934, 0.8913, 0.8874, 0.8859, 0.8848,
         0.8835],
        [0.9076, 0.9055, 0.9041, 0.8957, 0.8838, 0.8804, 0.8800, 0.8788, 0.8705,
         0.8698],
        [0.9284, 0.9117, 0.8990, 0.8932, 0.8913, 0.8762, 0.8732, 0.8720, 0.8708,
         0.8689],
        [0.8844, 0.8768, 0.8723, 0.8707, 0.8698, 0.8687, 0.8577, 0.8542, 0.8514,
         0.8496],
        [0.8485, 0.8339, 0.8230, 0.8013, 0.7908, 0.7887, 0.7854, 0.7668, 0.7613,
         0.7608],
        [0.9470, 0.9348, 0.9300, 0.9203, 0.9201, 0.9200, 0.9194, 0.9183, 0.9149,
         0.9125],
        [0.8561, 0.8378, 0.8308, 0.8260, 0.8249, 0.8213, 0.7959, 0.7956, 0.7933,
         0.7909],
        [0.9053, 0.8637, 0.8508, 0.8468, 0.8466, 0.8333, 0.8322, 0.8310, 0.8288,
         0.8274],
        [0.9227, 0.9226, 0.9136, 0.8975, 0.8844, 0.8837, 0.8828, 0.8822, 0.8819,
         0.8818],
        [0.8992, 0.8491, 0.8485, 0.8466, 0.8441, 0.8423, 0.8362, 0.8349, 0.8320,
         0.8288],
        [0.9100, 0.9077, 0.8949, 0.8809, 0.8751, 0.8699, 0.8664, 0.8656, 0.8629,
         0.8535],
        [0.9560, 0.9293, 0.9252, 0.9130, 0.9101, 0.8990, 0.8933, 0.8861, 0.8845,
         0.8738],
        [0.9377, 0.8986, 0.8869, 0.8867, 0.8855, 0.8805, 0.8742, 0.8717, 0.8713,
         0.8709],
        [0.9573, 0.9407, 0.9356, 0.9338, 0.9327, 0.9324, 0.9324, 0.9281, 0.9258,
         0.9226],
        [0.9412, 0.9204, 0.9145, 0.9119, 0.9094, 0.9092, 0.9081, 0.9064, 0.9049,
         0.9048],
        [0.9476, 0.9221, 0.9085, 0.9008, 0.8937, 0.8819, 0.8814, 0.8790, 0.8739,
         0.8739],
        [0.9512, 0.9077, 0.9009, 0.8835, 0.8744, 0.8742, 0.8715, 0.8707, 0.8701,
         0.8650],
        [0.9429, 0.8986, 0.8914, 0.8907, 0.8902, 0.8783, 0.8759, 0.8704, 0.8635,
         0.8616],
        [0.9273, 0.8573, 0.8570, 0.8520, 0.8443, 0.8414, 0.8408, 0.8397, 0.8326,
         0.8245],
        [0.9138, 0.9115, 0.9084, 0.9050, 0.8997, 0.8965, 0.8963, 0.8946, 0.8941,
         0.8926],
        [0.8564, 0.8151, 0.8138, 0.8041, 0.8041, 0.8002, 0.7999, 0.7967, 0.7957,
         0.7949],
        [0.8997, 0.8874, 0.8856, 0.8794, 0.8781, 0.8706, 0.8679, 0.8632, 0.8590,
         0.8587],
        [0.8752, 0.8733, 0.8456, 0.8450, 0.8448, 0.8421, 0.8352, 0.8330, 0.8318,
         0.8276]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 1, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 1, 1, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 1, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 0, 1],
        [0, 1, 1, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 1, 0, 1],
        [1, 1, 1, 1, 0, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 453898.1562,  229279.0938,  225175.9688,  218506.6719,  197889.5156,
          190915.8125,  185566.4531,  184031.4375,  168629.7031,  162333.4688],
        [ 919193.2500,  886034.6875,  882106.6875,  876760.8125,  847296.5000,
          840765.3750,  825618.4375,  821692.4375,  802624.3750,  787134.7500],
        [ 582130.6875,  512345.4062,  485894.6875,  379338.3438,  361655.9375,
          361496.9688,  307072.4688,  301690.2188,  273712.7500,  258905.4375],
        [ 932886.1250,  544149.7500,  540884.0000,  527302.9375,  516861.3750,
          512578.5000,  464979.4062,  428004.6562,  410532.7500,  403667.2500],
        [ 264082.3750,  209199.8438,  206572.7969,  161230.6250,  155490.6562,
          151056.2344,  129129.4844,  109702.0234,  104967.3125,  100254.1641],
        [ 332037.5000,  267981.0625,  239071.6562,  239029.4844,  239023.3281,
          206346.7500,  205039.8906,  164888.2656,  158591.1562,  157647.5000],
        [ 679730.0625,  645802.6250,  645111.9375,  457294.9688,  434460.0312,
          414699.5000,  412891.3438,  412578.3750,  383773.6250,  380293.1250],
        [ 672944.6250,  572952.0625,  543378.1250,  354444.1250,  312554.3125,
          296599.4062,  295435.7500,  249292.7188,  233154.6094,  232765.1406],
        [ 718758.6875,  686158.3750,  546347.1875,  529394.5000,  518917.4688,
          505854.6562,  497805.5000,  482115.6875,  476199.0625,  472651.0000],
        [ 892118.3125,  498380.2500,  497291.1562,  430016.3750,  428996.8438,
          422461.5000,  406493.9062,  373234.3750,  368410.0312,  344594.2812],
        [1007168.8125,  693750.8750,  691452.9375,  638473.6250,  610802.4375,
          591758.8125,  590859.9375,  581359.5625,  546493.1250,  541241.6250],
        [ 536496.7500,  505611.5625,  375231.1875,  371732.0625,  284996.2188,
          284386.9688,  284105.5938,  203024.6406,  198232.9062,  146560.4219],
        [ 948413.1875,  604747.8125,  587990.8750,  553148.0625,  539039.4375,
          507803.9688,  438651.6250,  417391.9688,  415377.0938,  402030.6250],
        [1139190.5000, 1015753.7500,  991190.0625,  988416.7500,  981694.2500,
          964844.4375,  955535.5000,  924356.9375,  923006.5000,  915621.9375],
        [ 796698.8125,  735971.3125,  729833.1875,  722296.8125,  717475.3125,
          713906.3125,  707990.6875,  685667.1250,  678172.8125,  670599.2500],
        [ 657966.1250,  628691.3125,  620034.3125,  579565.4375,  565260.9375,
          559067.7500,  557711.9375,  543134.5625,  540175.7500,  535428.4375],
        [1222139.1250, 1076468.5000, 1015656.8125, 1000368.3125,  925258.3125,
          872869.6875,  871157.4375,  858754.6250,  805154.3125,  804017.1875],
        [1083486.5000,  966034.0000,  898482.6250,  848603.3125,  825255.5625,
          824029.4375,  816824.2500,  813584.6250,  802203.4375,  791313.5625],
        [ 800710.7500,  795071.5000,  757016.1875,  671382.5000,  671084.8125,
          550494.6875,  526389.5000,  503072.9375,  480979.5312,  480463.7812],
        [ 657019.9375,  607471.5000,  559107.1875,  553673.1875,  537485.6250,
          528249.1875,  527633.9375,  504437.8125,  503606.7188,  501950.5938],
        [1169927.2500,  989688.2500,  978031.1875,  950617.2500,  866504.1250,
          841736.1250,  836222.5625,  800900.1250,  793183.4375,  773206.1250],
        [ 809034.3125,  718703.1875,  704975.6875,  675009.8125,  608142.1250,
          605274.5625,  598066.5625,  578840.1875,  554876.3125,  544164.2500],
        [1145025.3750,  878229.5000,  849622.8750,  823725.4375,  798552.6250,
          792682.1250,  704389.6250,  653407.0000,  636512.9375,  603143.7500],
        [ 856342.8750,  845523.0625,  792604.2500,  765237.2500,  750184.3750,
          710714.9375,  694574.4375,  683411.5625,  647783.8750,  640918.6875],
        [1071808.6250,  578443.9375,  551052.5000,  526368.3750,  510641.5000,
          387298.8750,  386349.6875,  385953.0625,  379962.9062,  375426.9688],
        [ 786192.4375,  762379.6250,  706244.1250,  660945.3125,  654115.9375,
          632504.6250,  628513.8750,  604625.0000,  596099.2500,  577404.5000],
        [ 854076.3125,  801963.2500,  691047.5000,  684553.1250,  636429.1250,
          584136.6250,  566037.2500,  562250.1875,  554521.8750,  531537.9375],
        [ 995754.8125,  858155.3750,  852933.5625,  761228.1250,  750469.1250,
          669857.8125,  648372.8750,  646914.0000,  644892.3750,  643649.3750],
        [ 930978.0000,  835635.0000,  783468.0625,  750265.8750,  714201.8750,
          703855.1250,  686142.6875,  677472.0625,  657724.0000,  654236.9375],
        [ 825106.8125,  795585.7500,  700652.5625,  672861.1875,  666023.0625,
          660976.8125,  656406.8125,  656200.2500,  640652.9375,  635174.6250],
        [1052873.8750,  943711.6875,  934725.1250,  881634.8125,  874096.7500,
          867717.2500,  854623.0000,  849300.5000,  818709.9375,  791700.0000],
        [ 803450.7500,  688664.6250,  686866.1250,  607034.2500,  597239.6250,
          589544.0000,  548079.8125,  520793.9375,  471958.6562,  469888.7188],
        [ 425062.5000,  372011.2188,  313391.5312,  293738.5938,  284640.4062,
          271277.0000,  269698.8125,  243690.2344,  239956.7812,  226883.6094],
        [ 658740.9375,  591547.2500,  531289.6250,  465240.2500,  463838.9688,
          456340.0312,  446238.9062,  424556.0625,  412819.2812,  387216.8750],
        [ 762309.1250,  674910.6875,  641198.7500,  613014.6875,  584218.5625,
          529840.9375,  508716.6562,  499356.5625,  495268.3125,  494614.5625],
        [ 598632.0625,  591908.9375,  561824.6250,  460165.7188,  458584.6250,
          449528.6875,  431708.9062,  413705.2500,  397056.6562,  378139.9062],
        [ 784113.8125,  314194.7500,  289574.0000,  279273.6875,  261255.5156,
          255281.6875,  248716.6094,  246359.4531,  232947.0156,  229651.9844],
        [ 881560.8125,  703946.4375,  545924.8125,  504021.3750,  422980.3438,
          385483.3438,  377493.1562,  367625.2500,  363781.9062,  362665.1562],
        [ 547965.8750,  472875.0625,  463338.5000,  461550.1562,  444117.8438,
          404555.1875,  313313.5312,  306578.2500,  286795.4375,  279167.6875],
        [1020882.3125,  928970.0625,  920001.0000,  919265.1875,  903348.1875,
          898489.5625,  892691.0625,  885383.4375,  877702.0000,  864739.1875],
        [ 439310.1250,  424339.1250,  407606.0938,  403170.9375,  345325.2812,
          301632.1250,  287955.2812,  267393.3750,  258526.9375,  241104.8594],
        [ 532107.5000,  412380.1250,  400759.7500,  377878.5312,  348781.6250,
          338833.7812,  320554.5625,  313566.4375,  308667.9375,  303013.7500],
        [ 427660.2812,  414686.8438,  406552.8125,  360770.5938,  304428.0938,
          289937.6562,  288167.0938,  283392.0312,  251702.8750,  249127.2969],
        [ 575194.5000,  453392.8438,  377937.2812,  347988.2188,  338754.9375,
          273095.8438,  261405.5469,  257007.7344,  252720.5625,  246026.2969],
        [ 306741.7188,  275219.6562,  258063.8438,  252256.3281,  249028.4688,
          245208.3438,  209544.2656,  199339.8438,  191676.2031,  186781.7812],
        [ 183849.0000,  149113.0938,  127739.7812,   93583.7812,   80580.6016,
           78263.4219,   74559.8516,   57180.4961,   52881.7969,   52528.3477],
        [ 750226.6250,  630748.1875,  588381.8750,  512885.1250,  511138.9688,
          510219.9375,  506216.0938,  497975.5000,  474690.0625,  458322.2812],
        [ 204769.4531,  157616.0781,  142725.0312,  133252.2500,  131164.8594,
          124529.8516,   86713.3594,   86270.2500,   83544.4141,   80709.5078],
        [ 413774.6875,  228263.9219,  189778.0000,  179442.5625,  178858.5938,
          147945.2031,  145524.2188,  143055.0781,  138720.9219,  135879.6250],
        [ 530522.5000,  529329.8750,  465845.8125,  369875.2188,  307068.9375,
          304041.3438,  300044.1250,  297259.7500,  295943.3125,  295540.8750],
        [ 379368.0000,  185397.1875,  183885.6562,  178843.4219,  172676.3906,
          168135.2656,  154205.1719,  151321.6719,  145266.7188,  138745.7969],
        [ 442125.3125,  428193.6875,  356655.9375,  291961.2500,  268851.9062,
          249553.8750,  237290.8281,  234632.7969,  225880.5625,  197321.5156],
        [ 853775.0625,  583265.5000,  550044.4375,  461566.4375,  443124.4688,
          377910.2500,  348573.1250,  314255.8750,  307269.5938,  263880.7500],
        [ 656821.3750,  376117.5625,  317934.7500,  317171.8750,  311633.7500,
          290370.4375,  265396.5000,  256025.5469,  254540.9531,  252897.2969],
        [ 870088.0000,  685977.1250,  637455.7500,  621831.0000,  611658.7500,
          609662.3125,  609662.3125,  573343.4375,  554671.0625,  529757.0625],
        [ 690484.3125,  513465.0312,  471487.2188,  454411.4062,  438767.9375,
          437322.5000,  430809.8438,  419983.7500,  411612.3750,  411019.2812],
        [ 756959.1250,  526136.5000,  433251.8438,  388130.8125,  350312.7188,
          296176.0000,  293956.0625,  283935.7500,  264030.5312,  264019.2188],
        [ 796524.1250,  427923.4688,  388541.5312,  302808.9375,  266204.8750,
          265419.7812,  255302.8594,  252246.6875,  250183.2188,  232772.9062],
        [ 707801.6875,  375658.3438,  339018.0000,  335751.5625,  333473.4375,
          281474.2500,  271707.0625,  251093.1875,  227596.8125,  221474.8750],
        [ 566058.3125,  208309.1406,  207600.1406,  193115.8125,  173185.5000,
          166117.0156,  164724.8125,  162165.8906,  146429.5156,  130427.7812],
        [ 466960.9375,  452091.0625,  432143.4688,  411694.0625,  381736.0625,
          364768.8750,  363792.3125,  355043.2812,  352547.1875,  344963.1875],
        [ 205674.4375,  114105.7969,  111971.8984,   97509.2422,   97388.5156,
           92230.8281,   91732.9297,   87730.2344,   86432.6484,   85464.3828],
        [ 382126.1562,  320465.9375,  312352.5625,  285615.2188,  280422.5938,
          252031.9531,  242416.5156,  226564.9062,  213390.2188,  212746.5156],
        [ 269103.5312,  261964.0625,  176225.6094,  174741.6719,  174214.3594,
          167804.3281,  151957.2500,  147168.9844,  144766.2188,  136306.2656]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[453898.1562,      0.0000],
         [229279.0938,      0.0000],
         [225175.9688,      0.0000],
         ...,
         [184031.4375,      0.0000],
         [168629.7031,      0.0000],
         [162333.4688,      0.0000]],

        [[919193.2500,      0.0000],
         [886034.6875,      0.0000],
         [882106.6875,      0.0000],
         ...,
         [821692.4375,      0.0000],
         [802624.3750,      0.0000],
         [787134.7500,      0.0000]],

        [[582130.6875,      0.0000],
         [512345.4062,      0.0000],
         [485894.6875,      0.0000],
         ...,
         [301690.2188,      0.0000],
         [273712.7500,      0.0000],
         [     0.0000, 258905.4375]],

        ...,

        [[     0.0000, 205674.4375],
         [114105.7969,      0.0000],
         [111971.8984,      0.0000],
         ...,
         [ 87730.2344,      0.0000],
         [ 86432.6484,      0.0000],
         [ 85464.3828,      0.0000]],

        [[382126.1562,      0.0000],
         [320465.9375,      0.0000],
         [     0.0000, 312352.5625],
         ...,
         [     0.0000, 226564.9062],
         [213390.2188,      0.0000],
         [     0.0000, 212746.5156]],

        [[     0.0000, 269103.5312],
         [     0.0000, 261964.0625],
         [     0.0000, 176225.6094],
         ...,
         [147168.9844,      0.0000],
         [144766.2188,      0.0000],
         [136306.2656,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[2018336.8750,  197889.5156],
        [8489227.0000,       0.0000],
        [3565337.5000,  258905.4375],
        [5281846.5000,       0.0000],
        [ 568031.6875, 1023653.7500],
        [1554139.7500,  655516.9375],
        [2931137.5000, 1935498.2500],
        [3763521.0000,       0.0000],
        [4715443.5000,  718758.6875],
        [4661997.0000,       0.0000],
        [6493362.0000,       0.0000],
        [3190378.2500,       0.0000],
        [5414594.5000,       0.0000],
        [9799611.0000,       0.0000],
        [7158612.0000,       0.0000],
        [5787036.5000,       0.0000],
        [9451844.0000,       0.0000],
        [8669818.0000,       0.0000],
        [6236666.5000,       0.0000],
        [5480635.5000,       0.0000],
        [9000016.0000,       0.0000],
        [6397087.0000,       0.0000],
        [7885291.5000,       0.0000],
        [7387296.0000,       0.0000],
        [5153306.5000,       0.0000],
        [6609025.0000,       0.0000],
        [6466553.0000,       0.0000],
        [7472227.5000,       0.0000],
        [7393980.0000,       0.0000],
        [6909641.0000,       0.0000],
        [8869093.0000,       0.0000],
        [5983521.0000,       0.0000],
        [2713467.0000,  226883.6094],
        [2937483.5000, 1900344.3750],
        [3287395.7500, 2516053.0000],
        [2744074.0000, 1997181.2500],
        [1834733.7500, 1306634.7500],
        [2784050.5000, 2131432.0000],
        [1747170.2500, 2233087.0000],
        [9111472.0000,       0.0000],
        [3376364.2500,       0.0000],
        [3656543.7500,       0.0000],
        [3276425.5000,       0.0000],
        [3383523.7500,       0.0000],
        [ 898108.3750, 1475752.1250],
        [ 489578.3125,  460701.8750],
        [4852422.5000,  588381.8750],
        [1005025.6250,  226269.4375],
        [ 778425.9375, 1122816.8750],
        [1729119.7500, 1966352.0000],
        [1401156.3750,  456688.9375],
        [1793296.7500, 1139171.0000],
        [2511060.7500, 1992604.6250],
        [2922792.5000,  376117.5625],
        [5774349.5000,  529757.0625],
        [4224952.0000,  454411.4062],
        [1731902.0000, 2125006.7500],
        [2356043.5000, 1081884.7500],
        [1854702.5000, 1490346.8750],
        [ 876137.3750, 1241996.5000],
        [3514046.5000,  411694.0625],
        [ 864566.5000,  205674.4375],
        [1690853.3750, 1037279.1875],
        [ 754413.0625, 1049839.1250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 59.375
Top1 accuracy for validation set is 59.375 size is torch.Size([64, 1])
Epoch 226/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:07<32:41, 67.64s/it]  7%|▋         | 2/30 [01:08<13:12, 28.29s/it] 10%|█         | 3/30 [01:09<07:05, 15.76s/it] 13%|█▎        | 4/30 [01:09<04:15,  9.83s/it] 17%|█▋        | 5/30 [01:10<02:43,  6.56s/it] 20%|██        | 6/30 [01:11<01:49,  4.58s/it] 23%|██▎       | 7/30 [01:12<01:16,  3.33s/it] 27%|██▋       | 8/30 [01:12<00:55,  2.51s/it] 30%|███       | 9/30 [01:13<00:41,  1.96s/it] 33%|███▎      | 10/30 [01:14<00:31,  1.58s/it] 37%|███▋      | 11/30 [01:15<00:25,  1.33s/it] 40%|████      | 12/30 [01:15<00:20,  1.15s/it] 43%|████▎     | 13/30 [01:16<00:17,  1.03s/it] 47%|████▋     | 14/30 [01:17<00:15,  1.06it/s] 50%|█████     | 15/30 [01:18<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:18<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:19<00:10,  1.23it/s] 60%|██████    | 18/30 [01:20<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:21<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:21<00:07,  1.30it/s] 70%|███████   | 21/30 [01:22<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:23<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:24<00:05,  1.32it/s] 80%|████████  | 24/30 [01:24<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:25<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:26<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:27<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:27<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:28<00:00,  1.34it/s]100%|██████████| 30/30 [01:29<00:00,  1.34it/s]100%|██████████| 30/30 [01:29<00:00,  2.99s/it]
Epoch loss is 2.861928606033325
Epoch 227/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:22, 60.79s/it]  7%|▋         | 2/30 [01:01<11:53, 25.47s/it] 10%|█         | 3/30 [01:02<06:22, 14.18s/it] 13%|█▎        | 4/30 [01:03<03:50,  8.88s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.95s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.8474867741266885
Epoch 228/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:07<32:24, 67.05s/it]  7%|▋         | 2/30 [01:07<13:05, 28.05s/it] 10%|█         | 3/30 [01:08<07:00, 15.58s/it] 13%|█▎        | 4/30 [01:09<04:12,  9.73s/it] 17%|█▋        | 5/30 [01:10<02:42,  6.49s/it] 20%|██        | 6/30 [01:10<01:48,  4.54s/it] 23%|██▎       | 7/30 [01:11<01:15,  3.30s/it] 27%|██▋       | 8/30 [01:12<00:54,  2.49s/it] 30%|███       | 9/30 [01:13<00:40,  1.94s/it] 33%|███▎      | 10/30 [01:13<00:31,  1.57s/it] 37%|███▋      | 11/30 [01:14<00:25,  1.32s/it] 40%|████      | 12/30 [01:15<00:20,  1.15s/it] 43%|████▎     | 13/30 [01:16<00:17,  1.03s/it] 47%|████▋     | 14/30 [01:16<00:15,  1.06it/s] 50%|█████     | 15/30 [01:17<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:18<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:19<00:10,  1.23it/s] 60%|██████    | 18/30 [01:19<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:20<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:21<00:07,  1.30it/s] 70%|███████   | 21/30 [01:22<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:22<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:23<00:05,  1.32it/s] 80%|████████  | 24/30 [01:24<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:24<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:25<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:26<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:27<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:27<00:00,  1.34it/s]100%|██████████| 30/30 [01:28<00:00,  1.34it/s]100%|██████████| 30/30 [01:28<00:00,  2.97s/it]
Epoch loss is 2.842527151107788
Epoch 229/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:39, 61.37s/it]  7%|▋         | 2/30 [01:03<12:25, 26.63s/it] 10%|█         | 3/30 [01:04<06:39, 14.81s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.26s/it] 17%|█▋        | 5/30 [01:05<02:34,  6.19s/it] 20%|██        | 6/30 [01:06<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.17s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.40s/it] 30%|███       | 9/30 [01:08<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Epoch loss is 2.7978416283925376
Epoch 230/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:12, 58.36s/it]  7%|▋         | 2/30 [01:00<11:40, 25.03s/it] 10%|█         | 3/30 [01:01<06:30, 14.48s/it] 13%|█▎        | 4/30 [01:02<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:03<02:31,  6.06s/it] 20%|██        | 6/30 [01:04<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.36s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.8336214939753215
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0245,  0.0077, -0.0036,  ...,  0.0087, -0.0091,  0.0176],
        [-0.0032,  0.0257,  0.0132,  ...,  0.0089, -0.0175, -0.0134],
        [-0.0386, -0.0312,  0.0099,  ...,  0.0762, -0.0284, -0.0281],
        ...,
        [ 0.0268, -0.0176, -0.0127,  ..., -0.0249, -0.0329, -0.0150],
        [-0.0286, -0.0043, -0.0125,  ..., -0.0057,  0.0114, -0.0129],
        [-0.0370, -0.0072,  0.0137,  ...,  0.0200,  0.0199, -0.0535]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9103, 0.8624, 0.8623, 0.8597, 0.8542, 0.8512, 0.8489, 0.8473, 0.8418,
         0.8410],
        [0.9589, 0.9579, 0.9579, 0.9568, 0.9543, 0.9533, 0.9525, 0.9504, 0.9500,
         0.9499],
        [0.9272, 0.9192, 0.9163, 0.8967, 0.8935, 0.8929, 0.8803, 0.8782, 0.8734,
         0.8690],
        [0.9609, 0.9230, 0.9220, 0.9192, 0.9191, 0.9159, 0.9120, 0.9041, 0.9003,
         0.8998],
        [0.8732, 0.8563, 0.8549, 0.8377, 0.8346, 0.8323, 0.8206, 0.8136, 0.8059,
         0.8040],
        [0.8865, 0.8745, 0.8616, 0.8603, 0.8597, 0.8519, 0.8515, 0.8378, 0.8346,
         0.8334],
        [0.9369, 0.9340, 0.9334, 0.9080, 0.9071, 0.9041, 0.9041, 0.9035, 0.8959,
         0.8955],
        [0.9374, 0.9241, 0.9223, 0.8919, 0.8839, 0.8797, 0.8770, 0.8673, 0.8623,
         0.8615],
        [0.9415, 0.9377, 0.9239, 0.9211, 0.9206, 0.9163, 0.9162, 0.9134, 0.9124,
         0.9119],
        [0.9586, 0.9172, 0.9169, 0.9072, 0.9064, 0.9062, 0.9038, 0.8970, 0.8970,
         0.8907],
        [0.9666, 0.9398, 0.9392, 0.9335, 0.9295, 0.9292, 0.9280, 0.9276, 0.9235,
         0.9234],
        [0.9224, 0.9176, 0.8951, 0.8942, 0.8785, 0.8754, 0.8750, 0.8530, 0.8508,
         0.8314],
        [0.9623, 0.9307, 0.9285, 0.9251, 0.9212, 0.9185, 0.9084, 0.9059, 0.9043,
         0.9012],
        [0.9754, 0.9678, 0.9661, 0.9652, 0.9649, 0.9646, 0.9628, 0.9605, 0.9605,
         0.9597],
        [0.9498, 0.9450, 0.9443, 0.9438, 0.9432, 0.9432, 0.9413, 0.9388, 0.9385,
         0.9381],
        [0.9359, 0.9340, 0.9339, 0.9293, 0.9276, 0.9261, 0.9242, 0.9234, 0.9230,
         0.9223],
        [0.9804, 0.9715, 0.9664, 0.9663, 0.9605, 0.9559, 0.9555, 0.9543, 0.9508,
         0.9505],
        [0.9722, 0.9643, 0.9592, 0.9550, 0.9527, 0.9526, 0.9520, 0.9516, 0.9497,
         0.9494],
        [0.9502, 0.9489, 0.9471, 0.9360, 0.9342, 0.9235, 0.9211, 0.9163, 0.9149,
         0.9148],
        [0.9365, 0.9311, 0.9253, 0.9230, 0.9217, 0.9212, 0.9198, 0.9179, 0.9178,
         0.9177],
        [0.9769, 0.9653, 0.9650, 0.9633, 0.9566, 0.9556, 0.9529, 0.9502, 0.9499,
         0.9490],
        [0.9505, 0.9420, 0.9412, 0.9376, 0.9307, 0.9297, 0.9292, 0.9263, 0.9251,
         0.9235],
        [0.9761, 0.9573, 0.9553, 0.9528, 0.9503, 0.9500, 0.9423, 0.9358, 0.9342,
         0.9304],
        [0.9563, 0.9542, 0.9496, 0.9472, 0.9462, 0.9418, 0.9409, 0.9392, 0.9357,
         0.9353],
        [0.9712, 0.9250, 0.9246, 0.9209, 0.9181, 0.8996, 0.8989, 0.8986, 0.8980,
         0.8964],
        [0.9502, 0.9474, 0.9415, 0.9388, 0.9373, 0.9327, 0.9324, 0.9318, 0.9300,
         0.9275],
        [0.9550, 0.9506, 0.9415, 0.9397, 0.9350, 0.9283, 0.9259, 0.9241, 0.9241,
         0.9218],
        [0.9670, 0.9567, 0.9558, 0.9465, 0.9464, 0.9381, 0.9364, 0.9362, 0.9355,
         0.9353],
        [0.9614, 0.9542, 0.9501, 0.9464, 0.9429, 0.9412, 0.9393, 0.9388, 0.9383,
         0.9365],
        [0.9532, 0.9494, 0.9411, 0.9382, 0.9380, 0.9376, 0.9370, 0.9364, 0.9346,
         0.9334],
        [0.9698, 0.9625, 0.9617, 0.9577, 0.9572, 0.9561, 0.9555, 0.9547, 0.9526,
         0.9499],
        [0.9523, 0.9395, 0.9393, 0.9318, 0.9295, 0.9288, 0.9236, 0.9221, 0.9140,
         0.9140],
        [0.9061, 0.8962, 0.8854, 0.8793, 0.8778, 0.8755, 0.8751, 0.8686, 0.8674,
         0.8639],
        [0.9363, 0.9284, 0.9221, 0.9115, 0.9114, 0.9108, 0.9106, 0.9061, 0.9050,
         0.9006],
        [0.9480, 0.9395, 0.9335, 0.9322, 0.9292, 0.9208, 0.9186, 0.9182, 0.9173,
         0.9168],
        [0.9298, 0.9287, 0.9241, 0.9126, 0.9104, 0.9088, 0.9050, 0.9013, 0.8999,
         0.8977],
        [0.9502, 0.8843, 0.8794, 0.8767, 0.8720, 0.8718, 0.8705, 0.8698, 0.8633,
         0.8597],
        [0.9553, 0.9405, 0.9231, 0.9180, 0.9054, 0.8983, 0.8973, 0.8949, 0.8937,
         0.8918],
        [0.9238, 0.9148, 0.9101, 0.9098, 0.9093, 0.9021, 0.8849, 0.8833, 0.8785,
         0.8754],
        [0.9668, 0.9609, 0.9599, 0.9593, 0.9586, 0.9574, 0.9573, 0.9570, 0.9566,
         0.9554],
        [0.9073, 0.9053, 0.9042, 0.9034, 0.8913, 0.8823, 0.8793, 0.8730, 0.8702,
         0.8638],
        [0.9225, 0.9035, 0.9016, 0.8973, 0.8913, 0.8913, 0.8861, 0.8846, 0.8835,
         0.8826],
        [0.9079, 0.9046, 0.9036, 0.8939, 0.8833, 0.8801, 0.8796, 0.8775, 0.8691,
         0.8690],
        [0.9264, 0.9104, 0.8975, 0.8908, 0.8886, 0.8756, 0.8720, 0.8707, 0.8684,
         0.8665],
        [0.8837, 0.8749, 0.8699, 0.8678, 0.8672, 0.8668, 0.8551, 0.8528, 0.8484,
         0.8473],
        [0.8465, 0.8355, 0.8239, 0.7993, 0.7933, 0.7891, 0.7861, 0.7694, 0.7615,
         0.7612],
        [0.9453, 0.9340, 0.9299, 0.9200, 0.9199, 0.9195, 0.9190, 0.9170, 0.9146,
         0.9123],
        [0.8545, 0.8339, 0.8287, 0.8232, 0.8231, 0.8192, 0.7931, 0.7923, 0.7917,
         0.7912],
        [0.9028, 0.8616, 0.8477, 0.8421, 0.8417, 0.8315, 0.8297, 0.8289, 0.8275,
         0.8257],
        [0.9216, 0.9189, 0.9100, 0.8958, 0.8809, 0.8803, 0.8793, 0.8790, 0.8781,
         0.8770],
        [0.8998, 0.8498, 0.8497, 0.8475, 0.8424, 0.8415, 0.8380, 0.8362, 0.8333,
         0.8289],
        [0.9093, 0.9067, 0.8913, 0.8778, 0.8743, 0.8666, 0.8624, 0.8620, 0.8610,
         0.8502],
        [0.9543, 0.9296, 0.9249, 0.9109, 0.9108, 0.8984, 0.8905, 0.8851, 0.8840,
         0.8731],
        [0.9370, 0.8971, 0.8873, 0.8859, 0.8844, 0.8807, 0.8737, 0.8706, 0.8699,
         0.8693],
        [0.9562, 0.9401, 0.9341, 0.9327, 0.9322, 0.9312, 0.9312, 0.9268, 0.9232,
         0.9213],
        [0.9392, 0.9187, 0.9118, 0.9091, 0.9079, 0.9069, 0.9051, 0.9042, 0.9027,
         0.9027],
        [0.9462, 0.9205, 0.9061, 0.8993, 0.8941, 0.8801, 0.8794, 0.8769, 0.8715,
         0.8693],
        [0.9505, 0.9061, 0.9013, 0.8839, 0.8748, 0.8737, 0.8711, 0.8692, 0.8682,
         0.8645],
        [0.9414, 0.8969, 0.8910, 0.8884, 0.8866, 0.8782, 0.8748, 0.8670, 0.8627,
         0.8608],
        [0.9260, 0.8550, 0.8547, 0.8504, 0.8417, 0.8390, 0.8386, 0.8384, 0.8314,
         0.8243],
        [0.9113, 0.9092, 0.9063, 0.9044, 0.8975, 0.8947, 0.8941, 0.8928, 0.8922,
         0.8907],
        [0.8574, 0.8135, 0.8080, 0.8032, 0.8000, 0.7972, 0.7969, 0.7969, 0.7968,
         0.7920],
        [0.8996, 0.8864, 0.8821, 0.8791, 0.8752, 0.8666, 0.8656, 0.8618, 0.8574,
         0.8545],
        [0.8715, 0.8697, 0.8452, 0.8426, 0.8405, 0.8378, 0.8309, 0.8308, 0.8249,
         0.8214]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 1, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 1],
        [1, 0, 1, 1, 0, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 1, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 0, 1],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 1, 0, 1],
        [1, 1, 1, 1, 0, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 444529.2812,  224083.8438,  223786.7812,  215749.1719,  199365.3281,
          191070.4531,  184777.7969,  180596.0781,  166899.7188,  165158.6562],
        [ 890195.0625,  876998.3750,  876752.4375,  863242.0000,  832596.3125,
          820949.9375,  811634.8125,  788153.2500,  783778.8750,  781855.8125],
        [ 565312.1875,  504602.8750,  483752.9688,  365751.1875,  349516.5000,
          346348.7188,  289271.5000,  280985.3125,  262239.0312,  246216.4062],
        [ 915359.1875,  532586.7500,  525451.0625,  504205.5000,  504162.7188,
          481516.5312,  455002.9062,  406698.2812,  384924.2188,  382341.5938],
        [ 261653.2188,  205513.6562,  201413.5781,  157590.0781,  150676.1094,
          145712.2500,  123296.6094,  111623.6875,  100020.8594,   97379.4141],
        [ 316341.7500,  266319.9062,  221520.9219,  217412.0938,  215520.2969,
          192864.9531,  191827.6094,  157783.0156,  150678.4062,  148064.3281],
        [ 649322.7500,  623273.6875,  618232.8750,  430235.4375,  424436.6562,
          406860.7812,  406488.8750,  402980.2812,  361531.7812,  359495.0312],
        [ 654463.4375,  541254.0000,  527536.3125,  341493.5625,  304503.0000,
          286788.8750,  275990.8125,  240477.0625,  223816.4531,  221236.5469],
        [ 694292.2500,  657589.7500,  539457.6250,  518644.3750,  514428.1250,
          484089.4375,  483342.1250,  464381.1562,  457563.7188,  454263.6250],
        [ 885489.8750,  490315.4375,  488062.5625,  424915.3750,  420208.9375,
          419026.3750,  404726.5312,  367459.4375,  367345.2188,  335657.1562],
        [ 993161.9375,  677311.2500,  671853.2500,  618916.0000,  584407.4375,
          581710.0625,  572434.2500,  568772.0625,  536875.5000,  535914.8125],
        [ 528098.0625,  493318.6250,  357651.8750,  352855.2812,  282139.1562,
          269799.6562,  268410.5000,  195949.9531,  190020.5000,  143923.7500],
        [ 933820.7500,  594603.7500,  576631.8750,  549186.4375,  518833.8438,
          499274.6250,  432406.5000,  417235.9688,  407978.2500,  390118.0625],
        [1126064.5000, 1009934.0625,  986105.3750,  973557.8125,  969817.7500,
          964908.0000,  940479.0625,  910759.7500,  910372.5000,  900309.6875],
        [ 781473.3750,  729542.2500,  721800.3750,  716992.4375,  710761.0000,
          710486.5000,  691628.3750,  667928.1250,  664348.9375,  661389.1875],
        [ 640787.9375,  623606.0625,  622851.8125,  583000.7500,  568997.1875,
          556522.8750,  541842.1875,  536114.6875,  532652.8125,  527690.7500],
        [1209543.5000, 1065097.8750,  990737.4375,  988478.0000,  909896.8125,
          852588.7500,  847305.4375,  832712.3125,  791875.1875,  788614.9375],
        [1075894.8750,  961185.5625,  893357.0625,  841387.8125,  814716.6875,
          813213.0625,  806256.9375,  801733.8750,  779896.5000,  776986.3125],
        [ 785802.6250,  771265.5625,  751998.0625,  640965.8125,  625423.8125,
          536193.9375,  518113.4062,  484045.1250,  474201.8750,  474015.5938],
        [ 645947.3125,  598143.0625,  550840.2500,  532752.8750,  522769.4688,
          519240.7188,  508562.4062,  495395.8750,  494310.4375,  493785.0938],
        [1150296.3750,  974681.8750,  970607.9375,  947941.2500,  860541.0000,
          848671.3750,  816806.3750,  785417.5625,  782146.6250,  772464.7500],
        [ 788843.6250,  699003.3125,  690507.3125,  656599.0625,  594819.2500,
          585909.1250,  582257.3125,  558124.3125,  549270.1875,  536613.4375],
        [1138012.3750,  869095.3125,  844971.7500,  815253.7500,  787137.6875,
          783944.0625,  702317.6250,  639421.1250,  625346.2500,  592073.2500],
        [ 857688.1250,  831611.5000,  779057.1875,  752701.1875,  742333.0625,
          697259.6250,  688302.8125,  671121.9375,  638692.1875,  634616.3750],
        [1060842.1250,  548022.3125,  544769.1875,  516690.3438,  496842.2188,
          381080.5938,  377807.1875,  375919.6250,  372599.5312,  364238.4062],
        [ 785609.3125,  755156.5625,  693698.6250,  667595.0000,  653890.1250,
          612099.8750,  609608.8750,  604090.0625,  588371.8125,  567720.1875],
        [ 841586.0625,  790006.0625,  694130.0625,  676171.5000,  631922.2500,
          574765.1250,  555423.7500,  540789.6250,  540761.8125,  523813.0000],
        [ 998958.3125,  862551.5625,  850513.0000,  745112.0000,  744547.2500,
          661268.6875,  644756.4375,  643304.5000,  636607.0000,  635315.1875],
        [ 922166.2500,  831620.2500,  785030.4375,  744000.0625,  708343.2500,
          690497.4375,  672697.5625,  667515.5000,  662767.5625,  646120.5000],
        [ 819710.7500,  776296.0000,  689576.1875,  661767.0625,  660382.0000,
          656021.3125,  650603.3750,  644700.5000,  628373.6250,  618450.4375],
        [1039451.9375,  936287.3125,  925532.7500,  874132.5625,  867885.2500,
          854230.2500,  847398.3125,  837537.8750,  812674.2500,  782274.9375],
        [ 809018.1875,  674243.5625,  672739.9375,  604070.5000,  584411.3125,
          578464.9375,  537297.5625,  526191.7500,  468676.3438,  468398.8750],
        [ 418238.6875,  363375.1875,  311349.7500,  285483.9688,  279362.3750,
          270431.8125,  268787.5312,  244962.2031,  240798.7969,  229034.7656],
        [ 643913.3750,  575640.6250,  525544.7500,  451778.1562,  451570.9375,
          447477.7188,  445981.5000,  418522.8125,  412074.2812,  386556.8125],
        [ 761090.2500,  674667.4375,  618491.7500,  607792.5000,  581968.6250,
          516426.7812,  500154.8750,  497509.3438,  491098.3438,  487348.1562],
        [ 586633.7500,  578028.1875,  541278.2500,  458889.1250,  444684.4688,
          435024.7188,  411842.8750,  390537.5625,  382781.5938,  370879.0625],
        [ 785115.0000,  306598.7188,  285661.8125,  275027.8438,  257195.5469,
          256171.8281,  251547.6250,  249182.1875,  227030.7969,  215596.5781],
        [ 845354.5625,  684079.9375,  533544.5000,  495964.5312,  414069.9375,
          374283.0000,  369087.3438,  356453.5938,  350635.9375,  341040.5312],
        [ 538820.5625,  473571.8750,  443132.5000,  440914.7500,  438185.0000,
          394986.6250,  309218.8750,  302028.7812,  282047.1562,  269987.0625],
        [ 996229.7500,  915219.5000,  902750.5000,  894127.5625,  885402.8750,
          870520.4375,  869772.7500,  866119.9375,  861432.7500,  845782.8125],
        [ 425617.8125,  413693.4062,  407063.7812,  402591.9375,  338651.5938,
          297931.2500,  285293.4375,  260646.8125,  250632.4219,  228727.2188],
        [ 528588.3125,  402974.9062,  392382.9062,  368882.8750,  338922.6562,
          338677.0938,  314259.4375,  307732.9688,  302925.6250,  298999.2188],
        [ 429323.4688,  409614.0625,  403759.2812,  351524.5312,  302038.8438,
          288786.6250,  286491.1875,  278189.4062,  246548.9062,  246322.1094],
        [ 558863.0625,  445211.0625,  370173.3750,  336051.7500,  325687.6562,
          270778.9375,  256926.8750,  252445.4688,  244356.5156,  237818.2500],
        [ 303681.4375,  268084.0625,  249318.6250,  242167.6719,  240048.7812,
          238549.4375,  202069.4531,  195334.8125,  183598.9688,  180705.8281],
        [ 178593.8906,  152560.4375,  129250.8359,   90972.8906,   83476.2422,
           78712.2344,   75331.8438,   59397.4922,   53058.4023,   52823.6328],
        [ 732533.8125,  623165.5000,  587706.1250,  510537.8125,  509741.3750,
          506673.4688,  502877.2188,  488599.5312,  472240.9688,  457250.5000],
        [ 200077.2969,  149084.0938,  138578.2656,  127954.2500,  127896.0547,
          120979.2031,   83278.0078,   82393.9531,   81638.2188,   81065.6562],
        [ 399029.8438,  221503.6094,  181632.9375,  167793.7656,  166830.1719,
          144188.7656,  140437.0000,  138877.1250,  136210.2188,  132600.5312],
        [ 522494.3125,  502606.8125,  442538.2812,  361160.6562,  292026.1250,
          289553.5625,  285174.0312,  283955.2500,  280610.4062,  276210.6875],
        [ 382364.5625,  187337.4844,  186898.1562,  181108.3438,  168470.5625,
          166376.3906,  158063.4531,  154087.2656,  147934.3438,  138791.4688],
        [ 438059.6562,  422062.4375,  338725.2500,  279435.1250,  265490.4375,
          238037.2188,  224127.0156,  222763.6562,  219584.5938,  188418.9844],
        [ 832775.8125,  585621.4375,  547245.6875,  447871.4062,  447612.1875,
          375100.5938,  334679.3438,  310111.0312,  305216.4688,  261152.8906],
        [ 650739.2500,  368122.0000,  320063.3750,  313549.9688,  307037.0312,
          291150.4375,  263287.2500,  252003.1094,  249657.1875,  247233.8438],
        [ 856536.4375,  680274.1250,  624654.2500,  611707.7500,  607498.6875,
          598600.6875,  598600.6875,  562034.6875,  534383.7500,  519925.5312],
        [ 670983.0625,  500827.3750,  454154.5000,  436651.9688,  429503.2500,
          423375.4375,  412681.5000,  406994.3125,  398693.5625,  398452.5938],
        [ 741870.2500,  513811.8438,  418401.8438,  379527.9375,  352356.9062,
          288480.5312,  285578.7188,  275718.2812,  255414.3906,  247302.4688],
        [ 788748.8125,  418545.1562,  390605.3750,  304590.1250,  267679.6562,
          263550.0312,  253908.1562,  247173.7188,  243545.5000,  231099.1562],
        [ 693005.0000,  366659.5625,  337369.2812,  324894.4375,  316715.4688,
          280702.2188,  267595.4375,  239465.9688,  225028.9219,  218970.8281],
        [ 555859.8750,  201768.6719,  200837.7656,  188769.1562,  166854.0312,
          160359.3125,  159518.5625,  159104.4062,  143855.9531,  130062.8516],
        [ 450586.2500,  437099.0000,  419917.6875,  408255.7500,  370299.0938,
          355629.1562,  352554.5625,  346278.6875,  343051.2188,  335700.0312],
        [ 208574.5156,  111509.8438,  103028.7891,   96199.5781,   91955.8516,
           88277.6094,   87881.0469,   87879.5469,   87807.9141,   82027.9688],
        [ 381559.5312,  315675.4375,  296813.5938,  284491.4375,  269255.0000,
          237861.7969,  234722.1094,  222216.0000,  208751.6250,  200289.7812],
        [ 255352.0469,  248628.8438,  175277.9375,  168959.2188,  163866.8125,
          157647.9531,  142996.7031,  142685.0156,  131160.2188,  124776.4062]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[444529.2812,      0.0000],
         [224083.8438,      0.0000],
         [223786.7812,      0.0000],
         ...,
         [180596.0781,      0.0000],
         [166899.7188,      0.0000],
         [165158.6562,      0.0000]],

        [[890195.0625,      0.0000],
         [876998.3750,      0.0000],
         [876752.4375,      0.0000],
         ...,
         [788153.2500,      0.0000],
         [783778.8750,      0.0000],
         [781855.8125,      0.0000]],

        [[565312.1875,      0.0000],
         [504602.8750,      0.0000],
         [483752.9688,      0.0000],
         ...,
         [280985.3125,      0.0000],
         [262239.0312,      0.0000],
         [     0.0000, 246216.4062]],

        ...,

        [[     0.0000, 208574.5156],
         [111509.8438,      0.0000],
         [103028.7891,      0.0000],
         ...,
         [ 87879.5469,      0.0000],
         [ 87807.9141,      0.0000],
         [ 82027.9688,      0.0000]],

        [[381559.5312,      0.0000],
         [315675.4375,      0.0000],
         [     0.0000, 296813.5938],
         ...,
         [     0.0000, 222216.0000],
         [208751.6250,      0.0000],
         [     0.0000, 200289.7812]],

        [[     0.0000, 255352.0469],
         [     0.0000, 248628.8438],
         [     0.0000, 175277.9375],
         ...,
         [142685.0156,      0.0000],
         [131160.2188,      0.0000],
         [124776.4062,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1996651.7500,  199365.3281],
        [8326156.0000,       0.0000],
        [3447780.5000,  246216.4062],
        [5092249.0000,       0.0000],
        [ 551357.8125, 1003521.6250],
        [1456144.2500,  622189.0625],
        [2835279.2500, 1847578.7500],
        [3617560.0000,       0.0000],
        [4573760.0000,  694292.2500],
        [4603207.0000,       0.0000],
        [6341356.5000,       0.0000],
        [3082167.2500,       0.0000],
        [5320090.5000,       0.0000],
        [9692308.0000,       0.0000],
        [7056350.5000,       0.0000],
        [5734067.5000,       0.0000],
        [9276850.0000,       0.0000],
        [8564629.0000,       0.0000],
        [6062026.0000,       0.0000],
        [5361747.0000,       0.0000],
        [8909575.0000,       0.0000],
        [6241947.0000,       0.0000],
        [7797573.5000,       0.0000],
        [7293384.0000,       0.0000],
        [5038811.5000,       0.0000],
        [6537840.5000,       0.0000],
        [6369369.0000,       0.0000],
        [7422934.0000,       0.0000],
        [7330758.5000,       0.0000],
        [6805881.5000,       0.0000],
        [8777406.0000,       0.0000],
        [5923513.0000,       0.0000],
        [2682790.2500,  229034.7656],
        [2893514.2500, 1865546.5000],
        [3231473.5000, 2505074.5000],
        [2642899.5000, 1957680.1250],
        [1811320.2500, 1297807.6250],
        [2701535.0000, 2062979.0000],
        [1694424.6250, 2198468.5000],
        [8907358.0000,       0.0000],
        [3310849.7500,       0.0000],
        [3594346.5000,       0.0000],
        [3242598.5000,       0.0000],
        [3298313.0000,       0.0000],
        [ 863908.6875, 1439650.3750],
        [ 493772.7188,  460405.1562],
        [4803620.5000,  587706.1250],
        [ 973301.0625,  219643.9219],
        [ 751850.2500, 1077253.7500],
        [1656515.5000, 1879814.5000],
        [1416235.6250,  455196.3750],
        [1711091.7500, 1125612.5000],
        [2461841.5000, 1985545.2500],
        [2894721.5000,  368122.0000],
        [5674291.0000,  519925.5312],
        [4095665.2500,  436651.9688],
        [1694181.7500, 2064281.5000],
        [2332615.5000, 1076830.2500],
        [1816686.8750, 1453720.2500],
        [ 853857.5000, 1213133.0000],
        [3075415.7500,  743955.7500],
        [ 836568.1250,  208574.5156],
        [1647825.5000, 1003810.8125],
        [ 705485.1250, 1005866.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 59.375
Top1 accuracy for validation set is 59.375 size is torch.Size([64, 1])
Epoch 231/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:02, 60.10s/it]  7%|▋         | 2/30 [01:01<11:50, 25.37s/it] 10%|█         | 3/30 [01:01<06:21, 14.13s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:39,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.81617050965627
Epoch 232/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:56, 59.87s/it]  7%|▋         | 2/30 [01:00<11:42, 25.09s/it] 10%|█         | 3/30 [01:01<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.92s/it] 20%|██        | 6/30 [01:03<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.8022653659184775
Epoch 233/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:11, 60.38s/it]  7%|▋         | 2/30 [01:01<11:48, 25.30s/it] 10%|█         | 3/30 [01:01<06:20, 14.09s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.82s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.91s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.8074393033981324
Epoch 234/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:58, 59.95s/it]  7%|▋         | 2/30 [01:00<11:43, 25.12s/it] 10%|█         | 3/30 [01:01<06:17, 13.99s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.77s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.786853543917338
Epoch 235/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:41, 57.29s/it]  7%|▋         | 2/30 [01:01<12:16, 26.32s/it] 10%|█         | 3/30 [01:02<06:35, 14.64s/it] 13%|█▎        | 4/30 [01:03<03:58,  9.16s/it] 17%|█▋        | 5/30 [01:04<02:33,  6.12s/it] 20%|██        | 6/30 [01:04<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:05<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.38s/it] 30%|███       | 9/30 [01:07<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.29s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.07it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.29it/s] 70%|███████   | 21/30 [01:16<00:06,  1.30it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.810406279563904
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0249,  0.0071, -0.0022,  ...,  0.0084, -0.0082,  0.0163],
        [-0.0027,  0.0255,  0.0142,  ...,  0.0091, -0.0166, -0.0138],
        [-0.0376, -0.0313,  0.0106,  ...,  0.0772, -0.0272, -0.0274],
        ...,
        [ 0.0259, -0.0167, -0.0116,  ..., -0.0244, -0.0316, -0.0152],
        [-0.0283, -0.0042, -0.0124,  ..., -0.0051,  0.0114, -0.0128],
        [-0.0371, -0.0077,  0.0136,  ...,  0.0215,  0.0206, -0.0532]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9088, 0.8617, 0.8602, 0.8592, 0.8535, 0.8524, 0.8467, 0.8451, 0.8423,
         0.8403],
        [0.9574, 0.9572, 0.9567, 0.9558, 0.9538, 0.9512, 0.9511, 0.9495, 0.9478,
         0.9476],
        [0.9255, 0.9184, 0.9159, 0.8948, 0.8914, 0.8912, 0.8766, 0.8743, 0.8715,
         0.8665],
        [0.9597, 0.9218, 0.9192, 0.9168, 0.9159, 0.9124, 0.9099, 0.9004, 0.8977,
         0.8956],
        [0.8719, 0.8555, 0.8533, 0.8357, 0.8340, 0.8279, 0.8180, 0.8142, 0.8026,
         0.8022],
        [0.8823, 0.8730, 0.8560, 0.8536, 0.8514, 0.8477, 0.8467, 0.8345, 0.8320,
         0.8287],
        [0.9335, 0.9311, 0.9300, 0.9049, 0.9034, 0.9029, 0.9025, 0.9013, 0.8933,
         0.8924],
        [0.9357, 0.9205, 0.9199, 0.8894, 0.8823, 0.8774, 0.8732, 0.8645, 0.8596,
         0.8580],
        [0.9385, 0.9348, 0.9228, 0.9196, 0.9196, 0.9138, 0.9131, 0.9105, 0.9099,
         0.9082],
        [0.9581, 0.9156, 0.9153, 0.9060, 0.9058, 0.9047, 0.9034, 0.8963, 0.8954,
         0.8881],
        [0.9653, 0.9377, 0.9367, 0.9309, 0.9286, 0.9256, 0.9253, 0.9252, 0.9230,
         0.9217],
        [0.9217, 0.9157, 0.8921, 0.8911, 0.8775, 0.8717, 0.8713, 0.8504, 0.8478,
         0.8306],
        [0.9610, 0.9291, 0.9275, 0.9245, 0.9181, 0.9174, 0.9065, 0.9054, 0.9028,
         0.8984],
        [0.9745, 0.9675, 0.9659, 0.9642, 0.9641, 0.9639, 0.9619, 0.9601, 0.9593,
         0.9588],
        [0.9482, 0.9443, 0.9434, 0.9419, 0.9418, 0.9415, 0.9391, 0.9379, 0.9374,
         0.9357],
        [0.9344, 0.9340, 0.9336, 0.9297, 0.9278, 0.9256, 0.9230, 0.9228, 0.9218,
         0.9205],
        [0.9795, 0.9708, 0.9655, 0.9643, 0.9591, 0.9542, 0.9535, 0.9522, 0.9497,
         0.9494],
        [0.9716, 0.9643, 0.9585, 0.9543, 0.9517, 0.9516, 0.9512, 0.9501, 0.9483,
         0.9480],
        [0.9493, 0.9467, 0.9467, 0.9325, 0.9297, 0.9215, 0.9199, 0.9141, 0.9139,
         0.9132],
        [0.9348, 0.9301, 0.9250, 0.9200, 0.9198, 0.9197, 0.9175, 0.9171, 0.9167,
         0.9166],
        [0.9756, 0.9645, 0.9636, 0.9631, 0.9562, 0.9561, 0.9515, 0.9487, 0.9487,
         0.9485],
        [0.9489, 0.9402, 0.9397, 0.9357, 0.9289, 0.9284, 0.9269, 0.9241, 0.9237,
         0.9229],
        [0.9758, 0.9567, 0.9548, 0.9520, 0.9493, 0.9492, 0.9420, 0.9344, 0.9335,
         0.9290],
        [0.9563, 0.9531, 0.9482, 0.9456, 0.9455, 0.9405, 0.9403, 0.9380, 0.9356,
         0.9346],
        [0.9704, 0.9234, 0.9210, 0.9193, 0.9158, 0.8981, 0.8979, 0.8963, 0.8952,
         0.8952],
        [0.9500, 0.9464, 0.9402, 0.9396, 0.9374, 0.9320, 0.9309, 0.9304, 0.9290,
         0.9264],
        [0.9540, 0.9497, 0.9415, 0.9390, 0.9342, 0.9273, 0.9249, 0.9224, 0.9212,
         0.9209],
        [0.9672, 0.9567, 0.9558, 0.9463, 0.9451, 0.9374, 0.9360, 0.9360, 0.9354,
         0.9344],
        [0.9606, 0.9536, 0.9503, 0.9458, 0.9427, 0.9399, 0.9391, 0.9383, 0.9380,
         0.9359],
        [0.9528, 0.9478, 0.9404, 0.9381, 0.9379, 0.9367, 0.9362, 0.9345, 0.9333,
         0.9323],
        [0.9686, 0.9617, 0.9610, 0.9568, 0.9568, 0.9548, 0.9540, 0.9533, 0.9516,
         0.9487],
        [0.9529, 0.9379, 0.9376, 0.9312, 0.9278, 0.9275, 0.9222, 0.9219, 0.9146,
         0.9141],
        [0.9049, 0.8945, 0.8849, 0.8778, 0.8762, 0.8751, 0.8742, 0.8690, 0.8670,
         0.8645],
        [0.9350, 0.9267, 0.9211, 0.9111, 0.9106, 0.9093, 0.9085, 0.9050, 0.9045,
         0.9004],
        [0.9471, 0.9395, 0.9316, 0.9307, 0.9289, 0.9183, 0.9178, 0.9169, 0.9169,
         0.9163],
        [0.9285, 0.9269, 0.9213, 0.9125, 0.9081, 0.9069, 0.9016, 0.8971, 0.8969,
         0.8960],
        [0.9503, 0.8824, 0.8781, 0.8751, 0.8738, 0.8698, 0.8698, 0.8691, 0.8618,
         0.8555],
        [0.9525, 0.9381, 0.9213, 0.9172, 0.9035, 0.8970, 0.8959, 0.8933, 0.8917,
         0.8879],
        [0.9224, 0.9147, 0.9094, 0.9066, 0.9056, 0.8993, 0.8830, 0.8820, 0.8773,
         0.8727],
        [0.9650, 0.9602, 0.9577, 0.9571, 0.9570, 0.9566, 0.9562, 0.9549, 0.9535,
         0.9534],
        [0.9056, 0.9041, 0.9035, 0.9027, 0.8903, 0.8814, 0.8785, 0.8719, 0.8680,
         0.8605],
        [0.9218, 0.9015, 0.8999, 0.8957, 0.8910, 0.8894, 0.8844, 0.8832, 0.8820,
         0.8819],
        [0.9081, 0.9034, 0.9030, 0.8920, 0.8826, 0.8802, 0.8789, 0.8764, 0.8680,
         0.8680],
        [0.9249, 0.9092, 0.8966, 0.8892, 0.8866, 0.8752, 0.8706, 0.8695, 0.8668,
         0.8654],
        [0.8833, 0.8733, 0.8676, 0.8672, 0.8639, 0.8636, 0.8530, 0.8504, 0.8467,
         0.8465],
        [0.8447, 0.8367, 0.8239, 0.7974, 0.7947, 0.7888, 0.7859, 0.7721, 0.7622,
         0.7618],
        [0.9435, 0.9329, 0.9296, 0.9195, 0.9195, 0.9192, 0.9181, 0.9158, 0.9142,
         0.9120],
        [0.8522, 0.8297, 0.8265, 0.8203, 0.8197, 0.8171, 0.7904, 0.7890, 0.7887,
         0.7878],
        [0.9001, 0.8598, 0.8444, 0.8378, 0.8365, 0.8303, 0.8297, 0.8253, 0.8242,
         0.8240],
        [0.9199, 0.9155, 0.9072, 0.8945, 0.8778, 0.8769, 0.8758, 0.8758, 0.8746,
         0.8727],
        [0.9006, 0.8515, 0.8508, 0.8487, 0.8410, 0.8406, 0.8396, 0.8358, 0.8345,
         0.8283],
        [0.9092, 0.9056, 0.8876, 0.8752, 0.8737, 0.8640, 0.8590, 0.8590, 0.8583,
         0.8478],
        [0.9526, 0.9297, 0.9249, 0.9115, 0.9089, 0.8977, 0.8877, 0.8839, 0.8833,
         0.8723],
        [0.9363, 0.8956, 0.8875, 0.8848, 0.8835, 0.8807, 0.8733, 0.8695, 0.8688,
         0.8681],
        [0.9551, 0.9394, 0.9326, 0.9317, 0.9315, 0.9299, 0.9299, 0.9251, 0.9209,
         0.9201],
        [0.9370, 0.9164, 0.9093, 0.9062, 0.9061, 0.9042, 0.9034, 0.9015, 0.9011,
         0.9008],
        [0.9448, 0.9188, 0.9040, 0.8978, 0.8945, 0.8781, 0.8769, 0.8749, 0.8692,
         0.8688],
        [0.9498, 0.9047, 0.9018, 0.8842, 0.8757, 0.8729, 0.8704, 0.8688, 0.8660,
         0.8639],
        [0.9398, 0.8951, 0.8901, 0.8859, 0.8829, 0.8775, 0.8735, 0.8640, 0.8613,
         0.8596],
        [0.9246, 0.8532, 0.8525, 0.8491, 0.8392, 0.8373, 0.8367, 0.8365, 0.8309,
         0.8244],
        [0.9086, 0.9065, 0.9037, 0.9036, 0.8951, 0.8926, 0.8916, 0.8908, 0.8898,
         0.8895],
        [0.8583, 0.8111, 0.8018, 0.8013, 0.7986, 0.7974, 0.7955, 0.7935, 0.7934,
         0.7872],
        [0.8996, 0.8853, 0.8789, 0.8786, 0.8721, 0.8633, 0.8632, 0.8604, 0.8562,
         0.8509],
        [0.8677, 0.8664, 0.8450, 0.8396, 0.8361, 0.8338, 0.8290, 0.8263, 0.8185,
         0.8179]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 1],
        [1, 0, 1, 1, 0, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 1, 0],
        [0, 1, 1, 1, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 1, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 0, 1],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 1, 0, 1, 0, 0, 1, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 1, 0, 1],
        [1, 1, 1, 1, 0, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 435136.3125,  221895.3906,  217087.6250,  214194.5781,  197236.4844,
          194173.4219,  178977.8750,  175038.2188,  168170.7188,  163349.7344],
        [ 871384.2500,  868418.4375,  862163.4375,  851619.2500,  827162.3125,
          797466.5625,  795468.1250,  777835.2500,  759020.8125,  756998.1250],
        [ 551834.5000,  498515.2812,  481637.2812,  355981.0625,  339308.5000,
          338280.7188,  274489.9062,  265827.1250,  255321.3594,  237787.3906],
        [ 900011.7500,  523477.8750,  504487.3750,  487274.2812,  481099.2500,
          457728.6562,  441817.1875,  385545.0938,  371140.1562,  360216.4062],
        [ 256833.5312,  203023.4844,  196921.2969,  153054.3125,  149390.7969,
          136910.4844,  118858.3828,  112570.7500,   95363.6016,   94780.4219],
        [ 297740.3750,  260653.0156,  204530.3594,  197739.7188,  191650.2500,
          181670.5312,  179022.2656,  150362.3125,  145163.9531,  138505.4688],
        [ 619236.5625,  597692.0000,  588409.9375,  411033.7812,  402871.9375,
          399829.7812,  397540.5000,  390930.6875,  348279.3750,  344207.3750],
        [ 638381.6875,  514029.4375,  509805.0625,  329800.2812,  297869.3125,
          277519.7812,  261556.1719,  230877.9844,  215390.6406,  210524.5781],
        [ 665014.5625,  630374.6875,  531049.5000,  507373.1562,  507234.7812,
          467075.3750,  462456.9062,  445350.8125,  441659.2188,  431366.4688],
        [ 878983.6250,  479392.7812,  476894.3750,  417627.2812,  416945.1875,
          410110.1250,  402388.8750,  363596.0000,  359228.4062,  323438.3438],
        [ 974690.2500,  657498.1875,  648229.4375,  596067.4375,  577418.8125,
          552932.3750,  550246.4375,  549515.4375,  532466.8750,  522912.5938],
        [ 522970.9062,  480144.0625,  342466.7500,  337731.7500,  278219.9062,
          256051.9062,  254365.7500,  188747.2031,  182038.2031,  142231.3906],
        [ 917363.0625,  581073.5625,  568030.5000,  544422.7500,  496882.0625,
          491394.9062,  420940.1250,  414043.5000,  399014.5938,  375053.0312],
        [1111423.5000, 1005782.8125,  983822.7500,  960090.8125,  958528.2500,
          955891.8750,  928731.7500,  904404.1875,  895355.3750,  887784.6875],
        [ 763967.0000,  721763.1875,  712991.8750,  697944.1875,  696429.6250,
          693627.1875,  670226.5000,  658878.5000,  654462.1875,  638218.5000],
        [ 626501.8750,  623552.0000,  619669.6250,  586605.1875,  570512.0625,
          553046.8125,  532566.4375,  531349.8750,  523919.4062,  514230.0000],
        [1194362.5000, 1053763.8750,  977518.3125,  960827.1875,  892070.6250,
          832150.1875,  823250.2500,  808065.8750,  780418.7500,  776897.4375],
        [1067302.2500,  960391.1250,  884386.8125,  833052.1875,  803179.5000,
          801049.8125,  797459.7500,  784143.7500,  764669.0000,  761558.5000],
        [ 775288.3750,  747398.6250,  746963.2500,  610011.3125,  586271.8750,
          521492.7500,  509654.8750,  468856.9688,  467868.9375,  463246.5938],
        [ 630270.7500,  589547.3750,  548128.4375,  510644.9062,  508876.7812,
          508366.5000,  492721.0312,  489965.3125,  486785.6250,  486449.1562],
        [1129703.3750,  964075.5625,  951382.6875,  944704.0000,  856034.1875,
          854191.1875,  800329.0000,  769393.0000,  769360.0000,  767099.7500],
        [ 771042.0000,  681336.3125,  675847.1875,  638751.9375,  579347.6875,
          575098.0000,  563481.6250,  541027.9375,  537673.8125,  532289.6875],
        [1131984.3750,  861511.6250,  838822.3750,  806249.2500,  775219.5625,
          774410.4375,  698996.0000,  626576.6250,  618749.5625,  580042.6250],
        [ 857520.5000,  819359.0625,  763209.6875,  735759.3750,  734487.6250,
          684007.5625,  682296.6875,  659943.1875,  637427.1250,  628892.1875],
        [1049213.2500,  535856.5625,  517852.1250,  505115.5938,  480372.1562,
          373119.7500,  372072.9375,  363872.4688,  358321.7188,  358213.0312],
        [ 783837.1875,  744173.1875,  681286.9375,  674980.8125,  654730.0000,
          605744.0625,  595998.6250,  592346.5625,  580049.2500,  559019.2500],
        [ 829804.4375,  780118.1250,  693672.1875,  669353.9375,  624793.6250,
          566206.2500,  547373.5625,  527932.3750,  519300.1250,  516727.3125],
        [1001199.6875,  862629.7500,  850754.7500,  742736.7500,  730799.1875,
          654046.0625,  641745.0625,  641124.1250,  635960.1875,  626835.3750],
        [ 912074.0000,  824441.3750,  787292.3750,  737338.4375,  705871.7500,
          678496.2500,  670212.3750,  662485.7500,  659913.0000,  640436.6875],
        [ 814912.5000,  759590.6875,  682856.5625,  660717.1250,  659502.7500,
          647809.1875,  643090.4375,  628005.7500,  616947.1250,  608326.6250],
        [1022204.2500,  926424.6250,  916244.7500,  863223.0625,  863177.0000,
          838704.0000,  829314.7500,  821018.0625,  801793.5000,  769362.8750],
        [ 816899.0000,  658639.7500,  655864.3125,  599101.5625,  570582.2500,
          568054.3750,  526892.2500,  524481.8125,  472325.1875,  469052.4062],
        [ 411338.0625,  354308.9375,  309243.9375,  279288.8750,  272943.8125,
          268814.7188,  265436.5000,  246373.3281,  239502.0625,  230881.2969],
        [ 632486.5625,  561969.8125,  518738.8438,  449589.1250,  446434.3125,
          437850.8125,  433039.1250,  411676.7812,  409044.5312,  385841.9375],
        [ 751309.8750,  674120.1250,  602455.6250,  594945.7500,  579479.7500,
          497826.4062,  494665.5312,  488313.5312,  488108.1875,  484221.0000],
        [ 575918.5000,  562886.5000,  519640.4688,  458289.9375,  430329.8125,
          423031.5938,  392198.4688,  368134.6562,  366843.1875,  362173.6562],
        [ 786316.1250,  298455.3750,  280498.8438,  268765.5000,  263888.5312,
          249208.5625,  249042.0000,  246541.3750,  222258.4062,  203148.4062],
        [ 811830.6875,  660553.9375,  519772.3125,  490405.2188,  403324.4062,
          367423.6875,  361869.8438,  348298.3125,  340564.0312,  322450.0000],
        [ 528085.9375,  473230.0938,  438360.9688,  421691.8750,  415467.0625,
          379604.6875,  300997.3438,  296593.7500,  277159.5625,  259568.2500],
        [ 970303.4375,  905983.9375,  873958.3750,  867528.6250,  866104.1875,
          860647.6875,  856524.1875,  840618.6875,  823491.3125,  822846.0000],
        [ 415750.4375,  406526.8438,  403431.7188,  398623.2188,  333986.1875,
          293898.3125,  282009.2188,  256788.7188,  242962.2812,  218080.3281],
        [ 523693.5938,  392023.0938,  383094.1875,  360419.8438,  337051.5312,
          329818.8438,  307022.9688,  301610.5625,  296735.2188,  296275.1562],
        [ 430825.4688,  402652.2188,  400131.1250,  342303.1875,  299130.1250,
          289047.5625,  283853.7188,  273833.9375,  242676.9844,  242635.5625],
        [ 547131.9375,  437529.4062,  365588.3438,  328610.7188,  316910.0312,
          269188.2500,  252128.1250,  248103.5000,  238623.8281,  233815.4844],
        [ 301946.1250,  261846.6875,  241455.5469,  239894.7812,  228914.2344,
          228005.4688,  195938.9219,  188872.3438,  179204.1719,  178676.3438],
        [ 173987.8906,  155358.5938,  129281.2891,   88615.2578,   85252.2422,
           78292.1641,   75174.5938,   61706.2773,   53525.9766,   53238.7969],
        [ 713606.1250,  613731.8750,  585396.9375,  506902.5625,  506885.1562,
          504503.7188,  496662.6875,  480340.5312,  470049.1875,  454962.9688],
        [ 193804.5312,  140530.6562,  134217.4688,  122784.7734,  121846.1016,
          117267.6875,   80125.0391,   78546.8125,   78240.8047,   77261.4609],
        [ 383905.7500,  215872.4688,  173274.3750,  157749.0156,  154882.0938,
          141723.6406,  140540.0312,  131970.3750,  129885.1016,  129445.6172],
        [ 509617.4375,  478299.0938,  425019.5312,  354568.5312,  279173.5625,
          275565.8125,  271608.0938,  271439.2500,  266747.7188,  259692.7812],
        [ 386735.2812,  191701.6094,  189784.1406,  184179.0938,  165135.1875,
          164052.7188,  161919.3906,  153212.9062,  150496.0156,  137774.1562],
        [ 437099.0000,  415334.3438,  321464.7188,  269256.5312,  263213.9375,
          229315.6094,  213415.4531,  213367.4531,  211468.2969,  181923.1406],
        [ 812753.3125,  586278.0000,  547044.8125,  451960.0000,  435351.7812,
          371195.0312,  321628.7812,  304825.8125,  302304.5312,  258049.3281],
        [ 644116.6875,  360343.5625,  320745.0938,  308707.9688,  303016.3438,
          291122.1250,  261845.4375,  248129.2969,  245763.1875,  243115.0156],
        [ 842396.2500,  673028.0625,  611111.8125,  603293.8750,  601425.1250,
          588031.2500,  588031.2500,  548808.9375,  517249.9375,  510812.9688],
        [ 650886.9375,  485114.0312,  437831.2188,  418929.3125,  418196.4375,
          407282.0000,  402660.6875,  391936.0000,  389403.9688,  387878.4375],
        [ 727278.3125,  501833.8125,  406173.0625,  371527.5938,  354420.4688,
          280467.0000,  275760.3750,  267856.6250,  247006.6562,  245710.9219],
        [ 781047.1875,  410210.2188,  393387.0312,  305953.1875,  271027.9688,
          260574.4688,  251187.0938,  245574.3438,  235811.4062,  228912.2656],
        [ 677238.9375,  357509.3438,  333072.9688,  313729.1250,  300496.5625,
          278273.7812,  262668.7812,  229161.0469,  220677.7188,  215461.7188],
        [ 545378.9375,  196591.7812,  194467.1562,  185279.4531,  160841.7656,
          156517.3750,  155175.5625,  154860.6875,  142923.3438,  130332.4062],
        [ 433491.1250,  420748.2812,  404075.5312,  404033.9062,  357637.9062,
          345234.7188,  340244.2812,  336186.0625,  331502.4688,  330242.4688],
        [ 211326.3594,  107702.3594,   94344.6719,   93632.0781,   90030.6562,
           88568.6250,   86164.5156,   83794.4844,   83608.8984,   76553.3203],
        [ 381302.7188,  311007.5938,  283682.1562,  282565.9375,  257530.3438,
          226933.1719,  226743.4531,  217679.1094,  204993.3594,  190142.8594],
        [ 241637.0625,  237285.1875,  174790.3438,  161855.7969,  154020.4062,
          149043.1406,  139060.8125,  133752.3594,  119661.6953,  118694.9297]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[435136.3125,      0.0000],
         [221895.3906,      0.0000],
         [217087.6250,      0.0000],
         ...,
         [175038.2188,      0.0000],
         [168170.7188,      0.0000],
         [163349.7344,      0.0000]],

        [[871384.2500,      0.0000],
         [868418.4375,      0.0000],
         [862163.4375,      0.0000],
         ...,
         [777835.2500,      0.0000],
         [759020.8125,      0.0000],
         [756998.1250,      0.0000]],

        [[551834.5000,      0.0000],
         [498515.2812,      0.0000],
         [481637.2812,      0.0000],
         ...,
         [265827.1250,      0.0000],
         [255321.3594,      0.0000],
         [     0.0000, 237787.3906]],

        ...,

        [[     0.0000, 211326.3594],
         [107702.3594,      0.0000],
         [ 94344.6719,      0.0000],
         ...,
         [ 83794.4844,      0.0000],
         [ 83608.8984,      0.0000],
         [ 76553.3203,      0.0000]],

        [[381302.7188,      0.0000],
         [311007.5938,      0.0000],
         [     0.0000, 283682.1562],
         ...,
         [     0.0000, 217679.1094],
         [204993.3594,      0.0000],
         [     0.0000, 190142.8594]],

        [[     0.0000, 241637.0625],
         [     0.0000, 237285.1875],
         [     0.0000, 174790.3438],
         ...,
         [133752.3594,      0.0000],
         [119661.6953,      0.0000],
         [118694.9297,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1968023.7500,  197236.4844],
        [8167536.0000,       0.0000],
        [3361195.7500,  237787.3906],
        [4912798.0000,       0.0000],
        [ 629499.6250,  888207.4375],
        [1498935.5000,  448102.6875],
        [3080383.0000, 1419649.0000],
        [3485755.0000,       0.0000],
        [4423941.0000,  665014.5625],
        [4528605.0000,       0.0000],
        [6161978.0000,       0.0000],
        [2984968.0000,       0.0000],
        [5208218.0000,       0.0000],
        [9591816.0000,       0.0000],
        [6908509.0000,       0.0000],
        [5681953.0000,       0.0000],
        [9099325.0000,       0.0000],
        [8457192.0000,       0.0000],
        [5897054.0000,       0.0000],
        [5251756.0000,       0.0000],
        [8806273.0000,       0.0000],
        [6095896.0000,       0.0000],
        [7712562.5000,       0.0000],
        [7202903.0000,       0.0000],
        [4914009.5000,       0.0000],
        [6472166.0000,       0.0000],
        [6275282.0000,       0.0000],
        [7387831.0000,       0.0000],
        [7278562.0000,       0.0000],
        [6721759.0000,       0.0000],
        [8651467.0000,       0.0000],
        [5861893.0000,       0.0000],
        [2647250.2500,  230881.2969],
        [2851420.2500, 1835251.6250],
        [3166315.0000, 2489130.7500],
        [2543424.2500, 1916022.5000],
        [1779049.7500, 1289073.3750],
        [2634335.5000, 1992157.0000],
        [1632796.8750, 2157962.5000],
        [8688006.0000,       0.0000],
        [3252057.2500,       0.0000],
        [3527745.0000,       0.0000],
        [3207090.0000,       0.0000],
        [3237629.7500,       0.0000],
        [ 658333.3750, 1586421.2500],
        [ 495805.3125,  458627.7812],
        [4747645.0000,  585396.9375],
        [ 931861.0625,  212764.2812],
        [ 724340.3750, 1034908.0625],
        [1586670.7500, 1805061.0000],
        [1434801.0000,  450189.5625],
        [1640211.2500, 1115647.2500],
        [2414061.2500, 1977330.2500],
        [2866561.2500,  360343.5625],
        [5573376.0000,  510812.9688],
        [3971922.7500,  418196.4375],
        [1659594.8750, 2018440.0000],
        [2309060.0000, 1074625.2500],
        [1774415.0000, 1413875.0000],
        [ 836487.5000, 1185881.0000],
        [3299362.7500,  404033.9062],
        [ 804399.6250,  211326.3594],
        [1608510.7500,  974070.0625],
        [ 665190.2500,  964611.5625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 59.375
Top1 accuracy for validation set is 59.375 size is torch.Size([64, 1])
Epoch 236/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:52, 59.75s/it]  7%|▋         | 2/30 [01:01<11:57, 25.62s/it] 10%|█         | 3/30 [01:02<06:25, 14.26s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.93s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.98s/it] 20%|██        | 6/30 [01:04<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.8087687492370605
Epoch 237/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:28, 56.84s/it]  7%|▋         | 2/30 [00:59<11:43, 25.11s/it] 10%|█         | 3/30 [01:00<06:17, 13.99s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.76s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.87s/it] 20%|██        | 6/30 [01:02<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:04<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 2.785902428627014
Epoch 238/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:55<26:59, 55.84s/it]  7%|▋         | 2/30 [00:59<11:37, 24.92s/it] 10%|█         | 3/30 [00:59<06:14, 13.88s/it] 13%|█▎        | 4/30 [01:00<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.83s/it] 20%|██        | 6/30 [01:02<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:02<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 2.7860153913497925
Epoch 239/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:14, 60.52s/it]  7%|▋         | 2/30 [01:01<11:50, 25.36s/it] 10%|█         | 3/30 [01:02<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:39,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.7862751881281533
Epoch 240/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:34, 57.06s/it]  7%|▋         | 2/30 [01:00<11:53, 25.49s/it] 10%|█         | 3/30 [01:01<06:24, 14.24s/it] 13%|█▎        | 4/30 [01:02<03:51,  8.91s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.97s/it] 20%|██        | 6/30 [01:03<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.799016586939494
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0249,  0.0064, -0.0004,  ...,  0.0077, -0.0073,  0.0148],
        [-0.0023,  0.0251,  0.0150,  ...,  0.0094, -0.0156, -0.0142],
        [-0.0367, -0.0313,  0.0115,  ...,  0.0780, -0.0260, -0.0268],
        ...,
        [ 0.0252, -0.0157, -0.0103,  ..., -0.0242, -0.0302, -0.0152],
        [-0.0281, -0.0041, -0.0122,  ..., -0.0045,  0.0115, -0.0127],
        [-0.0372, -0.0079,  0.0132,  ...,  0.0229,  0.0214, -0.0525]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9076, 0.8613, 0.8594, 0.8586, 0.8537, 0.8530, 0.8452, 0.8435, 0.8428,
         0.8390],
        [0.9568, 0.9564, 0.9546, 0.9546, 0.9533, 0.9500, 0.9490, 0.9487, 0.9467,
         0.9466],
        [0.9236, 0.9171, 0.9154, 0.8932, 0.8905, 0.8889, 0.8730, 0.8701, 0.8689,
         0.8642],
        [0.9590, 0.9209, 0.9160, 0.9151, 0.9128, 0.9097, 0.9074, 0.8974, 0.8955,
         0.8945],
        [0.8704, 0.8549, 0.8511, 0.8350, 0.8337, 0.8245, 0.8159, 0.8142, 0.8026,
         0.8017],
        [0.8770, 0.8713, 0.8511, 0.8476, 0.8438, 0.8433, 0.8424, 0.8305, 0.8292,
         0.8254],
        [0.9303, 0.9279, 0.9265, 0.9027, 0.9017, 0.9010, 0.8994, 0.8984, 0.8915,
         0.8905],
        [0.9341, 0.9179, 0.9170, 0.8870, 0.8809, 0.8750, 0.8702, 0.8616, 0.8575,
         0.8553],
        [0.9359, 0.9320, 0.9217, 0.9189, 0.9182, 0.9114, 0.9100, 0.9078, 0.9076,
         0.9045],
        [0.9579, 0.9145, 0.9136, 0.9059, 0.9052, 0.9037, 0.9034, 0.8958, 0.8941,
         0.8868],
        [0.9643, 0.9355, 0.9345, 0.9288, 0.9278, 0.9238, 0.9225, 0.9223, 0.9215,
         0.9201],
        [0.9213, 0.9142, 0.8895, 0.8886, 0.8768, 0.8682, 0.8678, 0.8477, 0.8447,
         0.8295],
        [0.9601, 0.9274, 0.9269, 0.9244, 0.9172, 0.9153, 0.9056, 0.9049, 0.9010,
         0.8958],
        [0.9734, 0.9672, 0.9658, 0.9639, 0.9637, 0.9626, 0.9611, 0.9593, 0.9580,
         0.9578],
        [0.9471, 0.9429, 0.9427, 0.9398, 0.9389, 0.9388, 0.9369, 0.9364, 0.9351,
         0.9342],
        [0.9344, 0.9329, 0.9320, 0.9297, 0.9275, 0.9248, 0.9224, 0.9220, 0.9196,
         0.9195],
        [0.9784, 0.9699, 0.9644, 0.9625, 0.9576, 0.9526, 0.9514, 0.9499, 0.9485,
         0.9483],
        [0.9711, 0.9642, 0.9581, 0.9536, 0.9513, 0.9510, 0.9500, 0.9489, 0.9476,
         0.9466],
        [0.9483, 0.9460, 0.9447, 0.9293, 0.9260, 0.9198, 0.9187, 0.9132, 0.9130,
         0.9102],
        [0.9328, 0.9294, 0.9245, 0.9188, 0.9182, 0.9165, 0.9163, 0.9159, 0.9158,
         0.9154],
        [0.9743, 0.9636, 0.9622, 0.9621, 0.9565, 0.9556, 0.9497, 0.9485, 0.9476,
         0.9471],
        [0.9479, 0.9388, 0.9386, 0.9337, 0.9273, 0.9269, 0.9246, 0.9232, 0.9226,
         0.9214],
        [0.9755, 0.9559, 0.9543, 0.9513, 0.9484, 0.9480, 0.9413, 0.9329, 0.9325,
         0.9275],
        [0.9561, 0.9520, 0.9468, 0.9446, 0.9438, 0.9395, 0.9390, 0.9368, 0.9353,
         0.9339],
        [0.9697, 0.9226, 0.9181, 0.9175, 0.9135, 0.8976, 0.8972, 0.8948, 0.8947,
         0.8940],
        [0.9501, 0.9457, 0.9402, 0.9390, 0.9375, 0.9320, 0.9284, 0.9283, 0.9281,
         0.9253],
        [0.9530, 0.9488, 0.9415, 0.9380, 0.9335, 0.9262, 0.9239, 0.9208, 0.9205,
         0.9187],
        [0.9670, 0.9567, 0.9559, 0.9460, 0.9437, 0.9368, 0.9359, 0.9356, 0.9353,
         0.9335],
        [0.9599, 0.9529, 0.9503, 0.9449, 0.9422, 0.9395, 0.9388, 0.9373, 0.9370,
         0.9353],
        [0.9520, 0.9467, 0.9396, 0.9389, 0.9375, 0.9353, 0.9351, 0.9328, 0.9317,
         0.9310],
        [0.9676, 0.9610, 0.9602, 0.9562, 0.9558, 0.9542, 0.9520, 0.9517, 0.9504,
         0.9475],
        [0.9532, 0.9361, 0.9357, 0.9305, 0.9263, 0.9262, 0.9210, 0.9205, 0.9146,
         0.9135],
        [0.9036, 0.8926, 0.8839, 0.8763, 0.8748, 0.8742, 0.8735, 0.8693, 0.8665,
         0.8646],
        [0.9335, 0.9250, 0.9201, 0.9108, 0.9103, 0.9067, 0.9058, 0.9038, 0.9037,
         0.8998],
        [0.9463, 0.9393, 0.9310, 0.9287, 0.9277, 0.9172, 0.9165, 0.9160, 0.9159,
         0.9151],
        [0.9271, 0.9250, 0.9188, 0.9116, 0.9057, 0.9050, 0.8983, 0.8946, 0.8946,
         0.8927],
        [0.9500, 0.8808, 0.8766, 0.8747, 0.8736, 0.8694, 0.8679, 0.8676, 0.8597,
         0.8547],
        [0.9495, 0.9356, 0.9198, 0.9164, 0.9014, 0.8958, 0.8946, 0.8916, 0.8894,
         0.8866],
        [0.9211, 0.9145, 0.9084, 0.9035, 0.9023, 0.8964, 0.8816, 0.8811, 0.8766,
         0.8711],
        [0.9633, 0.9590, 0.9557, 0.9555, 0.9554, 0.9551, 0.9549, 0.9529, 0.9513,
         0.9505],
        [0.9042, 0.9029, 0.9029, 0.9021, 0.8893, 0.8805, 0.8776, 0.8706, 0.8658,
         0.8586],
        [0.9214, 0.8996, 0.8984, 0.8942, 0.8902, 0.8877, 0.8829, 0.8823, 0.8819,
         0.8811],
        [0.9081, 0.9026, 0.9024, 0.8902, 0.8822, 0.8804, 0.8785, 0.8756, 0.8676,
         0.8666],
        [0.9231, 0.9072, 0.8955, 0.8878, 0.8848, 0.8747, 0.8697, 0.8681, 0.8650,
         0.8639],
        [0.8828, 0.8712, 0.8666, 0.8654, 0.8607, 0.8604, 0.8511, 0.8478, 0.8468,
         0.8450],
        [0.8432, 0.8375, 0.8237, 0.7961, 0.7951, 0.7885, 0.7858, 0.7743, 0.7624,
         0.7617],
        [0.9412, 0.9316, 0.9291, 0.9187, 0.9184, 0.9182, 0.9164, 0.9147, 0.9136,
         0.9114],
        [0.8499, 0.8259, 0.8248, 0.8176, 0.8172, 0.8151, 0.7896, 0.7880, 0.7848,
         0.7837],
        [0.8970, 0.8585, 0.8421, 0.8348, 0.8324, 0.8303, 0.8294, 0.8229, 0.8218,
         0.8214],
        [0.9185, 0.9117, 0.9041, 0.8930, 0.8745, 0.8734, 0.8725, 0.8720, 0.8709,
         0.8687],
        [0.9016, 0.8536, 0.8522, 0.8504, 0.8429, 0.8408, 0.8366, 0.8357, 0.8355,
         0.8281],
        [0.9087, 0.9040, 0.8837, 0.8731, 0.8726, 0.8611, 0.8567, 0.8555, 0.8551,
         0.8454],
        [0.9512, 0.9297, 0.9247, 0.9120, 0.9069, 0.8969, 0.8849, 0.8826, 0.8825,
         0.8717],
        [0.9353, 0.8943, 0.8877, 0.8843, 0.8823, 0.8810, 0.8729, 0.8685, 0.8677,
         0.8668],
        [0.9537, 0.9383, 0.9311, 0.9305, 0.9304, 0.9283, 0.9283, 0.9232, 0.9185,
         0.9182],
        [0.9347, 0.9141, 0.9064, 0.9046, 0.9025, 0.9023, 0.9018, 0.9007, 0.8992,
         0.8989],
        [0.9435, 0.9172, 0.9018, 0.8962, 0.8945, 0.8760, 0.8741, 0.8729, 0.8690,
         0.8668],
        [0.9493, 0.9031, 0.9022, 0.8844, 0.8767, 0.8722, 0.8699, 0.8680, 0.8636,
         0.8631],
        [0.9382, 0.8937, 0.8894, 0.8834, 0.8803, 0.8772, 0.8725, 0.8614, 0.8596,
         0.8587],
        [0.9230, 0.8514, 0.8504, 0.8476, 0.8366, 0.8364, 0.8345, 0.8344, 0.8301,
         0.8241],
        [0.9059, 0.9037, 0.9028, 0.9012, 0.8928, 0.8904, 0.8891, 0.8889, 0.8885,
         0.8877],
        [0.8589, 0.8089, 0.8001, 0.7998, 0.7984, 0.7957, 0.7908, 0.7894, 0.7893,
         0.7845],
        [0.8996, 0.8846, 0.8781, 0.8756, 0.8690, 0.8605, 0.8599, 0.8591, 0.8544,
         0.8477],
        [0.8641, 0.8632, 0.8452, 0.8365, 0.8318, 0.8296, 0.8277, 0.8216, 0.8153,
         0.8131]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 1, 0, 0, 0, 0, 1, 0],
        [1, 0, 1, 1, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 1, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [1, 0, 1, 0, 1, 0, 0, 1, 0, 1],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 1, 0, 1, 0, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 427746.7500,  220785.5000,  214636.2656,  212145.9688,  197953.7031,
          195906.0469,  175374.4062,  171025.2188,  169435.0625,  160336.0625],
        [ 863789.6250,  857889.4375,  836694.8125,  836111.6875,  821187.1875,
          782950.3750,  771967.6250,  769039.3750,  746830.0625,  746043.4375],
        [ 537025.0000,  489713.0625,  477553.4375,  347847.8750,  334779.9375,
          327376.7188,  260774.1094,  250338.1250,  246010.3438,  229928.7656],
        [ 891129.3750,  516959.9375,  482206.7188,  476126.8750,  460599.0312,
          440420.9688,  426284.0000,  369710.1562,  359644.9062,  354370.0938],
        [ 251103.4844,  201312.7656,  190678.5469,  151612.1719,  148671.0625,
          130463.9766,  115430.6797,  112509.2500,   95403.4453,   94170.8203],
        [ 276133.7812,  254694.1562,  190705.2812,  181436.7969,  171897.4062,
          170496.1094,  168452.5781,  142097.5781,  139474.6719,  132040.2500],
        [ 591506.0625,  571493.3125,  560395.8750,  398708.7812,  393030.8125,
          389221.6875,  380208.6562,  374640.8438,  339568.0938,  334977.2812],
        [ 624205.8125,  495178.1250,  489168.3438,  318434.8438,  292140.5938,
          268434.5625,  250701.2656,  221440.4531,  209075.7812,  202529.3906],
        [ 640628.4375,  605561.5625,  522701.1562,  502234.5312,  497518.3438,
          451400.0000,  442659.0000,  428725.6875,  427524.0938,  409184.2188],
        [ 876632.8750,  471606.8438,  465706.3438,  417117.8125,  412961.8125,
          404591.8438,  402510.5625,  361333.5938,  352729.7812,  317435.7500],
        [ 961454.1250,  636679.3125,  627543.5625,  578999.1875,  570374.4375,
          539180.8750,  528735.5000,  527751.6875,  521535.0312,  511069.7500],
        [ 520218.6562,  470020.0625,  330150.5312,  325713.7500,  275340.6875,
          243521.1094,  242043.4531,  181702.4062,  174103.0781,  140125.5469],
        [ 904762.1250,  567270.4375,  563334.3750,  543340.8125,  490033.5312,
          477003.5625,  415311.7812,  411128.2812,  388820.2812,  361047.3438],
        [1094837.5000, 1000995.3125,  981357.3125,  954902.3125,  952255.9375,
          938481.9375,  917951.1250,  895281.1250,  878497.5625,  875581.8125],
        [ 752018.1250,  707452.1250,  705654.3750,  676885.7500,  668296.3750,
          667316.2500,  650109.0000,  645131.6250,  632844.3750,  624865.7500],
        [ 626899.9375,  613591.3750,  606093.0625,  585899.6875,  567976.3750,
          546941.0000,  528182.6875,  525117.9375,  507276.8750,  506802.5000],
        [1175448.6250, 1041764.2500,  962188.0000,  936739.2500,  873296.8750,
          812544.8125,  799165.9375,  781807.3125,  766892.0000,  764246.8125],
        [1058497.7500,  959628.5000,  879359.1875,  825017.8750,  797782.3125,
          794278.8125,  783225.2500,  771749.7500,  756798.1875,  746147.3750],
        [ 764455.3125,  740004.7500,  726459.0000,  582673.9375,  555956.3750,
          508952.5312,  500626.8125,  462915.7812,  461471.7812,  443382.3125],
        [ 612814.1875,  583506.9375,  544471.5625,  501631.8438,  497513.1250,
          485709.3438,  484153.5938,  481354.9062,  480664.9688,  478047.8125],
        [1108216.5000,  951456.1875,  933229.5625,  931730.2500,  859260.9375,
          848147.8125,  780010.3125,  766524.9375,  757267.4375,  752009.5625],
        [ 760266.8125,  667931.2500,  665471.9375,  620890.5625,  566583.2500,
          563459.0000,  545343.5625,  534134.6250,  529409.6250,  520349.1250],
        [1127652.8750,  852787.1250,  833525.8125,  797587.5000,  766158.0625,
          761507.0000,  691706.8750,  613634.1250,  610370.3750,  568022.9375],
        [ 854298.7500,  806033.9375,  748170.2500,  724821.6875,  716725.8125,
          674103.4375,  669055.2500,  648574.5000,  635200.0625,  622579.8125],
        [1038746.4375,  529318.7500,  496468.9688,  492463.5938,  465241.1250,
          370342.5312,  368639.8750,  355900.6250,  355508.8125,  352139.9062],
        [ 783984.5000,  736257.0000,  680784.2500,  669949.1250,  655110.3750,
          605415.4375,  575338.2500,  574947.6875,  572773.4375,  550530.4375],
        [ 817598.1250,  770376.0625,  693435.3750,  659795.9375,  618840.4375,
          557866.7500,  539904.3125,  516124.5000,  513801.5312,  501008.9375],
        [ 999400.5000,  862153.5000,  852006.7500,  739544.1250,  715568.8125,
          648546.6875,  640103.2500,  637674.6250,  634867.5625,  619210.0000],
        [ 902982.1250,  816701.9375,  787269.1250,  728728.0000,  700343.8750,
          674562.5625,  667648.5000,  653050.6875,  650692.0625,  635301.8750],
        [ 806234.6250,  747279.6250,  675639.0625,  668187.3750,  655731.0625,
          634797.9375,  632773.1250,  612753.4375,  603062.6250,  597493.6875],
        [1006862.4375,  916993.0625,  906020.1875,  856267.6875,  851344.8125,
          832083.5625,  806053.1875,  803184.8750,  787892.5000,  755567.8750],
        [ 819774.8750,  642135.6250,  638957.8125,  592595.1875,  558028.0000,
          557395.0625,  517899.5312,  513881.9062,  472678.4688,  465059.6562],
        [ 403817.7812,  345222.2188,  304523.9062,  273355.3750,  267492.3438,
          265471.1875,  262619.1875,  247387.6094,  237715.7344,  231169.2500],
        [ 618979.7500,  547763.1875,  511402.2500,  447211.1250,  444257.1875,
          421949.3438,  416786.9688,  405020.3750,  404136.4062,  382305.5000],
        [ 743481.5625,  672581.5000,  597613.8750,  577893.1250,  570042.1875,
          490605.8750,  485321.3125,  482086.7500,  481522.5000,  475520.5938],
        [ 564648.3750,  548519.5625,  501615.1250,  452871.2500,  415767.8750,
          411674.4375,  374369.0312,  354870.6250,  354830.6875,  345386.2188],
        [ 783623.4375,  291362.6562,  274647.5312,  267302.8750,  262987.0938,
          247837.9375,  242553.6562,  241559.4219,  215589.9844,  200866.3125],
        [ 777579.3750,  637936.7500,  508564.8438,  484812.9375,  391533.6562,
          361261.9062,  355227.1562,  340373.4375,  329693.6875,  316855.6562],
        [ 518195.9062,  471900.1562,  432545.9062,  403061.4062,  396343.1562,
          364034.1875,  294860.4375,  292676.0000,  274627.6250,  253794.3906],
        [ 947562.5000,  891317.1875,  849375.7500,  847438.7500,  845744.1250,
          842618.0000,  840775.0000,  816107.0625,  797865.2500,  789401.2500],
        [ 407388.0625,  399980.0625,  399704.7500,  395043.8750,  329308.4375,
          290205.9688,  278375.1875,  252115.1406,  235200.9531,  212141.9375],
        [ 520913.1562,  381588.6250,  375092.3750,  352862.6875,  333309.4062,
          321567.4375,  300474.5000,  297669.4062,  296227.3750,  292765.9062],
        [ 430678.4062,  397927.4062,  396894.5938,  333428.6250,  297400.4062,
          289913.3125,  282287.1875,  270628.6875,  241279.4375,  238147.5625],
        [ 533776.0625,  424994.8125,  359803.0312,  322234.2188,  308551.6875,
          267069.9688,  248645.4531,  243098.3281,  232482.7344,  228863.1406],
        [ 299921.9688,  254028.2969,  238036.0781,  233925.8906,  218632.7812,
          217706.2812,  190624.7344,  181947.4375,  179326.7500,  174740.8438],
        [ 170394.0312,  157118.1250,  128925.8359,   86986.4375,   85715.7891,
           77932.1406,   75062.8359,   63626.8633,   53696.3828,   53175.7734],
        [ 690452.6875,  602017.3750,  581443.8750,  501010.8438,  498704.5312,
          497573.8750,  484917.4375,  473407.9688,  465838.7188,  451043.7188],
        [ 187511.9219,  133011.0156,  131074.1875,  118217.0781,  117466.6953,
          114095.0156,   79180.5234,   77424.2500,   74001.4297,   72847.2656],
        [ 367657.8438,  211951.6406,  167652.0469,  151035.2031,  146070.0938,
          141627.9844,  139978.6406,  127548.0469,  125507.2734,  124828.5469],
        [ 499323.6562,  453221.6250,  406580.3750,  346914.0000,  266370.7188,
          262267.0312,  258777.3125,  256946.4688,  253108.4219,  245094.0156],
        [ 392625.0625,  197641.3125,  193847.2188,  188934.1406,  169520.0938,
          164716.7969,  154970.3125,  153153.8750,  152527.9844,  137218.3281],
        [ 434386.2812,  406025.8750,  303973.5000,  261167.8438,  259276.8125,
          219991.8594,  206647.2812,  203230.3750,  201801.5781,  175764.5312],
        [ 797311.5000,  586202.5625,  545910.2500,  455292.4062,  423262.4062,
          366655.3750,  309295.2812,  299133.8438,  298692.8438,  255940.5781],
        [ 634771.9375,  353595.3438,  321583.6875,  306435.5938,  297696.0938,
          292340.4375,  260334.0312,  244583.8281,  241717.7344,  238843.3281],
        [ 825441.3125,  662468.0000,  598096.8125,  592803.7500,  591767.8750,
          574922.5000,  574922.5000,  533969.0625,  499512.7500,  497510.2812],
        [ 629606.8750,  469055.5625,  420132.3750,  409576.5625,  397699.0000,
          396292.8750,  393278.6562,  387575.6250,  379411.7812,  377287.6250],
        [ 713650.3750,  490561.4375,  393763.1562,  363493.0625,  354473.1875,
          272361.0938,  265018.6562,  260241.4531,  246339.4844,  238621.3438],
        [ 775210.7500,  401040.3438,  395740.7188,  306970.2812,  275101.0312,
          257666.4375,  249354.7656,  242829.7656,  228104.4062,  226470.9375],
        [ 661972.8750,  350473.4375,  329763.8125,  302665.1250,  289294.0938,
          276908.0312,  259093.8906,  221084.9062,  215381.0000,  212499.9375],
        [ 532402.3750,  191552.6719,  188882.6094,  181494.9375,  155120.5312,
          154516.0781,  150357.4375,  150269.7031,  141286.0000,  129748.4219],
        [ 417237.5625,  404424.7812,  398912.6250,  390244.9062,  345885.5938,
          334279.6562,  328338.5000,  327370.4688,  325311.4375,  321556.0938],
        [ 213071.3750,  104337.2500,   92023.3047,   91607.0312,   89803.1641,
           86488.2188,   80556.5547,   78942.1172,   78911.3281,   73602.2891],
        [ 381481.3125,  307627.3125,  280540.5625,  270534.5000,  246213.1406,
          218194.3281,  216317.6094,  213779.8750,  199790.7188,  181647.1562],
        [ 229575.1250,  226598.1719,  175380.9219,  154824.0469,  144780.7188,
          140203.8906,  136543.7031,  125147.6328,  114428.7891,  110773.0938]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[427746.7500,      0.0000],
         [220785.5000,      0.0000],
         [214636.2656,      0.0000],
         ...,
         [171025.2188,      0.0000],
         [169435.0625,      0.0000],
         [160336.0625,      0.0000]],

        [[863789.6250,      0.0000],
         [857889.4375,      0.0000],
         [836694.8125,      0.0000],
         ...,
         [769039.3750,      0.0000],
         [746830.0625,      0.0000],
         [746043.4375,      0.0000]],

        [[537025.0000,      0.0000],
         [489713.0625,      0.0000],
         [477553.4375,      0.0000],
         ...,
         [250338.1250,      0.0000],
         [246010.3438,      0.0000],
         [     0.0000, 229928.7656]],

        ...,

        [[     0.0000, 213071.3750],
         [104337.2500,      0.0000],
         [ 92023.3047,      0.0000],
         ...,
         [ 78942.1172,      0.0000],
         [ 78911.3281,      0.0000],
         [ 73602.2891,      0.0000]],

        [[381481.3125,      0.0000],
         [307627.3125,      0.0000],
         [     0.0000, 280540.5625],
         ...,
         [     0.0000, 213779.8750],
         [199790.7188,      0.0000],
         [181647.1562,      0.0000]],

        [[     0.0000, 229575.1250],
         [     0.0000, 226598.1719],
         [     0.0000, 175380.9219],
         ...,
         [125147.6328,      0.0000],
         [114428.7891,      0.0000],
         [110773.0938,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1949439.0000,  195906.0469],
        [8032503.5000,       0.0000],
        [3271418.5000,  229928.7656],
        [4777452.0000,       0.0000],
        [ 620321.5000,  871034.7500],
        [1409197.2500,  418231.3750],
        [2978382.7500, 1355368.6250],
        [3371309.2500,       0.0000],
        [4287509.0000,  640628.4375],
        [4482627.5000,       0.0000],
        [6003323.5000,       0.0000],
        [2902939.5000,       0.0000],
        [5122052.5000,       0.0000],
        [9490142.0000,       0.0000],
        [6730574.0000,       0.0000],
        [5614781.5000,       0.0000],
        [8914094.0000,       0.0000],
        [8372485.0000,       0.0000],
        [5746899.0000,       0.0000],
        [5149868.0000,       0.0000],
        [8687854.0000,       0.0000],
        [5973840.0000,       0.0000],
        [7622952.5000,       0.0000],
        [7099564.0000,       0.0000],
        [4824771.0000,       0.0000],
        [6405091.0000,       0.0000],
        [6188752.0000,       0.0000],
        [7349076.0000,       0.0000],
        [7217281.0000,       0.0000],
        [6633953.0000,       0.0000],
        [8522270.0000,       0.0000],
        [5778406.0000,       0.0000],
        [2607605.5000,  231169.2500],
        [2800583.0000, 1799229.1250],
        [3101190.7500, 2475478.5000],
        [2450547.7500, 1874005.3750],
        [1754470.0000, 1273861.0000],
        [2579758.5000, 1924081.0000],
        [1583659.7500, 2118379.5000],
        [8468205.0000,       0.0000],
        [3199464.2500,       0.0000],
        [3472471.0000,       0.0000],
        [3178585.5000,       0.0000],
        [3169519.5000,       0.0000],
        [ 634506.1250, 1554385.0000],
        [ 496196.1875,  456438.0000],
        [4664967.5000,  581443.8750],
        [ 896330.9375,  208498.4375],
        [ 706259.9375,  997597.3750],
        [1275749.2500, 1972854.2500],
        [1460438.5000,  444716.6250],
        [1570685.8750, 1101580.0000],
        [2372012.2500, 1965684.7500],
        [2838306.7500,  353595.3438],
        [5951415.0000,       0.0000],
        [3862217.7500,  397699.0000],
        [1622711.7500, 1975811.3750],
        [2286607.2500, 1071882.1250],
        [1952489.1250, 1166648.0000],
        [ 819207.1250, 1156423.6250],
        [3194649.0000,  398912.6250],
        [ 776271.2500,  213071.3750],
        [1751271.5000,  764854.9375],
        [ 631673.9375,  926582.1875]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 59.375
Top1 accuracy for validation set is 59.375 size is torch.Size([64, 1])
Epoch 241/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:50, 59.67s/it]  7%|▋         | 2/30 [01:00<11:40, 25.01s/it] 10%|█         | 3/30 [01:01<06:16, 13.94s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.73s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.85s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.784134848912557
Epoch 242/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:17, 56.47s/it]  7%|▋         | 2/30 [01:01<12:07, 25.98s/it] 10%|█         | 3/30 [01:01<06:30, 14.46s/it] 13%|█▎        | 4/30 [01:02<03:55,  9.05s/it] 17%|█▋        | 5/30 [01:03<02:31,  6.05s/it] 20%|██        | 6/30 [01:04<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.36s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.7374272108078004
Epoch 243/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:33, 63.24s/it]  7%|▋         | 2/30 [01:03<12:21, 26.48s/it] 10%|█         | 3/30 [01:04<06:37, 14.73s/it] 13%|█▎        | 4/30 [01:05<03:59,  9.21s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.16s/it] 20%|██        | 6/30 [01:06<01:43,  4.32s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.15s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.84s/it]
Epoch loss is 2.748064557711283
Epoch 244/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:22, 56.63s/it]  7%|▋         | 2/30 [00:57<11:05, 23.76s/it] 10%|█         | 3/30 [00:58<05:57, 13.25s/it] 13%|█▎        | 4/30 [00:58<03:36,  8.32s/it] 17%|█▋        | 5/30 [01:00<02:28,  5.95s/it] 20%|██        | 6/30 [01:01<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:02<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:02<00:51,  2.32s/it] 30%|███       | 9/30 [01:03<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:05<00:24,  1.27s/it] 40%|████      | 12/30 [01:05<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.08it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.65s/it]
Epoch loss is 2.754040026664734
Epoch 245/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:30, 59.00s/it]  7%|▋         | 2/30 [00:59<11:32, 24.73s/it] 10%|█         | 3/30 [01:00<06:12, 13.78s/it] 13%|█▎        | 4/30 [01:01<03:44,  8.64s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.79s/it] 20%|██        | 6/30 [01:02<01:37,  4.08s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 2.7560869455337524
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0248,  0.0060,  0.0014,  ...,  0.0069, -0.0061,  0.0139],
        [-0.0020,  0.0246,  0.0157,  ...,  0.0096, -0.0145, -0.0145],
        [-0.0361, -0.0315,  0.0124,  ...,  0.0784, -0.0247, -0.0263],
        ...,
        [ 0.0244, -0.0148, -0.0091,  ..., -0.0240, -0.0292, -0.0151],
        [-0.0279, -0.0038, -0.0122,  ..., -0.0038,  0.0114, -0.0126],
        [-0.0374, -0.0079,  0.0128,  ...,  0.0243,  0.0220, -0.0517]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9069, 0.8606, 0.8593, 0.8575, 0.8546, 0.8516, 0.8450, 0.8442, 0.8408,
         0.8379],
        [0.9562, 0.9555, 0.9536, 0.9528, 0.9527, 0.9486, 0.9485, 0.9464, 0.9457,
         0.9456],
        [0.9218, 0.9160, 0.9151, 0.8918, 0.8898, 0.8863, 0.8694, 0.8663, 0.8661,
         0.8614],
        [0.9583, 0.9205, 0.9137, 0.9136, 0.9099, 0.9073, 0.9050, 0.8954, 0.8941,
         0.8941],
        [0.8687, 0.8536, 0.8492, 0.8338, 0.8323, 0.8214, 0.8144, 0.8139, 0.8030,
         0.8002],
        [0.8711, 0.8692, 0.8468, 0.8410, 0.8398, 0.8378, 0.8351, 0.8262, 0.8257,
         0.8213],
        [0.9263, 0.9239, 0.9227, 0.9000, 0.8999, 0.8991, 0.8967, 0.8922, 0.8888,
         0.8882],
        [0.9324, 0.9159, 0.9136, 0.8847, 0.8799, 0.8731, 0.8670, 0.8584, 0.8552,
         0.8526],
        [0.9335, 0.9293, 0.9208, 0.9178, 0.9174, 0.9095, 0.9074, 0.9055, 0.9054,
         0.9028],
        [0.9577, 0.9131, 0.9117, 0.9058, 0.9044, 0.9029, 0.9027, 0.8951, 0.8922,
         0.8848],
        [0.9637, 0.9334, 0.9322, 0.9270, 0.9266, 0.9215, 0.9215, 0.9200, 0.9183,
         0.9181],
        [0.9207, 0.9125, 0.8874, 0.8863, 0.8755, 0.8647, 0.8643, 0.8448, 0.8414,
         0.8283],
        [0.9588, 0.9259, 0.9254, 0.9242, 0.9171, 0.9125, 0.9048, 0.9044, 0.8988,
         0.8924],
        [0.9725, 0.9665, 0.9654, 0.9632, 0.9630, 0.9614, 0.9608, 0.9585, 0.9574,
         0.9565],
        [0.9470, 0.9424, 0.9418, 0.9384, 0.9377, 0.9372, 0.9359, 0.9352, 0.9336,
         0.9331],
        [0.9344, 0.9323, 0.9299, 0.9294, 0.9270, 0.9241, 0.9220, 0.9211, 0.9187,
         0.9177],
        [0.9773, 0.9690, 0.9631, 0.9608, 0.9560, 0.9506, 0.9493, 0.9477, 0.9470,
         0.9466],
        [0.9706, 0.9642, 0.9579, 0.9529, 0.9508, 0.9499, 0.9485, 0.9477, 0.9467,
         0.9458],
        [0.9471, 0.9451, 0.9427, 0.9263, 0.9228, 0.9182, 0.9173, 0.9120, 0.9119,
         0.9095],
        [0.9307, 0.9287, 0.9239, 0.9177, 0.9157, 0.9151, 0.9149, 0.9140, 0.9139,
         0.9139],
        [0.9728, 0.9625, 0.9613, 0.9604, 0.9562, 0.9546, 0.9479, 0.9478, 0.9467,
         0.9465],
        [0.9465, 0.9378, 0.9373, 0.9315, 0.9262, 0.9247, 0.9223, 0.9222, 0.9220,
         0.9194],
        [0.9755, 0.9553, 0.9541, 0.9506, 0.9480, 0.9472, 0.9408, 0.9320, 0.9317,
         0.9270],
        [0.9559, 0.9511, 0.9454, 0.9439, 0.9420, 0.9387, 0.9375, 0.9357, 0.9354,
         0.9331],
        [0.9690, 0.9216, 0.9172, 0.9142, 0.9113, 0.8965, 0.8964, 0.8941, 0.8941,
         0.8914],
        [0.9501, 0.9451, 0.9409, 0.9378, 0.9375, 0.9320, 0.9270, 0.9267, 0.9264,
         0.9252],
        [0.9517, 0.9479, 0.9413, 0.9370, 0.9326, 0.9252, 0.9228, 0.9203, 0.9189,
         0.9187],
        [0.9668, 0.9567, 0.9562, 0.9454, 0.9424, 0.9363, 0.9356, 0.9349, 0.9346,
         0.9330],
        [0.9593, 0.9521, 0.9502, 0.9437, 0.9416, 0.9398, 0.9379, 0.9365, 0.9359,
         0.9346],
        [0.9509, 0.9457, 0.9394, 0.9388, 0.9371, 0.9345, 0.9334, 0.9313, 0.9300,
         0.9299],
        [0.9665, 0.9604, 0.9594, 0.9557, 0.9551, 0.9537, 0.9509, 0.9499, 0.9493,
         0.9470],
        [0.9533, 0.9348, 0.9345, 0.9290, 0.9255, 0.9253, 0.9202, 0.9186, 0.9140,
         0.9125],
        [0.9018, 0.8909, 0.8824, 0.8744, 0.8734, 0.8734, 0.8727, 0.8698, 0.8660,
         0.8645],
        [0.9320, 0.9225, 0.9192, 0.9103, 0.9097, 0.9043, 0.9030, 0.9030, 0.9027,
         0.8987],
        [0.9454, 0.9386, 0.9307, 0.9285, 0.9251, 0.9165, 0.9159, 0.9156, 0.9136,
         0.9130],
        [0.9256, 0.9234, 0.9166, 0.9109, 0.9030, 0.9028, 0.8950, 0.8934, 0.8921,
         0.8904],
        [0.9494, 0.8793, 0.8753, 0.8746, 0.8716, 0.8686, 0.8659, 0.8658, 0.8575,
         0.8556],
        [0.9464, 0.9329, 0.9183, 0.9155, 0.8995, 0.8944, 0.8935, 0.8901, 0.8874,
         0.8866],
        [0.9197, 0.9138, 0.9068, 0.9001, 0.8989, 0.8936, 0.8798, 0.8796, 0.8752,
         0.8700],
        [0.9617, 0.9576, 0.9545, 0.9541, 0.9535, 0.9531, 0.9530, 0.9509, 0.9503,
         0.9494],
        [0.9026, 0.9022, 0.9014, 0.9013, 0.8880, 0.8795, 0.8767, 0.8693, 0.8630,
         0.8576],
        [0.9210, 0.8976, 0.8971, 0.8927, 0.8891, 0.8859, 0.8820, 0.8813, 0.8812,
         0.8804],
        [0.9079, 0.9021, 0.9012, 0.8881, 0.8817, 0.8803, 0.8780, 0.8745, 0.8663,
         0.8648],
        [0.9219, 0.9051, 0.8942, 0.8874, 0.8830, 0.8742, 0.8692, 0.8670, 0.8636,
         0.8626],
        [0.8823, 0.8693, 0.8654, 0.8627, 0.8575, 0.8568, 0.8490, 0.8471, 0.8452,
         0.8424],
        [0.8416, 0.8389, 0.8237, 0.7977, 0.7927, 0.7881, 0.7853, 0.7758, 0.7627,
         0.7617],
        [0.9383, 0.9297, 0.9287, 0.9175, 0.9168, 0.9166, 0.9146, 0.9132, 0.9124,
         0.9100],
        [0.8476, 0.8240, 0.8221, 0.8152, 0.8147, 0.8127, 0.7882, 0.7874, 0.7803,
         0.7787],
        [0.8942, 0.8576, 0.8397, 0.8319, 0.8293, 0.8282, 0.8279, 0.8215, 0.8186,
         0.8182],
        [0.9173, 0.9081, 0.9005, 0.8916, 0.8709, 0.8700, 0.8697, 0.8683, 0.8674,
         0.8671],
        [0.9021, 0.8555, 0.8536, 0.8525, 0.8447, 0.8404, 0.8365, 0.8353, 0.8340,
         0.8275],
        [0.9081, 0.9028, 0.8799, 0.8726, 0.8697, 0.8582, 0.8551, 0.8523, 0.8519,
         0.8427],
        [0.9499, 0.9297, 0.9247, 0.9125, 0.9049, 0.8962, 0.8823, 0.8820, 0.8817,
         0.8711],
        [0.9344, 0.8931, 0.8876, 0.8835, 0.8813, 0.8808, 0.8725, 0.8674, 0.8666,
         0.8654],
        [0.9520, 0.9366, 0.9296, 0.9290, 0.9286, 0.9270, 0.9270, 0.9211, 0.9168,
         0.9165],
        [0.9326, 0.9117, 0.9036, 0.9031, 0.9010, 0.9005, 0.8994, 0.8990, 0.8976,
         0.8967],
        [0.9419, 0.9154, 0.8996, 0.8949, 0.8938, 0.8737, 0.8717, 0.8706, 0.8693,
         0.8643],
        [0.9488, 0.9027, 0.9014, 0.8845, 0.8774, 0.8713, 0.8694, 0.8672, 0.8623,
         0.8617],
        [0.9366, 0.8918, 0.8884, 0.8809, 0.8776, 0.8767, 0.8712, 0.8587, 0.8580,
         0.8574],
        [0.9213, 0.8499, 0.8490, 0.8466, 0.8355, 0.8342, 0.8322, 0.8319, 0.8290,
         0.8237],
        [0.9030, 0.9016, 0.9008, 0.8986, 0.8904, 0.8881, 0.8876, 0.8866, 0.8864,
         0.8859],
        [0.8595, 0.8065, 0.8017, 0.7995, 0.7984, 0.7896, 0.7860, 0.7854, 0.7853,
         0.7811],
        [0.8995, 0.8838, 0.8776, 0.8725, 0.8665, 0.8581, 0.8577, 0.8567, 0.8521,
         0.8447],
        [0.8604, 0.8600, 0.8455, 0.8337, 0.8276, 0.8269, 0.8255, 0.8166, 0.8130,
         0.8101]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 1, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 1, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 1, 0, 1, 0, 1],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 1, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 423270.8750,  218333.1562,  214449.2500,  208864.3438,  200512.4375,
          191967.2500,  174797.1719,  172869.8438,  164628.3750,  157938.2188],
        [ 855879.0625,  846890.1875,  825056.4375,  815315.1875,  814251.4375,
          768386.1250,  766977.5625,  744098.6250,  736499.3125,  735763.5625],
        [ 523893.4062,  482326.3125,  475947.9688,  340959.2188,  331582.1250,
          315392.8750,  247834.8594,  236885.4219,  236313.8906,  221030.9375],
        [ 882630.0625,  513931.4375,  466104.4688,  466002.2188,  441600.2500,
          425476.5625,  411642.6250,  359262.0000,  352537.0625,  352479.9375],
        [ 245167.6406,  197730.8594,  185613.5312,  148953.1875,  145771.8750,
          124742.3750,  112908.3516,  112146.9531,   95965.3672,   92221.5938],
        [ 253931.8906,  247075.9062,  179248.7812,  165141.4844,  162320.6250,
          157727.3438,  151645.2812,  133540.1406,  132740.2031,  124621.4531],
        [ 558736.2500,  539621.1875,  530417.3125,  383326.6250,  382767.7188,
          378612.9688,  365870.5000,  343112.0625,  326906.8438,  323920.1875],
        [ 608865.7500,  481344.8125,  466005.7812,  308146.4375,  287703.2812,
          261321.0625,  239236.5625,  211813.6250,  202113.9688,  194743.5000],
        [ 618492.3750,  582671.1250,  516157.4688,  494750.0000,  491437.0625,
          439176.5312,  426479.5938,  414684.0625,  414107.8438,  399403.7188],
        [ 874366.8750,  462249.2188,  453399.7500,  416454.4062,  408190.7500,
          399473.7812,  398715.2500,  357429.8750,  342962.9062,  308712.0938],
        [ 952647.5000,  617814.3750,  607396.7500,  563634.2500,  560800.0000,
          521108.9375,  521097.5000,  510037.0000,  497965.5312,  497008.1250],
        [ 515639.8438,  458411.5000,  320190.3750,  315502.0625,  270440.3438,
          231519.1562,  230196.4531,  174387.7344,  166131.1094,  137790.3281],
        [ 888521.6875,  555506.3750,  551570.3750,  541693.4375,  489468.8750,
          458597.3125,  410968.3438,  408559.9688,  376794.6562,  343887.1250],
        [1081093.0000,  992120.6250,  976120.0000,  946476.9375,  943437.2500,
          921709.9375,  914271.1875,  884415.5000,  871041.9375,  859582.1875],
        [ 750164.3750,  703143.9375,  696597.6875,  663331.5625,  657490.6875,
          652666.5000,  640625.3750,  634146.3125,  619593.9375,  615660.5000],
        [ 626700.3125,  608617.8750,  588161.4375,  583322.8125,  563773.5000,
          540891.7500,  524930.1875,  518268.1250,  501202.4688,  493819.4688],
        [1156665.6250, 1027656.5000,  945005.8125,  914540.6875,  853474.6875,
          790381.2500,  775606.3750,  757932.8750,  750719.0000,  746291.8125],
        [1051399.8750,  959484.8125,  876600.3125,  816881.1250,  791970.3125,
          782300.3125,  767204.3125,  758548.9375,  747462.8125,  737627.5000],
        [ 751585.8125,  730719.0625,  706137.0625,  558410.2500,  531340.8125,
          497235.6875,  490858.6250,  454977.2812,  454616.4062,  439062.1875],
        [ 594640.0000,  577954.8750,  539416.9375,  493659.8438,  480048.8125,
          475621.7500,  474649.3438,  468338.1250,  467929.1875,  467905.0938],
        [1084788.2500,  935976.6250,  920449.4375,  908579.6875,  856369.0000,
          836634.1875,  760213.8750,  759342.2500,  746816.5625,  745308.8750],
        [ 745257.6875,  658071.5625,  653812.1875,  601596.1250,  557559.8750,
          546062.8125,  527776.3125,  526791.7500,  525084.8750,  505967.5000],
        [1127803.5000,  845229.6250,  830643.6875,  789747.6250,  761143.1875,
          752747.8750,  686959.1250,  605401.0000,  602848.1250,  563790.1250],
        [ 852331.8750,  796268.9375,  733635.6875,  717820.2500,  698842.7500,
          666692.1875,  655592.3125,  638903.0625,  635831.6250,  615799.6875],
        [1027722.1875,  521903.6875,  490406.1562,  470046.0938,  450882.4375,
          364957.1250,  364501.7812,  352766.4688,  352605.3438,  339197.1875],
        [ 784169.1875,  730832.6250,  687806.7500,  658432.5625,  655196.0000,
          605633.6875,  564051.5000,  561243.5625,  558914.2500,  550015.5625],
        [ 802841.7500,  760311.0625,  691584.8125,  650562.3750,  610879.9375,
          549515.9375,  531045.4375,  512703.1562,  502712.7812,  500770.5312],
        [ 996040.7500,  862448.8125,  855993.3750,  733387.3125,  702663.3125,
          643741.4375,  638088.8750,  631003.8750,  629027.1250,  614646.1875],
        [ 895200.8750,  807239.4375,  785800.4375,  716319.8750,  694721.4375,
          677273.7500,  659405.3125,  646346.6250,  640238.1875,  629157.3125],
        [ 793742.6250,  737180.9375,  673369.0000,  667542.1875,  651173.1875,
          627557.9375,  618212.2500,  599406.6875,  588331.3750,  587865.8750],
        [ 991929.5625,  908871.7500,  895463.8750,  850057.2500,  842451.6875,
          826501.5625,  793745.6875,  782089.1875,  776040.6250,  750492.7500],
        [ 820697.9375,  630291.1875,  628190.2500,  580661.3750,  551676.6875,
          550123.1250,  511537.8750,  500458.7812,  468398.4688,  458530.0000],
        [ 393768.4062,  336699.4062,  298171.7188,  265965.8438,  262449.1875,
          262284.5312,  259536.8125,  248981.9219,  236006.6875,  231084.8281],
        [ 605523.4375,  529243.5625,  504406.0625,  444417.3750,  440511.6875,
          407872.8438,  400591.2188,  400239.5000,  398435.0938,  376522.0312],
        [ 733210.3750,  665852.1875,  595056.4375,  576362.4375,  549002.6250,
          485727.4062,  481618.4688,  479015.2812,  465911.5938,  461668.1250],
        [ 552672.5000,  535742.6250,  485893.2812,  447947.8750,  400402.9062,
          399227.3750,  356941.0625,  348870.7812,  342616.3750,  334213.0312],
        [ 776669.2500,  285311.4062,  269517.8125,  266915.9062,  255788.0781,
          244721.9375,  235642.8125,  235141.5156,  208960.7656,  203496.0781],
        [ 743711.3125,  613903.9375,  498239.5938,  478441.8750,  380827.7500,
          353919.2188,  349265.9375,  332760.5625,  320209.6250,  316913.3750],
        [ 507979.7812,  466923.5312,  422631.9688,  383866.5938,  377338.3750,
          350147.7188,  287214.4688,  286430.8125,  269045.5625,  249682.4375],
        [ 926581.0625,  873701.6875,  835294.8125,  830182.0000,  823730.8750,
          818559.3125,  817974.0625,  793197.0625,  786992.8750,  777230.1250],
        [ 398141.1250,  395729.4062,  391054.4688,  390878.1250,  322997.8438,
          285973.6562,  275038.8750,  247281.7031,  225915.0312,  209193.4531],
        [ 517486.7812,  370369.7188,  367879.5000,  345697.3125,  328043.9688,
          313654.6562,  296398.9062,  293588.4688,  293070.6562,  289738.6250],
        [ 429361.1250,  395039.7500,  390322.7188,  323604.0312,  295086.3125,
          289620.3750,  279906.6875,  266453.0312,  236976.2656,  231823.1719],
        [ 524600.3750,  412448.1875,  353111.1250,  320327.8125,  300692.6562,
          265188.0312,  247117.8750,  239402.7188,  227927.8281,  224805.1875],
        [ 297623.4062,  247333.5938,  233791.3906,  225105.7656,  208852.7812,
          206966.1719,  185175.5781,  180198.6719,  175309.0156,  168449.0469],
        [ 166548.0000,  160147.0312,  128997.1719,   88950.1250,   82824.8125,
           77511.0547,   74465.2656,   65090.0859,   53976.0391,   53196.5703],
        [ 663271.5000,  586323.3125,  577889.8125,  492711.6250,  487243.0938,
          485932.2188,  472421.5938,  463271.7812,  457817.7500,  442609.6250],
        [ 181512.9375,  129517.4922,  126091.5312,  114165.9844,  113436.8906,
          110257.3906,   77684.2969,   76749.8906,   69344.5625,   67828.8203],
        [ 353270.0938,  209159.5469,  162167.2812,  144939.0312,  139774.2656,
          137535.9062,  136952.9375,  125044.8047,  119931.3125,  119249.8750],
        [ 491008.4375,  430727.2500,  386219.6562,  339930.3125,  252957.5938,
          249759.8438,  248719.9219,  243785.9844,  240800.6250,  239707.9375],
        [ 395206.2812,  203183.2969,  197613.4219,  194447.6875,  174064.2344,
          163788.5312,  154910.0156,  152127.3281,  149391.9375,  136154.6406],
        [ 430862.8438,  399237.6875,  287880.5938,  259229.8281,  248807.4688,
          211049.4375,  201919.2031,  193963.9219,  193015.4688,  169241.7656],
        [ 781957.9375,  586264.0625,  545638.5625,  458759.1562,  411127.8750,
          363068.6250,  297791.2188,  296749.6562,  295216.6250,  253828.2656],
        [ 626794.6875,  347353.6250,  321157.6875,  302801.6875,  293714.5000,
          291673.1875,  259055.3594,  240627.0781,  238001.1250,  233799.2031],
        [ 805751.1250,  647215.1250,  585316.5625,  579969.6250,  577439.6875,
          563690.1250,  563690.1250,  518181.6250,  487833.1562,  485172.3125],
        [ 610705.7500,  453429.1562,  403931.4062,  400884.3438,  389220.1875,
          386118.7188,  380327.9688,  377935.8438,  370743.2500,  365984.9688],
        [ 698021.4375,  477582.1250,  381168.5625,  356564.7812,  351072.5625,
          263457.8125,  255989.1562,  252159.8594,  247415.2344,  230353.6719],
        [ 769854.6875,  398397.1250,  391293.5938,  307448.1250,  277500.4688,
          254609.4062,  247672.7656,  239978.7656,  223818.5781,  221786.6406],
        [ 647052.8125,  341279.3125,  324858.1875,  292039.2188,  278641.5938,
          275027.5938,  254263.3906,  212600.6719,  210423.2031,  208548.6719],
        [ 519955.2500,  187373.9219,  184956.9062,  178744.7031,  152539.9219,
          149728.9688,  145500.4844,  145006.3594,  139131.6562,  128897.0625],
        [ 400104.8125,  392582.0312,  388069.7188,  376064.4688,  334430.5000,
          323687.0625,  321247.4375,  316792.1875,  315804.0312,  313447.7188],
        [ 214926.7031,  100862.3516,   94166.6016,   91272.2578,   89877.6094,
           79181.3594,   75280.2734,   74642.0938,   74505.8359,   70212.5781],
        [ 380909.4688,  304135.3125,  278398.0312,  258932.0938,  237734.5625,
          210830.3594,  209670.8125,  206649.8438,  193590.4375,  174107.2344],
        [ 217954.7500,  216428.4062,  176178.0312,  148831.2188,  136417.7031,
          134936.7031,  132291.0781,  116556.4609,  110730.2109,  106209.2891]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[423270.8750,      0.0000],
         [218333.1562,      0.0000],
         [214449.2500,      0.0000],
         ...,
         [172869.8438,      0.0000],
         [164628.3750,      0.0000],
         [157938.2188,      0.0000]],

        [[855879.0625,      0.0000],
         [846890.1875,      0.0000],
         [825056.4375,      0.0000],
         ...,
         [744098.6250,      0.0000],
         [736499.3125,      0.0000],
         [735763.5625,      0.0000]],

        [[523893.4062,      0.0000],
         [482326.3125,      0.0000],
         [475947.9688,      0.0000],
         ...,
         [236885.4219,      0.0000],
         [236313.8906,      0.0000],
         [     0.0000, 221030.9375]],

        ...,

        [[     0.0000, 214926.7031],
         [100862.3516,      0.0000],
         [ 94166.6016,      0.0000],
         ...,
         [ 74642.0938,      0.0000],
         [ 74505.8359,      0.0000],
         [ 70212.5781,      0.0000]],

        [[380909.4688,      0.0000],
         [304135.3125,      0.0000],
         [     0.0000, 278398.0312],
         ...,
         [206649.8438,      0.0000],
         [193590.4375,      0.0000],
         [174107.2344,      0.0000]],

        [[     0.0000, 217954.7500],
         [     0.0000, 216428.4062],
         [     0.0000, 176178.0312],
         ...,
         [116556.4609,      0.0000],
         [110730.2109,      0.0000],
         [106209.2891,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1935663.7500,  191967.2500],
        [7909117.0000,       0.0000],
        [3191136.0000,  221030.9375],
        [4671667.0000,       0.0000],
        [ 607654.3750,  853567.3125],
        [1320521.0000,  387472.0312],
        [2852830.5000, 1280461.2500],
        [3261294.7500,       0.0000],
        [4178867.2500,  618492.3750],
        [4421955.0000,       0.0000],
        [5849510.0000,       0.0000],
        [2820208.7500,       0.0000],
        [5025568.0000,       0.0000],
        [9390268.0000,       0.0000],
        [6633421.0000,       0.0000],
        [5549688.0000,       0.0000],
        [8718274.0000,       0.0000],
        [8289480.0000,       0.0000],
        [5614943.5000,       0.0000],
        [5040164.0000,       0.0000],
        [8554478.0000,       0.0000],
        [5847981.0000,       0.0000],
        [7566313.5000,       0.0000],
        [7011718.0000,       0.0000],
        [4734988.5000,       0.0000],
        [6356295.5000,       0.0000],
        [6112927.5000,       0.0000],
        [7307041.0000,       0.0000],
        [7151703.5000,       0.0000],
        [6544382.0000,       0.0000],
        [8417644.0000,       0.0000],
        [5700565.5000,       0.0000],
        [2563864.5000,  231084.8281],
        [2749707.0000, 1758055.7500],
        [3038984.5000, 2454440.2500],
        [2034930.2500, 2169597.5000],
        [1729619.6250, 1252545.8750],
        [2532338.5000, 1855854.8750],
        [1533428.5000, 2067832.6250],
        [8283444.0000,       0.0000],
        [3142203.7500,       0.0000],
        [3415928.7500,       0.0000],
        [3138193.5000,       0.0000],
        [3115621.7500,       0.0000],
        [ 609267.5625, 1519537.8750],
        [ 496013.9688,  455692.1875],
        [4551602.5000,  577889.8125],
        [ 860322.4375,  206267.3750],
        [ 684247.0000,  963778.1250],
        [1228552.6250, 1895065.0000],
        [1480430.7500,  440456.5625],
        [1505877.8750, 1089330.3750],
        [2330969.2500, 1959432.5000],
        [2807624.5000,  347353.6250],
        [5814259.0000,       0.0000],
        [3761345.7500,  377935.8438],
        [1582617.3750, 1931167.7500],
        [2265169.0000, 1067191.2500],
        [1910491.5000, 1134243.0000],
        [ 803290.6875, 1128544.5000],
        [3089648.0000,  392582.0312],
        [ 750001.0000,  214926.7031],
        [1707957.2500,  747000.9375],
        [ 604850.3750,  891683.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 60.9375
Top1 accuracy for validation set is 60.9375 size is torch.Size([64, 1])
Epoch 246/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:39, 61.37s/it]  7%|▋         | 2/30 [01:02<11:59, 25.71s/it] 10%|█         | 3/30 [01:02<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:04<02:29,  6.00s/it] 20%|██        | 6/30 [01:05<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.734717035293579
Epoch 247/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:27, 60.95s/it]  7%|▋         | 2/30 [01:01<11:55, 25.54s/it] 10%|█         | 3/30 [01:02<06:27, 14.35s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.98s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.01s/it] 20%|██        | 6/30 [01:04<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.7823522965113323
Epoch 248/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:29, 56.88s/it]  7%|▋         | 2/30 [01:00<11:50, 25.36s/it] 10%|█         | 3/30 [01:01<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:02<02:29,  6.00s/it] 20%|██        | 6/30 [01:03<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.7407289266586305
Epoch 249/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:01, 62.11s/it]  7%|▋         | 2/30 [01:02<12:08, 26.01s/it] 10%|█         | 3/30 [01:03<06:30, 14.48s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 2.718489718437195
Epoch 250/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:02, 58.01s/it]  7%|▋         | 2/30 [00:58<11:21, 24.33s/it] 10%|█         | 3/30 [01:01<06:26, 14.30s/it] 13%|█▎        | 4/30 [01:01<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.99s/it] 20%|██        | 6/30 [01:03<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.34s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.728360644976298
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0247,  0.0057,  0.0029,  ...,  0.0060, -0.0047,  0.0131],
        [-0.0018,  0.0241,  0.0163,  ...,  0.0101, -0.0133, -0.0148],
        [-0.0356, -0.0321,  0.0134,  ...,  0.0787, -0.0235, -0.0257],
        ...,
        [ 0.0235, -0.0139, -0.0079,  ..., -0.0238, -0.0280, -0.0154],
        [-0.0277, -0.0034, -0.0122,  ..., -0.0033,  0.0112, -0.0129],
        [-0.0376, -0.0079,  0.0126,  ...,  0.0255,  0.0227, -0.0512]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9059, 0.8598, 0.8591, 0.8561, 0.8551, 0.8501, 0.8465, 0.8431, 0.8393,
         0.8369],
        [0.9555, 0.9545, 0.9523, 0.9523, 0.9508, 0.9480, 0.9473, 0.9448, 0.9446,
         0.9441],
        [0.9203, 0.9146, 0.9145, 0.8907, 0.8892, 0.8836, 0.8657, 0.8626, 0.8617,
         0.8592],
        [0.9578, 0.9199, 0.9125, 0.9116, 0.9072, 0.9052, 0.9025, 0.8940, 0.8939,
         0.8929],
        [0.8672, 0.8523, 0.8475, 0.8323, 0.8312, 0.8187, 0.8147, 0.8122, 0.8036,
         0.7986],
        [0.8675, 0.8661, 0.8431, 0.8358, 0.8347, 0.8334, 0.8278, 0.8218, 0.8215,
         0.8170],
        [0.9221, 0.9198, 0.9189, 0.8976, 0.8972, 0.8964, 0.8942, 0.8860, 0.8859,
         0.8856],
        [0.9306, 0.9141, 0.9108, 0.8825, 0.8788, 0.8716, 0.8635, 0.8556, 0.8527,
         0.8500],
        [0.9307, 0.9267, 0.9201, 0.9165, 0.9164, 0.9076, 0.9049, 0.9036, 0.9033,
         0.9015],
        [0.9574, 0.9114, 0.9093, 0.9053, 0.9034, 0.9023, 0.9016, 0.8942, 0.8897,
         0.8832],
        [0.9628, 0.9318, 0.9300, 0.9255, 0.9252, 0.9201, 0.9189, 0.9173, 0.9164,
         0.9150],
        [0.9199, 0.9110, 0.8850, 0.8840, 0.8742, 0.8613, 0.8608, 0.8422, 0.8385,
         0.8268],
        [0.9574, 0.9250, 0.9237, 0.9236, 0.9171, 0.9095, 0.9038, 0.9037, 0.8959,
         0.8894],
        [0.9718, 0.9659, 0.9650, 0.9626, 0.9626, 0.9604, 0.9603, 0.9577, 0.9572,
         0.9554],
        [0.9470, 0.9424, 0.9411, 0.9374, 0.9373, 0.9359, 0.9349, 0.9347, 0.9329,
         0.9316],
        [0.9343, 0.9318, 0.9291, 0.9279, 0.9265, 0.9232, 0.9214, 0.9201, 0.9183,
         0.9162],
        [0.9760, 0.9681, 0.9620, 0.9594, 0.9545, 0.9486, 0.9473, 0.9458, 0.9456,
         0.9449],
        [0.9703, 0.9640, 0.9577, 0.9522, 0.9502, 0.9491, 0.9475, 0.9467, 0.9459,
         0.9454],
        [0.9457, 0.9443, 0.9407, 0.9235, 0.9198, 0.9169, 0.9155, 0.9112, 0.9110,
         0.9090],
        [0.9287, 0.9275, 0.9228, 0.9167, 0.9140, 0.9135, 0.9133, 0.9127, 0.9125,
         0.9124],
        [0.9713, 0.9612, 0.9600, 0.9585, 0.9557, 0.9534, 0.9475, 0.9465, 0.9464,
         0.9454],
        [0.9452, 0.9370, 0.9358, 0.9292, 0.9250, 0.9227, 0.9215, 0.9215, 0.9198,
         0.9183],
        [0.9754, 0.9547, 0.9537, 0.9499, 0.9477, 0.9463, 0.9403, 0.9309, 0.9307,
         0.9265],
        [0.9555, 0.9503, 0.9442, 0.9434, 0.9403, 0.9377, 0.9361, 0.9351, 0.9349,
         0.9328],
        [0.9683, 0.9205, 0.9161, 0.9113, 0.9091, 0.8958, 0.8955, 0.8936, 0.8931,
         0.8893],
        [0.9499, 0.9447, 0.9415, 0.9373, 0.9364, 0.9318, 0.9262, 0.9253, 0.9247,
         0.9241],
        [0.9504, 0.9468, 0.9407, 0.9360, 0.9316, 0.9238, 0.9215, 0.9197, 0.9186,
         0.9170],
        [0.9667, 0.9565, 0.9562, 0.9447, 0.9410, 0.9355, 0.9355, 0.9346, 0.9333,
         0.9326],
        [0.9586, 0.9512, 0.9501, 0.9429, 0.9410, 0.9399, 0.9370, 0.9360, 0.9351,
         0.9344],
        [0.9501, 0.9446, 0.9394, 0.9380, 0.9365, 0.9341, 0.9318, 0.9305, 0.9296,
         0.9282],
        [0.9654, 0.9598, 0.9584, 0.9551, 0.9544, 0.9535, 0.9500, 0.9484, 0.9480,
         0.9466],
        [0.9530, 0.9338, 0.9336, 0.9272, 0.9249, 0.9242, 0.9193, 0.9166, 0.9129,
         0.9113],
        [0.9000, 0.8892, 0.8811, 0.8729, 0.8729, 0.8723, 0.8720, 0.8699, 0.8658,
         0.8641],
        [0.9307, 0.9204, 0.9182, 0.9099, 0.9092, 0.9027, 0.9026, 0.9018, 0.9002,
         0.8973],
        [0.9441, 0.9380, 0.9303, 0.9285, 0.9227, 0.9158, 0.9156, 0.9153, 0.9118,
         0.9117],
        [0.9244, 0.9216, 0.9142, 0.9099, 0.9009, 0.9002, 0.8921, 0.8917, 0.8897,
         0.8896],
        [0.9490, 0.8776, 0.8760, 0.8724, 0.8692, 0.8679, 0.8640, 0.8631, 0.8568,
         0.8555],
        [0.9435, 0.9301, 0.9164, 0.9145, 0.8974, 0.8933, 0.8922, 0.8883, 0.8864,
         0.8854],
        [0.9183, 0.9128, 0.9051, 0.8964, 0.8953, 0.8910, 0.8783, 0.8776, 0.8736,
         0.8690],
        [0.9600, 0.9562, 0.9535, 0.9525, 0.9518, 0.9511, 0.9506, 0.9498, 0.9489,
         0.9479],
        [0.9016, 0.9008, 0.9006, 0.8996, 0.8864, 0.8785, 0.8758, 0.8676, 0.8600,
         0.8564],
        [0.9205, 0.8958, 0.8956, 0.8916, 0.8880, 0.8847, 0.8821, 0.8802, 0.8798,
         0.8797],
        [0.9075, 0.9014, 0.8998, 0.8858, 0.8809, 0.8802, 0.8770, 0.8731, 0.8648,
         0.8630],
        [0.9204, 0.9028, 0.8927, 0.8870, 0.8818, 0.8733, 0.8686, 0.8658, 0.8621,
         0.8608],
        [0.8816, 0.8674, 0.8636, 0.8600, 0.8538, 0.8533, 0.8474, 0.8473, 0.8420,
         0.8396],
        [0.8400, 0.8396, 0.8232, 0.7986, 0.7904, 0.7885, 0.7852, 0.7773, 0.7634,
         0.7620],
        [0.9359, 0.9279, 0.9277, 0.9165, 0.9154, 0.9150, 0.9132, 0.9115, 0.9110,
         0.9084],
        [0.8451, 0.8229, 0.8186, 0.8130, 0.8121, 0.8104, 0.7863, 0.7861, 0.7758,
         0.7741],
        [0.8914, 0.8564, 0.8379, 0.8287, 0.8284, 0.8269, 0.8234, 0.8199, 0.8163,
         0.8156],
        [0.9162, 0.9049, 0.8977, 0.8903, 0.8672, 0.8668, 0.8668, 0.8661, 0.8643,
         0.8636],
        [0.9025, 0.8571, 0.8547, 0.8539, 0.8457, 0.8395, 0.8376, 0.8348, 0.8321,
         0.8273],
        [0.9076, 0.9018, 0.8764, 0.8724, 0.8668, 0.8557, 0.8533, 0.8493, 0.8488,
         0.8403],
        [0.9485, 0.9295, 0.9245, 0.9129, 0.9028, 0.8950, 0.8812, 0.8805, 0.8795,
         0.8712],
        [0.9336, 0.8913, 0.8874, 0.8824, 0.8812, 0.8797, 0.8716, 0.8663, 0.8657,
         0.8641],
        [0.9503, 0.9350, 0.9281, 0.9275, 0.9269, 0.9255, 0.9255, 0.9190, 0.9156,
         0.9148],
        [0.9308, 0.9091, 0.9016, 0.9010, 0.9001, 0.8993, 0.8970, 0.8960, 0.8952,
         0.8946],
        [0.9406, 0.9134, 0.8975, 0.8935, 0.8926, 0.8713, 0.8696, 0.8692, 0.8681,
         0.8617],
        [0.9484, 0.9031, 0.8995, 0.8845, 0.8777, 0.8706, 0.8691, 0.8662, 0.8615,
         0.8610],
        [0.9354, 0.8901, 0.8875, 0.8781, 0.8763, 0.8752, 0.8699, 0.8576, 0.8564,
         0.8557],
        [0.9198, 0.8485, 0.8476, 0.8452, 0.8343, 0.8325, 0.8301, 0.8296, 0.8278,
         0.8236],
        [0.9003, 0.9002, 0.8982, 0.8963, 0.8882, 0.8873, 0.8856, 0.8851, 0.8841,
         0.8837],
        [0.8605, 0.8050, 0.8028, 0.8001, 0.7978, 0.7842, 0.7830, 0.7825, 0.7822,
         0.7788],
        [0.8991, 0.8830, 0.8772, 0.8694, 0.8636, 0.8562, 0.8556, 0.8539, 0.8496,
         0.8419],
        [0.8577, 0.8558, 0.8458, 0.8309, 0.8255, 0.8239, 0.8215, 0.8118, 0.8105,
         0.8076]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 0, 0, 1, 0, 0, 1],
        [1, 1, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 1, 0, 0, 1, 0, 1, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 1, 1, 0, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 0, 0, 0, 1, 1, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [1, 0, 1, 0, 0, 1, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 417532.1250,  215809.4688,  213930.0000,  204713.0156,  201882.8125,
          187968.1406,  178646.7031,  170093.9844,  161093.9844,  155793.4688],
        [ 847241.5625,  835673.2500,  809755.3125,  809740.6250,  792242.2500,
          761700.8750,  753711.1250,  727650.8750,  725482.1250,  720083.5625],
        [ 512179.7812,  472311.6875,  472118.0312,  335630.8750,  328722.3125,
          303544.1875,  234905.2969,  224817.4219,  221982.1719,  214093.4844],
        [ 875725.4375,  509954.3125,  458237.0938,  452759.4062,  425026.8438,
          412849.1875,  397709.6250,  352084.1562,  351316.7500,  346694.0312],
        [ 240213.9062,  193885.5156,  181256.7656,  145857.6719,  143460.1562,
          120098.3125,  113453.3281,  109384.8594,   96723.8438,   90145.0078],
        [ 241051.7500,  236276.4844,  170198.8125,  153206.3281,  150936.7031,
          148192.6094,  136714.3906,  125552.4062,  124894.2734,  117143.2812],
        [ 525862.6250,  508909.8125,  502726.6562,  370767.3125,  368464.1250,
          364480.2500,  353173.7500,  313837.4688,  313329.6562,  312119.1250],
        [ 594134.9375,  469364.2812,  447553.7188,  298610.2500,  283463.9062,
          255520.1406,  227661.9375,  203344.5625,  195207.4219,  187845.5625],
        [ 594735.8750,  561405.2500,  510817.3125,  485353.2812,  484878.1250,
          427506.5625,  411545.2500,  403768.5000,  401768.8438,  391869.4375],
        [ 870512.1250,  451409.0312,  438027.9375,  413796.7812,  402834.6562,
          396114.9062,  392260.5625,  353086.1875,  331261.3438,  301474.8125],
        [ 940523.9375,  603635.7500,  588485.6875,  552177.7500,  549834.6875,
          511157.9688,  502727.6250,  490768.7500,  484532.3438,  475085.9062],
        [ 509450.7500,  448974.2812,  309507.4375,  305154.7812,  265246.2188,
          220729.7188,  219204.2031,  167898.1250,  159313.1719,  134830.9531],
        [ 870653.2500,  547942.8750,  538293.0625,  537257.5625,  489556.5938,
          439451.7812,  404828.0312,  404356.5312,  361748.3750,  329496.5938],
        [1070309.1250,  983093.0000,  970645.0000,  938518.6875,  938079.3125,
          908430.6250,  907422.8125,  874573.6250,  868091.3750,  845736.8125],
        [ 750169.3125,  702527.3125,  690289.4375,  654191.3750,  653261.1875,
          640300.4375,  631512.5625,  629871.7500,  613320.5000,  601909.5000],
        [ 625860.5000,  604307.9375,  581402.7500,  571280.2500,  560062.4375,
          533923.2500,  520792.9375,  510851.4062,  498253.8438,  483208.9062],
        [1135548.1250, 1014889.9375,  929461.0000,  896490.1250,  835300.3750,
          768399.3750,  753425.1250,  738108.8750,  736031.0000,  728554.3125],
        [1046922.3750,  956989.1250,  874401.8750,  808792.1250,  785418.2500,
          773004.1250,  755671.0000,  747313.8750,  739022.3750,  733465.6250],
        [ 737047.3750,  722098.4375,  685612.8125,  536606.2500,  509168.0625,
          488113.3125,  478493.0000,  449991.0938,  448489.0000,  436362.6562],
        [ 578087.1875,  568019.6875,  530953.7500,  487008.9688,  468172.0312,
          465252.1875,  463718.2188,  459922.6562,  458331.5000,  458154.4688],
        [1062774.2500,  919534.3750,  904065.2500,  884659.3125,  849414.6875,
          822127.5000,  755596.0000,  745308.1875,  743955.3750,  734052.8125],
        [ 731680.6250,  650781.4375,  639809.0625,  582251.7500,  548213.1250,
          530214.0000,  521733.5000,  521513.6562,  508935.5312,  498222.0312],
        [1126098.8750,  837787.8750,  825564.8750,  782226.4375,  757629.3750,
          743494.3125,  682122.3750,  596753.3750,  594907.7500,  559877.6875],
        [ 847096.9375,  787206.8125,  721570.5000,  712832.8125,  681594.3125,
          657206.6875,  642378.7500,  633584.0625,  631860.1875,  612665.1875],
        [1017248.5625,  514328.5938,  482883.6875,  450409.2500,  436888.9688,
          360952.6562,  359551.2812,  350090.2812,  347400.9688,  329378.4688],
        [ 782070.5625,  726279.6250,  693414.2500,  653419.5000,  645214.6875,
          604169.6250,  557589.1250,  550526.1875,  546168.0000,  541005.2500],
        [ 788257.8125,  748567.0625,  686376.3125,  641026.3125,  602623.4375,
          538515.9375,  521535.0312,  508114.0000,  500427.2812,  488595.8125],
        [ 993890.5625,  860171.7500,  856511.9375,  725950.0000,  688970.7500,
          637196.1875,  637133.0000,  628949.1875,  617357.9375,  610617.2500],
        [ 885290.6250,  796761.1875,  783994.9375,  708240.5625,  689224.4375,
          678594.0000,  650511.5625,  641492.3125,  632929.4375,  626950.8125],
        [ 784033.8125,  724983.5000,  673588.6250,  659754.4375,  645610.5000,
          623980.2500,  603962.1875,  592881.1875,  584973.3750,  574085.8750],
        [ 976861.2500,  901130.0000,  883104.0625,  843011.8125,  834396.6875,
          824038.9375,  783762.4375,  766006.8125,  761327.6250,  746232.0625],
        [ 817761.0625,  621158.8750,  619880.0625,  565992.9375,  546973.8750,
          541731.1250,  505252.4375,  486163.5000,  461402.7188,  450867.8125],
        [ 383779.4688,  328876.8750,  292857.2188,  260492.7344,  260352.6406,
          258330.2656,  257258.8438,  249441.5781,  235134.3438,  229626.3750],
        [ 594889.6250,  512933.0312,  497618.5000,  441808.3750,  437291.2188,
          398371.2812,  398214.0312,  393743.6250,  384811.1562,  368804.4375],
        [ 719702.5000,  659538.6250,  591000.8125,  575924.0000,  530188.7500,
          480379.4688,  479289.9375,  477335.3125,  453940.5625,  453484.9375],
        [ 543083.8125,  522195.4688,  469836.7812,  442081.8750,  388257.7812,
          384431.8750,  342683.0312,  340626.7500,  331026.6875,  330630.0938],
        [ 771874.0625,  278474.2188,  272323.6875,  258429.8281,  246827.4531,
          242343.4844,  229345.7812,  226226.5781,  206874.8281,  203050.6094],
        [ 714251.6250,  589533.3125,  484850.8438,  471799.8125,  369505.3438,
          348305.9688,  342912.5312,  324558.4062,  315868.4688,  311144.6250],
        [ 498226.7500,  460655.7188,  412672.8438,  364141.1562,  358485.0625,
          337454.8750,  281177.2500,  278599.3438,  262966.5625,  246270.1875],
        [ 904347.2500,  855976.2500,  823179.5625,  811967.7500,  803816.3125,
          795519.0000,  790681.3750,  781415.9375,  771656.2500,  759845.6875],
        [ 392111.6875,  387672.0938,  386772.1562,  381540.6250,  315641.1250,
          281920.7500,  271489.7500,  241393.6094,  216633.0469,  205782.5312],
        [ 513890.2500,  360943.0312,  360206.7812,  340163.5000,  323309.4375,
          308164.6875,  296977.2500,  288859.6250,  287322.1562,  287159.4375],
        [ 426677.2812,  391194.7500,  382623.1875,  313191.3438,  292134.1875,
          288849.7188,  275935.0312,  261335.2500,  231920.2500,  225949.7344],
        [ 513147.3438,  399216.3438,  345720.7188,  318631.6875,  295779.4062,
          261715.6094,  244800.8438,  235344.3281,  223065.0938,  219150.5000],
        [ 294893.8750,  240686.0625,  228079.8281,  216438.9375,  198325.1719,
          196946.8438,  180998.6875,  180561.1250,  167400.7344,  161823.7031],
        [ 162739.4375,  161750.0938,  128091.4844,   90054.0156,   80188.8672,
           77992.2891,   74370.9531,   66466.3594,   54515.9219,   53380.6875],
        [ 640691.3750,  571249.2500,  569981.8750,  485314.3750,  478024.5625,
          475442.1250,  462823.5312,  452022.0625,  448878.8438,  432133.5625],
        [ 175156.7812,  127467.9141,  119883.8672,  110670.8828,  109308.0000,
          106665.1562,   75561.7266,   75356.4844,   65079.1016,   63445.0859],
        [ 339233.4062,  205662.4688,  157976.4844,  138449.2031,  137889.4375,
          134963.7344,  128415.8516,  122083.7344,  116062.9609,  114803.6016],
        [ 483153.1562,  411123.5625,  370868.0938,  333922.8125,  239972.5781,
          238744.0156,  238650.0156,  236232.3281,  230360.7031,  228131.8125],
        [ 397702.4375,  207794.4531,  200852.1406,  198611.5625,  176660.2344,
          161568.1719,  157370.2031,  151042.6875,  145303.0156,  135718.1406],
        [ 427680.6875,  393555.1562,  273951.4375,  258738.0781,  238675.5000,
          203741.1562,  196727.7656,  186004.2656,  184561.3594,  163398.3438],
        [ 766837.1250,  584434.1875,  544265.4375,  461319.5625,  399219.4062,
          357285.0312,  293214.9375,  290407.5625,  286138.9688,  253984.2031],
        [ 619943.8750,  338922.9688,  320200.7500,  298110.8750,  293073.1875,
          287115.8750,  255695.6406,  237106.0312,  234905.9531,  229664.2500],
        [ 786638.6875,  632146.4375,  573123.0625,  568401.6875,  563308.0625,
          552101.4375,  552101.4375,  503400.7188,  478957.7500,  473883.5938],
        [ 595136.4375,  437012.7500,  392355.2188,  389076.9375,  383809.1250,
          379444.7188,  367368.6875,  362177.7812,  358097.5938,  355232.5938],
        [ 684939.7500,  464192.9688,  369827.2500,  349614.8750,  345146.8438,
          254552.0938,  248271.0781,  247149.9062,  243083.9375,  221841.4219],
        [ 765208.7500,  401165.8438,  380960.6875,  307278.1250,  278989.9375,
          251995.9062,  246653.3125,  236722.6094,  221150.2656,  219723.0625],
        [ 635680.6250,  332778.0312,  320684.8438,  280467.2812,  273480.8125,
          269076.8438,  249314.3438,  209215.0000,  205647.7500,  203643.4375],
        [ 509187.0000,  183650.6406,  181473.6562,  175367.5469,  150132.1875,
          146283.2344,  141234.0000,  140231.9688,  136697.0469,  128737.3594],
        [ 385304.3438,  384531.5938,  373583.3438,  363550.5625,  323814.5625,
          319913.8438,  312018.5625,  309848.2500,  305684.8750,  303858.1250],
        [ 218069.7344,   98649.2344,   95698.8672,   92033.4922,   89121.7266,
           73312.5391,   72119.3750,   71549.9531,   71286.3750,   67886.1562],
        [ 378875.9375,  300908.0938,  276991.5000,  247550.9375,  228032.4219,
          205108.1562,  203309.0781,  198558.1562,  186609.9688,  167318.0469],
        [ 209496.3125,  203992.1562,  176827.4531,  142898.9531,  132302.6875,
          129242.8281,  124928.6953,  108765.3125,  106744.8281,  102442.6953]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[417532.1250,      0.0000],
         [215809.4688,      0.0000],
         [213930.0000,      0.0000],
         ...,
         [170093.9844,      0.0000],
         [161093.9844,      0.0000],
         [155793.4688,      0.0000]],

        [[847241.5625,      0.0000],
         [835673.2500,      0.0000],
         [809755.3125,      0.0000],
         ...,
         [727650.8750,      0.0000],
         [725482.1250,      0.0000],
         [720083.5625,      0.0000]],

        [[512179.7812,      0.0000],
         [472311.6875,      0.0000],
         [472118.0312,      0.0000],
         ...,
         [224817.4219,      0.0000],
         [221982.1719,      0.0000],
         [     0.0000, 214093.4844]],

        ...,

        [[     0.0000, 218069.7344],
         [ 98649.2344,      0.0000],
         [ 95698.8672,      0.0000],
         ...,
         [ 71549.9531,      0.0000],
         [ 71286.3750,      0.0000],
         [ 67886.1562,      0.0000]],

        [[378875.9375,      0.0000],
         [300908.0938,      0.0000],
         [     0.0000, 276991.5000],
         ...,
         [198558.1562,      0.0000],
         [186609.9688,      0.0000],
         [167318.0469,      0.0000]],

        [[     0.0000, 209496.3125],
         [     0.0000, 203992.1562],
         [     0.0000, 176827.4531],
         ...,
         [108765.3125,      0.0000],
         [106744.8281,      0.0000],
         [102442.6953,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1919495.5000,  187968.1406],
        [7783282.0000,       0.0000],
        [3106211.7500,  214093.4844],
        [4582357.0000,       0.0000],
        [ 596285.0000,  838194.3750],
        [1242996.2500,  361170.7500],
        [2729490.2500, 1204180.3750],
        [3162706.7500,       0.0000],
        [4078912.5000,  594735.8750],
        [4350778.5000,       0.0000],
        [5698930.5000,       0.0000],
        [2740309.7500,       0.0000],
        [4923584.5000,       0.0000],
        [9304900.0000,       0.0000],
        [6567353.0000,       0.0000],
        [5489944.5000,       0.0000],
        [8536208.0000,       0.0000],
        [8221001.0000,       0.0000],
        [5491982.5000,       0.0000],
        [4937620.5000,       0.0000],
        [8421488.0000,       0.0000],
        [5733354.5000,       0.0000],
        [7506463.0000,       0.0000],
        [6927996.0000,       0.0000],
        [4649133.0000,       0.0000],
        [6299857.0000,       0.0000],
        [6024039.0000,       0.0000],
        [7256749.0000,       0.0000],
        [7093990.0000,       0.0000],
        [6467853.5000,       0.0000],
        [8319871.5000,       0.0000],
        [5617184.5000,       0.0000],
        [2526524.0000,  229626.3750],
        [2706725.5000, 1721759.8750],
        [2988284.5000, 2432500.5000],
        [1966142.0000, 2128712.0000],
        [1702416.1250, 1233354.5000],
        [2484095.2500, 1788635.8750],
        [1483776.1250, 2016873.7500],
        [8098406.0000,       0.0000],
        [3080957.2500,       0.0000],
        [3366996.0000,       0.0000],
        [3089811.0000,       0.0000],
        [3056571.7500,       0.0000],
        [ 582164.8750, 1483990.1250],
        [ 496969.0625,  452581.0000],
        [4446580.0000,  569981.8750],
        [ 825770.6250,  202824.4062],
        [ 777864.8750,  817676.0000],
        [1181618.2500, 1829540.8750],
        [1494231.7500,  438391.3750],
        [1447059.7500, 1079974.0000],
        [2290614.0000, 1946492.5000],
        [2775816.5000,  338922.9688],
        [5684063.0000,       0.0000],
        [3661614.2500,  358097.5938],
        [1540982.5000, 1887637.6250],
        [2248732.0000, 1061116.5000],
        [1871588.0000, 1108400.8750],
        [ 787081.4375, 1105913.2500],
        [2996803.7500,  385304.3438],
        [ 731657.6875,  218069.7344],
        [1663611.7500,  729650.6250],
        [ 579498.3750,  858143.5625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 60.9375
Top1 accuracy for validation set is 60.9375 size is torch.Size([64, 1])
Epoch 251/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:18, 62.69s/it]  7%|▋         | 2/30 [01:03<12:15, 26.26s/it] 10%|█         | 3/30 [01:04<06:34, 14.61s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.14s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.11s/it] 20%|██        | 6/30 [01:06<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 2.760227743784587
Epoch 252/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:18, 60.63s/it]  7%|▋         | 2/30 [01:01<11:51, 25.40s/it] 10%|█         | 3/30 [01:02<06:21, 14.15s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.86s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.7209447304407757
Epoch 253/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:30, 56.91s/it]  7%|▋         | 2/30 [00:57<11:08, 23.88s/it] 10%|█         | 3/30 [00:58<05:59, 13.32s/it] 13%|█▎        | 4/30 [00:59<03:37,  8.36s/it] 17%|█▋        | 5/30 [00:59<02:20,  5.61s/it] 20%|██        | 6/30 [01:00<01:35,  3.96s/it] 23%|██▎       | 7/30 [01:01<01:06,  2.91s/it] 27%|██▋       | 8/30 [01:02<00:48,  2.22s/it] 30%|███       | 9/30 [01:02<00:36,  1.76s/it] 33%|███▎      | 10/30 [01:03<00:28,  1.45s/it] 37%|███▋      | 11/30 [01:04<00:23,  1.23s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:05<00:16,  1.02it/s] 47%|████▋     | 14/30 [01:06<00:14,  1.10it/s] 50%|█████     | 15/30 [01:07<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:08<00:10,  1.24it/s] 60%|██████    | 18/30 [01:09<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:10<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:11<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:12<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:13<00:05,  1.33it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:14<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:15<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:16<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:17<00:00,  1.34it/s]100%|██████████| 30/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:18<00:00,  2.63s/it]
Epoch loss is 2.720435078938802
Epoch 254/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:43, 59.43s/it]  7%|▋         | 2/30 [01:01<11:57, 25.62s/it] 10%|█         | 3/30 [01:02<06:27, 14.34s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.03s/it] 20%|██        | 6/30 [01:04<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.746669562657674
Epoch 255/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:02, 60.08s/it]  7%|▋         | 2/30 [01:00<11:45, 25.18s/it] 10%|█         | 3/30 [01:01<06:18, 14.02s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.6946137666702272
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0245,  0.0055,  0.0043,  ...,  0.0050, -0.0034,  0.0124],
        [-0.0017,  0.0234,  0.0170,  ...,  0.0107, -0.0123, -0.0151],
        [-0.0353, -0.0328,  0.0143,  ...,  0.0790, -0.0225, -0.0252],
        ...,
        [ 0.0225, -0.0131, -0.0068,  ..., -0.0240, -0.0264, -0.0158],
        [-0.0277, -0.0030, -0.0121,  ..., -0.0028,  0.0114, -0.0132],
        [-0.0378, -0.0082,  0.0125,  ...,  0.0264,  0.0235, -0.0504]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9050, 0.8592, 0.8588, 0.8550, 0.8547, 0.8489, 0.8479, 0.8424, 0.8379,
         0.8365],
        [0.9548, 0.9534, 0.9515, 0.9509, 0.9488, 0.9476, 0.9461, 0.9441, 0.9435,
         0.9416],
        [0.9184, 0.9139, 0.9128, 0.8899, 0.8880, 0.8810, 0.8621, 0.8586, 0.8574,
         0.8573],
        [0.9571, 0.9193, 0.9112, 0.9099, 0.9046, 0.9034, 0.9004, 0.8934, 0.8929,
         0.8921],
        [0.8658, 0.8510, 0.8465, 0.8318, 0.8299, 0.8165, 0.8145, 0.8108, 0.8036,
         0.7986],
        [0.8659, 0.8619, 0.8401, 0.8321, 0.8294, 0.8291, 0.8214, 0.8182, 0.8172,
         0.8147],
        [0.9178, 0.9161, 0.9156, 0.8954, 0.8940, 0.8938, 0.8915, 0.8833, 0.8832,
         0.8797],
        [0.9286, 0.9123, 0.9083, 0.8802, 0.8778, 0.8701, 0.8599, 0.8527, 0.8499,
         0.8472],
        [0.9280, 0.9239, 0.9193, 0.9155, 0.9149, 0.9055, 0.9025, 0.9021, 0.9010,
         0.9005],
        [0.9568, 0.9095, 0.9066, 0.9044, 0.9024, 0.9009, 0.8998, 0.8933, 0.8877,
         0.8814],
        [0.9617, 0.9299, 0.9276, 0.9244, 0.9234, 0.9187, 0.9169, 0.9145, 0.9142,
         0.9125],
        [0.9188, 0.9096, 0.8829, 0.8821, 0.8723, 0.8578, 0.8573, 0.8390, 0.8351,
         0.8252],
        [0.9557, 0.9239, 0.9232, 0.9218, 0.9168, 0.9066, 0.9031, 0.9023, 0.8930,
         0.8864],
        [0.9714, 0.9654, 0.9648, 0.9624, 0.9622, 0.9598, 0.9592, 0.9572, 0.9571,
         0.9549],
        [0.9470, 0.9420, 0.9405, 0.9375, 0.9366, 0.9349, 0.9345, 0.9341, 0.9324,
         0.9306],
        [0.9340, 0.9312, 0.9287, 0.9258, 0.9258, 0.9225, 0.9207, 0.9193, 0.9181,
         0.9147],
        [0.9746, 0.9672, 0.9609, 0.9579, 0.9530, 0.9464, 0.9453, 0.9449, 0.9435,
         0.9431],
        [0.9702, 0.9638, 0.9576, 0.9515, 0.9495, 0.9482, 0.9466, 0.9459, 0.9452,
         0.9450],
        [0.9442, 0.9433, 0.9386, 0.9209, 0.9169, 0.9154, 0.9137, 0.9103, 0.9099,
         0.9088],
        [0.9266, 0.9263, 0.9213, 0.9159, 0.9129, 0.9119, 0.9117, 0.9114, 0.9110,
         0.9107],
        [0.9698, 0.9600, 0.9586, 0.9567, 0.9551, 0.9521, 0.9470, 0.9465, 0.9460,
         0.9440],
        [0.9441, 0.9363, 0.9344, 0.9269, 0.9240, 0.9210, 0.9206, 0.9205, 0.9177,
         0.9174],
        [0.9753, 0.9542, 0.9532, 0.9494, 0.9472, 0.9455, 0.9396, 0.9299, 0.9298,
         0.9261],
        [0.9549, 0.9496, 0.9429, 0.9429, 0.9383, 0.9366, 0.9347, 0.9346, 0.9341,
         0.9328],
        [0.9678, 0.9195, 0.9150, 0.9083, 0.9069, 0.8950, 0.8946, 0.8937, 0.8921,
         0.8880],
        [0.9495, 0.9442, 0.9417, 0.9370, 0.9352, 0.9316, 0.9250, 0.9250, 0.9228,
         0.9220],
        [0.9491, 0.9457, 0.9399, 0.9349, 0.9306, 0.9224, 0.9200, 0.9191, 0.9182,
         0.9152],
        [0.9667, 0.9563, 0.9561, 0.9443, 0.9398, 0.9354, 0.9347, 0.9344, 0.9322,
         0.9322],
        [0.9578, 0.9501, 0.9499, 0.9421, 0.9405, 0.9401, 0.9361, 0.9356, 0.9343,
         0.9341],
        [0.9491, 0.9434, 0.9391, 0.9368, 0.9357, 0.9335, 0.9301, 0.9296, 0.9290,
         0.9268],
        [0.9645, 0.9593, 0.9574, 0.9544, 0.9538, 0.9533, 0.9492, 0.9478, 0.9462,
         0.9461],
        [0.9526, 0.9328, 0.9326, 0.9256, 0.9242, 0.9231, 0.9182, 0.9147, 0.9119,
         0.9103],
        [0.8985, 0.8879, 0.8798, 0.8721, 0.8716, 0.8715, 0.8714, 0.8704, 0.8653,
         0.8636],
        [0.9296, 0.9180, 0.9172, 0.9093, 0.9088, 0.9019, 0.9014, 0.9006, 0.8975,
         0.8955],
        [0.9425, 0.9373, 0.9296, 0.9283, 0.9207, 0.9151, 0.9150, 0.9148, 0.9107,
         0.9106],
        [0.9232, 0.9200, 0.9121, 0.9087, 0.8989, 0.8972, 0.8909, 0.8887, 0.8886,
         0.8873],
        [0.9486, 0.8765, 0.8758, 0.8702, 0.8673, 0.8668, 0.8621, 0.8606, 0.8579,
         0.8550],
        [0.9407, 0.9272, 0.9145, 0.9133, 0.8951, 0.8916, 0.8904, 0.8863, 0.8861,
         0.8832],
        [0.9168, 0.9118, 0.9031, 0.8934, 0.8917, 0.8886, 0.8766, 0.8750, 0.8720,
         0.8678],
        [0.9586, 0.9548, 0.9523, 0.9511, 0.9500, 0.9493, 0.9491, 0.9484, 0.9470,
         0.9462],
        [0.9008, 0.8999, 0.8987, 0.8983, 0.8848, 0.8773, 0.8747, 0.8664, 0.8571,
         0.8553],
        [0.9201, 0.8949, 0.8936, 0.8906, 0.8872, 0.8836, 0.8823, 0.8793, 0.8792,
         0.8791],
        [0.9068, 0.9008, 0.8991, 0.8837, 0.8801, 0.8798, 0.8759, 0.8718, 0.8633,
         0.8622],
        [0.9189, 0.9010, 0.8911, 0.8867, 0.8806, 0.8727, 0.8681, 0.8649, 0.8609,
         0.8591],
        [0.8807, 0.8655, 0.8618, 0.8570, 0.8502, 0.8499, 0.8470, 0.8460, 0.8385,
         0.8363],
        [0.8402, 0.8383, 0.8225, 0.7993, 0.7886, 0.7883, 0.7851, 0.7788, 0.7634,
         0.7616],
        [0.9334, 0.9267, 0.9260, 0.9156, 0.9139, 0.9134, 0.9118, 0.9096, 0.9096,
         0.9063],
        [0.8427, 0.8218, 0.8153, 0.8108, 0.8096, 0.8081, 0.7847, 0.7838, 0.7721,
         0.7715],
        [0.8883, 0.8554, 0.8369, 0.8279, 0.8255, 0.8255, 0.8194, 0.8182, 0.8170,
         0.8130],
        [0.9154, 0.9011, 0.8944, 0.8887, 0.8648, 0.8642, 0.8640, 0.8639, 0.8609,
         0.8602],
        [0.9031, 0.8582, 0.8556, 0.8552, 0.8462, 0.8387, 0.8382, 0.8340, 0.8300,
         0.8268],
        [0.9073, 0.9011, 0.8732, 0.8721, 0.8641, 0.8532, 0.8514, 0.8466, 0.8462,
         0.8381],
        [0.9470, 0.9290, 0.9242, 0.9131, 0.9010, 0.8941, 0.8803, 0.8791, 0.8773,
         0.8710],
        [0.9326, 0.8898, 0.8868, 0.8812, 0.8809, 0.8785, 0.8708, 0.8651, 0.8645,
         0.8628],
        [0.9489, 0.9332, 0.9266, 0.9264, 0.9253, 0.9240, 0.9240, 0.9172, 0.9145,
         0.9134],
        [0.9291, 0.9063, 0.9001, 0.8997, 0.8981, 0.8974, 0.8947, 0.8942, 0.8931,
         0.8928],
        [0.9389, 0.9115, 0.8954, 0.8920, 0.8917, 0.8696, 0.8688, 0.8666, 0.8650,
         0.8586],
        [0.9478, 0.9034, 0.8979, 0.8849, 0.8779, 0.8697, 0.8683, 0.8647, 0.8603,
         0.8598],
        [0.9342, 0.8882, 0.8863, 0.8754, 0.8753, 0.8726, 0.8682, 0.8567, 0.8544,
         0.8538],
        [0.9185, 0.8472, 0.8461, 0.8439, 0.8331, 0.8306, 0.8279, 0.8278, 0.8268,
         0.8230],
        [0.8990, 0.8973, 0.8955, 0.8935, 0.8862, 0.8856, 0.8842, 0.8822, 0.8821,
         0.8820],
        [0.8609, 0.8033, 0.8030, 0.8001, 0.7966, 0.7801, 0.7794, 0.7793, 0.7788,
         0.7766],
        [0.8986, 0.8823, 0.8769, 0.8664, 0.8611, 0.8547, 0.8531, 0.8510, 0.8470,
         0.8396],
        [0.8552, 0.8516, 0.8463, 0.8279, 0.8238, 0.8202, 0.8176, 0.8078, 0.8074,
         0.8051]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 1, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 1, 0, 0, 0, 1, 0, 1],
        [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 1, 1, 1, 0, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 411737.6250,  213969.5781,  212849.2031,  201615.9531,  200720.7812,
          184814.2656,  182311.4844,  168417.5625,  158004.6562,  154828.7812],
        [ 839172.8750,  822840.5000,  799905.5000,  793023.8750,  769633.6875,
          756737.5625,  740594.3125,  719650.3750,  713887.2500,  694369.1250],
        [ 498630.8438,  467649.4688,  460536.2500,  331858.3125,  322926.4062,
          292472.3438,  223138.9219,  212323.7031,  208559.7969,  208239.6250],
        [ 866877.6875,  505265.9062,  450075.6562,  441818.0312,  409570.7188,
          402607.2812,  385818.7500,  349237.2812,  346398.5625,  342724.8750],
        [ 235296.7500,  190512.7656,  178713.6719,  144730.6094,  140791.7031,
          116342.1172,  113114.6328,  107185.0000,   96764.2578,   90057.6250],
        [ 235731.3594,  222404.2656,  163028.9688,  145486.4688,  139785.2031,
          139210.2344,  124811.8750,  119185.4062,  117541.5547,  113396.0000],
        [ 494328.8125,  483016.7812,  479278.9688,  359299.3438,  351916.3438,
          350996.2500,  339459.9375,  301939.7812,  301512.1875,  287163.5625],
        [ 577325.1875,  456913.1250,  431630.2500,  289133.5938,  279182.8750,
          250125.9688,  216377.0312,  195121.8125,  187410.3906,  180326.4062],
        [ 572261.8125,  539497.1875,  505598.0312,  478614.3750,  474589.1562,
          414951.9062,  397358.9375,  395387.9688,  388878.1250,  386013.4375],
        [ 863783.0000,  439040.0000,  421449.0625,  408543.5938,  396837.4375,
          388534.0938,  382653.4688,  348343.5000,  321557.6250,  294189.0938],
        [ 925821.5000,  587815.9375,  568508.5000,  543802.6875,  535543.3750,
          501277.0000,  487914.0938,  471858.3438,  469522.3125,  458505.0625],
        [ 501619.4375,  439820.7188,  300594.5938,  297090.2812,  258082.3125,
          209754.8125,  208360.2031,  160411.7812,  151689.9688,  131764.6406],
        [ 850230.0000,  539743.1875,  534378.1250,  523881.9375,  487731.7500,
          421230.8750,  400850.3125,  396428.5625,  347265.5000,  315926.5938],
        [1063780.2500,  975721.6250,  968198.6875,  935338.6250,  933151.3125,
          900714.1250,  893259.9375,  868354.6875,  867162.1875,  840035.2500],
        [ 750122.8125,  699162.6875,  683912.3125,  655170.3750,  646883.1250,
          631135.0625,  628192.6250,  623843.3750,  608850.6875,  593552.7500],
        [ 623650.0625,  599250.1250,  578211.2500,  554701.6875,  554476.9375,
          528744.1250,  515780.0625,  505257.7500,  496334.5625,  473435.9375],
        [1113407.2500, 1001376.3125,  916022.8125,  877522.8750,  817354.9375,
          744310.1875,  733026.5000,  728213.2500,  714140.6250,  709864.8125],
        [1045457.6875,  954641.8750,  873869.1875,  800184.0000,  778056.3125,
          763052.5000,  746711.1250,  739042.1250,  731995.4375,  729079.0625],
        [ 721518.8125,  712111.1875,  666004.6250,  517145.8750,  488441.5625,
          477834.9375,  466556.3125,  444626.8125,  441950.8125,  435189.4688],
        [ 561032.7500,  558437.4375,  520107.0312,  481661.1875,  461383.3438,
          454385.3750,  453601.3125,  451624.7812,  449008.9688,  446783.5625],
        [1038925.7500,  903183.6875,  885453.5000,  862523.6250,  842318.3125,
          807308.6875,  750812.0625,  744965.6250,  739891.8750,  719001.4375],
        [ 720574.0000,  643894.3750,  627234.1875,  563091.6250,  540238.1250,
          517908.3750,  514601.3750,  514242.7500,  493592.5312,  491845.4688],
        [1124328.2500,  831348.3125,  820015.6250,  777026.3125,  752190.2500,
          734833.7500,  675450.9375,  588254.5000,  587363.1875,  556452.8750],
        [ 840518.4375,  778934.6250,  708036.6250,  707865.1250,  662957.8125,
          646976.9375,  629925.1875,  628787.8125,  624090.3750,  612478.2500],
        [1010266.4375,  506713.0625,  475170.1875,  431955.1875,  423350.0000,
          356825.3438,  354839.8438,  350471.4375,  342806.9375,  323236.9688],
        [ 777626.8125,  721604.2500,  695793.6250,  650562.3750,  634458.4375,
          602387.2500,  548417.5625,  548214.6875,  531039.3125,  525336.8125],
        [ 773551.3125,  737032.6250,  678594.0000,  631757.7500,  593605.4375,
          528519.2500,  510611.2812,  503674.9062,  497442.4688,  476811.1562],
        [ 994374.1250,  856668.0000,  854630.3750,  721719.1250,  677535.3750,
          636313.2500,  629931.8125,  627084.6875,  607539.8750,  607171.4375],
        [ 876231.6875,  784894.1250,  782688.3125,  699618.8750,  683869.2500,
          680318.2500,  641980.1250,  638009.1250,  626286.2500,  624079.0000],
        [ 773551.3125,  713299.2500,  670090.3750,  648909.1875,  638850.0000,
          618665.7500,  589360.6875,  585703.0000,  580457.0625,  562310.2500],
        [ 963166.6875,  894833.8750,  870200.8750,  834193.0000,  827286.1875,
          820825.5000,  774756.1875,  759065.6250,  741689.8125,  741442.3125],
        [ 813196.8125,  612899.5625,  611226.0625,  552480.6875,  541901.1250,
          533803.6250,  497530.6875,  473173.2500,  454466.8438,  444330.9375],
        [ 375275.5625,  322636.4062,  287281.8750,  257531.3281,  255614.7031,
          255156.5625,  254722.1094,  251344.5156,  233743.0312,  227916.7656],
        [ 585147.4375,  495944.2188,  490519.8125,  438294.5000,  434686.3125,
          394097.1250,  391144.0000,  386566.0312,  369811.3750,  359391.8438],
        [ 703367.9375,  653618.2500,  585306.0000,  574372.8125,  515188.6250,
          475791.4062,  475109.0000,  473759.7812,  446671.4688,  446135.5000],
        [ 534258.3750,  510510.5000,  455744.1875,  434403.6875,  377709.9375,
          368400.5312,  336593.7812,  326412.7812,  325782.4062,  319858.3438],
        [ 768255.7500,  274117.9062,  271295.0938,  250415.7188,  240435.3125,
          238655.4844,  223071.2656,  218513.9688,  210157.4844,  201717.6875],
        [ 686228.3750,  565878.0000,  472123.4688,  463951.7812,  357747.0312,
          339932.5938,  334573.4062,  315173.6875,  314361.0625,  301641.3125],
        [ 487489.4375,  453932.7500,  401058.7188,  348789.2812,  340613.7500,
          326091.0625,  274543.3125,  268414.0938,  257065.5938,  242141.3281],
        [ 886144.5625,  839375.3750,  810160.8750,  795609.2500,  783310.3750,
          775947.4375,  773203.1875,  765556.2500,  750413.3125,  742270.0625],
        [ 387813.7500,  382965.2500,  376349.0000,  374133.1250,  308865.8125,
          277454.6875,  267289.0938,  237334.0625,  207659.5625,  202628.1250],
        [ 511357.8750,  356646.4062,  349975.4688,  335179.9062,  319430.6250,
          303584.7188,  297835.2188,  285194.7188,  285128.9062,  284714.7812],
        [ 422741.5938,  387970.9375,  378436.7812,  303939.0000,  288552.6250,
          287324.3438,  271814.3438,  256436.0781,  227051.3750,  223384.8594],
        [ 502128.6562,  389304.4688,  337848.6562,  317376.7500,  290852.6562,
          259645.0000,  243141.9219,  232429.9688,  219300.1875,  213703.6562],
        [ 291109.3438,  234294.9375,  222376.7031,  207569.6562,  188203.1250,
          187573.6406,  179989.4844,  177418.8281,  159329.4375,  154405.0000],
        [ 163109.8438,  158803.2031,  126727.2109,   91010.2031,   78073.8438,
           77759.0859,   74268.8125,   67892.1172,   54514.5195,   53110.6484],
        [ 618410.3750,  561260.1875,  556061.3125,  479186.1875,  467842.6250,
          464134.0938,  453873.0625,  440022.1250,  439580.0312,  419882.0625],
        [ 169185.4375,  125548.5781,  114394.9688,  107237.5625,  105376.9297,
          103237.7031,   73859.2188,   72948.2734,   61712.3945,   61156.5117],
        [ 324629.6250,  202796.1250,  155730.6250,  136914.4219,  132323.3750,
          132301.8125,  121326.1719,  119183.4688,  117221.2812,  110717.9609],
        [ 477849.9688,  389622.7812,  354049.2188,  326210.1875,  232116.5312,
          229878.7656,  229479.2500,  229096.7969,  219354.1562,  217338.9062],
        [ 400836.5312,  211184.5312,  203376.1875,  202207.2969,  177953.2812,
          159728.4844,  158563.4844,  149351.9062,  141063.3125,  134749.4688],
        [ 425715.2500,  389414.0312,  261555.1719,  257415.9062,  229562.2031,
          196633.9844,  191456.6094,  178821.2500,  177870.4688,  158332.5938],
        [ 750970.3125,  580446.5625,  541545.7500,  462291.1250,  389023.5312,
          352286.6875,  289438.4375,  284606.4688,  277386.9688,  253548.5781],
        [ 611321.6875,  331373.8125,  317585.0312,  293045.8125,  291962.6250,
          282234.9688,  252716.2188,  232803.5469,  231081.9688,  225510.5625],
        [ 770851.6250,  616473.6250,  560715.0000,  559514.6875,  550193.4375,
          540716.9375,  540716.9375,  490004.0938,  471702.1875,  464496.7500],
        [ 581230.9375,  419490.5938,  383895.1250,  381907.1875,  373003.0625,
          369321.4688,  355646.4688,  353057.5625,  347374.7812,  346007.3438],
        [ 669002.9375,  451836.3125,  359305.1562,  342271.1562,  340487.7500,
          248394.0000,  245559.5938,  238001.5781,  232689.6719,  212270.2500],
        [ 758823.8750,  402509.3750,  372381.4062,  309139.8750,  279673.2188,
          248694.5312,  243746.2500,  231745.5781,  217531.5312,  216071.0156],
        [ 625024.8750,  323978.5938,  315376.9375,  270060.1875,  269503.1562,
          259440.5312,  243621.6719,  206725.7188,  199881.4375,  198302.6875],
        [ 499366.5312,  180351.6719,  177472.4531,  172185.0312,  147444.9062,
          142366.5625,  136887.5156,  136629.7969,  134718.7500,  127588.2031],
        [ 377857.2812,  369022.1875,  359522.1250,  349619.5000,  314998.5000,
          312359.1250,  306195.4688,  297332.0625,  296948.6562,  296699.8125],
        [ 219309.5938,   96339.6797,   95993.2891,   92102.2422,   87545.2812,
           69141.0391,   68456.0000,   68424.9922,   67940.5000,   65808.8828],
        [ 375943.2500,  297984.1250,  275647.2812,  237155.5625,  219919.2812,
          200915.5469,  196366.0156,  190388.0000,  179863.5312,  161779.7031],
        [ 202341.7500,  192068.7031,  177998.0938,  136904.7500,  129200.7969,
          122688.5547,  118232.4141,  102702.5000,  102213.9453,   98809.4922]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[411737.6250,      0.0000],
         [213969.5781,      0.0000],
         [212849.2031,      0.0000],
         ...,
         [168417.5625,      0.0000],
         [158004.6562,      0.0000],
         [154828.7812,      0.0000]],

        [[839172.8750,      0.0000],
         [822840.5000,      0.0000],
         [799905.5000,      0.0000],
         ...,
         [719650.3750,      0.0000],
         [713887.2500,      0.0000],
         [694369.1250,      0.0000]],

        [[498630.8438,      0.0000],
         [467649.4688,      0.0000],
         [460536.2500,      0.0000],
         ...,
         [212323.7031,      0.0000],
         [     0.0000, 208559.7969],
         [208239.6250,      0.0000]],

        ...,

        [[     0.0000, 219309.5938],
         [ 96339.6797,      0.0000],
         [ 95993.2891,      0.0000],
         ...,
         [ 68424.9922,      0.0000],
         [ 67940.5000,      0.0000],
         [ 65808.8828,      0.0000]],

        [[375943.2500,      0.0000],
         [297984.1250,      0.0000],
         [     0.0000, 275647.2812],
         ...,
         [190388.0000,      0.0000],
         [179863.5312,      0.0000],
         [161779.7031,      0.0000]],

        [[     0.0000, 202341.7500],
         [     0.0000, 192068.7031],
         [     0.0000, 177998.0938],
         ...,
         [102702.5000,      0.0000],
         [102213.9453,      0.0000],
         [ 98809.4922,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1904455.5000,  184814.2656],
        [7649815.0000,       0.0000],
        [3017776.0000,  208559.7969],
        [4500395.0000,       0.0000],
        [ 588686.3125,  824822.8125],
        [1180635.5000,  339945.8125],
        [2616423.2500, 1132488.6250],
        [3063546.7500,       0.0000],
        [3980889.0000,  572261.8125],
        [4264931.0000,       0.0000],
        [5550569.0000,       0.0000],
        [2659188.7500,       0.0000],
        [4817667.0000,       0.0000],
        [9245717.0000,       0.0000],
        [6520826.0000,       0.0000],
        [5429842.5000,       0.0000],
        [8355239.0000,       0.0000],
        [8162089.0000,       0.0000],
        [5371380.5000,       0.0000],
        [4838026.0000,       0.0000],
        [8294385.0000,       0.0000],
        [5627223.0000,       0.0000],
        [7447264.0000,       0.0000],
        [6840571.0000,       0.0000],
        [4575635.5000,       0.0000],
        [6235441.0000,       0.0000],
        [5931600.0000,       0.0000],
        [7212968.0000,       0.0000],
        [7037975.0000,       0.0000],
        [6381197.0000,       0.0000],
        [8227460.0000,       0.0000],
        [5535010.0000,       0.0000],
        [2493306.0000,  227916.7656],
        [2665406.0000, 1680196.5000],
        [2942170.2500, 2407150.2500],
        [1902261.7500, 2087412.8750],
        [1676246.5000, 1220389.1250],
        [2427381.0000, 1724229.8750],
        [1434325.8750, 1965813.5000],
        [7921990.5000,       0.0000],
        [3022492.7500,       0.0000],
        [3329048.5000,       0.0000],
        [3047652.0000,       0.0000],
        [3005732.0000,       0.0000],
        [ 555102.2500, 1447167.8750],
        [ 496629.2188,  448640.2812],
        [4338992.0000,  561260.1875],
        [ 733537.3750,  261120.1875],
        [ 759270.5000,  793874.3750],
        [1144421.7500, 1760574.7500],
        [1504638.2500,  434376.2500],
        [1394232.2500, 1072545.1250],
        [2251376.0000, 1930168.5000],
        [2738262.5000,  331373.8125],
        [5565385.0000,       0.0000],
        [3910934.2500,       0.0000],
        [1499893.8750, 1839924.6250],
        [2225752.7500, 1054564.0000],
        [1827568.8750, 1084346.8750],
        [ 771587.8750, 1083423.5000],
        [2902697.5000,  377857.2812],
        [ 711751.8750,  219309.5938],
        [1622243.7500,  713718.3750],
        [ 555615.3125,  827545.6875]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 60.9375
Top1 accuracy for validation set is 60.9375 size is torch.Size([64, 1])
Epoch 256/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:43, 57.38s/it]  7%|▋         | 2/30 [00:58<11:16, 24.18s/it] 10%|█         | 3/30 [00:59<06:03, 13.48s/it] 13%|█▎        | 4/30 [00:59<03:41,  8.51s/it] 17%|█▋        | 5/30 [01:00<02:22,  5.71s/it] 20%|██        | 6/30 [01:01<01:36,  4.02s/it] 23%|██▎       | 7/30 [01:02<01:07,  2.95s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.25s/it] 30%|███       | 9/30 [01:03<00:37,  1.78s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.46s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.24s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.65s/it]
Epoch loss is 2.694075004259745
Epoch 257/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:38, 63.38s/it]  7%|▋         | 2/30 [01:04<12:23, 26.54s/it] 10%|█         | 3/30 [01:04<06:38, 14.76s/it] 13%|█▎        | 4/30 [01:05<03:59,  9.23s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.17s/it] 20%|██        | 6/30 [01:07<01:43,  4.33s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.84s/it]
Epoch loss is 2.673981714248657
Epoch 258/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:41, 59.35s/it]  7%|▋         | 2/30 [01:00<11:36, 24.88s/it] 10%|█         | 3/30 [01:00<06:14, 13.86s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.68s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.82s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.756437134742737
Epoch 259/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:32, 56.97s/it]  7%|▋         | 2/30 [01:00<11:47, 25.25s/it] 10%|█         | 3/30 [01:00<06:19, 14.06s/it] 13%|█▎        | 4/30 [01:01<03:48,  8.81s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.90s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.70s/it]
Epoch loss is 2.6859895547231036
Epoch 260/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:16, 56.45s/it]  7%|▋         | 2/30 [00:57<11:03, 23.68s/it] 10%|█         | 3/30 [00:58<06:03, 13.47s/it] 13%|█▎        | 4/30 [00:59<03:39,  8.45s/it] 17%|█▋        | 5/30 [01:00<02:25,  5.83s/it] 20%|██        | 6/30 [01:01<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:01<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:02<00:50,  2.29s/it] 30%|███       | 9/30 [01:03<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:04<00:23,  1.25s/it] 40%|████      | 12/30 [01:05<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:07<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:10<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:13<00:05,  1.33it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:16<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.65s/it]
Epoch loss is 2.6762646516164144
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0245,  0.0053,  0.0054,  ...,  0.0039, -0.0020,  0.0116],
        [-0.0015,  0.0229,  0.0177,  ...,  0.0110, -0.0112, -0.0153],
        [-0.0348, -0.0333,  0.0151,  ...,  0.0790, -0.0214, -0.0246],
        ...,
        [ 0.0218, -0.0125, -0.0058,  ..., -0.0243, -0.0252, -0.0158],
        [-0.0278, -0.0026, -0.0117,  ..., -0.0022,  0.0114, -0.0133],
        [-0.0379, -0.0085,  0.0127,  ...,  0.0273,  0.0241, -0.0495]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9040, 0.8586, 0.8582, 0.8543, 0.8541, 0.8488, 0.8475, 0.8419, 0.8381,
         0.8369],
        [0.9541, 0.9523, 0.9507, 0.9495, 0.9470, 0.9469, 0.9448, 0.9430, 0.9424,
         0.9400],
        [0.9168, 0.9134, 0.9113, 0.8896, 0.8869, 0.8785, 0.8587, 0.8567, 0.8551,
         0.8551],
        [0.9563, 0.9186, 0.9098, 0.9081, 0.9025, 0.9018, 0.8986, 0.8929, 0.8921,
         0.8914],
        [0.8644, 0.8499, 0.8454, 0.8317, 0.8287, 0.8147, 0.8143, 0.8098, 0.8038,
         0.7989],
        [0.8643, 0.8572, 0.8377, 0.8282, 0.8247, 0.8242, 0.8154, 0.8144, 0.8143,
         0.8122],
        [0.9130, 0.9127, 0.9123, 0.8931, 0.8911, 0.8909, 0.8888, 0.8808, 0.8804,
         0.8741],
        [0.9267, 0.9104, 0.9058, 0.8781, 0.8768, 0.8687, 0.8566, 0.8499, 0.8474,
         0.8456],
        [0.9255, 0.9214, 0.9191, 0.9144, 0.9134, 0.9034, 0.9011, 0.8999, 0.8994,
         0.8991],
        [0.9564, 0.9079, 0.9043, 0.9034, 0.9015, 0.8994, 0.8984, 0.8923, 0.8857,
         0.8798],
        [0.9606, 0.9282, 0.9252, 0.9234, 0.9215, 0.9172, 0.9150, 0.9120, 0.9116,
         0.9106],
        [0.9180, 0.9081, 0.8807, 0.8803, 0.8701, 0.8546, 0.8542, 0.8358, 0.8316,
         0.8240],
        [0.9538, 0.9226, 0.9224, 0.9202, 0.9164, 0.9039, 0.9027, 0.9011, 0.8908,
         0.8836],
        [0.9710, 0.9648, 0.9643, 0.9623, 0.9619, 0.9592, 0.9582, 0.9573, 0.9564,
         0.9545],
        [0.9469, 0.9416, 0.9400, 0.9373, 0.9357, 0.9345, 0.9338, 0.9332, 0.9317,
         0.9295],
        [0.9337, 0.9306, 0.9284, 0.9252, 0.9238, 0.9216, 0.9199, 0.9184, 0.9174,
         0.9133],
        [0.9732, 0.9661, 0.9599, 0.9560, 0.9514, 0.9439, 0.9437, 0.9434, 0.9413,
         0.9413],
        [0.9699, 0.9637, 0.9575, 0.9507, 0.9487, 0.9472, 0.9460, 0.9449, 0.9446,
         0.9445],
        [0.9428, 0.9423, 0.9366, 0.9185, 0.9146, 0.9140, 0.9121, 0.9094, 0.9089,
         0.9085],
        [0.9249, 0.9245, 0.9199, 0.9153, 0.9119, 0.9111, 0.9106, 0.9102, 0.9097,
         0.9096],
        [0.9682, 0.9586, 0.9569, 0.9549, 0.9542, 0.9508, 0.9463, 0.9459, 0.9454,
         0.9429],
        [0.9429, 0.9349, 0.9329, 0.9243, 0.9230, 0.9204, 0.9194, 0.9184, 0.9163,
         0.9155],
        [0.9752, 0.9535, 0.9526, 0.9489, 0.9465, 0.9449, 0.9389, 0.9288, 0.9287,
         0.9256],
        [0.9542, 0.9488, 0.9424, 0.9415, 0.9369, 0.9353, 0.9341, 0.9333, 0.9332,
         0.9326],
        [0.9672, 0.9183, 0.9138, 0.9055, 0.9046, 0.8938, 0.8933, 0.8932, 0.8908,
         0.8861],
        [0.9491, 0.9440, 0.9418, 0.9368, 0.9341, 0.9314, 0.9248, 0.9237, 0.9215,
         0.9207],
        [0.9477, 0.9447, 0.9394, 0.9339, 0.9294, 0.9210, 0.9186, 0.9186, 0.9176,
         0.9133],
        [0.9666, 0.9562, 0.9556, 0.9439, 0.9386, 0.9352, 0.9341, 0.9340, 0.9316,
         0.9315],
        [0.9571, 0.9497, 0.9489, 0.9411, 0.9399, 0.9398, 0.9352, 0.9348, 0.9336,
         0.9336],
        [0.9482, 0.9424, 0.9385, 0.9359, 0.9350, 0.9327, 0.9290, 0.9288, 0.9286,
         0.9260],
        [0.9635, 0.9587, 0.9564, 0.9537, 0.9533, 0.9530, 0.9484, 0.9470, 0.9457,
         0.9443],
        [0.9523, 0.9320, 0.9317, 0.9237, 0.9237, 0.9222, 0.9173, 0.9126, 0.9107,
         0.9092],
        [0.8969, 0.8864, 0.8783, 0.8718, 0.8708, 0.8706, 0.8702, 0.8701, 0.8651,
         0.8628],
        [0.9286, 0.9161, 0.9157, 0.9086, 0.9080, 0.9009, 0.9008, 0.8985, 0.8950,
         0.8948],
        [0.9406, 0.9365, 0.9289, 0.9279, 0.9187, 0.9147, 0.9143, 0.9134, 0.9094,
         0.9092],
        [0.9219, 0.9184, 0.9102, 0.9073, 0.8970, 0.8940, 0.8898, 0.8882, 0.8856,
         0.8854],
        [0.9486, 0.8767, 0.8740, 0.8679, 0.8668, 0.8645, 0.8600, 0.8587, 0.8582,
         0.8560],
        [0.9378, 0.9244, 0.9128, 0.9124, 0.8931, 0.8897, 0.8889, 0.8857, 0.8844,
         0.8808],
        [0.9157, 0.9107, 0.9014, 0.8907, 0.8882, 0.8867, 0.8750, 0.8724, 0.8706,
         0.8667],
        [0.9571, 0.9536, 0.9512, 0.9499, 0.9486, 0.9481, 0.9471, 0.9460, 0.9457,
         0.9452],
        [0.9000, 0.8991, 0.8971, 0.8967, 0.8833, 0.8764, 0.8739, 0.8652, 0.8544,
         0.8544],
        [0.9195, 0.8932, 0.8914, 0.8894, 0.8859, 0.8828, 0.8819, 0.8782, 0.8782,
         0.8781],
        [0.9060, 0.9000, 0.8980, 0.8817, 0.8794, 0.8792, 0.8742, 0.8703, 0.8618,
         0.8612],
        [0.9175, 0.8990, 0.8896, 0.8866, 0.8797, 0.8718, 0.8676, 0.8640, 0.8593,
         0.8573],
        [0.8797, 0.8635, 0.8603, 0.8542, 0.8469, 0.8466, 0.8465, 0.8446, 0.8349,
         0.8330],
        [0.8404, 0.8367, 0.8220, 0.7998, 0.7892, 0.7866, 0.7852, 0.7801, 0.7632,
         0.7619],
        [0.9312, 0.9255, 0.9242, 0.9145, 0.9126, 0.9117, 0.9103, 0.9078, 0.9078,
         0.9044],
        [0.8402, 0.8201, 0.8122, 0.8089, 0.8073, 0.8061, 0.7835, 0.7808, 0.7704,
         0.7691],
        [0.8853, 0.8544, 0.8355, 0.8274, 0.8242, 0.8223, 0.8177, 0.8171, 0.8155,
         0.8106],
        [0.9145, 0.8973, 0.8912, 0.8875, 0.8637, 0.8616, 0.8615, 0.8611, 0.8576,
         0.8574],
        [0.9035, 0.8593, 0.8566, 0.8562, 0.8463, 0.8381, 0.8378, 0.8329, 0.8286,
         0.8263],
        [0.9070, 0.9005, 0.8716, 0.8701, 0.8616, 0.8510, 0.8497, 0.8438, 0.8438,
         0.8358],
        [0.9458, 0.9285, 0.9239, 0.9127, 0.8989, 0.8929, 0.8795, 0.8777, 0.8755,
         0.8709],
        [0.9318, 0.8881, 0.8864, 0.8807, 0.8797, 0.8774, 0.8699, 0.8643, 0.8637,
         0.8616],
        [0.9477, 0.9315, 0.9251, 0.9250, 0.9234, 0.9224, 0.9224, 0.9153, 0.9135,
         0.9122],
        [0.9275, 0.9034, 0.8995, 0.8985, 0.8954, 0.8954, 0.8931, 0.8926, 0.8916,
         0.8915],
        [0.9376, 0.9094, 0.8938, 0.8908, 0.8903, 0.8693, 0.8668, 0.8641, 0.8620,
         0.8558],
        [0.9473, 0.9030, 0.8964, 0.8847, 0.8779, 0.8688, 0.8674, 0.8633, 0.8593,
         0.8592],
        [0.9330, 0.8860, 0.8846, 0.8747, 0.8727, 0.8703, 0.8663, 0.8558, 0.8526,
         0.8515],
        [0.9171, 0.8457, 0.8443, 0.8429, 0.8318, 0.8287, 0.8257, 0.8257, 0.8256,
         0.8224],
        [0.8975, 0.8943, 0.8928, 0.8912, 0.8853, 0.8834, 0.8831, 0.8810, 0.8805,
         0.8805],
        [0.8609, 0.8029, 0.8015, 0.7999, 0.7950, 0.7767, 0.7764, 0.7755, 0.7743,
         0.7741],
        [0.8979, 0.8812, 0.8762, 0.8637, 0.8582, 0.8528, 0.8502, 0.8478, 0.8439,
         0.8363],
        [0.8529, 0.8473, 0.8466, 0.8250, 0.8222, 0.8164, 0.8139, 0.8051, 0.8029,
         0.8025]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 1, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 1, 1, 1, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 1, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 406297.7812,  212284.0156,  210990.2500,  199621.2188,  199102.3594,
          184460.8750,  181279.2188,  167317.7344,  158371.4062,  155704.7969],
        [ 830265.9375,  809885.0625,  791155.8125,  777319.8750,  750239.4375,
          749038.4375,  727620.3750,  708513.5000,  702570.1875,  679373.6250],
        [ 487786.6562,  464547.7188,  450655.4688,  330373.8438,  317960.2188,
          282034.2500,  212576.5625,  206686.2969,  201975.8281,  201882.4219],
        [ 857348.8125,  500309.9062,  440885.7500,  430768.7500,  397743.3750,
          393736.1250,  376184.6562,  346290.2500,  342392.9375,  339117.9375],
        [ 230631.5156,  187471.8750,  175903.7031,  144624.9219,  138511.2656,
          113397.8359,  112786.7422,  105691.9531,   97063.5234,   90459.1797],
        [ 230304.9062,  208021.6875,  157546.1875,  137462.0781,  130866.6016,
          129957.4531,  114448.3281,  112908.8828,  112818.7969,  109416.7891],
        [ 461873.3438,  459645.5000,  457385.7188,  347637.6250,  337902.1562,
          336530.8750,  327010.3750,  291347.3750,  289919.9375,  265036.3438],
        [ 561807.4375,  445237.8438,  416568.4375,  280675.7188,  275262.7188,
          245346.5781,  206421.3438,  187545.3906,  180870.1250,  176337.8906],
        [ 551894.5625,  520536.7500,  503656.6875,  470876.5312,  464341.7500,
          402857.7188,  389837.5938,  382785.5938,  380161.8750,  378627.7812],
        [ 858547.4375,  429119.6250,  407676.4688,  402533.9688,  391907.9375,
          380394.0000,  375029.0625,  343572.4375,  312590.3750,  287412.3125],
        [ 911178.5000,  573548.5000,  549817.3750,  536135.1250,  521512.1250,
          490228.0000,  475506.5312,  455191.6562,  452907.5000,  446372.5625],
        [ 496178.8438,  430317.5312,  291238.7812,  289301.5625,  250313.7656,
          200491.0156,  199286.2344,  153202.2344,  144448.6094,  129522.3047],
        [ 827540.2500,  529888.0000,  528003.8750,  511591.5000,  484823.5625,
          405602.8750,  398364.8125,  389488.6875,  336479.2188,  303520.4688],
        [1057305.2500,  968053.7500,  961667.8750,  933976.6250,  928150.9375,
          893436.2500,  880305.6875,  869966.0625,  857994.9375,  835849.4375],
        [ 749123.4375,  694644.0000,  679328.9375,  653877.0000,  638300.6875,
          627986.0000,  621680.9375,  616020.5625,  603579.3125,  584617.0000],
        [ 620470.8750,  593799.0625,  575168.7500,  549753.8750,  538723.9375,
          522453.0000,  509824.5312,  498729.7188,  492033.1250,  464059.2812],
        [1090919.7500,  986064.8750,  902457.0000,  854042.1250,  799121.7500,
          718518.8125,  715847.3125,  712878.3125,  692217.0000,  691739.1875],
        [1041726.4375,  952313.2500,  871754.9375,  791474.2500,  768708.6875,
          752995.5625,  740263.8125,  728873.9375,  725042.9375,  724391.1875],
        [ 706557.3750,  702058.4375,  647368.8750,  499884.5000,  472605.4375,
          468266.2188,  455626.0000,  438753.2812,  435222.6562,  433020.1250],
        [ 547702.0625,  543932.3750,  509819.6250,  476902.5938,  454746.9375,
          449737.0938,  446095.9375,  443673.7500,  440662.5312,  439944.0625],
        [1015354.6250,  886349.0625,  864150.5000,  840472.7500,  832368.4375,
          792311.0000,  742828.8125,  739133.0000,  733661.5625,  707882.0625],
        [ 708040.0000,  631734.2500,  613521.1875,  542831.1250,  532995.8125,
          512982.9375,  506058.2500,  498472.0312,  484329.5625,  478809.3125],
        [1122325.0000,  824036.5625,  812873.4375,  770925.1250,  745382.7500,
          728606.3750,  668867.6875,  578539.3750,  578033.1250,  553075.8125],
        [ 832162.9375,  770611.1875,  702527.3125,  694055.9375,  649621.2500,
          635111.6250,  624406.4375,  617387.3750,  616448.3750,  610616.0625],
        [1001540.5625,  497890.5000,  466857.1875,  414726.7500,  409824.3125,
          351110.4375,  348549.8750,  348147.9062,  336490.1250,  314433.3438],
        [ 773937.2500,  719384.7500,  697264.9375,  649051.5625,  624348.7500,
          600586.6250,  546802.2500,  537726.6250,  521061.2500,  515831.7188],
        [ 758632.1875,  726126.5625,  673066.6250,  622640.4375,  583570.9375,
          517628.4062,  500528.4688,  500313.7188,  492854.9688,  464088.9688],
        [ 993564.5625,  855415.6250,  848660.8125,  717616.3125,  665995.1250,
          634393.0625,  624029.0625,  623494.8750,  602751.0000,  601642.6250],
        [ 866833.0625,  780296.0000,  771498.7500,  690138.0000,  678429.0000,
          676810.8125,  634402.7500,  630703.0625,  620121.3125,  619680.8125],
        [ 763653.0625,  702542.6875,  664742.5625,  640631.5625,  632117.5000,
          611833.7500,  580199.1875,  578745.2500,  577005.9375,  556368.5000],
        [ 950438.6875,  887271.0000,  859007.6875,  826314.8125,  821587.5000,
          817324.5000,  765922.0625,  750457.0000,  736792.9375,  722413.2500],
        [ 809027.4375,  605650.5000,  603584.5000,  538031.8125,  537888.1875,
          526314.1875,  491293.1875,  459335.7500,  447127.5312,  437597.4375],
        [ 366807.8750,  315979.0312,  281312.6875,  256174.5156,  252812.6562,
          251845.2812,  250429.5625,  250297.0625,  232886.5938,  225278.8438],
        [ 576946.0000,  482809.0625,  479703.7812,  433344.4062,  430025.0312,
          388504.8438,  387724.9688,  375205.7812,  357030.9375,  356127.4062],
        [ 685304.3125,  646224.6250,  579302.9375,  571359.3125,  500704.1562,
          472825.0000,  470337.0625,  464723.1562,  438475.5625,  437464.7500],
        [ 524731.9375,  499063.2500,  443365.8438,  425652.7188,  367342.0625,
          351769.6875,  331550.5000,  324141.1562,  312307.3125,  311400.8125],
        [ 767374.1250,  274881.5312,  264690.5625,  242605.0312,  238638.8594,
          230941.6250,  216684.5000,  212590.5312,  211101.5625,  204604.3125],
        [ 658091.6875,  543153.7500,  460452.3438,  457623.9062,  347563.6875,
          330840.1562,  327226.2812,  312786.0000,  307039.6562,  291679.3125],
        [ 479724.3438,  446999.1875,  391255.1875,  335602.7500,  324102.1875,
          317237.8125,  268496.7500,  258391.3750,  251931.7656,  238237.7500],
        [ 867424.3125,  824763.0000,  797325.1875,  782250.3125,  767923.1875,
          762009.6250,  751917.6875,  740248.3125,  736472.6250,  731669.5000],
        [ 383737.0312,  378434.2812,  367711.8438,  365797.9375,  302218.0625,
          273679.3438,  264243.6250,  233299.6250,  200025.7812,  199868.8594],
        [ 506838.2812,  347985.9062,  339217.8750,  329472.0938,  313772.2188,
          300134.2812,  296264.1250,  280894.7812,  280767.2812,  280399.0938],
        [ 418040.5312,  383780.9375,  372965.7188,  295489.3125,  285879.0312,
          285117.1875,  265446.1250,  250871.7969,  222098.4219,  220250.9062],
        [ 492759.0938,  378175.5938,  330660.6875,  316924.2500,  287104.4062,
          256297.6875,  241457.6094,  229172.8438,  214399.9531,  208435.1250],
        [ 286812.9375,  227745.7656,  217380.9844,  199389.0938,  179683.5000,
          178964.7188,  178713.6719,  173757.0781,  151364.8281,  147302.8125],
        [ 163792.2812,  155194.0781,  125788.3828,   91644.9531,   78771.0312,
           75876.7109,   74367.7578,   69126.4688,   54336.3320,   53328.9922],
        [ 598642.3750,  551966.6250,  541533.8125,  472083.4062,  458915.8125,
          453041.0312,  444208.4688,  428724.0625,  428626.3750,  408498.4062],
        [ 163333.0625,  122539.1172,  109484.3203,  104440.2812,  101940.5000,
          100292.7891,   72642.3359,   69829.8125,   60186.9531,   59105.3516],
        [ 310821.6562,  199885.6250,  152721.8750,  136028.2188,  129914.9609,
          126334.3203,  118418.6016,  117296.5469,  114759.8125,  106999.4297],
        [ 471621.2188,  369151.7500,  338008.5312,  320710.5312,  228478.4531,
          221617.7031,  221130.4531,  219938.1562,  209199.6406,  208670.0156],
        [ 402896.1562,  214314.1094,  206302.2812,  205241.2188,  177961.4219,
          158352.9844,  157683.2812,  146967.1719,  138261.9688,  133789.0938],
        [ 423865.5000,  386456.1875,  255784.1719,  250033.4219,  221586.6406,
          190315.7500,  187009.5781,  171951.6875,  171845.7812,  153345.3438],
        [ 737446.7500,  576523.0000,  539271.3750,  459862.5938,  377474.7812,
          346388.0000,  286323.2188,  278982.2188,  270247.7500,  253089.8281],
        [ 603666.8125,  323567.0000,  315585.4062,  291181.5625,  286974.9062,
          277892.1562,  249338.5938,  230461.5625,  228475.6250,  221629.5469],
        [ 757824.5000,  601550.8125,  548860.7500,  548354.3125,  535919.4375,
          528274.3750,  528274.3750,  477096.3750,  465408.4375,  456593.3750],
        [ 568058.1250,  402634.1562,  380539.8438,  375639.3438,  359341.1250,
          359195.1562,  347636.2812,  345195.8750,  340114.1875,  339906.0312],
        [ 656177.7500,  438770.4375,  351186.0938,  336422.0938,  333718.7500,
          247331.9375,  238595.8594,  229505.7344,  222810.1875,  204080.8750],
        [ 753680.2500,  400548.4062,  364488.9375,  308416.9688,  279546.5312,
          245445.5781,  240781.5625,  226990.7500,  214532.2812,  214061.4219],
        [ 614318.0000,  314093.7812,  307962.2500,  267219.0312,  259825.0625,
          250779.2344,  237116.4219,  203998.7500,  194776.5625,  191907.0312],
        [ 489525.8125,  176557.8281,  172970.0938,  169685.0625,  144799.0781,
          138545.4844,  132774.7812,  132663.5156,  132577.2656,  126577.9297],
        [ 369845.2500,  353636.8438,  346093.4688,  338045.5938,  310903.5000,
          302399.4062,  301390.2812,  292339.8750,  290316.7188,  290197.1250],
        [ 219338.0469,   95864.8438,   93943.7031,   91719.9844,   85532.3828,
           65928.6172,   65574.0156,   64726.0469,   63668.2578,   63473.5312],
        [ 372292.6562,  293047.7500,  272986.2188,  228244.3281,  210931.9219,
          195441.0156,  188193.9688,  182047.7500,  172089.8281,  154278.4219],
        [ 195642.2344,  180693.7656,  178900.7344,  131443.8438,  126228.8125,
          116227.8984,  112100.8672,   98837.0078,   95730.5469,   95200.0391]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[406297.7812,      0.0000],
         [212284.0156,      0.0000],
         [210990.2500,      0.0000],
         ...,
         [167317.7344,      0.0000],
         [158371.4062,      0.0000],
         [155704.7969,      0.0000]],

        [[830265.9375,      0.0000],
         [809885.0625,      0.0000],
         [791155.8125,      0.0000],
         ...,
         [708513.5000,      0.0000],
         [702570.1875,      0.0000],
         [679373.6250,      0.0000]],

        [[487786.6562,      0.0000],
         [464547.7188,      0.0000],
         [450655.4688,      0.0000],
         ...,
         [206686.2969,      0.0000],
         [201975.8281,      0.0000],
         [     0.0000, 201882.4219]],

        ...,

        [[     0.0000, 219338.0469],
         [ 95864.8438,      0.0000],
         [ 93943.7031,      0.0000],
         ...,
         [ 64726.0469,      0.0000],
         [ 63668.2578,      0.0000],
         [ 63473.5312,      0.0000]],

        [[372292.6562,      0.0000],
         [293047.7500,      0.0000],
         [     0.0000, 272986.2188],
         ...,
         [182047.7500,      0.0000],
         [172089.8281,      0.0000],
         [154278.4219,      0.0000]],

        [[     0.0000, 195642.2344],
         [     0.0000, 180693.7656],
         [     0.0000, 178900.7344],
         ...,
         [ 98837.0078,      0.0000],
         [ 95730.5469,      0.0000],
         [ 95200.0391,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1894150.5000,  181279.2188],
        [7525982.5000,       0.0000],
        [2954597.0000,  201882.4219],
        [4424778.5000,       0.0000],
        [ 583445.6250,  813096.8750],
        [1235730.0000,  208021.6875],
        [2774514.0000,  799775.5000],
        [2976073.5000,       0.0000],
        [3893682.5000,  551894.5625],
        [4188783.5000,       0.0000],
        [5412398.0000,       0.0000],
        [2584300.7500,       0.0000],
        [4715303.5000,       0.0000],
        [9186707.0000,       0.0000],
        [6469158.0000,       0.0000],
        [5365015.5000,       0.0000],
        [8163806.5000,       0.0000],
        [8097545.0000,       0.0000],
        [5259363.0000,       0.0000],
        [4753217.0000,       0.0000],
        [8154512.0000,       0.0000],
        [5509774.0000,       0.0000],
        [7382665.5000,       0.0000],
        [6752948.0000,       0.0000],
        [4489571.0000,       0.0000],
        [6185996.0000,       0.0000],
        [5839451.0000,       0.0000],
        [7167563.0000,       0.0000],
        [6968913.5000,       0.0000],
        [6307840.0000,       0.0000],
        [8137530.0000,       0.0000],
        [5455850.5000,       0.0000],
        [2458545.2500,  225278.8438],
        [2979168.2500, 1288254.0000],
        [2891007.7500, 2375713.2500],
        [1841883.1250, 2049442.1250],
        [1649529.2500, 1214583.5000],
        [2374759.0000, 1661697.8750],
        [1389900.8750, 1922078.2500],
        [7762003.5000,       0.0000],
        [2969016.5000,       0.0000],
        [3275746.0000,       0.0000],
        [2999940.0000,       0.0000],
        [2955387.0000,       0.0000],
        [ 530437.3750, 1410678.0000],
        [ 497452.2500,  444774.7500],
        [4234274.0000,  551966.6250],
        [ 649320.7500,  314473.7500],
        [ 742752.4375,  770428.6250],
        [ 900759.0000, 1907767.3750],
        [1511365.6250,  430404.0312],
        [1346088.2500, 1066105.8750],
        [2214782.0000, 1910827.5000],
        [2705206.0000,  323567.0000],
        [5448156.5000,       0.0000],
        [3818260.2500,       0.0000],
        [1464003.6250, 1794596.0000],
        [2203908.7500, 1044583.8750],
        [1782122.2500, 1059873.7500],
        [ 756394.0625, 1060282.8750],
        [2532983.0000,  662185.1250],
        [ 690431.3750,  219338.0469],
        [1572882.2500,  696671.5625],
        [ 532224.3125,  798781.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 60.9375
Top1 accuracy for validation set is 60.9375 size is torch.Size([64, 1])
Epoch 261/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:39, 61.35s/it]  7%|▋         | 2/30 [01:02<11:59, 25.70s/it] 10%|█         | 3/30 [01:02<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:04<02:29,  6.00s/it] 20%|██        | 6/30 [01:05<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 2.6600292285283405
Epoch 262/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:04, 60.16s/it]  7%|▋         | 2/30 [01:00<11:45, 25.21s/it] 10%|█         | 3/30 [01:01<06:19, 14.04s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.79s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.73s/it]
Epoch loss is 2.659976609547933
Epoch 263/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:27, 58.89s/it]  7%|▋         | 2/30 [01:00<11:37, 24.92s/it] 10%|█         | 3/30 [01:00<06:14, 13.88s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.83s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.685028886795044
Epoch 264/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:38, 63.40s/it]  7%|▋         | 2/30 [01:04<12:23, 26.55s/it] 10%|█         | 3/30 [01:04<06:38, 14.77s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.24s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.17s/it] 20%|██        | 6/30 [01:07<01:43,  4.33s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.84s/it]
Epoch loss is 2.649319585164388
Epoch 265/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:15, 60.55s/it]  7%|▋         | 2/30 [01:01<11:50, 25.37s/it] 10%|█         | 3/30 [01:02<06:21, 14.13s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.6724438031514484
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0245,  0.0050,  0.0061,  ...,  0.0032, -0.0009,  0.0106],
        [-0.0011,  0.0224,  0.0185,  ...,  0.0116, -0.0102, -0.0157],
        [-0.0344, -0.0338,  0.0159,  ...,  0.0790, -0.0203, -0.0242],
        ...,
        [ 0.0207, -0.0115, -0.0047,  ..., -0.0247, -0.0241, -0.0154],
        [-0.0278, -0.0021, -0.0114,  ..., -0.0020,  0.0114, -0.0135],
        [-0.0382, -0.0088,  0.0128,  ...,  0.0282,  0.0246, -0.0483]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9032, 0.8577, 0.8573, 0.8537, 0.8529, 0.8496, 0.8465, 0.8422, 0.8393,
         0.8366],
        [0.9535, 0.9512, 0.9500, 0.9483, 0.9463, 0.9453, 0.9437, 0.9420, 0.9415,
         0.9390],
        [0.9157, 0.9131, 0.9102, 0.8889, 0.8860, 0.8761, 0.8567, 0.8560, 0.8529,
         0.8523],
        [0.9556, 0.9176, 0.9085, 0.9063, 0.9005, 0.9005, 0.8974, 0.8923, 0.8915,
         0.8904],
        [0.8632, 0.8484, 0.8453, 0.8323, 0.8281, 0.8149, 0.8126, 0.8098, 0.8035,
         0.7994],
        [0.8633, 0.8525, 0.8358, 0.8246, 0.8206, 0.8198, 0.8141, 0.8109, 0.8101,
         0.8096],
        [0.9094, 0.9092, 0.9088, 0.8909, 0.8886, 0.8879, 0.8863, 0.8788, 0.8780,
         0.8720],
        [0.9250, 0.9083, 0.9036, 0.8760, 0.8759, 0.8677, 0.8536, 0.8467, 0.8452,
         0.8443],
        [0.9226, 0.9191, 0.9190, 0.9135, 0.9118, 0.9020, 0.9002, 0.8986, 0.8975,
         0.8974],
        [0.9559, 0.9065, 0.9025, 0.9023, 0.9003, 0.8977, 0.8968, 0.8910, 0.8836,
         0.8784],
        [0.9595, 0.9267, 0.9226, 0.9223, 0.9199, 0.9160, 0.9134, 0.9098, 0.9091,
         0.9088],
        [0.9172, 0.9062, 0.8791, 0.8791, 0.8679, 0.8511, 0.8507, 0.8322, 0.8279,
         0.8223],
        [0.9520, 0.9216, 0.9215, 0.9183, 0.9161, 0.9021, 0.9013, 0.8998, 0.8887,
         0.8823],
        [0.9706, 0.9642, 0.9639, 0.9621, 0.9615, 0.9589, 0.9575, 0.9573, 0.9557,
         0.9540],
        [0.9467, 0.9409, 0.9395, 0.9371, 0.9346, 0.9344, 0.9326, 0.9324, 0.9311,
         0.9283],
        [0.9332, 0.9300, 0.9281, 0.9246, 0.9220, 0.9210, 0.9190, 0.9176, 0.9169,
         0.9126],
        [0.9717, 0.9649, 0.9590, 0.9542, 0.9497, 0.9431, 0.9418, 0.9409, 0.9395,
         0.9394],
        [0.9696, 0.9635, 0.9574, 0.9498, 0.9478, 0.9463, 0.9455, 0.9444, 0.9443,
         0.9439],
        [0.9416, 0.9413, 0.9349, 0.9162, 0.9125, 0.9125, 0.9104, 0.9085, 0.9081,
         0.9080],
        [0.9237, 0.9221, 0.9184, 0.9142, 0.9109, 0.9104, 0.9092, 0.9092, 0.9088,
         0.9086],
        [0.9666, 0.9573, 0.9552, 0.9534, 0.9533, 0.9494, 0.9456, 0.9451, 0.9448,
         0.9419],
        [0.9417, 0.9335, 0.9315, 0.9222, 0.9219, 0.9198, 0.9182, 0.9163, 0.9155,
         0.9140],
        [0.9749, 0.9527, 0.9522, 0.9483, 0.9458, 0.9442, 0.9382, 0.9279, 0.9276,
         0.9253],
        [0.9534, 0.9482, 0.9416, 0.9405, 0.9360, 0.9339, 0.9334, 0.9325, 0.9325,
         0.9316],
        [0.9666, 0.9172, 0.9129, 0.9025, 0.9024, 0.8929, 0.8926, 0.8919, 0.8896,
         0.8847],
        [0.9488, 0.9441, 0.9419, 0.9367, 0.9329, 0.9312, 0.9247, 0.9223, 0.9210,
         0.9209],
        [0.9463, 0.9435, 0.9387, 0.9332, 0.9281, 0.9195, 0.9182, 0.9172, 0.9170,
         0.9118],
        [0.9664, 0.9558, 0.9550, 0.9435, 0.9373, 0.9352, 0.9337, 0.9334, 0.9316,
         0.9311],
        [0.9564, 0.9494, 0.9477, 0.9399, 0.9396, 0.9388, 0.9347, 0.9337, 0.9334,
         0.9329],
        [0.9472, 0.9416, 0.9378, 0.9351, 0.9342, 0.9322, 0.9291, 0.9282, 0.9274,
         0.9255],
        [0.9626, 0.9580, 0.9556, 0.9531, 0.9527, 0.9525, 0.9476, 0.9463, 0.9452,
         0.9428],
        [0.9519, 0.9310, 0.9308, 0.9233, 0.9217, 0.9210, 0.9164, 0.9106, 0.9096,
         0.9090],
        [0.8955, 0.8850, 0.8767, 0.8711, 0.8710, 0.8696, 0.8689, 0.8686, 0.8645,
         0.8619],
        [0.9276, 0.9150, 0.9138, 0.9080, 0.9072, 0.9001, 0.8998, 0.8968, 0.8952,
         0.8937],
        [0.9391, 0.9358, 0.9281, 0.9275, 0.9168, 0.9146, 0.9139, 0.9125, 0.9084,
         0.9080],
        [0.9207, 0.9170, 0.9086, 0.9060, 0.8953, 0.8909, 0.8889, 0.8882, 0.8833,
         0.8831],
        [0.9489, 0.8765, 0.8720, 0.8659, 0.8656, 0.8616, 0.8590, 0.8579, 0.8571,
         0.8553],
        [0.9355, 0.9218, 0.9116, 0.9110, 0.8911, 0.8880, 0.8875, 0.8850, 0.8829,
         0.8786],
        [0.9144, 0.9098, 0.8998, 0.8877, 0.8851, 0.8847, 0.8735, 0.8702, 0.8692,
         0.8653],
        [0.9557, 0.9527, 0.9503, 0.9489, 0.9479, 0.9461, 0.9452, 0.9451, 0.9437,
         0.9435],
        [0.8994, 0.8985, 0.8956, 0.8951, 0.8817, 0.8757, 0.8734, 0.8640, 0.8536,
         0.8518],
        [0.9186, 0.8912, 0.8890, 0.8880, 0.8843, 0.8821, 0.8806, 0.8769, 0.8766,
         0.8764],
        [0.9053, 0.8989, 0.8971, 0.8796, 0.8789, 0.8782, 0.8727, 0.8689, 0.8604,
         0.8601],
        [0.9162, 0.8966, 0.8879, 0.8866, 0.8787, 0.8709, 0.8668, 0.8627, 0.8572,
         0.8557],
        [0.8789, 0.8615, 0.8589, 0.8516, 0.8460, 0.8439, 0.8433, 0.8432, 0.8314,
         0.8296],
        [0.8407, 0.8353, 0.8209, 0.7994, 0.7897, 0.7847, 0.7843, 0.7809, 0.7636,
         0.7622],
        [0.9289, 0.9242, 0.9221, 0.9132, 0.9109, 0.9096, 0.9085, 0.9059, 0.9058,
         0.9021],
        [0.8381, 0.8185, 0.8095, 0.8072, 0.8058, 0.8039, 0.7825, 0.7783, 0.7688,
         0.7680],
        [0.8824, 0.8532, 0.8342, 0.8273, 0.8232, 0.8189, 0.8187, 0.8160, 0.8121,
         0.8106],
        [0.9135, 0.8937, 0.8877, 0.8862, 0.8623, 0.8592, 0.8591, 0.8578, 0.8560,
         0.8541],
        [0.9036, 0.8601, 0.8574, 0.8573, 0.8463, 0.8384, 0.8368, 0.8317, 0.8270,
         0.8259],
        [0.9066, 0.8997, 0.8710, 0.8668, 0.8594, 0.8483, 0.8482, 0.8415, 0.8413,
         0.8335],
        [0.9447, 0.9279, 0.9234, 0.9122, 0.8971, 0.8915, 0.8788, 0.8762, 0.8733,
         0.8708],
        [0.9305, 0.8865, 0.8856, 0.8806, 0.8782, 0.8762, 0.8691, 0.8635, 0.8627,
         0.8601],
        [0.9466, 0.9300, 0.9239, 0.9235, 0.9215, 0.9209, 0.9209, 0.9136, 0.9125,
         0.9114],
        [0.9262, 0.9006, 0.8992, 0.8972, 0.8935, 0.8930, 0.8924, 0.8907, 0.8906,
         0.8902],
        [0.9365, 0.9072, 0.8922, 0.8901, 0.8885, 0.8690, 0.8647, 0.8615, 0.8593,
         0.8536],
        [0.9468, 0.9024, 0.8948, 0.8846, 0.8777, 0.8675, 0.8664, 0.8617, 0.8598,
         0.8578],
        [0.9316, 0.8842, 0.8828, 0.8738, 0.8700, 0.8678, 0.8641, 0.8548, 0.8509,
         0.8494],
        [0.9158, 0.8443, 0.8420, 0.8419, 0.8299, 0.8260, 0.8238, 0.8236, 0.8236,
         0.8214],
        [0.8958, 0.8914, 0.8901, 0.8890, 0.8843, 0.8820, 0.8818, 0.8814, 0.8789,
         0.8787],
        [0.8612, 0.8022, 0.7997, 0.7992, 0.7928, 0.7736, 0.7728, 0.7724, 0.7721,
         0.7694],
        [0.8973, 0.8797, 0.8753, 0.8612, 0.8552, 0.8508, 0.8472, 0.8447, 0.8406,
         0.8345],
        [0.8503, 0.8469, 0.8432, 0.8218, 0.8197, 0.8127, 0.8097, 0.8025, 0.7999,
         0.7982]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 1, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 0, 1, 1, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 1, 1, 1, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 401603.3750,  209476.5469,  208324.8438,  197920.6562,  195741.8906,
          186792.1094,  178585.7188,  168027.0625,  161157.4531,  154991.1406],
        [ 823839.3125,  796979.2500,  783430.6250,  764270.1250,  743068.2500,
          732112.7500,  716250.2500,  698338.3750,  693646.3750,  669199.4375],
        [ 479784.7500,  462433.9688,  443423.3750,  327116.4375,  314102.1562,
          272610.3125,  206653.7656,  204652.3125,  195774.9375,  194031.4531],
        [ 848945.7500,  493498.3750,  433130.4062,  419811.5938,  386416.7500,
          386284.8438,  369338.7188,  343391.6250,  339788.3750,  334288.9062],
        [ 226803.5625,  183601.5938,  175638.3594,  145882.7188,  137335.1094,
          113661.4766,  110105.2422,  105646.9062,   96670.3594,   91189.8750],
        [ 227038.3750,  194550.0781,  153350.8906,  130544.2656,  123271.4609,
          121976.0859,  112352.0625,  107348.0703,  106215.1719,  105396.6406],
        [ 438886.3438,  437495.2188,  435159.5625,  336910.4375,  326069.5938,
          322641.0625,  315326.7188,  283135.1250,  280251.2188,  257054.3125],
        [ 548147.2500,  431904.9375,  403919.4688,  272304.4688,  271888.0000,
          241771.4531,  197757.0781,  179023.6250,  175225.7969,  172977.3594],
        [ 529711.1250,  503809.9375,  502824.5000,  465201.1875,  453941.4375,
          394640.5938,  384440.3125,  375922.0938,  370297.6875,  369390.1562],
        [ 852666.8125,  420967.4375,  397440.7812,  396056.3750,  385085.7500,
          371314.0000,  366482.6562,  337039.9375,  303228.5000,  281581.6562],
        [ 897389.9375,  561271.3750,  529827.3750,  527511.1875,  509733.1250,
          482254.1250,  464262.9375,  441334.1875,  436588.6875,  434810.2812],
        [ 490307.0000,  419116.7188,  284358.7812,  284343.3125,  242437.7969,
          190731.4688,  189574.8594,  145565.0156,  136821.7344,  126410.9688],
        [ 806623.0000,  521863.3750,  521232.1875,  497953.6562,  482891.9688,
          395344.2812,  390738.7188,  382235.5000,  326349.0000,  297620.0312],
        [1051238.3750,  959420.7500,  955609.3125,  931542.0000,  923539.2500,
          889647.6250,  871766.6250,  869645.8750,  850285.9375,  829939.8125],
        [ 747229.7500,  688144.0000,  673941.3750,  651272.5625,  628982.7500,
          626685.3125,  611280.2500,  608948.8125,  597804.8750,  574785.4375],
        [ 616560.6875,  588859.0625,  573123.6875,  544743.7500,  525232.6250,
          517993.8438,  503325.3438,  492804.7188,  488196.6562,  459202.1250],
        [1067781.7500,  969448.8125,  890808.1875,  832227.9375,  779718.0000,
          710344.9375,  696641.5000,  687918.3125,  674014.6875,  673713.8750],
        [1036103.8125,  950654.4375,  871190.6875,  781249.0625,  759178.6250,
          743166.8125,  734391.0000,  723456.3750,  722077.1250,  718555.8750],
        [ 694607.5000,  692259.8750,  631440.8750,  483126.9062,  458568.0312,
          458550.0625,  445142.3125,  432903.2812,  430556.8125,  429964.7188],
        [ 538021.5625,  526261.9375,  498538.0938,  469656.2188,  447999.5312,
          445227.6562,  437287.4688,  437278.3125,  435091.5000,  433357.6250],
        [ 992760.4375,  869549.6250,  843616.6250,  821926.0625,  821579.6250,
          776259.0000,  735210.8750,  729983.5000,  727727.9375,  697400.6250],
        [ 696303.4375,  619005.1250,  601510.0625,  526631.0000,  524675.4375,
          509083.0938,  497748.0938,  484000.3125,  478441.4375,  468100.1562],
        [1118545.6250,  814628.1250,  808040.4375,  764507.0625,  738129.9375,
          720894.3750,  662178.0625,  571187.6875,  568668.4375,  550574.0000],
        [ 822806.8125,  763650.8750,  694923.5625,  683800.8125,  641389.5625,
          622057.5625,  617987.6250,  609974.0625,  609886.8125,  602317.7500],
        [ 992572.0000,  490334.5938,  461136.1250,  397504.4688,  397155.8750,
          346296.1875,  345253.8438,  341620.5938,  330433.0938,  308038.3125],
        [ 770240.9375,  720503.9375,  697523.0000,  647424.3750,  613737.7500,
          598635.5000,  545419.5000,  527603.6875,  517810.1250,  517189.7500],
        [ 742725.3750,  714135.1250,  666621.0000,  616100.5000,  573318.2500,
          506970.2500,  497269.3438,  490155.5312,  488677.3750,  453839.7188],
        [ 989912.8750,  850890.3125,  841460.0625,  713567.3750,  653807.1875,
          633934.6250,  620897.6875,  618106.7500,  602609.0000,  597657.2500],
        [ 858605.5625,  776384.1875,  757906.8750,  678507.9375,  675654.5000,
          667715.3750,  629991.8750,  620704.0625,  617814.3750,  613885.8125],
        [ 752894.3125,  694842.0625,  658553.1250,  632850.3750,  624828.8125,
          607512.5625,  581332.9375,  573464.2500,  566998.9375,  551686.1875],
        [ 937445.2500,  877783.2500,  848187.5000,  819359.8125,  814686.3750,
          812092.4375,  756522.5000,  743411.3125,  731724.6250,  706620.0625],
        [ 805176.5625,  597162.1250,  595150.0625,  535139.0000,  523057.6875,
          517530.6875,  484984.5000,  446304.4375,  439885.7500,  435974.1562],
        [ 359518.3438,  309630.8438,  275004.7812,  253621.8594,  253605.6562,
          248376.7031,  245790.3750,  244928.3438,  231017.6250,  222558.3281],
        [ 568648.9375,  474853.0625,  467307.0938,  430225.1875,  424786.5000,
          383893.3125,  382674.6250,  366159.8750,  358120.8125,  350368.8438],
        [ 670000.2500,  639182.7500,  572626.5000,  567844.1875,  487681.9688,
          472234.2188,  468041.6562,  458579.4062,  432356.5625,  430244.8750],
        [ 515262.8438,  489059.6562,  433759.1250,  417850.7812,  358797.3750,
          336886.3438,  327485.4062,  323984.4375,  302168.7812,  301232.2500],
        [ 770790.6250,  274327.6562,  256992.7812,  235557.1875,  234558.9844,
          221732.7188,  213630.5000,  210189.9375,  207899.7188,  202413.5312],
        [ 636682.9375,  523482.9062,  452744.7188,  449083.9375,  337578.4688,
          323007.7188,  320590.3438,  309376.4062,  300208.9688,  282571.5938],
        [ 470961.4062,  441098.1250,  382390.0938,  321895.1250,  309804.8125,
          308132.3438,  262835.6562,  250527.2812,  247035.1719,  233502.8438],
        [ 849524.8750,  814407.5000,  787173.7500,  770704.5625,  760204.4375,
          741436.6250,  731957.6875,  730914.1875,  715648.6250,  713941.6875],
        [ 380094.4375,  375437.3750,  359971.5625,  357430.5938,  295285.9062,
          271168.8750,  262166.7500,  229211.1094,  197661.8594,  192683.1250],
        [ 500552.3438,  338390.7500,  327867.8750,  322882.3438,  306437.0312,
          297041.0000,  290816.0625,  275790.8750,  274657.5000,  273714.8750],
        [ 413546.6562,  377794.9375,  368027.5938,  286607.3438,  283777.1250,
          281049.9062,  259598.6875,  245815.4531,  217911.0938,  216838.0938],
        [ 483382.2188,  365582.7500,  322760.4375,  316622.4375,  282884.9062,
          252961.6875,  238705.7812,  225178.7656,  208211.0312,  203711.6250],
        [ 283749.2500,  221170.7344,  213055.5312,  192172.5781,  177384.4844,
          172140.2188,  170571.0938,  170472.7031,  143879.5625,  140225.1406],
        [ 164295.1094,  152275.5312,  123837.0391,   91101.2969,   79326.0234,
           73909.4609,   73473.2422,   70002.5000,   54661.5352,   53540.3672],
        [ 579167.0625,  541932.6875,  526120.9375,  463190.9375,  448059.7812,
          440037.2500,  433199.8125,  417129.7188,  416699.1562,  395187.8438],
        [ 158284.5781,  119729.5000,  105259.0234,  101913.5703,   99818.0781,
           97187.0938,   71562.6484,   67406.0625,   58890.0859,   58167.4258],
        [ 298099.7812,  196582.2344,  149751.2500,  135714.3750,  128055.4453,
          120457.4453,  119992.6406,  115514.9219,  109296.4375,  106993.0000],
        [ 464830.0000,  350626.9062,  321734.0000,  315106.3750,  223776.1094,
          214107.5625,  213665.7500,  210012.6250,  204572.5000,  199094.9531],
        [ 403770.0312,  217003.1875,  208554.4219,  208274.7656,  178111.8594,
          159164.0469,  155504.4531,  144586.5781,  135230.4219,  132971.0625],
        [ 421426.5625,  382051.0938,  253483.5625,  238576.7344,  214826.0938,
          183251.0469,  182938.3125,  166290.1094,  165810.1250,  148231.4688],
        [ 726513.7500,  571565.3125,  535923.0000,  456512.8438,  367908.6250,
          339674.0000,  283443.0938,  273138.3125,  261816.4688,  252533.8438],
        [ 593111.9375,  316039.9062,  312371.3438,  290641.6562,  280883.7812,
          272834.5000,  246663.9219,  227535.6094,  225003.1719,  217020.3438],
        [ 746746.0625,  588560.8750,  539832.8125,  536717.8125,  521745.9375,
          516779.5312,  516779.5312,  465800.5000,  458509.8750,  451353.9375],
        [ 557868.3750,  386632.7812,  379116.2812,  368488.7188,  349347.8750,
          347056.5938,  344071.5000,  335879.6875,  335307.1250,  333482.6875],
        [ 646469.3125,  425002.9062,  343166.4062,  333009.7812,  325494.8438,
          246131.2031,  231652.5625,  221246.4531,  214536.1875,  197660.5469],
        [ 748193.0625,  396940.4375,  356159.6875,  307795.7812,  278995.2188,
          241027.6094,  237202.1406,  221821.1094,  215996.2188,  209833.4375],
        [ 602423.4375,  305851.0938,  299951.4375,  263693.0938,  249680.5312,
          242089.1562,  229489.3281,  201034.0000,  190058.3750,  186263.2500],
        [ 480516.9375,  173075.2031,  167372.1562,  167298.7500,  140839.5000,
          133306.5312,  129197.2344,  128818.2969,  128805.0234,  124763.3203],
        [ 360963.3438,  339055.2188,  332818.9688,  327859.7500,  306670.0625,
          296603.0625,  295595.8125,  293950.7188,  283558.2812,  282960.7188],
        [ 220326.9531,   94860.6406,   91577.6875,   90876.8125,   82873.4766,
           63042.9102,   62346.3008,   61940.9336,   61695.0352,   59399.7578],
        [ 369202.7812,  286782.5938,  269671.8125,  220439.6250,  202300.0625,
          190018.3281,  180445.7812,  174166.8438,  164254.2031,  150562.2031],
        [ 188515.3281,  179651.1250,  170486.8438,  125436.3203,  121857.3750,
          110183.7031,  105632.6094,   95261.3359,   91844.7969,   89598.5391]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[401603.3750,      0.0000],
         [209476.5469,      0.0000],
         [208324.8438,      0.0000],
         ...,
         [168027.0625,      0.0000],
         [161157.4531,      0.0000],
         [154991.1406,      0.0000]],

        [[823839.3125,      0.0000],
         [796979.2500,      0.0000],
         [783430.6250,      0.0000],
         ...,
         [698338.3750,      0.0000],
         [693646.3750,      0.0000],
         [669199.4375,      0.0000]],

        [[479784.7500,      0.0000],
         [462433.9688,      0.0000],
         [443423.3750,      0.0000],
         ...,
         [204652.3125,      0.0000],
         [     0.0000, 195774.9375],
         [194031.4531,      0.0000]],

        ...,

        [[     0.0000, 220326.9531],
         [ 94860.6406,      0.0000],
         [ 91577.6875,      0.0000],
         ...,
         [ 61940.9336,      0.0000],
         [ 61695.0352,      0.0000],
         [ 59399.7578,      0.0000]],

        [[369202.7812,      0.0000],
         [286782.5938,      0.0000],
         [     0.0000, 269671.8125],
         ...,
         [174166.8438,      0.0000],
         [164254.2031,      0.0000],
         [150562.2031,      0.0000]],

        [[     0.0000, 188515.3281],
         [     0.0000, 179651.1250],
         [     0.0000, 170486.8438],
         ...,
         [ 95261.3359,      0.0000],
         [ 91844.7969,      0.0000],
         [ 89598.5391,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1884035.1250,  178585.7188],
        [7421135.0000,       0.0000],
        [2904808.2500,  195774.9375],
        [4354895.0000,       0.0000],
        [ 581183.3125,  805351.8750],
        [1187493.0000,  194550.0781],
        [2671700.5000,  761229.1250],
        [2894919.2500,       0.0000],
        [3820467.7500,  529711.1250],
        [4111863.7500,       0.0000],
        [5284983.5000,       0.0000],
        [2509667.7500,       0.0000],
        [4622851.5000,       0.0000],
        [9132636.0000,       0.0000],
        [6409075.0000,       0.0000],
        [5310042.5000,       0.0000],
        [7982618.0000,       0.0000],
        [8040023.5000,       0.0000],
        [5157120.0000,       0.0000],
        [4668720.0000,       0.0000],
        [8016014.0000,       0.0000],
        [5405498.0000,       0.0000],
        [7317354.0000,       0.0000],
        [6668796.0000,       0.0000],
        [4410345.0000,       0.0000],
        [6156088.5000,       0.0000],
        [5749812.5000,       0.0000],
        [7122843.0000,       0.0000],
        [6897171.0000,       0.0000],
        [6244963.5000,       0.0000],
        [8047833.0000,       0.0000],
        [5380365.0000,       0.0000],
        [2421494.5000,  222558.3281],
        [2943346.0000, 1263692.1250],
        [2849531.0000, 2349261.5000],
        [1788144.3750, 2018342.6250],
        [1614844.2500, 1213249.3750],
        [2326078.2500, 1609249.7500],
        [1349002.5000, 1879180.3750],
        [7615914.0000,       0.0000],
        [2921111.7500,       0.0000],
        [3208150.7500,       0.0000],
        [2950966.7500,       0.0000],
        [2900001.7500,       0.0000],
        [ 508192.3750, 1376629.0000],
        [ 496014.4375,  440407.6562],
        [4118792.5000,  541932.6875],
        [ 629868.3750,  308349.6562],
        [ 729031.2500,  751426.2500],
        [ 873937.5625, 1843589.2500],
        [1515805.3750,  427365.5625],
        [1299923.8750, 1056961.2500],
        [2180338.0000, 1888691.2500],
        [2666066.2500,  316039.9062],
        [5342827.0000,       0.0000],
        [3737251.5000,       0.0000],
        [1430984.1250, 1753386.1250],
        [2181869.2500, 1032095.3125],
        [1549699.5000, 1220834.2500],
        [ 739302.3750, 1034690.5000],
        [2463477.0000,  656559.1250],
        [ 668613.5625,  220326.9531],
        [1527714.5000,  680129.7500],
        [ 508745.7812,  769722.1875]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 60.9375
Top1 accuracy for validation set is 60.9375 size is torch.Size([64, 1])
Epoch 266/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:39, 63.43s/it]  7%|▋         | 2/30 [01:04<12:23, 26.56s/it] 10%|█         | 3/30 [01:04<06:38, 14.78s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.24s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.18s/it] 20%|██        | 6/30 [01:07<01:43,  4.33s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:25<00:00,  2.85s/it]
Epoch loss is 2.647475822766622
Epoch 267/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:57, 57.83s/it]  7%|▋         | 2/30 [01:01<12:11, 26.14s/it] 10%|█         | 3/30 [01:02<06:32, 14.55s/it] 13%|█▎        | 4/30 [01:03<03:56,  9.10s/it] 17%|█▋        | 5/30 [01:04<02:32,  6.09s/it] 20%|██        | 6/30 [01:04<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.37s/it] 30%|███       | 9/30 [01:07<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.6572516838709515
Epoch 268/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:01, 62.12s/it]  7%|▋         | 2/30 [01:02<12:08, 26.02s/it] 10%|█         | 3/30 [01:03<06:30, 14.48s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 2.6143027464548747
Epoch 269/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:03, 58.07s/it]  7%|▋         | 2/30 [01:00<11:49, 25.36s/it] 10%|█         | 3/30 [01:01<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.92s/it] 20%|██        | 6/30 [01:03<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.6755524476369223
Epoch 270/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:46, 63.67s/it]  7%|▋         | 2/30 [01:04<12:26, 26.66s/it] 10%|█         | 3/30 [01:05<06:40, 14.83s/it] 13%|█▎        | 4/30 [01:05<04:01,  9.27s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.20s/it] 20%|██        | 6/30 [01:07<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:08<01:12,  3.17s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.40s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.85s/it]
Epoch loss is 2.651793384552002
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0246,  0.0045,  0.0066,  ...,  0.0027,  0.0001,  0.0097],
        [-0.0009,  0.0219,  0.0192,  ...,  0.0121, -0.0093, -0.0160],
        [-0.0340, -0.0342,  0.0165,  ...,  0.0790, -0.0195, -0.0237],
        ...,
        [ 0.0197, -0.0106, -0.0036,  ..., -0.0244, -0.0232, -0.0150],
        [-0.0279, -0.0014, -0.0109,  ..., -0.0015,  0.0114, -0.0134],
        [-0.0382, -0.0090,  0.0128,  ...,  0.0293,  0.0251, -0.0471]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9022, 0.8568, 0.8563, 0.8539, 0.8515, 0.8506, 0.8458, 0.8421, 0.8402,
         0.8362],
        [0.9529, 0.9500, 0.9492, 0.9472, 0.9457, 0.9437, 0.9426, 0.9409, 0.9405,
         0.9380],
        [0.9145, 0.9129, 0.9088, 0.8883, 0.8849, 0.8743, 0.8569, 0.8532, 0.8510,
         0.8506],
        [0.9552, 0.9166, 0.9076, 0.9047, 0.8994, 0.8990, 0.8960, 0.8915, 0.8911,
         0.8891],
        [0.8623, 0.8474, 0.8455, 0.8321, 0.8276, 0.8152, 0.8112, 0.8096, 0.8034,
         0.8002],
        [0.8629, 0.8483, 0.8342, 0.8217, 0.8168, 0.8158, 0.8138, 0.8087, 0.8076,
         0.8048],
        [0.9066, 0.9064, 0.9048, 0.8891, 0.8867, 0.8849, 0.8837, 0.8771, 0.8759,
         0.8703],
        [0.9236, 0.9066, 0.9017, 0.8751, 0.8743, 0.8672, 0.8510, 0.8436, 0.8432,
         0.8431],
        [0.9201, 0.9193, 0.9166, 0.9132, 0.9101, 0.9009, 0.8992, 0.8978, 0.8963,
         0.8962],
        [0.9555, 0.9051, 0.9016, 0.9006, 0.8993, 0.8963, 0.8954, 0.8899, 0.8817,
         0.8772],
        [0.9585, 0.9251, 0.9210, 0.9202, 0.9184, 0.9151, 0.9121, 0.9078, 0.9075,
         0.9071],
        [0.9164, 0.9043, 0.8780, 0.8775, 0.8658, 0.8478, 0.8476, 0.8289, 0.8244,
         0.8209],
        [0.9503, 0.9206, 0.9202, 0.9167, 0.9158, 0.9015, 0.8992, 0.8982, 0.8864,
         0.8812],
        [0.9702, 0.9636, 0.9635, 0.9621, 0.9612, 0.9584, 0.9575, 0.9564, 0.9551,
         0.9537],
        [0.9465, 0.9403, 0.9389, 0.9367, 0.9345, 0.9335, 0.9315, 0.9313, 0.9305,
         0.9270],
        [0.9329, 0.9298, 0.9280, 0.9240, 0.9204, 0.9204, 0.9181, 0.9167, 0.9167,
         0.9123],
        [0.9703, 0.9639, 0.9582, 0.9526, 0.9480, 0.9422, 0.9403, 0.9389, 0.9383,
         0.9376],
        [0.9694, 0.9634, 0.9575, 0.9490, 0.9472, 0.9454, 0.9452, 0.9443, 0.9439,
         0.9431],
        [0.9405, 0.9403, 0.9334, 0.9139, 0.9112, 0.9109, 0.9091, 0.9078, 0.9077,
         0.9074],
        [0.9225, 0.9200, 0.9169, 0.9128, 0.9100, 0.9099, 0.9086, 0.9084, 0.9078,
         0.9076],
        [0.9652, 0.9561, 0.9536, 0.9525, 0.9520, 0.9482, 0.9449, 0.9443, 0.9439,
         0.9409],
        [0.9406, 0.9323, 0.9304, 0.9215, 0.9197, 0.9192, 0.9172, 0.9146, 0.9144,
         0.9132],
        [0.9748, 0.9520, 0.9517, 0.9478, 0.9452, 0.9437, 0.9375, 0.9270, 0.9265,
         0.9250],
        [0.9527, 0.9475, 0.9409, 0.9394, 0.9353, 0.9327, 0.9324, 0.9323, 0.9317,
         0.9305],
        [0.9659, 0.9161, 0.9121, 0.9001, 0.9001, 0.8929, 0.8914, 0.8908, 0.8885,
         0.8840],
        [0.9483, 0.9441, 0.9418, 0.9363, 0.9319, 0.9311, 0.9242, 0.9211, 0.9209,
         0.9203],
        [0.9449, 0.9424, 0.9381, 0.9327, 0.9271, 0.9183, 0.9174, 0.9165, 0.9159,
         0.9115],
        [0.9661, 0.9553, 0.9543, 0.9431, 0.9361, 0.9349, 0.9333, 0.9328, 0.9317,
         0.9304],
        [0.9559, 0.9490, 0.9463, 0.9393, 0.9388, 0.9377, 0.9345, 0.9331, 0.9326,
         0.9321],
        [0.9463, 0.9408, 0.9371, 0.9343, 0.9335, 0.9315, 0.9292, 0.9273, 0.9262,
         0.9247],
        [0.9617, 0.9572, 0.9547, 0.9525, 0.9523, 0.9520, 0.9467, 0.9457, 0.9448,
         0.9419],
        [0.9515, 0.9303, 0.9299, 0.9230, 0.9202, 0.9199, 0.9157, 0.9087, 0.9087,
         0.9085],
        [0.8942, 0.8838, 0.8754, 0.8713, 0.8704, 0.8687, 0.8676, 0.8674, 0.8640,
         0.8615],
        [0.9267, 0.9139, 0.9121, 0.9073, 0.9064, 0.8993, 0.8988, 0.8956, 0.8948,
         0.8931],
        [0.9376, 0.9352, 0.9274, 0.9270, 0.9152, 0.9144, 0.9135, 0.9118, 0.9074,
         0.9069],
        [0.9194, 0.9160, 0.9075, 0.9046, 0.8939, 0.8883, 0.8881, 0.8880, 0.8818,
         0.8811],
        [0.9491, 0.8761, 0.8700, 0.8648, 0.8629, 0.8589, 0.8589, 0.8582, 0.8560,
         0.8524],
        [0.9331, 0.9195, 0.9109, 0.9095, 0.8891, 0.8864, 0.8862, 0.8841, 0.8818,
         0.8784],
        [0.9129, 0.9089, 0.8985, 0.8852, 0.8839, 0.8815, 0.8721, 0.8682, 0.8678,
         0.8639],
        [0.9543, 0.9519, 0.9495, 0.9480, 0.9469, 0.9447, 0.9444, 0.9434, 0.9420,
         0.9416],
        [0.8989, 0.8980, 0.8944, 0.8936, 0.8803, 0.8752, 0.8728, 0.8628, 0.8529,
         0.8495],
        [0.9177, 0.8892, 0.8868, 0.8865, 0.8825, 0.8815, 0.8789, 0.8753, 0.8751,
         0.8746],
        [0.9044, 0.8980, 0.8963, 0.8782, 0.8774, 0.8771, 0.8713, 0.8675, 0.8591,
         0.8589],
        [0.9147, 0.8945, 0.8863, 0.8861, 0.8773, 0.8698, 0.8656, 0.8613, 0.8552,
         0.8548],
        [0.8783, 0.8594, 0.8573, 0.8490, 0.8454, 0.8419, 0.8405, 0.8401, 0.8275,
         0.8261],
        [0.8410, 0.8339, 0.8201, 0.7989, 0.7899, 0.7843, 0.7819, 0.7817, 0.7638,
         0.7624],
        [0.9266, 0.9230, 0.9201, 0.9117, 0.9098, 0.9083, 0.9068, 0.9045, 0.9041,
         0.9000],
        [0.8365, 0.8171, 0.8067, 0.8056, 0.8042, 0.8017, 0.7815, 0.7760, 0.7687,
         0.7655],
        [0.8794, 0.8522, 0.8331, 0.8270, 0.8226, 0.8195, 0.8166, 0.8152, 0.8113,
         0.8091],
        [0.9124, 0.8905, 0.8850, 0.8847, 0.8608, 0.8572, 0.8568, 0.8550, 0.8550,
         0.8510],
        [0.9036, 0.8608, 0.8586, 0.8579, 0.8467, 0.8388, 0.8362, 0.8307, 0.8258,
         0.8253],
        [0.9061, 0.8990, 0.8701, 0.8635, 0.8576, 0.8472, 0.8453, 0.8393, 0.8386,
         0.8312],
        [0.9437, 0.9273, 0.9228, 0.9117, 0.8953, 0.8900, 0.8780, 0.8745, 0.8712,
         0.8708],
        [0.9294, 0.8851, 0.8845, 0.8803, 0.8769, 0.8750, 0.8680, 0.8626, 0.8616,
         0.8586],
        [0.9457, 0.9285, 0.9228, 0.9222, 0.9198, 0.9191, 0.9191, 0.9121, 0.9116,
         0.9106],
        [0.9251, 0.8989, 0.8978, 0.8959, 0.8918, 0.8913, 0.8907, 0.8899, 0.8889,
         0.8888],
        [0.9354, 0.9050, 0.8906, 0.8894, 0.8868, 0.8688, 0.8626, 0.8593, 0.8568,
         0.8521],
        [0.9462, 0.9019, 0.8931, 0.8843, 0.8771, 0.8661, 0.8650, 0.8600, 0.8600,
         0.8566],
        [0.9302, 0.8824, 0.8809, 0.8726, 0.8674, 0.8657, 0.8615, 0.8536, 0.8491,
         0.8478],
        [0.9148, 0.8428, 0.8411, 0.8399, 0.8283, 0.8238, 0.8222, 0.8219, 0.8217,
         0.8206],
        [0.8943, 0.8883, 0.8872, 0.8869, 0.8833, 0.8822, 0.8809, 0.8793, 0.8772,
         0.8771],
        [0.8612, 0.8011, 0.7984, 0.7980, 0.7908, 0.7709, 0.7706, 0.7694, 0.7687,
         0.7652],
        [0.8965, 0.8782, 0.8743, 0.8585, 0.8526, 0.8487, 0.8439, 0.8415, 0.8372,
         0.8340],
        [0.8478, 0.8469, 0.8394, 0.8186, 0.8174, 0.8088, 0.8059, 0.7997, 0.7973,
         0.7934]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 0, 1, 1, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 1, 0, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 1, 1, 1, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 395546.0000,  206917.2344,  205443.8906,  198519.3281,  191757.7500,
          189373.1875,  176832.5000,  167806.0781,  163286.8125,  154151.3438],
        [ 816523.6250,  783519.5625,  774447.3750,  752483.7500,  736367.2500,
          715784.5000,  705205.6250,  687457.8750,  684188.2500,  660032.5625],
        [ 471882.6250,  461180.0938,  434702.0625,  324629.9375,  309059.3750,
          265631.2188,  207158.5156,  196528.7969,  190571.2969,  189438.0625],
        [ 843777.5625,  486288.2188,  427780.6250,  410164.4688,  380127.0625,
          378348.0312,  362228.5938,  339558.3750,  337537.5625,  328186.6562],
        [ 223674.3438,  181007.6719,  176088.3438,  145394.2188,  136244.2656,
          114134.9609,  107851.4922,  105391.1094,   96527.9375,   92224.4141],
        [ 225614.8906,  183306.6250,  149853.9688,  125359.0625,  116839.6953,
          115163.4922,  111962.1875,  104036.3828,  102486.5703,   98497.8828],
        [ 421213.9688,  420056.6875,  410991.4688,  328075.2500,  316939.3750,
          309096.5625,  303970.5938,  276559.6562,  271837.4062,  250794.5156],
        [ 536925.1875,  421401.2500,  392965.9688,  268640.9688,  265750.8438,
          240117.7031,  190353.3281,  171344.3750,  170357.7969,  170015.6719],
        [ 511242.3125,  505590.8125,  486402.3438,  462956.4062,  443050.0938,
          388555.2188,  379207.4062,  371877.1250,  363740.6250,  363054.7812],
        [ 847032.3125,  412338.4688,  392521.0000,  386723.4688,  379698.4375,
          363623.4062,  359210.9375,  331829.5312,  295257.1562,  276911.7500],
        [ 884968.1250,  549019.3750,  517607.6562,  511502.2188,  498859.1250,
          475918.9688,  455955.4688,  428854.9062,  426593.8750,  424268.2812],
        [ 484795.3438,  407679.9688,  280093.0625,  278003.2500,  235137.4844,
          182037.6719,  181480.0625,  138870.3750,  130175.1484,  123895.8672],
        [ 786985.3125,  515015.2188,  511735.0000,  487023.8438,  480683.7812,
          391691.2188,  379390.4375,  373798.9688,  315716.6875,  293135.2500],
        [1046048.0625,  951167.6875,  950356.1875,  930626.4375,  919778.1875,
          883207.6875,  872188.1875,  857963.0000,  842874.4375,  825380.7500],
        [ 745087.8750,  682492.5625,  668388.1875,  648183.6875,  627662.0625,
          618659.8750,  601529.5625,  599912.8750,  592687.8750,  564183.8125],
        [ 613828.4375,  586653.8750,  572225.8125,  540180.9375,  513577.6875,
          513349.0000,  496772.1250,  487194.3438,  486649.6250,  457328.1250],
        [1047244.9375,  955119.0625,  881111.1875,  812847.8750,  761299.3125,
          701179.2500,  682471.1250,  668098.8750,  662456.6250,  656268.5000],
        [1033062.9375,  949215.8750,  871924.5625,  772271.7500,  752741.3750,
          733906.4375,  731029.8750,  722546.9375,  717825.0625,  709685.4375],
        [ 684324.6875,  681604.0625,  617598.8125,  467583.0312,  450201.8438,
          448027.7500,  436566.1875,  428460.0000,  428355.4375,  426150.2812],
        [ 528904.0000,  510365.9062,  488440.6250,  460775.6562,  442469.5000,
          441749.7812,  433867.9375,  432645.7188,  428531.5312,  427436.0312],
        [ 973998.0000,  855320.1250,  824515.3125,  812486.6875,  806278.5000,
          763388.0625,  728472.3125,  722074.4375,  718285.2500,  687544.4375],
        [ 685046.1875,  608281.9375,  592231.3750,  521158.1250,  508267.1250,
          504489.3125,  490311.6875,  472635.2188,  471115.5000,  463394.5938],
        [1116666.5000,  805622.0625,  803028.6250,  759041.7500,  731894.1875,
          715523.1250,  655582.2500,  564422.2500,  559806.1250,  548389.8125],
        [ 813856.2500,  755544.8750,  688337.6250,  673121.1875,  634678.1250,
          611507.6250,  608879.6875,  608485.0000,  603436.0000,  592972.8125],
        [ 983149.3125,  482860.6562,  456080.7188,  384316.4062,  384125.1562,
          346457.7188,  339173.8750,  336416.0000,  325441.7812,  305337.5938],
        [ 764852.0625,  720485.4375,  696349.9375,  644460.1250,  605260.6875,
          598358.1250,  542193.1875,  518207.3125,  516650.9375,  512749.6562],
        [ 727870.1875,  702942.8125,  661342.5000,  611516.4375,  564929.0000,
          498004.4688,  491887.1875,  485793.6875,  481196.0938,  452082.8438],
        [ 986639.6250,  845596.5000,  832627.3125,  710142.3750,  642128.2500,
          631003.8750,  617037.7500,  612645.9375,  603012.5625,  591977.8125],
        [ 852186.3750,  772491.1875,  743583.0000,  672326.3125,  667761.2500,
          657522.6875,  627551.3125,  615060.1875,  611304.1250,  606229.5000],
        [ 743442.5000,  686713.5000,  651512.3125,  625725.6875,  619115.5625,
          601526.6875,  581848.1875,  566798.3750,  557646.5625,  545766.0625],
        [ 926390.1875,  867780.1250,  837809.4375,  811356.2500,  809493.5625,
          806651.4375,  747476.3750,  737051.6250,  726958.0000,  698240.5000],
        [ 800634.4375,  591292.8750,  588254.5000,  532906.8125,  511925.8125,
          509479.4062,  479680.9062,  434379.2500,  434055.4062,  433112.6250],
        [ 352866.0625,  304498.0625,  269895.9062,  254456.9688,  251190.9062,
          245210.2031,  241568.4062,  240732.8750,  229225.5156,  221247.0938],
        [ 561832.1250,  467840.8438,  456089.8750,  425380.4375,  420223.3438,
          379830.3125,  376769.1250,  360040.2500,  356136.5938,  347674.7500],
        [ 656239.1250,  634479.0000,  567431.1250,  564233.9375,  476395.7188,
          471319.0625,  465239.3438,  454255.8125,  426255.1250,  423294.2812],
        [ 506124.8438,  481928.5938,  427196.4062,  409671.0938,  351464.5312,
          324408.0312,  323389.2812,  322972.5938,  295731.7188,  292649.2188],
        [ 773017.4375,  272541.6875,  249928.0625,  231924.2344,  225861.6094,
          213066.0938,  213064.2656,  210945.7969,  204620.9062,  194300.6875],
        [ 615254.9375,  506784.6250,  448252.0938,  439378.8438,  328382.3438,
          315898.5938,  314979.5625,  305623.0938,  295849.0938,  281638.3125],
        [ 460950.5625,  435795.4062,  375285.2188,  310491.0312,  304633.1250,
          294483.5625,  257631.2969,  243439.8281,  242151.5000,  229021.2188],
        [ 833203.1875,  805262.6250,  777508.9375,  761020.5625,  749259.1875,
          726347.5000,  723197.6875,  712655.3750,  698349.0625,  694724.8125],
        [ 377378.3125,  372946.5312,  354227.5312,  349871.6875,  289514.0938,
          269051.9375,  260112.9062,  225323.3125,  195678.6250,  186311.4062],
        [ 493689.5000,  328589.0938,  317473.2812,  316313.0938,  298676.9062,
          294362.2812,  283819.9062,  269556.1250,  268877.5312,  266997.8750],
        [ 408394.7812,  372594.5312,  363721.5312,  280746.4062,  277886.8750,
          276580.5000,  254454.2969,  240961.6562,  213801.7031,  213162.8438],
        [ 473052.7500,  354590.1875,  315447.0000,  314532.0312,  277348.3438,
          249296.5000,  234703.5312,  220789.9062,  202254.9219,  201057.5938],
        [ 281390.7812,  214698.0781,  208320.6719,  185030.8281,  175816.3281,
          167320.1250,  163851.6562,  163055.8750,  136200.8594,  133438.8125],
        [ 165175.9688,  149203.1406,  122421.2578,   90488.3359,   79520.2266,
           73470.5156,   71018.4062,   70770.8828,   54815.6406,   53683.2734],
        [ 561166.0000,  532584.1875,  510731.5938,  453507.0312,  440857.5625,
          431598.5625,  422429.2500,  409075.3750,  406422.1875,  383332.1250],
        [ 154732.3906,  117305.0469,  101133.5859,   99617.3281,   97633.2734,
           94140.9219,   70587.0703,   65210.2539,   58738.7539,   56110.7930],
        [ 285598.5938,  193685.5469,  147492.1562,  135145.9688,  126867.0000,
          121520.4844,  116457.7969,  114136.3750,  108076.5703,  104628.3984],
        [ 457625.2500,  334733.9375,  309637.0312,  308291.0625,  219186.2344,
          208004.2188,  206816.2344,  201647.8594,  201577.3125,  190342.4219],
        [ 403738.8438,  219038.7031,  212155.9062,  210237.2500,  179064.5938,
          159888.0625,  154219.4219,  142597.2812,  132808.3438,  131905.5781],
        [ 418397.4688,  377885.0312,  250292.2812,  227800.7188,  209332.7656,
          180265.0156,  175657.9531,  161130.7188,  159631.9531,  143529.6875],
        [ 715998.8750,  566207.3125,  531428.4375,  453428.2812,  358655.0625,
          332471.6250,  280171.8750,  266595.1250,  254225.5625,  252618.3906],
        [ 583775.1875,  309918.0000,  307474.5312,  289372.4688,  275910.0312,
          268146.7188,  242912.4688,  224630.7344,  221555.1562,  212332.7969],
        [ 736713.5625,  576564.2500,  531332.1250,  526619.0000,  508641.0000,
          504179.0312,  504179.0312,  455627.7188,  452446.9062,  446307.8750],
        [ 548588.1250,  377353.1250,  371705.8438,  361876.0625,  341064.5938,
          338594.7500,  335826.2188,  332144.8438,  327381.7188,  326675.3125],
        [ 636221.0000,  411851.0938,  335540.6250,  329784.8750,  317686.2188,
          245662.8906,  224856.0000,  214455.5781,  206949.6094,  193329.9375],
        [ 742474.6875,  394094.5000,  347488.4375,  306495.2188,  276512.7500,
          236226.9219,  232684.1406,  216702.7031,  216501.4844,  206170.5000],
        [ 590075.5625,  298107.4688,  291822.6250,  259152.9688,  240647.0469,
          234929.9219,  221422.7188,  197731.4375,  185324.8750,  182036.6406],
        [ 474052.2188,  169340.8906,  165446.3438,  162556.2344,  137636.2812,
          129080.4766,  126142.6484,  125649.5469,  125293.5625,  123358.3672],
        [ 353477.6875,  324508.3125,  319268.6250,  317899.8750,  302186.9375,
          297501.3750,  291936.4375,  285153.6562,  277018.4688,  276574.4688],
        [ 220331.5781,   93348.0234,   89802.5625,   89294.9453,   80533.5078,
           60633.9648,   60419.5781,   59329.7812,   58805.7344,   55910.1055],
        [ 364930.6562,  280783.8750,  265556.0000,  212052.7344,  194889.5469,
          184275.5625,  172083.0781,  166341.6562,  156265.2969,  149335.0938],
        [ 181965.2969,  179690.8750,  161473.1250,  119844.5391,  117904.6328,
          104173.6953,   99942.4766,   91540.2266,   88371.2734,   83632.5000]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[395546.0000,      0.0000],
         [206917.2344,      0.0000],
         [205443.8906,      0.0000],
         ...,
         [167806.0781,      0.0000],
         [163286.8125,      0.0000],
         [154151.3438,      0.0000]],

        [[816523.6250,      0.0000],
         [783519.5625,      0.0000],
         [774447.3750,      0.0000],
         ...,
         [687457.8750,      0.0000],
         [684188.2500,      0.0000],
         [660032.5625,      0.0000]],

        [[471882.6250,      0.0000],
         [461180.0938,      0.0000],
         [434702.0625,      0.0000],
         ...,
         [196528.7969,      0.0000],
         [     0.0000, 190571.2969],
         [189438.0625,      0.0000]],

        ...,

        [[     0.0000, 220331.5781],
         [ 93348.0234,      0.0000],
         [ 89802.5625,      0.0000],
         ...,
         [ 59329.7812,      0.0000],
         [ 58805.7344,      0.0000],
         [ 55910.1055,      0.0000]],

        [[364930.6562,      0.0000],
         [280783.8750,      0.0000],
         [     0.0000, 265556.0000],
         ...,
         [166341.6562,      0.0000],
         [156265.2969,      0.0000],
         [149335.0938,      0.0000]],

        [[     0.0000, 181965.2969],
         [     0.0000, 179690.8750],
         [     0.0000, 161473.1250],
         ...,
         [ 91540.2266,      0.0000],
         [ 88371.2734,      0.0000],
         [ 83632.5000,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1872801.5000,  176832.5000],
        [7316010.0000,       0.0000],
        [2860210.7500,  190571.2969],
        [4293997.0000,       0.0000],
        [ 578242.3750,  800296.4375],
        [1149814.0000,  183306.6250],
        [2581604.5000,  727930.8750],
        [2827873.0000,       0.0000],
        [3764434.7500,  511242.3125],
        [4045146.5000,       0.0000],
        [5173548.0000,       0.0000],
        [2442168.2500,       0.0000],
        [4535176.0000,       0.0000],
        [9079591.0000,       0.0000],
        [6348788.5000,       0.0000],
        [5267760.0000,       0.0000],
        [7828097.0000,       0.0000],
        [7994210.0000,       0.0000],
        [5068872.0000,       0.0000],
        [4595186.5000,       0.0000],
        [7892363.0000,       0.0000],
        [5316931.0000,       0.0000],
        [7259977.0000,       0.0000],
        [6590819.0000,       0.0000],
        [4343359.5000,       0.0000],
        [6119567.5000,       0.0000],
        [5677565.0000,       0.0000],
        [7072812.0000,       0.0000],
        [6826015.5000,       0.0000],
        [6180095.0000,       0.0000],
        [7969208.0000,       0.0000],
        [5315722.0000,       0.0000],
        [2610892.0000,       0.0000],
        [2914210.7500, 1237607.0000],
        [2812871.2500, 2326271.0000],
        [1746182.1250, 1989354.2500],
        [1579446.0000, 1209824.8750],
        [2008984.7500, 1843056.7500],
        [1313729.2500, 1840153.5000],
        [7481529.0000,       0.0000],
        [2880416.2500,       0.0000],
        [3138355.5000,       0.0000],
        [2902305.0000,       0.0000],
        [2843072.7500,       0.0000],
        [ 485083.3125, 1344040.6250],
        [ 440084.0312,  490483.6250],
        [4019120.0000,  532584.1875],
        [ 612467.7500,  302741.6562],
        [ 718756.0000,  734852.8750],
        [ 849615.5625, 1788246.0000],
        [1521052.0000,  424601.9688],
        [1257348.7500, 1046574.7500],
        [2147705.2500, 1864095.3750],
        [2628553.7500,  307474.5312],
        [5242610.0000,       0.0000],
        [3661210.5000,       0.0000],
        [1401197.6250, 1715140.2500],
        [2157255.7500, 1018095.6875],
        [1508884.2500, 1192367.0000],
        [ 724215.7500, 1014340.8750],
        [2394546.7500,  650979.0625],
        [ 648078.2500,  220331.5781],
        [1484629.2500,  661884.3125],
        [ 485622.3125,  742916.3125]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 62.5
Top1 accuracy for validation set is 62.5 size is torch.Size([64, 1])
Epoch 271/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:10, 62.44s/it]  7%|▋         | 2/30 [01:03<12:12, 26.15s/it] 10%|█         | 3/30 [01:03<06:33, 14.56s/it] 13%|█▎        | 4/30 [01:04<03:56,  9.11s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.09s/it] 20%|██        | 6/30 [01:06<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:15<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 2.632555373509725
Epoch 272/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:14, 60.49s/it]  7%|▋         | 2/30 [01:01<11:49, 25.35s/it] 10%|█         | 3/30 [01:01<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.6504641930262247
Epoch 273/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:32, 56.98s/it]  7%|▋         | 2/30 [00:58<11:28, 24.60s/it] 10%|█         | 3/30 [00:59<06:10, 13.71s/it] 13%|█▎        | 4/30 [01:00<03:43,  8.59s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.77s/it] 20%|██        | 6/30 [01:01<01:37,  4.06s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 2.649275263150533
Epoch 274/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:41, 57.28s/it]  7%|▋         | 2/30 [01:00<11:50, 25.39s/it] 10%|█         | 3/30 [01:01<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:01<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.93s/it] 20%|██        | 6/30 [01:03<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.6367271105448404
Epoch 275/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:11, 60.40s/it]  7%|▋         | 2/30 [01:02<12:16, 26.31s/it] 10%|█         | 3/30 [01:03<06:38, 14.75s/it] 13%|█▎        | 4/30 [01:04<03:59,  9.22s/it] 17%|█▋        | 5/30 [01:05<02:34,  6.17s/it] 20%|██        | 6/30 [01:06<01:43,  4.32s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.15s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.39s/it] 30%|███       | 9/30 [01:08<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 2.6325311183929445
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0246,  0.0042,  0.0070,  ...,  0.0020,  0.0013,  0.0093],
        [-0.0009,  0.0213,  0.0200,  ...,  0.0123, -0.0083, -0.0160],
        [-0.0336, -0.0345,  0.0173,  ...,  0.0789, -0.0185, -0.0230],
        ...,
        [ 0.0187, -0.0097, -0.0028,  ..., -0.0242, -0.0222, -0.0147],
        [-0.0280, -0.0006, -0.0104,  ..., -0.0010,  0.0114, -0.0134],
        [-0.0382, -0.0092,  0.0128,  ...,  0.0305,  0.0257, -0.0460]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9012, 0.8559, 0.8551, 0.8539, 0.8508, 0.8500, 0.8448, 0.8416, 0.8408,
         0.8354],
        [0.9523, 0.9489, 0.9480, 0.9460, 0.9451, 0.9421, 0.9415, 0.9397, 0.9394,
         0.9370],
        [0.9133, 0.9129, 0.9072, 0.8879, 0.8835, 0.8723, 0.8564, 0.8506, 0.8501,
         0.8501],
        [0.9545, 0.9157, 0.9067, 0.9036, 0.8983, 0.8976, 0.8945, 0.8906, 0.8902,
         0.8880],
        [0.8611, 0.8466, 0.8457, 0.8314, 0.8276, 0.8157, 0.8095, 0.8095, 0.8035,
         0.8010],
        [0.8628, 0.8443, 0.8329, 0.8189, 0.8139, 0.8123, 0.8122, 0.8069, 0.8041,
         0.8039],
        [0.9038, 0.9036, 0.9008, 0.8871, 0.8847, 0.8816, 0.8810, 0.8755, 0.8737,
         0.8685],
        [0.9223, 0.9047, 0.8997, 0.8743, 0.8723, 0.8666, 0.8484, 0.8420, 0.8414,
         0.8407],
        [0.9192, 0.9177, 0.9144, 0.9127, 0.9085, 0.8995, 0.8984, 0.8970, 0.8955,
         0.8951],
        [0.9551, 0.9035, 0.9009, 0.8989, 0.8984, 0.8949, 0.8940, 0.8889, 0.8800,
         0.8760],
        [0.9575, 0.9233, 0.9197, 0.9180, 0.9170, 0.9140, 0.9107, 0.9059, 0.9058,
         0.9057],
        [0.9154, 0.9026, 0.8775, 0.8765, 0.8637, 0.8447, 0.8447, 0.8260, 0.8212,
         0.8190],
        [0.9486, 0.9197, 0.9189, 0.9156, 0.9153, 0.9007, 0.8974, 0.8967, 0.8841,
         0.8802],
        [0.9699, 0.9630, 0.9628, 0.9619, 0.9609, 0.9576, 0.9575, 0.9554, 0.9545,
         0.9532],
        [0.9464, 0.9399, 0.9386, 0.9366, 0.9348, 0.9324, 0.9305, 0.9304, 0.9298,
         0.9261],
        [0.9326, 0.9293, 0.9280, 0.9235, 0.9198, 0.9191, 0.9175, 0.9166, 0.9160,
         0.9118],
        [0.9690, 0.9629, 0.9572, 0.9511, 0.9463, 0.9413, 0.9391, 0.9383, 0.9358,
         0.9358],
        [0.9691, 0.9631, 0.9574, 0.9481, 0.9466, 0.9450, 0.9446, 0.9439, 0.9435,
         0.9422],
        [0.9398, 0.9387, 0.9318, 0.9118, 0.9101, 0.9092, 0.9077, 0.9073, 0.9071,
         0.9066],
        [0.9212, 0.9178, 0.9154, 0.9117, 0.9094, 0.9093, 0.9082, 0.9076, 0.9066,
         0.9065],
        [0.9639, 0.9549, 0.9520, 0.9515, 0.9507, 0.9468, 0.9443, 0.9436, 0.9428,
         0.9400],
        [0.9396, 0.9313, 0.9293, 0.9210, 0.9187, 0.9176, 0.9161, 0.9139, 0.9131,
         0.9125],
        [0.9747, 0.9515, 0.9513, 0.9474, 0.9444, 0.9431, 0.9369, 0.9263, 0.9254,
         0.9248],
        [0.9519, 0.9467, 0.9403, 0.9382, 0.9346, 0.9321, 0.9320, 0.9311, 0.9308,
         0.9306],
        [0.9653, 0.9149, 0.9113, 0.8978, 0.8977, 0.8927, 0.8897, 0.8896, 0.8872,
         0.8833],
        [0.9480, 0.9443, 0.9417, 0.9361, 0.9314, 0.9310, 0.9238, 0.9212, 0.9198,
         0.9195],
        [0.9433, 0.9413, 0.9376, 0.9320, 0.9260, 0.9170, 0.9167, 0.9160, 0.9145,
         0.9112],
        [0.9660, 0.9548, 0.9536, 0.9425, 0.9347, 0.9346, 0.9327, 0.9320, 0.9315,
         0.9299],
        [0.9554, 0.9487, 0.9452, 0.9389, 0.9378, 0.9367, 0.9341, 0.9328, 0.9316,
         0.9310],
        [0.9454, 0.9401, 0.9364, 0.9336, 0.9330, 0.9309, 0.9294, 0.9266, 0.9252,
         0.9242],
        [0.9609, 0.9562, 0.9539, 0.9520, 0.9519, 0.9516, 0.9459, 0.9449, 0.9444,
         0.9410],
        [0.9511, 0.9297, 0.9291, 0.9227, 0.9194, 0.9182, 0.9149, 0.9083, 0.9078,
         0.9076],
        [0.8928, 0.8828, 0.8741, 0.8715, 0.8696, 0.8675, 0.8663, 0.8662, 0.8633,
         0.8617],
        [0.9258, 0.9131, 0.9105, 0.9064, 0.9055, 0.8983, 0.8974, 0.8957, 0.8925,
         0.8923],
        [0.9362, 0.9344, 0.9269, 0.9268, 0.9145, 0.9135, 0.9129, 0.9109, 0.9062,
         0.9058],
        [0.9184, 0.9150, 0.9067, 0.9030, 0.8924, 0.8874, 0.8871, 0.8856, 0.8799,
         0.8790],
        [0.9490, 0.8758, 0.8682, 0.8636, 0.8604, 0.8591, 0.8589, 0.8558, 0.8542,
         0.8494],
        [0.9306, 0.9172, 0.9100, 0.9076, 0.8872, 0.8855, 0.8842, 0.8836, 0.8807,
         0.8785],
        [0.9115, 0.9080, 0.8972, 0.8828, 0.8826, 0.8783, 0.8705, 0.8672, 0.8651,
         0.8625],
        [0.9531, 0.9511, 0.9485, 0.9471, 0.9459, 0.9441, 0.9426, 0.9418, 0.9405,
         0.9400],
        [0.8983, 0.8973, 0.8932, 0.8920, 0.8786, 0.8744, 0.8720, 0.8615, 0.8520,
         0.8471],
        [0.9170, 0.8873, 0.8855, 0.8851, 0.8810, 0.8810, 0.8776, 0.8740, 0.8738,
         0.8735],
        [0.9037, 0.8968, 0.8958, 0.8776, 0.8762, 0.8756, 0.8701, 0.8664, 0.8583,
         0.8571],
        [0.9131, 0.8923, 0.8860, 0.8845, 0.8761, 0.8687, 0.8647, 0.8599, 0.8548,
         0.8538],
        [0.8776, 0.8575, 0.8558, 0.8463, 0.8449, 0.8403, 0.8372, 0.8371, 0.8240,
         0.8225],
        [0.8411, 0.8322, 0.8189, 0.7982, 0.7898, 0.7839, 0.7822, 0.7787, 0.7639,
         0.7634],
        [0.9242, 0.9217, 0.9179, 0.9101, 0.9083, 0.9067, 0.9047, 0.9027, 0.9021,
         0.8979],
        [0.8345, 0.8148, 0.8040, 0.8032, 0.8029, 0.7988, 0.7809, 0.7732, 0.7680,
         0.7632],
        [0.8766, 0.8511, 0.8321, 0.8264, 0.8221, 0.8204, 0.8143, 0.8141, 0.8120,
         0.8073],
        [0.9115, 0.8875, 0.8840, 0.8817, 0.8594, 0.8550, 0.8543, 0.8541, 0.8524,
         0.8481],
        [0.9032, 0.8613, 0.8593, 0.8582, 0.8469, 0.8389, 0.8356, 0.8294, 0.8247,
         0.8243],
        [0.9055, 0.8985, 0.8690, 0.8609, 0.8558, 0.8463, 0.8426, 0.8373, 0.8365,
         0.8294],
        [0.9429, 0.9268, 0.9221, 0.9112, 0.8935, 0.8886, 0.8771, 0.8729, 0.8708,
         0.8692],
        [0.9283, 0.8846, 0.8827, 0.8802, 0.8759, 0.8733, 0.8675, 0.8616, 0.8607,
         0.8572],
        [0.9446, 0.9273, 0.9219, 0.9209, 0.9180, 0.9174, 0.9174, 0.9108, 0.9107,
         0.9098],
        [0.9237, 0.8981, 0.8953, 0.8948, 0.8902, 0.8900, 0.8892, 0.8885, 0.8878,
         0.8871],
        [0.9340, 0.9028, 0.8891, 0.8887, 0.8849, 0.8685, 0.8609, 0.8567, 0.8539,
         0.8511],
        [0.9456, 0.9010, 0.8914, 0.8841, 0.8761, 0.8645, 0.8632, 0.8601, 0.8578,
         0.8560],
        [0.9288, 0.8807, 0.8790, 0.8715, 0.8644, 0.8636, 0.8590, 0.8526, 0.8476,
         0.8466],
        [0.9139, 0.8414, 0.8402, 0.8381, 0.8270, 0.8216, 0.8205, 0.8201, 0.8198,
         0.8197],
        [0.8928, 0.8853, 0.8847, 0.8842, 0.8827, 0.8822, 0.8797, 0.8771, 0.8755,
         0.8754],
        [0.8614, 0.8002, 0.7976, 0.7967, 0.7896, 0.7688, 0.7683, 0.7667, 0.7658,
         0.7614],
        [0.8959, 0.8767, 0.8732, 0.8561, 0.8503, 0.8471, 0.8406, 0.8385, 0.8342,
         0.8337],
        [0.8471, 0.8457, 0.8358, 0.8159, 0.8156, 0.8051, 0.8026, 0.7972, 0.7948,
         0.7896]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 1, 1, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 390078.2500,  204353.9219,  201976.0156,  198515.1719,  189803.5156,
          187806.8750,  174420.0000,  166609.1562,  164733.6094,  152352.0938],
        [ 809681.1875,  770865.5625,  761583.1875,  739830.5000,  730610.3125,
          700202.3125,  693498.8750,  676196.0000,  673680.5000,  650756.0000],
        [ 463628.8750,  461363.0938,  425126.9375,  322761.3438,  302996.9688,
          258154.4219,  205627.7656,  189267.4062,  187984.4531,  187951.4688],
        [ 835114.0000,  480229.2500,  422306.0312,  403935.2500,  374249.0938,
          370362.6562,  354635.5000,  335158.7812,  333580.3125,  323148.1875],
        [ 220034.2344,  178865.4219,  176494.3594,  143930.8906,  136365.0156,
          115021.1328,  105342.7734,  105248.2812,   96556.4766,   93172.1875],
        [ 225369.5312,  172982.8125,  147008.3750,  120422.7656,  112060.5703,
          109643.2500,  109475.2344,  101360.3984,   97393.1562,   97231.5859],
        [ 404862.4062,  403749.6250,  388089.3438,  318958.7812,  308047.7188,
          295045.7812,  292441.9375,  270273.0000,  263547.2812,  244475.1406],
        [ 527142.0000,  410392.1875,  382027.7500,  265519.2812,  258092.6250,
          238011.5625,  183584.9531,  167553.2656,  165958.6719,  164276.4531],
        [ 504821.8438,  493806.3125,  470870.7188,  460080.1250,  432951.1875,
          380746.7500,  375049.7812,  367561.8125,  359688.7812,  357738.8438],
        [ 842071.7500,  403292.0938,  388525.5938,  377614.8438,  374758.7500,
          356799.5000,  351818.6562,  327401.0625,  288122.8125,  272070.8438],
        [ 872574.2500,  535134.5000,  507914.9062,  496062.0000,  488705.3125,
          468498.5000,  446667.2500,  417417.4375,  416909.0312,  416222.9375],
        [ 477802.1250,  398128.5625,  277976.7188,  274109.2812,  228476.0625,
          174180.4688,  174158.0469,  133173.7344,  124395.4844,  120579.7344],
        [ 767926.8750,  508493.5312,  502065.5000,  479194.8438,  476881.6562,
          387622.9375,  369271.8125,  366045.0000,  305581.6875,  288829.8750],
        [1040532.0000,  942869.6250,  941125.0625,  928271.2500,  915037.0625,
          873839.1875,  872204.0000,  846449.3125,  836008.8750,  819791.2500],
        [ 744230.6875,  678157.2500,  665534.7500,  647238.0000,  630083.8125,
          609308.9375,  593015.2500,  592084.5625,  587306.6250,  556494.2500],
        [ 611312.3125,  583035.7500,  571800.8750,  536553.0625,  509066.5938,
          503605.2812,  492197.3750,  485947.4688,  482247.6562,  453654.5000],
        [1027346.8750,  941323.4375,  868182.4375,  795802.7500,  743543.9375,
          691533.3750,  670116.5625,  662990.0625,  639460.1250,  639422.3750],
        [1029403.4375,  944710.3125,  871305.3125,  762407.3125,  746400.0000,
          729113.1250,  725027.0625,  717733.3125,  713638.8125,  701210.0000],
        [ 676794.6875,  666819.3750,  604314.8125,  453852.2812,  442783.1250,
          437215.3125,  428014.0625,  425491.1875,  424490.0938,  421621.1250],
        [ 519466.0938,  494796.2500,  477893.3125,  453350.9062,  438688.8750,
          437808.6562,  431245.1562,  427551.0000,  421255.7812,  420632.3438],
        [ 956097.8750,  840711.6875,  806039.3750,  800785.5625,  790734.8750,
          748066.8750,  721678.5000,  715328.0000,  706782.5000,  679596.5000],
        [ 674923.5625,  599951.1875,  582970.7500,  517523.7500,  500760.5312,
          493277.6875,  482988.6875,  467619.5625,  462326.3750,  458525.1875],
        [1114999.1250,  799913.8750,  797885.8125,  755101.8750,  723624.0625,
          710222.3125,  649580.3750,  558621.6875,  551493.1250,  546892.5000],
        [ 804894.0625,  747790.8125,  681567.0625,  662215.9375,  628334.0625,
          606487.3750,  605803.5625,  598246.8750,  595846.8750,  594142.3125],
        [ 974158.6875,  474785.1562,  451011.4375,  371557.0000,  371307.9688,
          345636.9688,  331251.2188,  330341.7188,  319237.5312,  302225.8438],
        [ 761373.3125,  721824.4375,  695717.3125,  642206.0625,  600291.6875,
          597201.4375,  538921.2500,  519111.5000,  508827.3125,  506933.4688],
        [ 712389.6875,  691441.0625,  655789.2500,  606142.7500,  555699.2500,
          489215.4375,  486746.1875,  481952.0312,  471769.6875,  450059.7812],
        [ 984934.2500,  839623.5625,  824527.8750,  703799.3750,  629270.7500,
          628902.3750,  612056.1250,  605727.8750,  601057.6250,  587471.3125],
        [ 845834.3750,  768722.6250,  731345.7500,  668503.5625,  657858.2500,
          647776.4375,  624402.8750,  612610.8750,  602322.3125,  597558.0625],
        [ 733923.2500,  680368.1875,  645025.1875,  619577.3750,  614248.8750,
          595992.3750,  583643.8125,  560754.5625,  549474.0000,  541897.0000],
        [ 915679.6250,  856257.9375,  828054.1875,  805637.4375,  805254.8750,
          802019.1250,  739233.8750,  728699.5000,  723465.3125,  689234.9375],
        [ 795930.3125,  586623.6875,  581551.9375,  530460.3125,  505704.6250,
          497272.1562,  474373.7500,  431860.8438,  428503.7500,  427589.3438],
        [ 345821.2812,  299998.0625,  264747.5938,  255256.1250,  248401.5781,
          241025.7812,  237067.7969,  236729.8281,  226891.1875,  222007.7812],
        [ 554102.1250,  462277.0000,  445652.0312,  420329.5938,  414611.2812,
          374346.2188,  369568.0938,  360578.0000,  344727.7500,  343728.1250],
        [ 643129.6875,  627283.8750,  563132.4375,  562053.9375,  471966.3438,
          464891.6250,  461217.9375,  448054.2188,  419254.2500,  416492.9688],
        [ 498610.8438,  475240.4062,  421898.6562,  400590.8125,  344178.4688,
          320359.0000,  318911.0312,  312161.4062,  287951.1562,  284296.6875],
        [ 772438.1875,  271347.6250,  243633.5312,  227881.3438,  217732.2500,
          213694.2812,  213201.6719,  204031.8281,  199230.0000,  186219.0312],
        [ 593736.7500,  490400.0625,  442606.6875,  427553.8750,  319607.0625,
          311647.6875,  306109.9375,  303215.2188,  291107.9688,  281969.4062],
        [ 451885.8750,  429661.3750,  368565.6875,  299993.7500,  299318.1875,
          281220.6875,  251563.9375,  240023.8281,  233039.2188,  224479.5625],
        [ 818507.7500,  796183.1250,  767121.6875,  751115.0000,  739090.0625,
          720108.2500,  704845.2500,  696789.0000,  684121.1250,  679258.9375],
        [ 374091.0312,  368971.1875,  347928.5000,  342087.7812,  282691.5625,
          266131.0312,  257228.6719,  221305.9688,  193128.7031,  180019.0000],
        [ 489251.8438,  319805.2500,  311770.1562,  309952.5625,  292558.5312,
          292232.2812,  278329.2500,  264508.8750,  263811.2812,  262503.7500],
        [ 404083.2188,  366372.2500,  361038.0312,  278360.8438,  273103.6562,
          270808.3750,  250207.0938,  237171.8281,  211276.5938,  207657.1719],
        [ 462631.5938,  343654.3750,  314038.6562,  307284.8438,  272729.9375,
          245177.2344,  231659.8438,  216159.0156,  201147.7344,  198258.2500],
        [ 278410.7812,  208962.3594,  203836.7656,  177968.3750,  174532.1562,
          163527.4219,  156296.4688,  156112.4844,  129478.9609,  126835.5391],
        [ 165414.1562,  145670.5625,  120331.7266,   89528.6641,   79497.2578,
           73010.1484,   71262.4453,   67791.7031,   54883.4297,   54458.4023],
        [ 542274.3750,  522846.7500,  495136.5625,  443068.2812,  431944.4688,
          422028.6250,  410371.0625,  398464.3750,  395375.5625,  372048.8125],
        [ 150385.1094,  113538.8359,   97373.7500,   96241.8828,   95785.6094,
           90290.1641,   69987.5469,   62636.5938,   58227.0938,   54348.9297],
        [ 274624.2188,  190610.7188,  145336.1406,  134043.8750,  125980.3516,
          123036.0859,  112698.6797,  112479.1094,  109109.9141,  102044.5781],
        [ 452266.5625,  320885.8438,  305125.9688,  295319.1250,  214647.9062,
          201613.4531,  199633.4062,  199019.3906,  194204.7344,  182708.5000],
        [ 401240.8125,  220583.4531,  214340.0469,  211169.8281,  179680.7656,
          160159.5625,  152787.1406,  139930.1719,  130766.6719,  130076.9844],
        [ 414934.0625,  375137.0625,  246242.7188,  219401.2188,  204018.2344,
          178072.6094,  168896.3906,  156487.5156,  154722.2188,  139788.1250],
        [ 707432.5625,  562284.0000,  525749.8125,  450280.8438,  349634.8438,
          326060.9062,  276407.2500,  260281.9062,  252853.1562,  247054.9375],
        [ 574555.2500,  307740.5938,  299670.6562,  289134.9688,  271981.0938,
          262054.0156,  240965.1094,  221640.5312,  218625.2969,  208110.3750],
        [ 725356.2500,  566324.5000,  524312.7500,  516862.3438,  496067.2188,
          491843.5938,  491843.5938,  447622.4375,  446872.1562,  441257.5625],
        [ 537804.0625,  373350.7812,  358698.8125,  356231.7188,  333352.9375,
          332384.4375,  328467.2188,  325277.6250,  322015.7812,  319118.5312],
        [ 623038.3750,  399048.8438,  328253.6562,  326560.3750,  309188.8125,
          244712.3750,  219253.3594,  206603.9062,  198634.0938,  190708.5625],
        [ 735504.0000,  389285.9062,  339164.8438,  305448.2500,  272456.4375,
          231057.2812,  226801.8438,  217005.0312,  209991.7969,  204691.7344],
        [ 578905.3125,  291290.1562,  283918.6875,  255209.3906,  230780.4688,
          228136.1719,  213545.5625,  194796.8125,  181522.9844,  178727.1406],
        [ 467997.4688,  166102.9219,  163173.3125,  158496.6562,  135090.9531,
          125143.3359,  123158.2969,  122533.3828,  121984.0000,  121856.5625],
        [ 346075.3125,  310847.7500,  308021.5625,  306038.1250,  299584.6250,
          297558.1562,  286988.3438,  276670.2188,  270311.4062,  269953.0625],
        [ 220849.7188,   92172.0938,   88823.9844,   87738.5234,   79277.3906,
           58890.4805,   58454.5430,   57138.9023,   56367.3203,   52970.7344],
        [ 361681.4375,  275127.0312,  261436.7188,  204762.6250,  188643.7344,
          180114.2969,  164142.7188,  159299.5000,  149884.1250,  148675.1719],
        [ 180248.0000,  176508.6719,  153298.7031,  115360.9141,  114805.1328,
           98915.4688,   95393.1641,   88250.9219,   85298.9219,   79170.7109]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[390078.2500,      0.0000],
         [204353.9219,      0.0000],
         [201976.0156,      0.0000],
         ...,
         [166609.1562,      0.0000],
         [164733.6094,      0.0000],
         [152352.0938,      0.0000]],

        [[809681.1875,      0.0000],
         [770865.5625,      0.0000],
         [761583.1875,      0.0000],
         ...,
         [676196.0000,      0.0000],
         [673680.5000,      0.0000],
         [650756.0000,      0.0000]],

        [[463628.8750,      0.0000],
         [461363.0938,      0.0000],
         [425126.9375,      0.0000],
         ...,
         [189267.4062,      0.0000],
         [187984.4531,      0.0000],
         [187951.4688,      0.0000]],

        ...,

        [[     0.0000, 220849.7188],
         [ 92172.0938,      0.0000],
         [ 88823.9844,      0.0000],
         ...,
         [ 57138.9023,      0.0000],
         [ 56367.3203,      0.0000],
         [ 52970.7344,      0.0000]],

        [[361681.4375,      0.0000],
         [275127.0312,      0.0000],
         [     0.0000, 261436.7188],
         ...,
         [159299.5000,      0.0000],
         [149884.1250,      0.0000],
         [148675.1719,      0.0000]],

        [[     0.0000, 180248.0000],
         [     0.0000, 176508.6719],
         [     0.0000, 153298.7031],
         ...,
         [ 88250.9219,      0.0000],
         [ 85298.9219,      0.0000],
         [     0.0000,  79170.7109]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1856228.5000,  174420.0000],
        [7206904.5000,       0.0000],
        [3004862.7500,       0.0000],
        [4232719.0000,       0.0000],
        [ 575367.3750,  795663.5000],
        [1119964.8750,  172982.8125],
        [2493354.0000,  696137.0625],
        [2762558.7500,       0.0000],
        [3709510.0000,  493806.3125],
        [3982476.0000,       0.0000],
        [5066106.0000,       0.0000],
        [2382980.2500,       0.0000],
        [4451913.5000,       0.0000],
        [9016128.0000,       0.0000],
        [6303454.0000,       0.0000],
        [5229420.5000,       0.0000],
        [7679722.0000,       0.0000],
        [7940949.0000,       0.0000],
        [4981396.0000,       0.0000],
        [4522688.5000,       0.0000],
        [7765821.5000,       0.0000],
        [5240867.5000,       0.0000],
        [7208334.5000,       0.0000],
        [6525329.0000,       0.0000],
        [4271513.0000,       0.0000],
        [6092407.5000,       0.0000],
        [5601205.0000,       0.0000],
        [7017371.0000,       0.0000],
        [6756935.0000,       0.0000],
        [6124904.5000,       0.0000],
        [7893537.0000,       0.0000],
        [5259871.0000,       0.0000],
        [2577947.0000,       0.0000],
        [2880210.5000, 1209709.7500],
        [2773043.5000, 2304433.7500],
        [1703828.1250, 1960370.2500],
        [1546037.7500, 1203372.1250],
        [1974294.5000, 1793660.1250],
        [1278757.0000, 1800995.1250],
        [7357140.0000,       0.0000],
        [2833583.5000,       0.0000],
        [3084723.7500,       0.0000],
        [2860079.2500,       0.0000],
        [2792741.5000,       0.0000],
        [ 463559.8438, 1312401.5000],
        [ 435973.6562,  485874.8438],
        [3910712.0000,  522846.7500],
        [ 592713.1250,  296102.4062],
        [ 608238.1250,  821725.5625],
        [ 829179.8125, 1736245.1250],
        [1519732.2500,  421003.2500],
        [1221386.2500, 1036313.8750],
        [2119259.5000, 1838780.7500],
        [2594807.2500,  299670.6562],
        [5148363.0000,       0.0000],
        [3586702.0000,       0.0000],
        [1373964.7500, 1672037.6250],
        [2130500.0000, 1000907.2500],
        [1469541.0000, 1167291.6250],
        [ 710058.8750,  995478.0000],
        [2326388.5000,  645659.9375],
        [ 631834.0000,  220849.7188],
        [1447453.7500,  646313.6250],
        [ 387270.4375,  799980.1875]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 62.5
Top1 accuracy for validation set is 62.5 size is torch.Size([64, 1])
Epoch 276/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:58, 59.96s/it]  7%|▋         | 2/30 [01:00<11:43, 25.13s/it] 10%|█         | 3/30 [01:01<06:17, 14.00s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.77s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.6080971399943036
Epoch 277/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:51, 59.69s/it]  7%|▋         | 2/30 [01:00<11:40, 25.02s/it] 10%|█         | 3/30 [01:01<06:18, 14.03s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.79s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.629461391766866
Epoch 278/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:34, 59.12s/it]  7%|▋         | 2/30 [00:59<11:33, 24.78s/it] 10%|█         | 3/30 [01:03<06:45, 15.03s/it] 13%|█▎        | 4/30 [01:04<04:04,  9.39s/it] 17%|█▋        | 5/30 [01:04<02:36,  6.28s/it] 20%|██        | 6/30 [01:05<01:45,  4.40s/it] 23%|██▎       | 7/30 [01:06<01:13,  3.21s/it] 27%|██▋       | 8/30 [01:07<00:53,  2.42s/it] 30%|███       | 9/30 [01:07<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.55s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.30s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.616127355893453
Epoch 279/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:51, 59.69s/it]  7%|▋         | 2/30 [01:00<11:40, 25.02s/it] 10%|█         | 3/30 [01:01<06:16, 13.94s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.73s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.85s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.5788888295491534
Epoch 280/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:00, 60.02s/it]  7%|▋         | 2/30 [01:00<11:48, 25.29s/it] 10%|█         | 3/30 [01:01<06:20, 14.08s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.82s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.91s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.6133463859558104
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 2.4654e-02,  3.9509e-03,  7.4331e-03,  ...,  1.4163e-03,
          2.4519e-03,  8.9618e-03],
        [-8.0633e-04,  2.0670e-02,  2.0577e-02,  ...,  1.2561e-02,
         -7.3037e-03, -1.5995e-02],
        [-3.3319e-02, -3.4807e-02,  1.7920e-02,  ...,  7.8941e-02,
         -1.7130e-02, -2.2358e-02],
        ...,
        [ 1.7780e-02, -8.9307e-03, -1.7597e-03,  ..., -2.4051e-02,
         -2.1195e-02, -1.4361e-02],
        [-2.8122e-02,  5.9728e-05, -9.7582e-03,  ..., -6.8301e-04,
          1.1581e-02, -1.3418e-02],
        [-3.8095e-02, -9.7057e-03,  1.3165e-02,  ...,  3.1482e-02,
          2.6440e-02, -4.4927e-02]], device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.9003, 0.8550, 0.8542, 0.8538, 0.8506, 0.8489, 0.8439, 0.8419, 0.8410,
         0.8342],
        [0.9517, 0.9477, 0.9468, 0.9446, 0.9443, 0.9405, 0.9403, 0.9384, 0.9383,
         0.9359],
        [0.9127, 0.9120, 0.9058, 0.8871, 0.8820, 0.8704, 0.8562, 0.8501, 0.8498,
         0.8481],
        [0.9540, 0.9145, 0.9057, 0.9024, 0.8972, 0.8960, 0.8930, 0.8898, 0.8890,
         0.8869],
        [0.8598, 0.8464, 0.8457, 0.8307, 0.8270, 0.8163, 0.8091, 0.8081, 0.8033,
         0.8018],
        [0.8625, 0.8403, 0.8311, 0.8161, 0.8136, 0.8089, 0.8080, 0.8052, 0.8045,
         0.8010],
        [0.9013, 0.9011, 0.8968, 0.8854, 0.8820, 0.8790, 0.8787, 0.8741, 0.8718,
         0.8670],
        [0.9207, 0.9027, 0.8976, 0.8733, 0.8700, 0.8657, 0.8459, 0.8411, 0.8391,
         0.8384],
        [0.9191, 0.9150, 0.9123, 0.9121, 0.9068, 0.8982, 0.8975, 0.8962, 0.8946,
         0.8943],
        [0.9546, 0.9020, 0.9000, 0.8974, 0.8972, 0.8933, 0.8924, 0.8882, 0.8783,
         0.8749],
        [0.9565, 0.9218, 0.9184, 0.9158, 0.9155, 0.9129, 0.9095, 0.9044, 0.9044,
         0.9043],
        [0.9144, 0.9008, 0.8764, 0.8749, 0.8613, 0.8418, 0.8416, 0.8228, 0.8178,
         0.8171],
        [0.9468, 0.9188, 0.9170, 0.9153, 0.9135, 0.8997, 0.8954, 0.8951, 0.8812,
         0.8791],
        [0.9694, 0.9624, 0.9621, 0.9618, 0.9604, 0.9575, 0.9570, 0.9544, 0.9541,
         0.9527],
        [0.9463, 0.9393, 0.9382, 0.9364, 0.9350, 0.9313, 0.9295, 0.9295, 0.9292,
         0.9250],
        [0.9323, 0.9289, 0.9279, 0.9230, 0.9193, 0.9176, 0.9168, 0.9166, 0.9152,
         0.9112],
        [0.9678, 0.9617, 0.9561, 0.9497, 0.9445, 0.9403, 0.9379, 0.9377, 0.9342,
         0.9341],
        [0.9689, 0.9629, 0.9575, 0.9471, 0.9461, 0.9447, 0.9438, 0.9435, 0.9432,
         0.9413],
        [0.9390, 0.9373, 0.9301, 0.9097, 0.9089, 0.9076, 0.9069, 0.9066, 0.9062,
         0.9057],
        [0.9200, 0.9158, 0.9139, 0.9106, 0.9090, 0.9085, 0.9075, 0.9067, 0.9053,
         0.9053],
        [0.9628, 0.9537, 0.9504, 0.9503, 0.9494, 0.9455, 0.9435, 0.9429, 0.9416,
         0.9393],
        [0.9384, 0.9300, 0.9281, 0.9203, 0.9182, 0.9155, 0.9154, 0.9130, 0.9127,
         0.9113],
        [0.9745, 0.9511, 0.9506, 0.9472, 0.9435, 0.9426, 0.9362, 0.9256, 0.9246,
         0.9245],
        [0.9512, 0.9460, 0.9397, 0.9370, 0.9340, 0.9318, 0.9312, 0.9307, 0.9306,
         0.9294],
        [0.9647, 0.9138, 0.9106, 0.8954, 0.8952, 0.8923, 0.8881, 0.8878, 0.8858,
         0.8823],
        [0.9475, 0.9440, 0.9415, 0.9357, 0.9307, 0.9306, 0.9233, 0.9212, 0.9191,
         0.9182],
        [0.9418, 0.9403, 0.9370, 0.9314, 0.9248, 0.9162, 0.9158, 0.9154, 0.9132,
         0.9107],
        [0.9658, 0.9544, 0.9530, 0.9419, 0.9346, 0.9333, 0.9323, 0.9312, 0.9311,
         0.9294],
        [0.9550, 0.9482, 0.9441, 0.9386, 0.9368, 0.9357, 0.9336, 0.9323, 0.9305,
         0.9300],
        [0.9446, 0.9395, 0.9357, 0.9327, 0.9324, 0.9302, 0.9295, 0.9258, 0.9243,
         0.9240],
        [0.9602, 0.9554, 0.9531, 0.9515, 0.9514, 0.9513, 0.9451, 0.9439, 0.9439,
         0.9401],
        [0.9507, 0.9292, 0.9285, 0.9222, 0.9185, 0.9166, 0.9141, 0.9080, 0.9074,
         0.9070],
        [0.8917, 0.8819, 0.8727, 0.8718, 0.8686, 0.8664, 0.8653, 0.8651, 0.8625,
         0.8620],
        [0.9247, 0.9119, 0.9086, 0.9054, 0.9042, 0.8971, 0.8963, 0.8956, 0.8920,
         0.8912],
        [0.9346, 0.9336, 0.9264, 0.9263, 0.9145, 0.9123, 0.9116, 0.9099, 0.9050,
         0.9048],
        [0.9174, 0.9141, 0.9059, 0.9015, 0.8911, 0.8871, 0.8864, 0.8827, 0.8785,
         0.8783],
        [0.9490, 0.8754, 0.8663, 0.8627, 0.8599, 0.8592, 0.8579, 0.8530, 0.8523,
         0.8465],
        [0.9285, 0.9153, 0.9089, 0.9058, 0.8855, 0.8846, 0.8828, 0.8822, 0.8793,
         0.8785],
        [0.9102, 0.9070, 0.8962, 0.8820, 0.8805, 0.8752, 0.8693, 0.8660, 0.8625,
         0.8608],
        [0.9517, 0.9502, 0.9473, 0.9461, 0.9449, 0.9437, 0.9407, 0.9400, 0.9388,
         0.9386],
        [0.8973, 0.8963, 0.8921, 0.8902, 0.8770, 0.8736, 0.8711, 0.8604, 0.8509,
         0.8446],
        [0.9164, 0.8855, 0.8845, 0.8837, 0.8806, 0.8797, 0.8762, 0.8726, 0.8725,
         0.8725],
        [0.9031, 0.8958, 0.8951, 0.8767, 0.8753, 0.8739, 0.8690, 0.8652, 0.8573,
         0.8549],
        [0.9117, 0.8901, 0.8857, 0.8831, 0.8751, 0.8677, 0.8637, 0.8585, 0.8547,
         0.8535],
        [0.8765, 0.8557, 0.8541, 0.8441, 0.8437, 0.8390, 0.8345, 0.8337, 0.8208,
         0.8193],
        [0.8415, 0.8309, 0.8176, 0.7980, 0.7896, 0.7836, 0.7824, 0.7758, 0.7641,
         0.7637],
        [0.9220, 0.9206, 0.9156, 0.9085, 0.9071, 0.9052, 0.9028, 0.9007, 0.9001,
         0.8965],
        [0.8326, 0.8127, 0.8023, 0.8016, 0.8000, 0.7959, 0.7807, 0.7703, 0.7679,
         0.7610],
        [0.8740, 0.8501, 0.8308, 0.8257, 0.8216, 0.8207, 0.8133, 0.8119, 0.8119,
         0.8061],
        [0.9106, 0.8843, 0.8829, 0.8787, 0.8577, 0.8528, 0.8528, 0.8520, 0.8500,
         0.8454],
        [0.9028, 0.8616, 0.8597, 0.8584, 0.8469, 0.8389, 0.8346, 0.8279, 0.8241,
         0.8229],
        [0.9049, 0.8979, 0.8679, 0.8581, 0.8536, 0.8452, 0.8399, 0.8348, 0.8339,
         0.8272],
        [0.9420, 0.9260, 0.9213, 0.9105, 0.8919, 0.8871, 0.8760, 0.8711, 0.8706,
         0.8677],
        [0.9271, 0.8839, 0.8809, 0.8801, 0.8749, 0.8719, 0.8669, 0.8606, 0.8597,
         0.8556],
        [0.9437, 0.9260, 0.9210, 0.9199, 0.9163, 0.9158, 0.9158, 0.9098, 0.9092,
         0.9091],
        [0.9225, 0.8974, 0.8936, 0.8927, 0.8887, 0.8886, 0.8885, 0.8866, 0.8865,
         0.8856],
        [0.9324, 0.9007, 0.8880, 0.8876, 0.8829, 0.8681, 0.8590, 0.8541, 0.8509,
         0.8502],
        [0.9451, 0.9001, 0.8898, 0.8839, 0.8750, 0.8629, 0.8614, 0.8602, 0.8555,
         0.8553],
        [0.9276, 0.8793, 0.8772, 0.8705, 0.8619, 0.8619, 0.8569, 0.8515, 0.8464,
         0.8454],
        [0.9127, 0.8400, 0.8391, 0.8364, 0.8254, 0.8191, 0.8188, 0.8187, 0.8187,
         0.8175],
        [0.8911, 0.8830, 0.8825, 0.8823, 0.8813, 0.8811, 0.8784, 0.8750, 0.8738,
         0.8738],
        [0.8615, 0.7991, 0.7969, 0.7953, 0.7887, 0.7667, 0.7655, 0.7638, 0.7629,
         0.7575],
        [0.8953, 0.8751, 0.8722, 0.8538, 0.8477, 0.8450, 0.8372, 0.8356, 0.8331,
         0.8310],
        [0.8471, 0.8433, 0.8326, 0.8134, 0.8134, 0.8010, 0.7995, 0.7947, 0.7920,
         0.7888]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0, 1],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 385351.0000,  201505.8125,  199262.8750,  198261.6406,  189273.9062,
          184732.6875,  172149.0781,  167306.0938,  165055.6719,  149798.3906],
        [ 802524.8750,  757804.9375,  748062.5000,  724880.4375,  722398.8125,
          683883.0000,  681893.3750,  664165.8750,  663057.0625,  640359.6875],
        [ 459752.9375,  455109.1875,  416789.7500,  319063.1562,  296450.6562,
          251325.5938,  205050.6562,  187972.7969,  187258.0000,  182620.1719],
        [ 829923.9375,  471758.4062,  416228.5000,  396708.0625,  368419.5000,
          362449.0312,  347204.5625,  331749.1250,  327742.5312,  317999.3438],
        [ 215822.8438,  178291.3125,  176559.0156,  142487.4375,  135105.0000,
          116034.7422,  104664.0312,  103119.5156,   96314.0469,   94242.8828],
        [ 224504.8281,  163563.6094,  143371.2656,  115667.7109,  111606.2344,
          104380.8359,  103111.0625,   98967.3594,   98062.0547,   93190.9297],
        [ 390529.0000,  389669.2500,  366293.2812,  311512.1875,  296361.0312,
          284085.2812,  282895.1562,  265050.0000,  256499.6719,  239208.5000],
        [ 515740.2188,  398573.4375,  370431.9062,  262020.7812,  249782.2344,
          234995.1406,  176942.3125,  165337.8281,  160676.5000,  159138.3906],
        [ 503998.7812,  475045.5938,  457266.6562,  456135.5000,  422869.4375,
          373547.7188,  370055.5000,  363457.3438,  354801.9375,  353502.9688],
        [ 837152.9375,  394528.4688,  383260.8125,  369515.2500,  368426.5312,
          348503.0000,  343873.3438,  324220.0000,  281275.4062,  268074.5938],
        [ 859979.8750,  523362.5625,  499106.5625,  480593.4688,  478710.6875,
          461111.0625,  439225.1250,  408519.4375,  408484.3750,  407912.9062],
        [ 470976.7188,  387848.4688,  273600.0000,  268076.9062,  220794.9688,
          166998.1094,  166432.2500,  127340.2188,  118480.1641,  117315.1172],
        [ 748247.3750,  502021.9375,  489093.2500,  477164.1875,  465020.2188,
          381668.3438,  359070.1875,  357604.4688,  293255.2188,  284686.2812],
        [1034225.1250,  935674.9375,  930919.3750,  927765.9375,  908838.8125,
          871625.3125,  865253.9375,  834719.8750,  830342.0000,  814216.4375],
        [ 743443.9375,  672616.1250,  661703.3750,  645539.6875,  632375.5625,
          600182.9375,  584610.3750,  584432.5000,  581683.3750,  548456.8125],
        [ 608115.4375,  579633.4375,  571551.1250,  533074.0625,  505429.7812,
          493501.6875,  487865.7188,  486186.6875,  476797.0625,  449994.0938],
        [1009873.4375,  926321.3125,  854601.0625,  780290.7500,  724670.3125,
          681640.5000,  659012.3750,  657260.0000,  625374.8750,  624021.8750],
        [1026960.9375,  941823.6250,  871936.1875,  751997.3125,  740875.4375,
          726384.8750,  717452.0625,  713995.5000,  710986.7500,  691757.6875],
        [ 669896.1250,  653018.8750,  589809.4375,  440643.6250,  435632.9062,
          427538.7812,  423309.6562,  421358.6250,  418861.0000,  416052.6875],
        [ 510188.7812,  480744.7500,  467454.5938,  446441.9375,  435827.4062,
          433077.9375,  426838.8438,  421760.2500,  413777.4375,  413485.1562],
        [ 940022.6250,  825832.6250,  787546.1875,  786232.1250,  776644.0625,
          734758.7500,  713779.0000,  707713.3125,  695037.5625,  672068.5625],
        [ 664006.2500,  588522.6875,  572738.4375,  512419.1562,  497710.0938,
          478536.7812,  477698.6875,  462106.8438,  459709.9688,  450691.5625],
        [1112052.2500,  795902.1875,  790101.6875,  752268.4375,  714275.4375,
          705148.5000,  643594.7500,  552934.5000,  545382.0625,  543854.0000],
        [ 797369.2500,  740390.9375,  675789.1875,  650698.3125,  623472.2500,
          604313.1250,  598873.0625,  594566.3125,  593396.0000,  584036.9375],
        [ 966328.8750,  466973.8438,  446215.5000,  359160.2500,  357993.8125,
          343723.8750,  323702.1562,  322242.8125,  312949.8125,  297784.6875],
        [ 756338.5625,  718945.1875,  693597.4375,  639068.7500,  595051.8750,
          593497.8750,  534897.1875,  519156.0625,  503754.2188,  497042.2500],
        [ 697237.6875,  681630.7500,  650448.8750,  600402.7500,  546852.3750,
          483599.8438,  480915.8125,  477891.0000,  462796.1875,  446941.6562],
        [ 982220.5625,  834839.2500,  817648.8125,  697988.8750,  628384.4375,
          616908.3125,  608081.8125,  598960.4375,  598417.4375,  583442.3750],
        [ 841143.1250,  764054.4375,  719596.8125,  665646.5000,  648382.7500,
          638553.9375,  619972.2500,  608681.7500,  593081.4375,  588383.5625],
        [ 724894.3125,  674413.9375,  638586.2500,  612106.3125,  609423.4375,
          590784.5000,  584533.9375,  554262.3125,  542569.7500,  540356.0625],
        [ 905844.0000,  846243.5000,  818821.6250,  800106.9375,  798867.9375,
          797913.1250,  730583.8750,  718541.4375,  718130.3750,  680292.9375],
        [ 790838.2500,  582445.0000,  575847.6875,  526949.5000,  499665.2500,
          486497.4375,  468960.7188,  429836.8125,  426350.2812,  423576.5625],
        [ 340550.4062,  296148.0312,  259632.3594,  256451.7031,  244875.7969,
          237201.2344,  233471.2188,  232898.3594,  224332.7344,  222944.9375],
        [ 546037.3125,  454775.9688,  433787.6875,  414285.2500,  407441.6875,
          368049.0000,  363662.5625,  360125.4062,  342215.0312,  338109.4375],
        [ 628781.8750,  619536.0625,  558914.7500,  558152.5625,  471478.2188,
          457205.5938,  452548.7188,  441959.1875,  411848.3438,  410914.2500],
        [ 491802.3125,  468890.0625,  417031.8750,  391810.7812,  337918.5938,
          318774.8438,  315585.7188,  299508.3438,  282223.3750,  281170.0000],
        [ 772078.8125,  269996.3125,  237066.0000,  224964.1094,  216193.2344,
          213976.9219,  210250.0938,  196090.1562,  194149.5312,  178605.4688],
        [ 575856.4375,  477055.0000,  435706.9062,  416403.9688,  311643.8438,
          307851.5625,  300001.7812,  297250.6875,  285151.1875,  282049.0000],
        [ 443727.5000,  423726.0625,  363200.9375,  296731.7812,  290459.5938,
          269237.0312,  247253.1875,  236107.5312,  224490.2656,  219121.8594],
        [ 802742.2500,  785656.5625,  753632.8125,  740753.9375,  728844.1250,
          715648.6250,  685943.1250,  679078.8750,  667712.1875,  665863.0000],
        [ 369001.4375,  363874.5312,  342397.5312,  333306.1875,  275993.1875,
          262981.3438,  253761.4531,  217842.5312,  190179.8438,  173807.7812],
        [ 484566.0938,  311726.4688,  307388.9062,  303767.1562,  290691.0000,
          287087.1250,  273096.6562,  259411.3438,  259004.4531,  259000.7500],
        [ 400943.1875,  361126.9062,  357447.2812,  274880.5000,  269426.0938,
          264237.3438,  246345.3750,  233181.2969,  208283.7188,  201447.5938],
        [ 453299.4688,  332827.8438,  312534.9375,  301411.5625,  268785.0000,
          241697.2188,  228322.5000,  212078.8281,  200884.7031,  197265.8281],
        [ 274212.8438,  203771.8438,  199131.9844,  172681.6562,  171688.2031,
          160448.6562,  150500.0312,  148773.1875,  123741.7734,  121092.8984],
        [ 166194.6719,  142931.2500,  118235.0000,   89317.0938,   79248.2891,
           72725.9922,   71449.3750,   65072.9609,   55054.6406,   54684.6836],
        [ 525374.8750,  514388.9062,  479489.6875,  433141.5312,  424337.0938,
          413336.5312,  399442.9375,  387556.4062,  384074.5938,  364731.3125],
        [ 146474.9219,  110260.9609,   94990.1797,   94019.8828,   91939.9766,
           86686.9844,   69783.4062,   60168.4727,   58067.6055,   52618.5469],
        [ 264347.2188,  188132.5938,  142630.3438,  132675.1562,  125145.8438,
          123505.2734,  111092.1641,  108943.0312,  108887.9844,  100326.9453],
        [ 446257.1875,  306448.1562,  300356.1562,  283060.5938,  209679.0000,
          195523.9688,  195405.4219,  193055.7812,  187698.8906,  175743.5781],
        [ 398956.4062,  221473.4062,  215591.8281,  211715.4844,  179598.1875,
          160237.0156,  150568.0938,  136973.8281,  129689.4141,  127423.1875],
        [ 411223.9375,  371979.5938,  242521.5000,  210787.1250,  197663.5625,
          175359.0156,  162484.6406,  151196.0312,  149088.2188,  135472.0625],
        [ 698624.8125,  556354.1250,  519633.5625,  445720.8750,  341602.6562,
          319142.2812,  272388.8750,  253679.6719,  251832.5469,  241735.2500],
        [ 565224.3125,  304926.6875,  292025.8438,  288665.4688,  267960.8750,
          256879.5781,  238860.4062,  218343.7812,  215626.3750,  203511.0312],
        [ 715731.2500,  556208.2500,  517706.9062,  509640.2500,  484233.9375,
          480463.3125,  480463.3125,  441270.1875,  437671.7500,  436628.2188],
        [ 528708.3125,  369562.4688,  350251.5938,  345638.3125,  326264.3125,
          326107.8750,  325485.8438,  316916.9688,  316253.3750,  312170.0312],
        [ 609321.7500,  387499.4688,  323206.4688,  321160.7500,  300471.9062,
          243071.4375,  213428.4844,  198934.3906,  190045.5000,  188331.5000],
        [ 730428.5000,  383901.0000,  331463.5625,  304689.7812,  268412.7812,
          225616.8281,  220884.9062,  217233.6406,  203110.2500,  202459.0938],
        [ 569159.4375,  285445.8750,  276753.8438,  251554.5938,  222429.7188,
          222402.3750,  207190.5156,  191859.2812,  178313.5938,  175891.9688],
        [ 459851.5938,  162793.1250,  160639.1094,  154507.8125,  132077.5312,
          120813.4062,  120273.5625,  120019.8828,  119995.5000,  118088.0625],
        [ 337897.9688,  300990.1875,  298838.1562,  297963.6562,  293415.2188,
          292659.5312,  281639.6562,  268425.5938,  263673.7188,  263588.9688],
        [ 221196.6875,   90792.1797,   87944.3516,   85970.7188,   78168.4609,
           57141.7891,   56124.4375,   54788.9336,   54075.2734,   50087.1523],
        [ 358814.4688,  268541.0625,  257818.8438,  198093.8125,  181566.4375,
          174782.0156,  156269.3281,  152783.4844,  147457.2656,  143207.5469],
        [ 180017.9688,  170591.2656,  146396.5625,  111321.8828,  111229.8750,
           93263.7500,   91226.7500,   85159.9297,   81960.4062,   78375.7578]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[385351.0000,      0.0000],
         [201505.8125,      0.0000],
         [199262.8750,      0.0000],
         ...,
         [167306.0938,      0.0000],
         [165055.6719,      0.0000],
         [149798.3906,      0.0000]],

        [[802524.8750,      0.0000],
         [757804.9375,      0.0000],
         [748062.5000,      0.0000],
         ...,
         [664165.8750,      0.0000],
         [663057.0625,      0.0000],
         [640359.6875,      0.0000]],

        [[459752.9375,      0.0000],
         [455109.1875,      0.0000],
         [416789.7500,      0.0000],
         ...,
         [187972.7969,      0.0000],
         [187258.0000,      0.0000],
         [182620.1719,      0.0000]],

        ...,

        [[     0.0000, 221196.6875],
         [ 90792.1797,      0.0000],
         [ 87944.3516,      0.0000],
         ...,
         [ 54788.9336,      0.0000],
         [ 54075.2734,      0.0000],
         [ 50087.1523,      0.0000]],

        [[358814.4688,      0.0000],
         [268541.0625,      0.0000],
         [     0.0000, 257818.8438],
         ...,
         [152783.4844,      0.0000],
         [147457.2656,      0.0000],
         [143207.5469,      0.0000]],

        [[     0.0000, 180017.9688],
         [     0.0000, 170591.2656],
         [     0.0000, 146396.5625],
         ...,
         [ 85159.9297,      0.0000],
         [ 81960.4062,      0.0000],
         [     0.0000,  78375.7578]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1840548.1250,  172149.0781],
        [7089031.0000,       0.0000],
        [2961393.0000,       0.0000],
        [4170183.0000,       0.0000],
        [ 571268.8750,  791372.0000],
        [1092862.2500,  163563.6094],
        [2419449.0000,  662654.3125],
        [2693638.5000,       0.0000],
        [3655636.0000,  475045.5938],
        [3918830.2500,       0.0000],
        [4967006.0000,       0.0000],
        [2317863.0000,       0.0000],
        [4357831.5000,       0.0000],
        [8953582.0000,       0.0000],
        [6255044.0000,       0.0000],
        [5192149.0000,       0.0000],
        [7543066.0000,       0.0000],
        [7894170.0000,       0.0000],
        [4896122.0000,       0.0000],
        [4449597.0000,       0.0000],
        [7639634.5000,       0.0000],
        [5164140.5000,       0.0000],
        [7155514.0000,       0.0000],
        [6462905.5000,       0.0000],
        [4197075.5000,       0.0000],
        [6051349.0000,       0.0000],
        [5528717.0000,       0.0000],
        [6966892.0000,       0.0000],
        [6687496.0000,       0.0000],
        [6071930.5000,       0.0000],
        [7815346.0000,       0.0000],
        [5210967.5000,       0.0000],
        [2548506.7500,       0.0000],
        [3180416.5000,  848072.9375],
        [2732628.7500, 2278711.0000],
        [1388540.3750, 2216175.7500],
        [1514848.5000, 1198522.1250],
        [1937606.0000, 1751364.5000],
        [1245688.5000, 1768367.3750],
        [7225876.0000,       0.0000],
        [2783145.7500,       0.0000],
        [3035740.0000,       0.0000],
        [2817319.2500,       0.0000],
        [2749108.0000,       0.0000],
        [ 444203.1562, 1281839.8750],
        [ 432498.4062,  482415.5625],
        [3811485.0000,  514388.9062],
        [ 574280.4375,  290730.5000],
        [ 601361.5000,  804325.1250],
        [ 809699.6250, 1683529.1250],
        [1514877.2500,  417349.5938],
        [1182050.6250, 1025725.0000],
        [2090149.3750, 1810565.2500],
        [2559998.5000,  292025.8438],
        [5060017.5000,       0.0000],
        [3517359.2500,       0.0000],
        [1346599.1250, 1628872.5000],
        [2104423.0000,  983777.3125],
        [1435206.5000, 1145794.7500],
        [ 695525.1250,  973534.4375],
        [2260204.5000,  638888.1250],
        [ 615093.3125,  221196.6875],
        [1408639.6250,  630694.6875],
        [ 371705.9688,  777838.1875]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 62.5
Top1 accuracy for validation set is 62.5 size is torch.Size([64, 1])
Epoch 281/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:42, 59.40s/it]  7%|▋         | 2/30 [01:00<11:50, 25.39s/it] 10%|█         | 3/30 [01:01<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:03<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.5938948154449464
Epoch 282/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:57, 57.83s/it]  7%|▋         | 2/30 [00:58<11:19, 24.25s/it] 10%|█         | 3/30 [01:01<06:34, 14.62s/it] 13%|█▎        | 4/30 [01:02<03:57,  9.15s/it] 17%|█▋        | 5/30 [01:03<02:32,  6.12s/it] 20%|██        | 6/30 [01:03<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:04<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:05<00:52,  2.37s/it] 30%|███       | 9/30 [01:06<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.619443964958191
Epoch 283/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:12, 60.44s/it]  7%|▋         | 2/30 [01:01<11:53, 25.49s/it] 10%|█         | 3/30 [01:02<06:23, 14.19s/it] 13%|█▎        | 4/30 [01:02<03:51,  8.89s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.95s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.55680414835612
Epoch 284/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:17, 60.60s/it]  7%|▋         | 2/30 [01:02<12:15, 26.28s/it] 10%|█         | 3/30 [01:03<06:34, 14.62s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.15s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.12s/it] 20%|██        | 6/30 [01:05<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.567011268933614
Epoch 285/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:47, 63.71s/it]  7%|▋         | 2/30 [01:04<12:26, 26.68s/it] 10%|█         | 3/30 [01:05<06:40, 14.84s/it] 13%|█▎        | 4/30 [01:05<04:01,  9.27s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.20s/it] 20%|██        | 6/30 [01:07<01:44,  4.35s/it] 23%|██▎       | 7/30 [01:08<01:12,  3.17s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.40s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.33it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.85s/it]
Epoch loss is 2.5698287804921467
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0247,  0.0036,  0.0078,  ...,  0.0010,  0.0033,  0.0084],
        [-0.0006,  0.0201,  0.0211,  ...,  0.0128, -0.0063, -0.0162],
        [-0.0329, -0.0353,  0.0184,  ...,  0.0791, -0.0159, -0.0217],
        ...,
        [ 0.0169, -0.0083, -0.0008,  ..., -0.0241, -0.0199, -0.0139],
        [-0.0283,  0.0008, -0.0093,  ..., -0.0004,  0.0118, -0.0132],
        [-0.0379, -0.0100,  0.0135,  ...,  0.0325,  0.0273, -0.0438]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8993, 0.8545, 0.8539, 0.8524, 0.8501, 0.8476, 0.8432, 0.8428, 0.8404,
         0.8341],
        [0.9511, 0.9464, 0.9453, 0.9434, 0.9432, 0.9391, 0.9389, 0.9373, 0.9372,
         0.9346],
        [0.9124, 0.9108, 0.9044, 0.8861, 0.8803, 0.8684, 0.8558, 0.8502, 0.8495,
         0.8456],
        [0.9537, 0.9132, 0.9048, 0.9011, 0.8959, 0.8946, 0.8915, 0.8890, 0.8878,
         0.8861],
        [0.8584, 0.8471, 0.8446, 0.8293, 0.8262, 0.8171, 0.8086, 0.8067, 0.8034,
         0.8028],
        [0.8623, 0.8363, 0.8297, 0.8134, 0.8133, 0.8056, 0.8050, 0.8035, 0.8033,
         0.7981],
        [0.8986, 0.8985, 0.8930, 0.8837, 0.8793, 0.8764, 0.8762, 0.8728, 0.8698,
         0.8654],
        [0.9192, 0.9007, 0.8954, 0.8726, 0.8678, 0.8650, 0.8434, 0.8402, 0.8379,
         0.8371],
        [0.9189, 0.9123, 0.9113, 0.9103, 0.9053, 0.8967, 0.8965, 0.8955, 0.8935,
         0.8933],
        [0.9543, 0.9006, 0.8991, 0.8965, 0.8957, 0.8919, 0.8910, 0.8873, 0.8768,
         0.8738],
        [0.9554, 0.9200, 0.9170, 0.9139, 0.9133, 0.9118, 0.9083, 0.9030, 0.9026,
         0.9026],
        [0.9135, 0.8992, 0.8753, 0.8737, 0.8592, 0.8395, 0.8391, 0.8201, 0.8160,
         0.8148],
        [0.9450, 0.9185, 0.9157, 0.9151, 0.9117, 0.8988, 0.8939, 0.8932, 0.8785,
         0.8782],
        [0.9691, 0.9619, 0.9616, 0.9614, 0.9599, 0.9575, 0.9563, 0.9536, 0.9536,
         0.9523],
        [0.9463, 0.9386, 0.9376, 0.9363, 0.9351, 0.9303, 0.9285, 0.9285, 0.9285,
         0.9240],
        [0.9318, 0.9285, 0.9278, 0.9225, 0.9189, 0.9168, 0.9165, 0.9162, 0.9145,
         0.9105],
        [0.9666, 0.9606, 0.9550, 0.9486, 0.9426, 0.9392, 0.9371, 0.9367, 0.9327,
         0.9327],
        [0.9688, 0.9627, 0.9573, 0.9460, 0.9456, 0.9442, 0.9430, 0.9429, 0.9428,
         0.9402],
        [0.9383, 0.9361, 0.9285, 0.9078, 0.9078, 0.9066, 0.9064, 0.9061, 0.9048,
         0.9048],
        [0.9188, 0.9139, 0.9123, 0.9096, 0.9085, 0.9079, 0.9068, 0.9057, 0.9042,
         0.9041],
        [0.9616, 0.9523, 0.9493, 0.9485, 0.9482, 0.9445, 0.9427, 0.9421, 0.9405,
         0.9388],
        [0.9373, 0.9285, 0.9268, 0.9196, 0.9179, 0.9145, 0.9134, 0.9128, 0.9113,
         0.9109],
        [0.9743, 0.9508, 0.9498, 0.9469, 0.9427, 0.9421, 0.9356, 0.9250, 0.9243,
         0.9236],
        [0.9506, 0.9454, 0.9392, 0.9359, 0.9333, 0.9316, 0.9305, 0.9305, 0.9300,
         0.9282],
        [0.9639, 0.9126, 0.9098, 0.8933, 0.8929, 0.8917, 0.8865, 0.8857, 0.8846,
         0.8813],
        [0.9471, 0.9435, 0.9411, 0.9353, 0.9301, 0.9300, 0.9231, 0.9212, 0.9182,
         0.9168],
        [0.9407, 0.9395, 0.9364, 0.9308, 0.9238, 0.9157, 0.9151, 0.9147, 0.9117,
         0.9104],
        [0.9655, 0.9540, 0.9524, 0.9412, 0.9345, 0.9318, 0.9317, 0.9306, 0.9299,
         0.9288],
        [0.9545, 0.9476, 0.9430, 0.9383, 0.9356, 0.9345, 0.9334, 0.9320, 0.9294,
         0.9289],
        [0.9437, 0.9390, 0.9348, 0.9321, 0.9318, 0.9298, 0.9296, 0.9249, 0.9238,
         0.9236],
        [0.9593, 0.9546, 0.9523, 0.9510, 0.9509, 0.9508, 0.9441, 0.9435, 0.9431,
         0.9393],
        [0.9501, 0.9289, 0.9278, 0.9220, 0.9178, 0.9153, 0.9133, 0.9076, 0.9073,
         0.9063],
        [0.8904, 0.8807, 0.8721, 0.8711, 0.8677, 0.8651, 0.8640, 0.8636, 0.8623,
         0.8618],
        [0.9237, 0.9110, 0.9066, 0.9045, 0.9029, 0.8960, 0.8955, 0.8953, 0.8915,
         0.8910],
        [0.9329, 0.9327, 0.9259, 0.9254, 0.9140, 0.9117, 0.9098, 0.9090, 0.9038,
         0.9037],
        [0.9164, 0.9131, 0.9052, 0.9002, 0.8898, 0.8870, 0.8857, 0.8800, 0.8787,
         0.8770],
        [0.9489, 0.8748, 0.8643, 0.8615, 0.8608, 0.8588, 0.8554, 0.8506, 0.8501,
         0.8435],
        [0.9267, 0.9134, 0.9079, 0.9038, 0.8839, 0.8836, 0.8817, 0.8802, 0.8785,
         0.8777],
        [0.9091, 0.9062, 0.8953, 0.8812, 0.8790, 0.8725, 0.8682, 0.8652, 0.8599,
         0.8590],
        [0.9504, 0.9494, 0.9462, 0.9450, 0.9440, 0.9431, 0.9388, 0.9382, 0.9376,
         0.9373],
        [0.8964, 0.8954, 0.8911, 0.8885, 0.8754, 0.8727, 0.8702, 0.8591, 0.8499,
         0.8434],
        [0.9155, 0.8837, 0.8831, 0.8819, 0.8800, 0.8781, 0.8746, 0.8717, 0.8714,
         0.8712],
        [0.9023, 0.8946, 0.8943, 0.8760, 0.8741, 0.8721, 0.8680, 0.8639, 0.8559,
         0.8531],
        [0.9100, 0.8878, 0.8851, 0.8814, 0.8738, 0.8667, 0.8625, 0.8569, 0.8545,
         0.8530],
        [0.8755, 0.8542, 0.8529, 0.8435, 0.8415, 0.8377, 0.8321, 0.8310, 0.8179,
         0.8166],
        [0.8413, 0.8297, 0.8164, 0.7975, 0.7892, 0.7833, 0.7821, 0.7732, 0.7648,
         0.7635],
        [0.9198, 0.9195, 0.9134, 0.9070, 0.9058, 0.9036, 0.9010, 0.8989, 0.8981,
         0.8951],
        [0.8314, 0.8110, 0.8006, 0.8005, 0.7967, 0.7929, 0.7803, 0.7679, 0.7676,
         0.7584],
        [0.8714, 0.8495, 0.8292, 0.8249, 0.8206, 0.8205, 0.8122, 0.8116, 0.8098,
         0.8046],
        [0.9096, 0.8820, 0.8811, 0.8762, 0.8559, 0.8512, 0.8506, 0.8498, 0.8472,
         0.8422],
        [0.9021, 0.8618, 0.8602, 0.8585, 0.8471, 0.8387, 0.8336, 0.8266, 0.8237,
         0.8213],
        [0.9042, 0.8975, 0.8669, 0.8557, 0.8517, 0.8442, 0.8375, 0.8323, 0.8317,
         0.8256],
        [0.9410, 0.9253, 0.9205, 0.9098, 0.8903, 0.8855, 0.8751, 0.8702, 0.8693,
         0.8661],
        [0.9262, 0.8831, 0.8800, 0.8792, 0.8738, 0.8709, 0.8664, 0.8597, 0.8588,
         0.8544],
        [0.9426, 0.9246, 0.9199, 0.9188, 0.9149, 0.9141, 0.9141, 0.9090, 0.9081,
         0.9077],
        [0.9213, 0.8966, 0.8922, 0.8904, 0.8879, 0.8873, 0.8872, 0.8852, 0.8847,
         0.8840],
        [0.9312, 0.8988, 0.8874, 0.8864, 0.8815, 0.8675, 0.8573, 0.8519, 0.8493,
         0.8483],
        [0.9446, 0.8990, 0.8885, 0.8837, 0.8737, 0.8611, 0.8600, 0.8596, 0.8546,
         0.8532],
        [0.9265, 0.8775, 0.8755, 0.8691, 0.8598, 0.8591, 0.8548, 0.8505, 0.8451,
         0.8443],
        [0.9116, 0.8387, 0.8380, 0.8344, 0.8238, 0.8176, 0.8170, 0.8168, 0.8167,
         0.8151],
        [0.8895, 0.8834, 0.8804, 0.8798, 0.8794, 0.8783, 0.8770, 0.8731, 0.8720,
         0.8720],
        [0.8616, 0.7985, 0.7964, 0.7942, 0.7877, 0.7648, 0.7630, 0.7609, 0.7602,
         0.7538],
        [0.8949, 0.8734, 0.8715, 0.8512, 0.8451, 0.8427, 0.8341, 0.8326, 0.8325,
         0.8279],
        [0.8470, 0.8408, 0.8299, 0.8115, 0.8109, 0.7971, 0.7962, 0.7921, 0.7893,
         0.7885]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 0, 0, 0, 1, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 1, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 1, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 379918.6875,  200210.1406,  198507.4062,  194373.5156,  187975.5000,
          181395.2812,  170427.3438,  169366.0938,  163701.8594,  149598.7969],
        [ 795269.4375,  744209.3750,  732651.1875,  713248.3125,  710815.2500,
          670245.6875,  668274.0625,  653666.9375,  652719.4375,  629108.1250],
        [ 457810.7500,  447537.9375,  408443.0938,  314452.2500,  289627.3125,
          244204.3750,  204009.6562,  188246.5625,  186416.9844,  176408.3750],
        [ 825390.9375,  463274.8750,  410719.5312,  389785.1875,  361892.9375,
          355211.2500,  339670.4375,  327740.3125,  322032.3750,  314648.4062],
        [ 211605.4688,  180168.0781,  173844.0938,  139691.1250,  133555.2969,
          117333.1328,  103921.2578,  101112.6641,   96440.7109,   95683.8984],
        [ 223957.5938,  154267.3750,  140442.3594,  111328.8906,  111065.0391,
           99543.1562,   98778.7812,   96685.8438,   96297.5234,   89408.7031],
        [ 376182.1250,  375527.5938,  347228.0938,  303697.0625,  285146.2812,
          273581.5000,  273124.0000,  259866.2031,  248959.1250,  233925.2031],
        [ 504260.3438,  387584.5000,  359294.8750,  259275.0625,  242202.5469,
          232536.1562,  170895.2656,  163159.4688,  157973.6250,  156114.8594],
        [ 502421.3438,  457069.1250,  451013.1562,  444559.8125,  413944.8125,
          365918.3125,  364879.1562,  359876.4688,  349444.8438,  348294.0000],
        [ 833526.6875,  386898.3438,  378557.3438,  364736.8438,  360456.2500,
          341406.2812,  337056.0312,  319966.0312,  275372.9688,  263940.1250],
        [ 846356.5000,  510090.0312,  489230.8438,  467969.8125,  463961.9375,
          454223.7812,  431805.2500,  400574.7812,  398082.2500,  397919.0312],
        [ 464768.8125,  379369.8125,  269513.4375,  263385.6875,  213974.2656,
          161580.9531,  160717.8750,  122485.8359,  115573.4375,  113546.3047],
        [ 729909.0625,  499395.0938,  479667.6250,  476031.0312,  453411.4375,
          376861.8438,  351449.4688,  347953.0625,  282039.0625,  280740.2188],
        [1028841.0625,  928745.9375,  924207.9375,  921765.3125,  902186.8125,
          871455.6875,  857621.1250,  825279.1875,  824262.9375,  809249.6250],
        [ 742655.3125,  665748.6875,  656572.1250,  643740.2500,  632899.2500,
          591188.5625,  576564.2500,  576429.0000,  575872.3750,  540542.1250],
        [ 604445.6250,  576585.1250,  570771.6875,  529233.4375,  502183.7812,
          487492.7188,  485286.6250,  483663.0625,  471981.1875,  445391.5938],
        [ 993788.2500,  911530.5625,  840945.8125,  768171.5000,  704338.5625,
          671362.0000,  652033.8125,  648132.3750,  611741.5625,  611517.6250],
        [1024621.8750,  939180.3750,  869318.3125,  740356.3125,  735868.8750,
          721371.6250,  709008.3125,  707657.9375,  706488.0000,  681112.8125],
        [ 663253.8125,  642286.3125,  576305.8125,  428626.3750,  428476.7500,
          421320.8438,  420295.0938,  418526.0000,  410876.6250,  410567.9375],
        [ 501504.1562,  467655.6875,  457216.0625,  439647.1250,  433050.6875,
          429425.8438,  422848.0625,  416276.5312,  407364.7500,  406646.2812],
        [ 924577.3750,  809632.5000,  775508.7500,  766613.4375,  763689.5000,
          723837.3125,  706145.8125,  699811.8125,  684381.5000,  667543.5000],
        [ 653312.9375,  576160.7500,  562573.1250,  507333.0000,  495069.0625,
          472093.2500,  464280.2188,  460647.3750,  450594.8438,  448258.0938],
        [1108445.7500,  792128.1875,  780756.0000,  749944.0625,  706012.5000,
          699590.2500,  638192.3750,  548284.2500,  542961.6250,  537008.1250],
        [ 790663.2500,  733984.1250,  671006.0625,  640085.5625,  617523.4375,
          602339.0000,  593294.1250,  592989.8125,  588471.1250,  573426.5625],
        [ 956074.1875,  459104.0312,  441392.6875,  348282.3750,  346541.3125,
          340672.2188,  316184.3125,  312699.2188,  307734.4375,  293794.6250],
        [ 751214.5625,  714291.8125,  690276.8750,  634734.3750,  589501.8125,
          588909.0000,  533326.3125,  519292.2188,  497207.6875,  487502.9375],
        [ 685522.6250,  673915.6875,  645308.8750,  595363.5000,  538794.8750,
          479884.0312,  475899.8750,  473345.6562,  453230.6875,  444646.7500],
        [ 977408.2500,  829842.4375,  810954.7500,  690862.3750,  627465.7500,
          604054.3750,  603288.6875,  594161.5625,  587937.0625,  578595.1250],
        [ 834898.1875,  757060.8750,  709075.1875,  663079.8750,  637577.9375,
          627380.1875,  617939.8750,  605640.0625,  583429.6250,  579690.8750],
        [ 716336.3125,  669058.4375,  630545.5000,  606423.8125,  604056.6875,
          586771.3750,  585293.6875,  547270.2500,  538503.0625,  536903.1250],
        [ 895305.0000,  837065.1250,  809803.9375,  795059.3750,  793482.3125,
          792583.8750,  720372.0000,  713963.5625,  709378.8750,  672162.8125],
        [ 784600.0000,  579613.0000,  570896.3750,  525413.0000,  494552.7812,
          477058.6250,  463544.9062,  427754.1250,  425771.6875,  419475.4375],
        [ 334558.0938,  291053.2812,  257308.6562,  253829.4688,  241815.4844,
          232885.5000,  229317.5625,  228099.1875,  223727.0312,  222212.6250],
        [ 538101.1250,  448993.1250,  421380.7188,  409147.1562,  399917.0938,
          362142.9062,  359849.3438,  358616.0625,  339586.2188,  337192.0312],
        [ 613922.6875,  611954.0000,  555156.8750,  551617.2500,  468516.8125,
          453006.4375,  440863.8438,  436068.5312,  404624.6562,  404473.7812],
        [ 484665.0312,  462277.8750,  413242.3125,  384507.7812,  331413.3125,
          318572.7188,  312702.7812,  288349.3438,  282842.5625,  276152.2188],
        [ 771178.0625,  267733.0000,  230344.0156,  221406.4531,  218995.6875,
          212940.1406,  202666.1875,  189268.4844,  188005.6094,  171053.4219],
        [ 561483.4375,  464287.7188,  429376.7188,  404989.4688,  304696.1562,
          303442.9062,  295286.4688,  289166.9688,  282206.1562,  278908.5312],
        [ 436778.9688,  418798.2812,  358487.4688,  293357.0312,  284159.2500,
          258905.9219,  243506.9375,  233335.2188,  216173.4531,  213539.2500],
        [ 788095.4375,  776379.6875,  741599.3125,  729049.1250,  718603.8125,
          710110.6250,  667336.6250,  661897.1250,  656604.0000,  653190.8125],
        [ 364438.5625,  358956.8438,  337885.4062,  325586.7500,  269769.8438,
          259814.9062,  250570.0469,  213919.5781,  187565.2344,  170875.5625],
        [ 478838.5625,  304035.5625,  301380.8125,  296318.0938,  288260.5312,
          280560.6562,  266992.3125,  255941.0781,  255058.7812,  254034.3438],
        [ 396303.8125,  354877.0625,  353614.5625,  272086.1562,  264743.5625,
          257462.5469,  242673.9688,  228988.2344,  204297.7969,  196285.2969],
        [ 442440.3750,  322085.1875,  310067.2812,  293883.7500,  263800.4688,
          238360.6875,  224498.8125,  207316.6250,  200127.6719,  196082.6875],
        [ 270240.7812,  199466.6875,  195705.5000,  171176.9688,  166346.8750,
          157434.2969,  145467.8750,  143205.5000,  118621.3750,  116425.3672],
        [ 165907.8750,  140556.6562,  116155.5391,   88748.6250,   78792.1484,
           72427.2656,   71201.9844,   62662.6484,   55575.4648,   54561.2266],
        [ 508903.5000,  506572.5000,  464672.6250,  423599.1875,  416468.7188,
          403882.5000,  388885.5625,  377767.2188,  373186.6562,  357506.9375],
        [ 143888.0625,  107518.5547,   92764.8672,   92594.9922,   87694.4375,
           83036.8438,   69355.2109,   58120.7930,   57868.0938,   50764.0547],
        [ 254894.1406,  186462.8438,  139579.1250,  131166.4844,  123287.2109,
          123270.2812,  109456.4453,  108465.2266,  105791.8906,   98183.9844],
        [ 439983.9375,  296685.1250,  292775.6562,  272921.1562,  204175.2969,
          191119.4844,  189472.9219,  187099.1250,  180322.9688,  168037.9688],
        [ 395182.1875,  222135.0781,  217210.0312,  212080.6562,  180011.9531,
          159692.3906,  148567.7344,  134383.9688,  128959.5234,  124597.6797],
        [ 407439.7500,  369992.0000,  238954.0469,  203735.3281,  192242.2500,
          172869.8438,  157113.9219,  145760.6250,  144545.0781,  132433.2031],
        [ 689248.7500,  550315.1875,  514286.3750,  441456.6562,  333963.2500,
          311854.0312,  268707.8438,  250510.2969,  247369.9375,  236376.3281],
        [ 557847.0625,  301107.0312,  288084.6562,  284963.8750,  263730.8125,
          253073.6562,  237221.1562,  215630.5000,  212982.6094,  199852.8438],
        [ 705094.0000,  545294.6875,  509976.2500,  501577.3125,  474357.4375,
          469220.1875,  469220.1875,  436286.9062,  430534.6562,  428267.5938],
        [ 519870.9688,  365291.4062,  343241.3438,  334505.1250,  322537.6875,
          319961.1250,  319444.9375,  310570.4062,  308323.4062,  304990.3750],
        [ 598849.6250,  376854.3125,  320374.2812,  315856.0938,  294424.8750,
          241123.4844,  208289.0625,  192880.7656,  185855.8594,  183164.3750],
        [ 725444.0625,  378042.5312,  325385.0000,  303879.3125,  263572.6250,
          220023.1250,  216680.3750,  215484.7500,  200423.1562,  196500.7031],
        [ 559734.5625,  278179.3125,  270275.0625,  246802.5156,  216057.0000,
          213733.2031,  201108.6094,  189112.7812,  175084.8125,  173072.8906],
        [ 452392.5000,  159820.8281,  158210.4844,  150286.0469,  129074.0781,
          118261.5000,  117169.3203,  116852.9531,  116603.7109,  113989.0781],
        [ 329997.8750,  302443.2500,  289764.9062,  287367.3438,  285798.5938,
          281327.1875,  276244.6562,  261054.7812,  257052.8438,  256904.8281],
        [ 221704.5938,   89903.4141,   87329.2188,   84606.8750,   77120.7109,
           55590.7812,   54149.4336,   52579.6680,   52076.5391,   47499.8203],
        [ 356332.2812,  262264.0312,  255284.3594,  191044.9531,  174972.4688,
          169046.9062,  149669.2969,  146462.7656,  146290.9219,  136993.0469],
        [ 179842.4219,  164697.3281,  140885.7188,  108248.4297,  107408.6875,
           88200.0234,   87030.6641,   82105.5234,   78928.9453,   78036.1094]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[379918.6875,      0.0000],
         [200210.1406,      0.0000],
         [198507.4062,      0.0000],
         ...,
         [     0.0000, 169366.0938],
         [163701.8594,      0.0000],
         [149598.7969,      0.0000]],

        [[795269.4375,      0.0000],
         [744209.3750,      0.0000],
         [732651.1875,      0.0000],
         ...,
         [653666.9375,      0.0000],
         [652719.4375,      0.0000],
         [629108.1250,      0.0000]],

        [[457810.7500,      0.0000],
         [447537.9375,      0.0000],
         [408443.0938,      0.0000],
         ...,
         [188246.5625,      0.0000],
         [186416.9844,      0.0000],
         [     0.0000, 176408.3750]],

        ...,

        [[     0.0000, 221704.5938],
         [ 89903.4141,      0.0000],
         [ 87329.2188,      0.0000],
         ...,
         [ 52579.6680,      0.0000],
         [ 52076.5391,      0.0000],
         [ 47499.8203,      0.0000]],

        [[356332.2812,      0.0000],
         [262264.0312,      0.0000],
         [     0.0000, 255284.3594],
         ...,
         [146462.7656,      0.0000],
         [146290.9219,      0.0000],
         [136993.0469,      0.0000]],

        [[     0.0000, 179842.4219],
         [     0.0000, 164697.3281],
         [     0.0000, 140885.7188],
         ...,
         [ 82105.5234,      0.0000],
         [ 78928.9453,      0.0000],
         [     0.0000,  78036.1094]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1826108.5000,  169366.0938],
        [6970208.0000,       0.0000],
        [2740748.7500,  176408.3750],
        [4110366.2500,       0.0000],
        [ 566483.6875,  786872.0000],
        [1067507.8750,  154267.3750],
        [2344862.7500,  632374.3750],
        [2633296.5000,       0.0000],
        [3600351.7500,  457069.1250],
        [3861917.0000,       0.0000],
        [4860214.0000,       0.0000],
        [2264916.2500,       0.0000],
        [4277457.5000,       0.0000],
        [8893616.0000,       0.0000],
        [6202212.0000,       0.0000],
        [5157035.0000,       0.0000],
        [7413561.5000,       0.0000],
        [7834984.0000,       0.0000],
        [4820535.5000,       0.0000],
        [4381635.0000,       0.0000],
        [7521742.0000,       0.0000],
        [5090323.0000,       0.0000],
        [7103323.0000,       0.0000],
        [6403783.5000,       0.0000],
        [4122479.5000,       0.0000],
        [6006257.5000,       0.0000],
        [5465912.5000,       0.0000],
        [6904570.5000,       0.0000],
        [6615772.5000,       0.0000],
        [6021162.0000,       0.0000],
        [7739177.0000,       0.0000],
        [5168680.0000,       0.0000],
        [2514807.0000,       0.0000],
        [3144397.7500,  830527.8750],
        [2690654.5000, 2249550.5000],
        [1358192.8750, 2196533.2500],
        [1480751.2500, 1192840.0000],
        [1900877.7500, 1712966.8750],
        [1215310.8750, 1741730.8750],
        [7102866.0000,       0.0000],
        [2739382.7500,       0.0000],
        [2981420.7500,       0.0000],
        [2771333.0000,       0.0000],
        [2698663.5000,       0.0000],
        [ 428173.7500, 1255917.5000],
        [ 428393.9062,  478195.5312],
        [3714873.0000,  506572.5000],
        [ 558100.0000,  285505.9062],
        [ 592972.3125,  787585.3125],
        [ 788344.8750, 1634248.7500],
        [1634169.2500,  288651.9062],
        [1148700.2500, 1016385.8125],
        [2060413.7500, 1783674.8750],
        [2529530.2500,  284963.8750],
        [4969829.0000,       0.0000],
        [3448736.7500,       0.0000],
        [1324800.2500, 1592872.5000],
        [2078435.5000,  967000.1250],
        [1399211.5000, 1123949.2500],
        [ 681127.6875,  951532.8125],
        [2195515.0000,  632441.1250],
        [ 600856.4375,  221704.5938],
        [1372984.7500,  615376.1875],
        [ 357482.9375,  757900.9375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 62.5
Top1 accuracy for validation set is 62.5 size is torch.Size([64, 1])
Epoch 286/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:26, 60.91s/it]  7%|▋         | 2/30 [01:01<11:54, 25.52s/it] 10%|█         | 3/30 [01:02<06:23, 14.21s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.89s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.96s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.590402356783549
Epoch 287/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:31, 56.95s/it]  7%|▋         | 2/30 [00:57<11:08, 23.89s/it] 10%|█         | 3/30 [01:01<06:32, 14.53s/it] 13%|█▎        | 4/30 [01:01<03:56,  9.09s/it] 17%|█▋        | 5/30 [01:02<02:32,  6.08s/it] 20%|██        | 6/30 [01:03<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.36s/it] 30%|███       | 9/30 [01:05<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:07<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.567292801539103
Epoch 288/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:03, 62.20s/it]  7%|▋         | 2/30 [01:02<12:09, 26.05s/it] 10%|█         | 3/30 [01:03<06:31, 14.50s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.07s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.07s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 2.6137731949488323
Epoch 289/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:28, 58.92s/it]  7%|▋         | 2/30 [01:00<11:38, 24.95s/it] 10%|█         | 3/30 [01:00<06:15, 13.90s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.5865147590637205
Epoch 290/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:34, 59.11s/it]  7%|▋         | 2/30 [00:59<11:33, 24.78s/it] 10%|█         | 3/30 [01:01<06:22, 14.16s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.86s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.94s/it] 20%|██        | 6/30 [01:03<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.558875735600789
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0248,  0.0033,  0.0083,  ...,  0.0006,  0.0041,  0.0079],
        [-0.0006,  0.0194,  0.0218,  ...,  0.0131, -0.0056, -0.0162],
        [-0.0325, -0.0357,  0.0191,  ...,  0.0793, -0.0150, -0.0210],
        ...,
        [ 0.0162, -0.0077,  0.0001,  ..., -0.0241, -0.0189, -0.0135],
        [-0.0284,  0.0015, -0.0089,  ..., -0.0001,  0.0121, -0.0131],
        [-0.0377, -0.0103,  0.0138,  ...,  0.0334,  0.0280, -0.0428]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8983, 0.8547, 0.8531, 0.8512, 0.8497, 0.8464, 0.8440, 0.8417, 0.8401,
         0.8344],
        [0.9505, 0.9452, 0.9438, 0.9427, 0.9420, 0.9379, 0.9374, 0.9364, 0.9361,
         0.9337],
        [0.9122, 0.9097, 0.9029, 0.8852, 0.8787, 0.8664, 0.8553, 0.8501, 0.8492,
         0.8443],
        [0.9537, 0.9120, 0.9038, 0.9001, 0.8952, 0.8932, 0.8901, 0.8882, 0.8865,
         0.8856],
        [0.8570, 0.8478, 0.8438, 0.8282, 0.8253, 0.8173, 0.8082, 0.8050, 0.8037,
         0.8027],
        [0.8622, 0.8325, 0.8285, 0.8134, 0.8111, 0.8051, 0.8024, 0.8014, 0.7985,
         0.7950],
        [0.8963, 0.8962, 0.8895, 0.8819, 0.8769, 0.8739, 0.8739, 0.8714, 0.8681,
         0.8641],
        [0.9174, 0.8988, 0.8932, 0.8718, 0.8656, 0.8644, 0.8407, 0.8391, 0.8373,
         0.8347],
        [0.9186, 0.9108, 0.9097, 0.9086, 0.9039, 0.8958, 0.8954, 0.8947, 0.8927,
         0.8923],
        [0.9540, 0.8993, 0.8983, 0.8958, 0.8943, 0.8902, 0.8897, 0.8864, 0.8758,
         0.8729],
        [0.9546, 0.9185, 0.9159, 0.9129, 0.9110, 0.9109, 0.9073, 0.9019, 0.9013,
         0.9013],
        [0.9124, 0.8981, 0.8747, 0.8731, 0.8575, 0.8376, 0.8372, 0.8174, 0.8149,
         0.8119],
        [0.9434, 0.9180, 0.9149, 0.9142, 0.9099, 0.8978, 0.8925, 0.8913, 0.8772,
         0.8760],
        [0.9686, 0.9614, 0.9613, 0.9607, 0.9594, 0.9574, 0.9556, 0.9532, 0.9526,
         0.9517],
        [0.9461, 0.9378, 0.9369, 0.9359, 0.9350, 0.9294, 0.9278, 0.9276, 0.9275,
         0.9231],
        [0.9316, 0.9282, 0.9277, 0.9220, 0.9183, 0.9169, 0.9158, 0.9153, 0.9140,
         0.9106],
        [0.9656, 0.9596, 0.9539, 0.9475, 0.9404, 0.9383, 0.9364, 0.9358, 0.9316,
         0.9313],
        [0.9685, 0.9624, 0.9571, 0.9451, 0.9449, 0.9438, 0.9425, 0.9423, 0.9420,
         0.9393],
        [0.9375, 0.9350, 0.9269, 0.9065, 0.9062, 0.9056, 0.9054, 0.9050, 0.9040,
         0.9033],
        [0.9175, 0.9118, 0.9107, 0.9086, 0.9079, 0.9072, 0.9060, 0.9047, 0.9031,
         0.9027],
        [0.9604, 0.9510, 0.9483, 0.9473, 0.9468, 0.9435, 0.9419, 0.9413, 0.9394,
         0.9384],
        [0.9363, 0.9270, 0.9255, 0.9194, 0.9174, 0.9136, 0.9126, 0.9112, 0.9105,
         0.9099],
        [0.9741, 0.9506, 0.9490, 0.9467, 0.9419, 0.9416, 0.9350, 0.9244, 0.9240,
         0.9228],
        [0.9500, 0.9447, 0.9385, 0.9347, 0.9327, 0.9312, 0.9302, 0.9300, 0.9292,
         0.9273],
        [0.9633, 0.9113, 0.9092, 0.8912, 0.8909, 0.8905, 0.8846, 0.8839, 0.8831,
         0.8801],
        [0.9467, 0.9432, 0.9407, 0.9348, 0.9296, 0.9294, 0.9228, 0.9211, 0.9173,
         0.9153],
        [0.9395, 0.9385, 0.9359, 0.9301, 0.9228, 0.9151, 0.9146, 0.9135, 0.9102,
         0.9101],
        [0.9652, 0.9537, 0.9518, 0.9406, 0.9343, 0.9312, 0.9304, 0.9300, 0.9286,
         0.9281],
        [0.9539, 0.9469, 0.9419, 0.9380, 0.9342, 0.9332, 0.9331, 0.9316, 0.9287,
         0.9283],
        [0.9428, 0.9382, 0.9341, 0.9314, 0.9311, 0.9297, 0.9291, 0.9239, 0.9233,
         0.9227],
        [0.9587, 0.9539, 0.9516, 0.9506, 0.9505, 0.9504, 0.9434, 0.9430, 0.9421,
         0.9383],
        [0.9496, 0.9283, 0.9274, 0.9216, 0.9170, 0.9139, 0.9121, 0.9072, 0.9072,
         0.9057],
        [0.8897, 0.8800, 0.8724, 0.8698, 0.8668, 0.8640, 0.8630, 0.8625, 0.8624,
         0.8610],
        [0.9229, 0.9101, 0.9049, 0.9034, 0.9014, 0.8955, 0.8947, 0.8941, 0.8909,
         0.8907],
        [0.9322, 0.9309, 0.9254, 0.9248, 0.9136, 0.9109, 0.9080, 0.9079, 0.9025,
         0.9023],
        [0.9155, 0.9121, 0.9045, 0.8989, 0.8886, 0.8871, 0.8850, 0.8790, 0.8774,
         0.8756],
        [0.9489, 0.8743, 0.8624, 0.8616, 0.8605, 0.8586, 0.8529, 0.8488, 0.8472,
         0.8404],
        [0.9247, 0.9112, 0.9065, 0.9019, 0.8826, 0.8822, 0.8803, 0.8788, 0.8781,
         0.8760],
        [0.9080, 0.9054, 0.8942, 0.8804, 0.8773, 0.8701, 0.8667, 0.8646, 0.8574,
         0.8574],
        [0.9491, 0.9486, 0.9451, 0.9440, 0.9431, 0.9424, 0.9368, 0.9368, 0.9365,
         0.9362],
        [0.8955, 0.8943, 0.8902, 0.8870, 0.8738, 0.8718, 0.8693, 0.8578, 0.8487,
         0.8424],
        [0.9148, 0.8819, 0.8819, 0.8804, 0.8789, 0.8765, 0.8731, 0.8710, 0.8708,
         0.8695],
        [0.9015, 0.8936, 0.8931, 0.8753, 0.8729, 0.8703, 0.8670, 0.8627, 0.8547,
         0.8513],
        [0.9083, 0.8857, 0.8846, 0.8797, 0.8722, 0.8658, 0.8614, 0.8552, 0.8544,
         0.8524],
        [0.8745, 0.8524, 0.8516, 0.8430, 0.8389, 0.8362, 0.8299, 0.8281, 0.8150,
         0.8135],
        [0.8412, 0.8284, 0.8153, 0.7972, 0.7892, 0.7830, 0.7819, 0.7711, 0.7651,
         0.7630],
        [0.9184, 0.9177, 0.9113, 0.9055, 0.9044, 0.9021, 0.8992, 0.8973, 0.8962,
         0.8936],
        [0.8303, 0.8086, 0.8000, 0.7994, 0.7935, 0.7899, 0.7796, 0.7670, 0.7651,
         0.7561],
        [0.8687, 0.8489, 0.8279, 0.8238, 0.8204, 0.8196, 0.8114, 0.8113, 0.8077,
         0.8030],
        [0.9087, 0.8814, 0.8776, 0.8739, 0.8541, 0.8499, 0.8486, 0.8478, 0.8443,
         0.8402],
        [0.9011, 0.8617, 0.8606, 0.8586, 0.8472, 0.8381, 0.8329, 0.8250, 0.8229,
         0.8219],
        [0.9037, 0.8970, 0.8658, 0.8531, 0.8496, 0.8433, 0.8349, 0.8301, 0.8294,
         0.8245],
        [0.9401, 0.9246, 0.9199, 0.9090, 0.8887, 0.8838, 0.8743, 0.8697, 0.8678,
         0.8640],
        [0.9253, 0.8823, 0.8800, 0.8774, 0.8727, 0.8697, 0.8659, 0.8587, 0.8578,
         0.8532],
        [0.9416, 0.9233, 0.9189, 0.9178, 0.9132, 0.9126, 0.9126, 0.9083, 0.9070,
         0.9062],
        [0.9200, 0.8956, 0.8906, 0.8882, 0.8872, 0.8859, 0.8857, 0.8839, 0.8828,
         0.8825],
        [0.9297, 0.8967, 0.8869, 0.8852, 0.8798, 0.8669, 0.8555, 0.8496, 0.8484,
         0.8456],
        [0.9444, 0.8977, 0.8871, 0.8835, 0.8727, 0.8601, 0.8594, 0.8581, 0.8539,
         0.8510],
        [0.9253, 0.8756, 0.8738, 0.8675, 0.8577, 0.8565, 0.8524, 0.8492, 0.8439,
         0.8431],
        [0.9105, 0.8375, 0.8368, 0.8325, 0.8221, 0.8162, 0.8158, 0.8150, 0.8142,
         0.8127],
        [0.8881, 0.8836, 0.8785, 0.8785, 0.8767, 0.8757, 0.8756, 0.8713, 0.8705,
         0.8703],
        [0.8615, 0.7977, 0.7957, 0.7929, 0.7867, 0.7624, 0.7601, 0.7577, 0.7571,
         0.7499],
        [0.8944, 0.8717, 0.8706, 0.8487, 0.8422, 0.8402, 0.8319, 0.8310, 0.8296,
         0.8255],
        [0.8469, 0.8385, 0.8271, 0.8094, 0.8085, 0.7932, 0.7927, 0.7893, 0.7882,
         0.7862]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 1, 0, 0, 1],
        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 374390.0938,  200728.0625,  196355.7031,  190943.6719,  186871.7656,
          178346.0781,  172208.1875,  166707.7031,  163031.7656,  150243.0625],
        [ 789306.4375,  731276.0000,  717289.8750,  705689.3750,  698388.3750,
          658896.1250,  654513.3750,  645159.3125,  641895.6250,  620537.1250],
        [ 456521.0938,  440457.5000,  399713.5000,  310235.2812,  282840.9375,
          237300.3438,  202550.6406,  188017.4375,  185536.1875,  172994.3594],
        [ 826186.3750,  455206.8750,  404829.5938,  384011.5938,  358021.4688,
          348160.8438,  333156.2188,  323900.0938,  316310.6875,  312065.2812],
        [ 207565.3125,  181960.7812,  171782.0469,  137459.1875,  131875.1406,
          117662.6875,  103269.5078,   98769.0703,   96938.8203,   95524.0703],
        [ 223531.4531,  146209.0469,  138038.2656,  111287.5938,  107714.5781,
           98806.0000,   95054.7031,   93765.9375,   89996.4062,   85609.2578],
        [ 363680.2812,  363439.6562,  330125.6562,  296310.4375,  275832.4062,
          264113.6250,  264033.8125,  254855.7344,  243022.2969,  229806.6562],
        [ 491679.4375,  376915.0625,  348229.9062,  256288.1562,  234746.9531,
          230539.5938,  164493.8906,  160718.0312,  156559.1562,  150935.6875],
        [ 500175.3750,  447205.1250,  440210.1562,  433918.8125,  405654.7188,
          361093.4688,  359058.1875,  355763.1562,  345402.0312,  343791.3750],
        [ 828964.4375,  379803.8438,  374165.2500,  361069.7188,  353361.0625,
          333310.0312,  331000.4688,  315832.6250,  271391.0938,  260565.5156],
        [ 836148.3750,  499613.3125,  481018.5312,  460826.6562,  449063.3750,
          448194.4062,  425870.3438,  394107.6562,  390954.1875,  390493.6250],
        [ 457832.1562,  373267.4688,  267056.2188,  260983.0938,  209047.0625,
          157192.0000,  156278.5781,  117921.0469,  113734.5547,  108931.6016],
        [ 713037.4375,  496107.4062,  474447.0625,  469519.2188,  442092.8438,
          371756.9062,  344461.2188,  338782.0938,  276964.3125,  272018.1875],
        [1021910.8750,  922282.3750,  920549.5625,  912509.0000,  895578.3125,
          870659.0625,  849226.6875,  819989.8750,  813185.1250,  802922.1875],
        [ 740739.1250,  657906.5625,  649624.3750,  640900.3750,  632587.9375,
          583809.1875,  570833.1875,  568984.7500,  567823.6250,  533652.4375],
        [ 601931.2500,  573567.0625,  569990.5625,  525358.8750,  498416.8750,
          488428.5312,  480432.1562,  477343.0312,  468329.2188,  446069.1562],
        [ 978652.5625,  898209.4375,  828116.6250,  756448.9375,  682999.8125,
          662500.2500,  645084.2500,  639149.1875,  601908.3125,  599517.6875],
        [1020430.6250,  934729.5625,  866598.3125,  729975.1250,  728682.8125,
          717060.8125,  703908.8750,  701596.6250,  699205.3125,  672232.6875],
        [ 655567.2500,  632028.9375,  563141.5625,  421003.1562,  419140.6875,
          415576.0312,  414098.3750,  411957.5625,  405785.5000,  402186.3125],
        [ 492220.8438,  453750.5625,  447153.0938,  433555.2188,  429332.0938,
          425016.6875,  417882.6562,  410123.4062,  400975.3438,  398553.6875],
        [ 909249.7500,  795044.2500,  764254.1250,  754193.5625,  748196.6875,
          714315.6250,  698228.5000,  692034.8125,  673574.5000,  664139.9375],
        [ 644142.4375,  563990.7500,  552142.5000,  505811.2188,  491634.4062,
          465984.9062,  458971.8438,  450311.7500,  445834.3750,  442068.4062],
        [1104810.1250,  790490.6250,  772710.0625,  747195.5625,  697787.8125,
          694377.0000,  631909.5625,  543779.8125,  540638.5625,  531550.0625],
        [ 783201.3125,  726000.5625,  664759.0000,  629929.4375,  611892.6875,
          599245.0000,  590400.8750,  588598.5000,  582287.8125,  566360.6875],
        [ 947764.0000,  450620.6562,  437321.2500,  338323.9062,  336984.6875,
          334732.9688,  307727.9688,  304502.6875,  301380.8125,  288655.8438],
        [ 747062.2500,  710449.9375,  685805.1250,  630068.1875,  585196.0000,
          584006.3125,  531329.6250,  518523.1875,  491294.6250,  477389.9375],
        [ 674446.8125,  664573.9375,  640230.2500,  589786.3750,  530956.8125,
          476113.6875,  472403.1250,  465286.3750,  443608.1875,  442777.2500],
        [ 973071.4375,  826223.3750,  803820.1250,  684518.5000,  625944.0625,
          599045.5625,  592434.7500,  588554.6875,  577348.3125,  572919.2500],
        [ 828319.6250,  749483.5625,  697815.0625,  660289.4375,  625461.3750,
          616200.9375,  615039.0625,  602166.1250,  577674.3750,  574607.3125],
        [ 707246.3750,  661612.5000,  623859.5000,  600624.4375,  597983.3125,
          586419.5000,  580830.3125,  539370.6250,  535302.8750,  530441.6250],
        [ 887514.6875,  828673.6250,  802048.1875,  789713.7500,  788948.9375,
          787417.7500,  713129.8750,  709145.5625,  699866.5000,  663206.3125],
        [ 779396.0625,  574866.5625,  567044.3750,  522127.2188,  488698.7812,
          468067.1250,  456049.4062,  425177.6250,  425021.9688,  415777.8125],
        [ 330949.6562,  288398.8438,  258489.2188,  249185.9688,  238716.9375,
          229267.0625,  225982.0469,  224580.6094,  224250.5938,  219768.9531],
        [ 531967.4375,  443079.6562,  411365.9375,  402581.9688,  391323.0938,
          359398.3438,  355442.0000,  352524.9688,  336528.0000,  335714.4688],
        [ 607799.4375,  595949.1875,  551191.8125,  546805.3750,  465658.3750,
          448386.3438,  430007.3438,  429251.8438,  397500.6875,  396105.4688],
        [ 478526.7500,  455874.1562,  408982.9375,  377681.4688,  326060.5625,
          318946.3125,  309553.7812,  284115.6250,  277536.4688,  270505.5938],
        [ 770817.0000,  265648.9688,  224247.8125,  221498.9531,  218166.8750,
          212324.9062,  195646.3438,  184437.8281,  180358.9062,  163743.5469],
        [ 545471.5000,  449855.0625,  420831.3438,  394143.0000,  299033.1562,
          297355.3125,  289414.4062,  283262.0312,  280483.8438,  272353.0312],
        [ 429995.8750,  414263.5000,  353258.3125,  289869.0938,  277357.5938,
          250189.6562,  238253.8750,  231297.6094,  208721.7656,  208706.8281],
        [ 773493.8125,  768204.4375,  730404.1250,  719041.8750,  709590.0000,
          703099.7500,  649125.8750,  648896.8125,  646256.0625,  643334.5625],
        [ 359663.0625,  353349.2812,  333267.4375,  318547.5312,  263657.8750,
          256487.1875,  247309.5312,  209856.2500,  184411.6250,  168323.1406],
        [ 473681.1562,  296283.3125,  295973.5312,  289830.0938,  283700.8125,
          274301.7500,  261061.7500,  253417.3125,  252637.1719,  248146.5781],
        [ 391610.9375,  349998.8125,  347618.7188,  269479.0000,  260456.9688,
          250942.8594,  239436.7500,  224948.2344,  200800.2344,  191254.5938],
        [ 431580.8438,  312458.3438,  307571.2812,  286849.3438,  257639.4062,
          235190.4062,  220804.8750,  202277.5000,  199871.9062,  194289.5781],
        [ 266403.0000,  194287.3438,  192080.9844,  169770.2031,  160193.4688,
          154105.6250,  140926.8281,  137333.9219,  113944.6328,  111426.9297],
        [ 165506.0000,  137914.3125,  114303.9062,   88327.7188,   78764.0391,
           72086.1641,   71014.3438,   60812.8477,   55854.6797,   54180.2188],
        [ 499070.4062,  494035.2188,  450639.5625,  414722.8125,  408357.7812,
          394967.0625,  378904.4688,  369267.5938,  363506.9062,  350126.3750],
        [ 141785.7031,  103922.4531,   91862.5000,   91098.0859,   83790.0156,
           79566.9609,   68630.2656,   57390.3320,   55815.2266,   49072.2617],
        [ 245224.0000,  184884.9531,  136853.5781,  129217.0703,  122964.4141,
          121650.0000,  108239.9609,  108000.1172,  102639.7422,   95998.2266],
        [ 434036.7812,  294129.6562,  278669.7812,  263994.5000,  199153.6250,
          187584.0156,  183993.3438,  181811.7812,  173185.5000,  163201.0156],
        [ 389474.5625,  221937.2969,  218457.2969,  212228.9531,  180384.3594,
          158485.7812,  146995.2031,  131382.4375,  127566.9062,  125731.6562],
        [ 404310.6562,  367467.1562,  235446.7031,  196133.5469,  186659.2656,
          170610.7812,  151284.8750,  141278.3281,  139918.4375,  130408.1250],
        [ 680071.1250,  545012.9375,  509693.7188,  436391.3750,  326536.4062,
          304405.4375,  265707.0000,  248794.4062,  242050.8281,  229292.2188],
        [ 550714.1875,  297778.1562,  288138.5000,  277865.9375,  259525.9219,
          248829.5469,  235517.2188,  212642.8594,  209736.4062,  196549.7969],
        [ 694897.6875,  535077.8125,  502677.7812,  494264.2500,  463227.5938,
          458987.1562,  458987.1562,  431844.3750,  423584.2500,  418895.3438],
        [ 510322.6250,  360166.9375,  335451.0625,  324017.5000,  319476.6562,
          313479.1250,  312808.0938,  304521.8750,  299983.4375,  298524.2500],
        [ 586038.8125,  365646.1875,  317863.5000,  310385.3125,  287396.4062,
          239169.9688,  203161.5781,  186680.4688,  183548.5469,  176264.7656],
        [ 723466.6875,  371040.7188,  319086.8750,  302783.8125,  259504.1562,
          216757.0469,  214675.7656,  210751.9688,  198376.8281,  190581.1094],
        [ 550219.1875,  270670.7500,  263829.1562,  241216.6250,  209552.0625,
          206044.5156,  194365.5625,  185601.6875,  172165.0000,  170058.6406],
        [ 445297.7188,  157089.8125,  155464.8594,  146144.0781,  126060.7578,
          115819.0391,  115111.1172,  113846.1172,  112510.7578,  110147.9844],
        [ 323600.6250,  303598.0312,  282183.0000,  281915.3750,  275093.9688,
          271070.3750,  270797.7812,  254555.7656,  251454.5625,  251045.3125],
        [ 221376.4844,   88903.3906,   86455.5703,   83008.3438,   75982.6484,
           53716.7656,   52014.1992,   50191.9688,   49790.2109,   44935.1016],
        [ 353859.8438,  255799.7812,  251943.0469,  184200.1719,  168024.0312,
          163258.1562,  144906.5469,  143188.5781,  140244.6719,  132269.8906],
        [ 179608.6406,  159383.0781,  135311.4375,  105074.8750,  103843.4922,
           83357.2266,   82804.1953,   78924.3516,   77643.1172,   75444.7188]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[374390.0938,      0.0000],
         [200728.0625,      0.0000],
         [196355.7031,      0.0000],
         ...,
         [     0.0000, 166707.7031],
         [163031.7656,      0.0000],
         [150243.0625,      0.0000]],

        [[789306.4375,      0.0000],
         [731276.0000,      0.0000],
         [717289.8750,      0.0000],
         ...,
         [645159.3125,      0.0000],
         [641895.6250,      0.0000],
         [620537.1250,      0.0000]],

        [[456521.0938,      0.0000],
         [440457.5000,      0.0000],
         [399713.5000,      0.0000],
         ...,
         [188017.4375,      0.0000],
         [185536.1875,      0.0000],
         [     0.0000, 172994.3594]],

        ...,

        [[     0.0000, 221376.4844],
         [ 88903.3906,      0.0000],
         [ 86455.5703,      0.0000],
         ...,
         [ 50191.9688,      0.0000],
         [ 49790.2109,      0.0000],
         [ 44935.1016,      0.0000]],

        [[353859.8438,      0.0000],
         [255799.7812,      0.0000],
         [     0.0000, 251943.0469],
         ...,
         [143188.5781,      0.0000],
         [140244.6719,      0.0000],
         [132269.8906,      0.0000]],

        [[     0.0000, 179608.6406],
         [     0.0000, 159383.0781],
         [     0.0000, 135311.4375],
         ...,
         [ 78924.3516,      0.0000],
         [     0.0000,  77643.1172],
         [ 75444.7188,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1813118.2500,  166707.7031],
        [6862952.0000,       0.0000],
        [2703172.7500,  172994.3594],
        [4061849.0000,       0.0000],
        [ 560566.2500,  782240.3750],
        [1043804.1250,  146209.0469],
        [2279262.5000,  605958.0625],
        [2571105.7500,       0.0000],
        [3552062.2500,  440210.1562],
        [3809464.0000,       0.0000],
        [4776291.0000,       0.0000],
        [2222244.0000,       0.0000],
        [4199186.5000,       0.0000],
        [8828813.0000,       0.0000],
        [6146861.5000,       0.0000],
        [5129866.5000,       0.0000],
        [7292587.0000,       0.0000],
        [7774421.0000,       0.0000],
        [4740485.5000,       0.0000],
        [4308563.5000,       0.0000],
        [7413232.0000,       0.0000],
        [5020892.5000,       0.0000],
        [7055249.0000,       0.0000],
        [6342676.0000,       0.0000],
        [4048015.0000,       0.0000],
        [5961125.5000,       0.0000],
        [5400183.0000,       0.0000],
        [6843880.0000,       0.0000],
        [6547056.5000,       0.0000],
        [5963691.0000,       0.0000],
        [7669665.0000,       0.0000],
        [5122227.0000,       0.0000],
        [2489590.0000,       0.0000],
        [3105978.0000,  813947.8750],
        [2648057.0000, 2220598.7500],
        [1329976.7500, 2177807.0000],
        [1448928.8750, 1187962.3750],
        [1859471.1250, 1672731.5000],
        [1188785.0000, 1713129.2500],
        [6991447.0000,       0.0000],
        [2694873.0000,       0.0000],
        [2929033.5000,       0.0000],
        [2726547.2500,       0.0000],
        [2648533.2500,       0.0000],
        [ 411472.0000, 1229001.0000],
        [ 425185.3125,  473578.9062],
        [3624527.5000,  499070.4062],
        [ 543918.5000,  279015.3125],
        [ 584471.3750,  771200.7500],
        [ 770423.2500, 1589336.7500],
        [1626591.7500,  286052.6875],
        [1116293.3750, 1007224.5000],
        [2029707.0000, 1758248.3750],
        [2499432.5000,  277865.9375],
        [4882443.5000,       0.0000],
        [3378751.5000,       0.0000],
        [1302355.2500, 1553800.2500],
        [2057393.1250,  949631.7500],
        [1361728.2500, 1101994.8750],
        [ 667572.6250,  929919.5625],
        [2138116.0000,  627198.6250],
        [ 584998.1875,  221376.4844],
        [1338293.3750,  599401.3750],
        [ 342801.1562,  738593.9375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 62.5
Top1 accuracy for validation set is 62.5 size is torch.Size([64, 1])
Epoch 291/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:53, 57.70s/it]  7%|▋         | 2/30 [01:00<11:55, 25.55s/it] 10%|█         | 3/30 [01:01<06:24, 14.22s/it] 13%|█▎        | 4/30 [01:02<03:51,  8.90s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.96s/it] 20%|██        | 6/30 [01:03<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.5756730238596597
Epoch 292/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:42, 57.34s/it]  7%|▋         | 2/30 [00:58<11:13, 24.05s/it] 10%|█         | 3/30 [00:58<06:02, 13.41s/it] 13%|█▎        | 4/30 [01:00<03:51,  8.92s/it] 17%|█▋        | 5/30 [01:01<02:29,  5.97s/it] 20%|██        | 6/30 [01:02<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:03<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:03<00:51,  2.33s/it] 30%|███       | 9/30 [01:04<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.27s/it] 40%|████      | 12/30 [01:06<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:07<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 2.5600024700164794
Epoch 293/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:02, 62.17s/it]  7%|▋         | 2/30 [01:02<12:09, 26.04s/it] 10%|█         | 3/30 [01:03<06:31, 14.49s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.07s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.07s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 2.5621602137883506
Epoch 294/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:00, 62.08s/it]  7%|▋         | 2/30 [01:02<12:08, 26.00s/it] 10%|█         | 3/30 [01:03<06:30, 14.47s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.5886439879735312
Epoch 295/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:42, 59.40s/it]  7%|▋         | 2/30 [01:01<11:53, 25.49s/it] 10%|█         | 3/30 [01:01<06:23, 14.19s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.88s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.95s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.5786100228627524
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 2.4777e-02,  3.0091e-03,  8.7385e-03,  ...,  4.9933e-04,
          4.6501e-03,  7.3462e-03],
        [-6.8879e-04,  1.8674e-02,  2.2553e-02,  ...,  1.3430e-02,
         -5.1242e-03, -1.6204e-02],
        [-3.2183e-02, -3.5953e-02,  1.9652e-02,  ...,  7.9511e-02,
         -1.4330e-02, -2.0463e-02],
        ...,
        [ 1.5624e-02, -7.3340e-03,  9.0983e-04,  ..., -2.3772e-02,
         -1.8018e-02, -1.2887e-02],
        [-2.8634e-02,  2.2061e-03, -8.3707e-03,  ...,  8.8483e-05,
          1.2294e-02, -1.2952e-02],
        [-3.7573e-02, -1.0522e-02,  1.4309e-02,  ...,  3.4380e-02,
          2.8854e-02, -4.1661e-02]], device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8973, 0.8550, 0.8522, 0.8501, 0.8495, 0.8452, 0.8446, 0.8405, 0.8396,
         0.8345],
        [0.9500, 0.9440, 0.9424, 0.9419, 0.9409, 0.9367, 0.9360, 0.9355, 0.9349,
         0.9334],
        [0.9121, 0.9089, 0.9019, 0.8841, 0.8774, 0.8649, 0.8553, 0.8501, 0.8490,
         0.8432],
        [0.9537, 0.9107, 0.9030, 0.8989, 0.8941, 0.8916, 0.8889, 0.8873, 0.8854,
         0.8847],
        [0.8557, 0.8482, 0.8430, 0.8275, 0.8247, 0.8176, 0.8079, 0.8048, 0.8033,
         0.8023],
        [0.8616, 0.8288, 0.8273, 0.8131, 0.8087, 0.8051, 0.7995, 0.7994, 0.7936,
         0.7918],
        [0.8939, 0.8938, 0.8861, 0.8800, 0.8748, 0.8715, 0.8715, 0.8701, 0.8662,
         0.8634],
        [0.9157, 0.8969, 0.8913, 0.8710, 0.8639, 0.8634, 0.8383, 0.8380, 0.8369,
         0.8325],
        [0.9184, 0.9104, 0.9074, 0.9072, 0.9025, 0.8949, 0.8943, 0.8941, 0.8919,
         0.8914],
        [0.9537, 0.8980, 0.8975, 0.8952, 0.8930, 0.8886, 0.8885, 0.8857, 0.8749,
         0.8723],
        [0.9535, 0.9169, 0.9144, 0.9118, 0.9098, 0.9089, 0.9062, 0.9007, 0.9002,
         0.8999],
        [0.9111, 0.8966, 0.8737, 0.8719, 0.8553, 0.8354, 0.8349, 0.8146, 0.8136,
         0.8110],
        [0.9417, 0.9176, 0.9147, 0.9129, 0.9084, 0.8970, 0.8911, 0.8895, 0.8766,
         0.8739],
        [0.9681, 0.9610, 0.9609, 0.9599, 0.9587, 0.9573, 0.9550, 0.9529, 0.9515,
         0.9512],
        [0.9459, 0.9370, 0.9361, 0.9357, 0.9348, 0.9287, 0.9272, 0.9266, 0.9265,
         0.9220],
        [0.9312, 0.9278, 0.9274, 0.9214, 0.9177, 0.9170, 0.9153, 0.9139, 0.9133,
         0.9105],
        [0.9646, 0.9587, 0.9527, 0.9467, 0.9381, 0.9374, 0.9359, 0.9349, 0.9304,
         0.9299],
        [0.9682, 0.9621, 0.9567, 0.9444, 0.9441, 0.9434, 0.9419, 0.9418, 0.9411,
         0.9383],
        [0.9367, 0.9339, 0.9253, 0.9060, 0.9054, 0.9047, 0.9037, 0.9037, 0.9033,
         0.9022],
        [0.9163, 0.9098, 0.9092, 0.9077, 0.9073, 0.9065, 0.9051, 0.9038, 0.9021,
         0.9015],
        [0.9593, 0.9498, 0.9472, 0.9465, 0.9452, 0.9425, 0.9410, 0.9405, 0.9383,
         0.9380],
        [0.9352, 0.9256, 0.9241, 0.9190, 0.9170, 0.9129, 0.9123, 0.9101, 0.9092,
         0.9086],
        [0.9738, 0.9504, 0.9484, 0.9466, 0.9410, 0.9410, 0.9342, 0.9237, 0.9237,
         0.9219],
        [0.9494, 0.9440, 0.9378, 0.9337, 0.9321, 0.9308, 0.9299, 0.9293, 0.9286,
         0.9267],
        [0.9628, 0.9102, 0.9088, 0.8907, 0.8895, 0.8881, 0.8830, 0.8822, 0.8820,
         0.8790],
        [0.9462, 0.9426, 0.9403, 0.9342, 0.9290, 0.9288, 0.9226, 0.9209, 0.9166,
         0.9140],
        [0.9383, 0.9374, 0.9353, 0.9292, 0.9216, 0.9145, 0.9139, 0.9124, 0.9095,
         0.9088],
        [0.9648, 0.9535, 0.9512, 0.9398, 0.9339, 0.9306, 0.9291, 0.9290, 0.9273,
         0.9272],
        [0.9535, 0.9464, 0.9407, 0.9377, 0.9330, 0.9328, 0.9321, 0.9314, 0.9286,
         0.9274],
        [0.9420, 0.9374, 0.9334, 0.9307, 0.9304, 0.9296, 0.9285, 0.9229, 0.9229,
         0.9218],
        [0.9580, 0.9532, 0.9509, 0.9502, 0.9502, 0.9498, 0.9426, 0.9425, 0.9411,
         0.9378],
        [0.9491, 0.9278, 0.9270, 0.9211, 0.9160, 0.9127, 0.9110, 0.9069, 0.9067,
         0.9047],
        [0.8889, 0.8795, 0.8727, 0.8684, 0.8660, 0.8631, 0.8629, 0.8622, 0.8613,
         0.8604],
        [0.9219, 0.9091, 0.9031, 0.9024, 0.9001, 0.8952, 0.8935, 0.8930, 0.8901,
         0.8900],
        [0.9316, 0.9293, 0.9249, 0.9242, 0.9130, 0.9101, 0.9067, 0.9066, 0.9011,
         0.9009],
        [0.9145, 0.9112, 0.9039, 0.8978, 0.8874, 0.8873, 0.8844, 0.8792, 0.8748,
         0.8742],
        [0.9489, 0.8739, 0.8625, 0.8606, 0.8600, 0.8588, 0.8506, 0.8470, 0.8446,
         0.8376],
        [0.9231, 0.9094, 0.9053, 0.9004, 0.8817, 0.8807, 0.8792, 0.8790, 0.8763,
         0.8746],
        [0.9071, 0.9045, 0.8931, 0.8796, 0.8760, 0.8679, 0.8653, 0.8637, 0.8556,
         0.8551],
        [0.9479, 0.9478, 0.9442, 0.9431, 0.9421, 0.9419, 0.9360, 0.9357, 0.9351,
         0.9349],
        [0.8948, 0.8934, 0.8894, 0.8856, 0.8725, 0.8711, 0.8687, 0.8568, 0.8475,
         0.8414],
        [0.9137, 0.8808, 0.8797, 0.8789, 0.8776, 0.8748, 0.8713, 0.8702, 0.8698,
         0.8677],
        [0.9004, 0.8928, 0.8918, 0.8746, 0.8719, 0.8687, 0.8659, 0.8613, 0.8534,
         0.8494],
        [0.9066, 0.8841, 0.8833, 0.8780, 0.8707, 0.8647, 0.8602, 0.8540, 0.8533,
         0.8517],
        [0.8736, 0.8506, 0.8501, 0.8423, 0.8364, 0.8347, 0.8277, 0.8252, 0.8123,
         0.8106],
        [0.8406, 0.8272, 0.8142, 0.7968, 0.7890, 0.7828, 0.7819, 0.7688, 0.7654,
         0.7630],
        [0.9172, 0.9157, 0.9091, 0.9038, 0.9029, 0.9007, 0.8971, 0.8955, 0.8945,
         0.8921],
        [0.8292, 0.8066, 0.7993, 0.7980, 0.7905, 0.7871, 0.7791, 0.7665, 0.7620,
         0.7540],
        [0.8662, 0.8485, 0.8262, 0.8229, 0.8201, 0.8186, 0.8114, 0.8106, 0.8062,
         0.8017],
        [0.9079, 0.8804, 0.8746, 0.8720, 0.8524, 0.8489, 0.8468, 0.8459, 0.8418,
         0.8384],
        [0.9000, 0.8619, 0.8608, 0.8587, 0.8472, 0.8377, 0.8322, 0.8236, 0.8227,
         0.8223],
        [0.9030, 0.8966, 0.8649, 0.8506, 0.8476, 0.8423, 0.8321, 0.8279, 0.8273,
         0.8234],
        [0.9391, 0.9238, 0.9193, 0.9083, 0.8872, 0.8822, 0.8736, 0.8693, 0.8664,
         0.8620],
        [0.9245, 0.8817, 0.8799, 0.8758, 0.8718, 0.8687, 0.8653, 0.8579, 0.8568,
         0.8520],
        [0.9406, 0.9221, 0.9180, 0.9167, 0.9116, 0.9111, 0.9111, 0.9076, 0.9060,
         0.9045],
        [0.9188, 0.8947, 0.8889, 0.8869, 0.8860, 0.8846, 0.8843, 0.8829, 0.8812,
         0.8811],
        [0.9284, 0.8949, 0.8866, 0.8843, 0.8786, 0.8667, 0.8544, 0.8479, 0.8475,
         0.8434],
        [0.9443, 0.8966, 0.8856, 0.8833, 0.8719, 0.8601, 0.8580, 0.8566, 0.8533,
         0.8491],
        [0.9240, 0.8741, 0.8726, 0.8663, 0.8555, 0.8538, 0.8503, 0.8482, 0.8429,
         0.8420],
        [0.9097, 0.8363, 0.8359, 0.8308, 0.8206, 0.8150, 0.8147, 0.8132, 0.8116,
         0.8105],
        [0.8867, 0.8840, 0.8774, 0.8767, 0.8745, 0.8741, 0.8730, 0.8696, 0.8689,
         0.8685],
        [0.8613, 0.7966, 0.7950, 0.7910, 0.7855, 0.7599, 0.7571, 0.7548, 0.7540,
         0.7461],
        [0.8938, 0.8700, 0.8696, 0.8461, 0.8396, 0.8379, 0.8312, 0.8280, 0.8268,
         0.8233],
        [0.8468, 0.8363, 0.8246, 0.8071, 0.8066, 0.7894, 0.7891, 0.7878, 0.7869,
         0.7842]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 0, 1, 1, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 1, 0, 0, 1],
        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 1, 0, 1, 0, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 368964.5000,  201639.0156,  193720.8281,  188025.3281,  186384.0938,
          175285.4531,  173795.5156,  164031.7656,  161829.4062,  150551.1406],
        [ 783086.3125,  718995.2500,  703184.8750,  697849.6875,  688231.9375,
          647425.0625,  641148.6250,  636576.6875,  631383.6875,  618124.3750],
        [ 455807.2188,  435457.2188,  393995.6562,  305714.6250,  277518.2188,
          232227.6562,  202651.6875,  187900.0469,  185170.2969,  170247.8438],
        [ 826050.0625,  447040.5312,  400412.4375,  377485.9375,  352664.5312,
          340368.9062,  327350.5000,  319834.2188,  311137.1875,  308231.4062],
        [ 203582.0781,  183090.3125,  169907.7188,  136068.9688,  130784.6328,
          118241.4297,  102854.1406,   98399.4922,   96351.8125,   95008.5781],
        [ 221630.8125,  138743.4219,  135792.1875,  110847.1641,  104118.3672,
           98792.3438,   91268.4375,   91069.3281,   83912.4453,   81787.3750],
        [ 351730.4375,  350998.5938,  314398.8438,  288199.2188,  267730.9688,
          255091.8438,  255070.2031,  250087.0781,  236675.2031,  227267.5625],
        [ 480217.7812,  367074.1875,  338648.0312,  253558.0000,  229017.0781,
          227247.1875,  158944.1094,  158076.5625,  155601.7656,  146243.3438],
        [ 498763.9688,  444808.7500,  426008.0625,  424894.2812,  397669.0312,
          356408.3750,  353699.5938,  352535.0625,  341736.9062,  339102.4375],
        [ 825711.3750,  372838.4062,  370138.7812,  358269.4375,  347220.4688,
          325760.3438,  325377.8438,  312700.1250,  267822.6562,  258273.1094],
        [ 824112.0000,  488347.0625,  471259.2812,  453886.4375,  441450.3438,
          435326.4375,  419151.8750,  387517.2188,  384624.4062,  382726.8438],
        [ 449316.0938,  365087.3125,  263213.9375,  256656.0312,  202621.9375,
          152386.2344,  151278.2500,  113227.7500,  111550.8984,  107553.9375],
        [ 695912.4375,  493184.1250,  473240.0312,  461175.2812,  432576.4062,
          367197.7500,  337870.5938,  330137.3125,  274408.7812,  264341.1875],
        [1014884.1250,  916429.1250,  915332.0625,  902687.6875,  887501.9375,
          869599.3750,  841068.5625,  816893.5625,  800221.4375,  797288.6875],
        [ 738761.6875,  650403.5625,  642105.6250,  638294.0000,  630724.7500,
          577604.3750,  565498.2500,  561095.3125,  559813.5625,  525418.0000],
        [ 598833.1250,  570462.5625,  567549.1250,  521025.9375,  494204.3750,
          489015.8125,  477223.3438,  467997.0312,  463969.4375,  445786.7500],
        [ 964783.7500,  887288.6875,  814260.7500,  747605.3750,  661278.8125,
          654240.0625,  640782.5000,  631176.5625,  592291.8125,  588062.1250],
        [1016195.5625,  931281.6875,  862256.3125,  722702.0000,  720180.3750,
          713245.5000,  698193.8750,  696862.8125,  690103.7500,  662974.2500],
        [ 647962.4375,  622261.0625,  550259.5625,  418050.0938,  414567.0312,
          410028.7500,  404393.1875,  404319.1562,  402143.3750,  396040.0938],
        [ 483831.9062,  441464.2500,  437428.8438,  428243.9375,  425438.8438,
          421052.1250,  412633.5000,  405109.5938,  395055.5625,  391906.0625],
        [ 894472.9375,  781269.1875,  752664.5625,  744816.4375,  731915.1250,
          704004.1875,  689090.3125,  684116.5625,  663299.9375,  659574.5000],
        [ 634010.2500,  552776.8750,  541072.8125,  502882.5000,  489052.6250,
          461320.4375,  457263.1250,  443152.3438,  437473.9375,  433939.5000],
        [1101352.2500,  788455.5000,  765178.1875,  746017.8750,  688961.5000,
          688767.7500,  625376.0625,  538358.7500,  537946.6250,  524754.0000],
        [ 776410.0625,  718854.6875,  658528.6250,  620859.1875,  606554.5000,
          595540.1250,  587805.3125,  582736.7500,  576956.9375,  561985.9375],
        [ 940475.4375,  443757.9688,  435158.7500,  335716.6875,  330046.0000,
          323365.5312,  300923.5938,  297528.3438,  296620.0312,  284205.8438],
        [ 742097.3750,  705299.1250,  681850.5000,  624701.2500,  580630.9375,
          579113.5000,  529887.0000,  516601.1562,  486170.4688,  468445.7812],
        [ 662873.7500,  654697.5625,  634653.3125,  582392.2500,  522078.4375,
          471619.4375,  467976.0312,  457725.6562,  439221.3438,  435140.0625],
        [ 968404.6250,  823038.3125,  796977.7500,  677118.7500,  622315.6250,
          594072.0625,  581442.6875,  580427.7500,  566696.1875,  565630.9375],
        [ 823620.9375,  744068.1875,  686168.8750,  657194.1875,  614725.9375,
          612620.2500,  606566.6250,  600324.9375,  577047.1875,  566992.4375],
        [ 698684.8125,  654103.4375,  617962.9375,  594751.7500,  592339.8125,
          585339.5000,  575915.1875,  531951.2500,  531944.1250,  523811.0000],
        [ 878861.2500,  819882.0000,  793923.6250,  785222.8125,  785146.4375,
          781345.1875,  704872.8125,  703904.8125,  689853.0625,  658359.0625],
        [ 773127.3125,  570839.6875,  563814.8750,  518394.1562,  482021.8750,
          459847.2188,  448831.7500,  423425.9375,  422154.5938,  410436.4062],
        [ 327364.2188,  286292.3438,  259779.9844,  244278.2188,  236060.9219,
          226463.5938,  225758.2344,  223471.1406,  220771.3906,  217665.4062],
        [ 524686.4375,  437006.0625,  400930.9688,  397021.4375,  383922.2500,
          357934.0625,  349708.5625,  347137.0312,  333096.4688,  332643.1875],
        [ 602444.6875,  582586.6875,  547685.8750,  541957.0000,  462074.6875,
          443357.3750,  421755.0312,  421253.3438,  389670.7188,  388248.9062],
        [ 472059.9375,  449802.7188,  405208.5312,  371490.0625,  320237.4062,
          319828.4062,  306732.6562,  284842.6875,  267477.7812,  265200.9375],
        [ 771072.1875,  264154.9375,  224334.4375,  218434.1719,  216681.1875,
          212772.6875,  189281.6562,  179843.1250,  173813.4219,  157305.3906],
        [ 533722.6250,  438594.7188,  413870.5938,  385908.8750,  295461.4062,
          291259.0312,  285055.2188,  284122.1250,  273199.0312,  266772.6250],
        [ 424280.4062,  408794.9688,  347691.3438,  286632.2188,  272138.5625,
          242368.9219,  233573.4531,  228450.5625,  203333.5312,  201978.3281],
        [ 760194.3125,  759715.9375,  720856.5625,  709617.0625,  699969.3125,
          697503.0625,  641264.1875,  638823.2500,  633138.9375,  631583.6250],
        [ 355921.6562,  348897.7500,  329411.7812,  312187.3125,  258868.4062,
          253916.1562,  245083.7188,  206763.5781,  181237.0469,  166005.3594],
        [ 466548.2812,  291612.2812,  287173.9375,  283595.8750,  278640.2500,
          267420.4062,  254405.7656,  250598.2500,  249173.6250,  241797.7344],
        [ 385719.7812,  346010.2812,  340975.5000,  266800.3750,  256620.7812,
          245294.4062,  235613.8125,  220702.9844,  197193.2344,  186266.4531],
        [ 421193.0938,  305747.8750,  302254.0938,  280146.2188,  252237.5469,
          231511.4375,  217173.5625,  198925.2812,  196947.9531,  192292.6719],
        [ 262953.0312,  189361.0938,  187914.7344,  168310.6250,  154574.1406,
          150959.8750,  136458.5625,  131739.3906,  109537.3672,  106885.0078],
        [ 164234.6406,  135573.5156,  112517.7344,   87798.2031,   78553.0312,
           71908.8828,   70977.9844,   58854.9961,   56086.4453,   54187.6094],
        [ 490607.7500,  479792.5312,  436794.4062,  405189.1875,  400013.5938,
          387157.0625,  368074.6250,  359682.6250,  354307.9375,  342686.6250],
        [ 139390.2344,  101052.2188,   90963.0000,   89383.4688,   80278.5469,
           76430.5469,   68193.2734,   56925.2539,   53438.9062,   47635.2773],
        [ 236718.3281,  183816.3906,  133704.9062,  127445.5547,  122527.5469,
          119869.2266,  108097.8047,  106881.5312,  100429.8594,   94129.1562],
        [ 429461.0625,  289751.0625,  266864.0000,  256961.6719,  194262.1562,
          184948.2812,  179280.4062,  177099.1562,  167100.8750,  159000.2031],
        [ 383417.2812,  222658.5156,  219159.8906,  212717.2969,  180445.2812,
          157506.3906,  145630.0156,  128792.7422,  127203.7969,  126475.0000],
        [ 400158.5938,  365308.4688,  232258.6719,  189400.1094,  181342.3438,
          168134.0000,  145359.7188,  136939.3594,  135777.0469,  128423.9375],
        [ 670881.3125,  538740.3750,  505610.0938,  431492.8125,  319293.9062,
          297415.4375,  263143.9062,  247408.8438,  237406.7344,  222881.1719],
        [ 544284.6875,  295172.4375,  287658.0312,  271366.5312,  256334.5938,
          245124.8594,  233664.1250,  210266.9219,  206967.3594,  193053.5781],
        [ 685278.8125,  525798.9375,  496045.8750,  486688.6250,  452731.3125,
          449587.8750,  449587.8750,  427750.4375,  417651.1875,  409227.9375],
        [ 501370.7500,  355776.0625,  327390.1250,  318015.4062,  313937.7500,
          307747.9062,  306252.7188,  300512.6250,  293062.2812,  292624.6875],
        [ 575525.3750,  356758.3438,  316687.9688,  306581.4688,  282633.5938,
          238326.8125,  199911.3594,  182081.7812,  181288.3906,  170866.5938],
        [ 722489.0625,  365333.5625,  312413.3750,  302226.1562,  256655.5312,
          216912.5469,  210540.6250,  206424.5000,  196779.1719,  185468.0938],
        [ 540572.0625,  264845.3438,  259236.5156,  237018.3125,  203175.3438,
          198272.0469,  188470.3906,  183012.9844,  169548.7031,  167427.8750],
        [ 440324.3438,  154336.1094,  153492.9688,  142644.4844,  123438.3906,
          113801.1797,  113425.1953,  111051.7031,  108545.7344,  106849.3281],
        [ 317317.0938,  304993.2812,  277563.7188,  274851.9375,  266443.6250,
          265057.5938,  260802.4688,  248370.0781,  246001.2031,  244651.4844],
        [ 220723.6094,   87495.8672,   85589.5000,   80826.2031,   74745.8828,
           51801.7344,   49831.0625,   48171.8828,   47652.7266,   42541.8320],
        [ 350815.8750,  249932.8281,  248321.9844,  177503.2656,  161748.5469,
          158018.5312,  143571.8438,  137071.4531,  134717.3438,  128178.4844],
        [ 179299.7344,  154359.3594,  130562.0625,  101770.6094,  100941.4531,
           78987.9766,   78617.1797,   77204.0078,   76219.6797,   73348.0000]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[368964.5000,      0.0000],
         [201639.0156,      0.0000],
         [193720.8281,      0.0000],
         ...,
         [     0.0000, 164031.7656],
         [161829.4062,      0.0000],
         [150551.1406,      0.0000]],

        [[783086.3125,      0.0000],
         [718995.2500,      0.0000],
         [703184.8750,      0.0000],
         ...,
         [636576.6875,      0.0000],
         [631383.6875,      0.0000],
         [618124.3750,      0.0000]],

        [[455807.2188,      0.0000],
         [435457.2188,      0.0000],
         [393995.6562,      0.0000],
         ...,
         [187900.0469,      0.0000],
         [185170.2969,      0.0000],
         [     0.0000, 170247.8438]],

        ...,

        [[     0.0000, 220723.6094],
         [ 87495.8672,      0.0000],
         [ 85589.5000,      0.0000],
         ...,
         [ 48171.8828,      0.0000],
         [ 47652.7266,      0.0000],
         [ 42541.8320,      0.0000]],

        [[350815.8750,      0.0000],
         [249932.8281,      0.0000],
         [     0.0000, 248321.9844],
         ...,
         [137071.4531,      0.0000],
         [134717.3438,      0.0000],
         [128178.4844,      0.0000]],

        [[     0.0000, 179299.7344],
         [     0.0000, 154359.3594],
         [     0.0000, 130562.0625],
         ...,
         [     0.0000,  77204.0078],
         [ 76219.6797,      0.0000],
         [     0.0000,  73348.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1800195.3750,  164031.7656],
        [6766006.5000,       0.0000],
        [2676442.7500,  170247.8438],
        [4010575.5000,       0.0000],
        [ 556613.5000,  777675.6875],
        [1019218.5000,  138743.4219],
        [2215120.2500,  582129.8125],
        [2514628.0000,       0.0000],
        [3509618.5000,  426008.0625],
        [3764112.5000,       0.0000],
        [4688401.5000,       0.0000],
        [2172892.2500,       0.0000],
        [4130043.5000,       0.0000],
        [8761907.0000,       0.0000],
        [6089719.0000,       0.0000],
        [5096067.5000,       0.0000],
        [7181770.0000,       0.0000],
        [7713996.0000,       0.0000],
        [4670025.0000,       0.0000],
        [4242164.5000,       0.0000],
        [7305224.0000,       0.0000],
        [4952944.5000,       0.0000],
        [7005168.5000,       0.0000],
        [6286232.0000,       0.0000],
        [3987798.5000,       0.0000],
        [5914797.0000,       0.0000],
        [5328378.0000,       0.0000],
        [6776124.5000,       0.0000],
        [6489329.5000,       0.0000],
        [5906804.0000,       0.0000],
        [7601371.0000,       0.0000],
        [5072893.5000,       0.0000],
        [2467905.5000,       0.0000],
        [3066134.0000,  797952.3750],
        [2606242.5000, 2194792.0000],
        [1302718.7500, 2160162.2500],
        [1423005.0000, 1184688.2500],
        [1825617.7500, 1642348.3750],
        [1162763.5000, 1686478.7500],
        [6892666.0000,       0.0000],
        [2658293.0000,       0.0000],
        [2870966.5000,       0.0000],
        [2681197.5000,       0.0000],
        [2598429.7500,       0.0000],
        [ 395850.8750, 1202843.0000],
        [ 422280.6875,  468412.3750],
        [3533698.5000,  490607.7500],
        [ 529884.7500,  273806.0312],
        [ 577153.7500,  756466.6250],
        [ 755562.1250, 1549166.7500],
        [1620024.7500,  283981.3750],
        [1085376.5000,  997725.7500],
        [1999205.6250, 1735069.0000],
        [2472526.7500,  271366.5312],
        [4800349.0000,       0.0000],
        [3316690.5000,       0.0000],
        [1287102.7500, 1523558.8750],
        [2040840.1250,  934402.5000],
        [1330855.5000, 1080724.0000],
        [ 655744.3125,  912165.0625],
        [2083742.0000,  622310.3750],
        [ 568656.6875,  220723.6094],
        [1306036.3750,  583843.7500],
        [ 256607.4688,  794702.5625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 62.5
Top1 accuracy for validation set is 62.5 size is torch.Size([64, 1])
Epoch 296/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:01, 62.11s/it]  7%|▋         | 2/30 [01:03<12:25, 26.63s/it] 10%|█         | 3/30 [01:04<06:39, 14.81s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.26s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.19s/it] 20%|██        | 6/30 [01:06<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.17s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.40s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.83s/it]
Epoch loss is 2.478830909729004
Epoch 297/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:55<26:58, 55.81s/it]  7%|▋         | 2/30 [00:59<11:43, 25.11s/it] 10%|█         | 3/30 [01:00<06:17, 13.99s/it] 13%|█▎        | 4/30 [01:00<03:47,  8.76s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.87s/it] 20%|██        | 6/30 [01:02<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.30s/it] 30%|███       | 9/30 [01:04<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 2.554883941014608
Epoch 298/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:57, 59.90s/it]  7%|▋         | 2/30 [01:00<11:43, 25.14s/it] 10%|█         | 3/30 [01:01<06:18, 14.00s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.77s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.5698960463205975
Epoch 299/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:51, 61.78s/it]  7%|▋         | 2/30 [01:02<12:04, 25.88s/it] 10%|█         | 3/30 [01:03<06:28, 14.41s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.03s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.5343668778737385
Epoch 300/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:28, 56.84s/it]  7%|▋         | 2/30 [00:59<11:38, 24.95s/it] 10%|█         | 3/30 [01:00<06:15, 13.90s/it] 13%|█▎        | 4/30 [01:00<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.84s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.5683359066645304
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0248,  0.0027,  0.0091,  ...,  0.0003,  0.0053,  0.0068],
        [-0.0008,  0.0179,  0.0231,  ...,  0.0139, -0.0047, -0.0162],
        [-0.0320, -0.0363,  0.0202,  ...,  0.0795, -0.0137, -0.0198],
        ...,
        [ 0.0151, -0.0070,  0.0015,  ..., -0.0236, -0.0172, -0.0124],
        [-0.0289,  0.0028, -0.0081,  ...,  0.0005,  0.0125, -0.0129],
        [-0.0373, -0.0108,  0.0146,  ...,  0.0353,  0.0295, -0.0405]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8961, 0.8553, 0.8511, 0.8490, 0.8489, 0.8450, 0.8439, 0.8395, 0.8391,
         0.8342],
        [0.9496, 0.9430, 0.9412, 0.9410, 0.9398, 0.9355, 0.9346, 0.9345, 0.9336,
         0.9331],
        [0.9119, 0.9081, 0.9006, 0.8836, 0.8764, 0.8638, 0.8554, 0.8498, 0.8486,
         0.8420],
        [0.9538, 0.9092, 0.9022, 0.8975, 0.8932, 0.8900, 0.8876, 0.8865, 0.8842,
         0.8841],
        [0.8544, 0.8486, 0.8422, 0.8262, 0.8244, 0.8176, 0.8075, 0.8053, 0.8019,
         0.8014],
        [0.8611, 0.8266, 0.8253, 0.8128, 0.8067, 0.8052, 0.7975, 0.7962, 0.7900,
         0.7888],
        [0.8915, 0.8912, 0.8823, 0.8779, 0.8725, 0.8689, 0.8688, 0.8688, 0.8646,
         0.8630],
        [0.9143, 0.8954, 0.8893, 0.8703, 0.8631, 0.8612, 0.8377, 0.8365, 0.8355,
         0.8307],
        [0.9180, 0.9099, 0.9058, 0.9049, 0.9011, 0.8940, 0.8934, 0.8934, 0.8914,
         0.8905],
        [0.9535, 0.8969, 0.8968, 0.8948, 0.8918, 0.8873, 0.8871, 0.8850, 0.8742,
         0.8718],
        [0.9526, 0.9154, 0.9127, 0.9105, 0.9088, 0.9066, 0.9051, 0.8993, 0.8990,
         0.8985],
        [0.9096, 0.8951, 0.8727, 0.8708, 0.8533, 0.8332, 0.8325, 0.8120, 0.8116,
         0.8103],
        [0.9400, 0.9171, 0.9144, 0.9117, 0.9069, 0.8960, 0.8899, 0.8881, 0.8760,
         0.8721],
        [0.9676, 0.9608, 0.9604, 0.9591, 0.9582, 0.9572, 0.9544, 0.9526, 0.9508,
         0.9505],
        [0.9458, 0.9362, 0.9353, 0.9352, 0.9347, 0.9278, 0.9266, 0.9259, 0.9255,
         0.9211],
        [0.9308, 0.9274, 0.9271, 0.9208, 0.9172, 0.9169, 0.9151, 0.9126, 0.9125,
         0.9106],
        [0.9636, 0.9579, 0.9516, 0.9458, 0.9367, 0.9360, 0.9354, 0.9339, 0.9295,
         0.9288],
        [0.9679, 0.9619, 0.9563, 0.9438, 0.9431, 0.9430, 0.9414, 0.9414, 0.9402,
         0.9374],
        [0.9359, 0.9327, 0.9237, 0.9055, 0.9043, 0.9040, 0.9029, 0.9024, 0.9018,
         0.9011],
        [0.9153, 0.9080, 0.9078, 0.9068, 0.9068, 0.9058, 0.9043, 0.9030, 0.9013,
         0.9013],
        [0.9582, 0.9486, 0.9463, 0.9456, 0.9438, 0.9416, 0.9403, 0.9397, 0.9376,
         0.9375],
        [0.9342, 0.9242, 0.9229, 0.9185, 0.9167, 0.9123, 0.9122, 0.9099, 0.9073,
         0.9073],
        [0.9735, 0.9502, 0.9477, 0.9464, 0.9404, 0.9401, 0.9336, 0.9233, 0.9228,
         0.9211],
        [0.9487, 0.9433, 0.9371, 0.9328, 0.9316, 0.9303, 0.9294, 0.9286, 0.9280,
         0.9261],
        [0.9623, 0.9093, 0.9086, 0.8903, 0.8880, 0.8858, 0.8815, 0.8812, 0.8807,
         0.8780],
        [0.9456, 0.9421, 0.9397, 0.9334, 0.9283, 0.9282, 0.9225, 0.9204, 0.9157,
         0.9129],
        [0.9371, 0.9364, 0.9346, 0.9283, 0.9206, 0.9136, 0.9132, 0.9112, 0.9089,
         0.9073],
        [0.9645, 0.9530, 0.9506, 0.9390, 0.9335, 0.9300, 0.9283, 0.9275, 0.9267,
         0.9259],
        [0.9530, 0.9458, 0.9395, 0.9373, 0.9324, 0.9319, 0.9312, 0.9311, 0.9285,
         0.9265],
        [0.9411, 0.9365, 0.9328, 0.9300, 0.9298, 0.9292, 0.9279, 0.9225, 0.9220,
         0.9211],
        [0.9572, 0.9525, 0.9502, 0.9499, 0.9498, 0.9493, 0.9420, 0.9419, 0.9402,
         0.9374],
        [0.9485, 0.9274, 0.9266, 0.9206, 0.9149, 0.9115, 0.9100, 0.9066, 0.9062,
         0.9038],
        [0.8882, 0.8790, 0.8732, 0.8673, 0.8654, 0.8634, 0.8625, 0.8616, 0.8607,
         0.8599],
        [0.9209, 0.9081, 0.9016, 0.9016, 0.8988, 0.8948, 0.8924, 0.8920, 0.8896,
         0.8894],
        [0.9308, 0.9278, 0.9244, 0.9236, 0.9125, 0.9095, 0.9056, 0.9051, 0.9000,
         0.8996],
        [0.9138, 0.9102, 0.9033, 0.8968, 0.8876, 0.8866, 0.8838, 0.8795, 0.8731,
         0.8724],
        [0.9488, 0.8736, 0.8633, 0.8596, 0.8589, 0.8588, 0.8485, 0.8454, 0.8421,
         0.8356],
        [0.9215, 0.9076, 0.9042, 0.8990, 0.8810, 0.8797, 0.8791, 0.8785, 0.8744,
         0.8731],
        [0.9060, 0.9036, 0.8919, 0.8785, 0.8744, 0.8655, 0.8640, 0.8631, 0.8541,
         0.8528],
        [0.9471, 0.9467, 0.9432, 0.9422, 0.9413, 0.9413, 0.9353, 0.9351, 0.9343,
         0.9337],
        [0.8941, 0.8925, 0.8886, 0.8844, 0.8713, 0.8706, 0.8681, 0.8558, 0.8466,
         0.8406],
        [0.9126, 0.8796, 0.8777, 0.8774, 0.8762, 0.8730, 0.8694, 0.8694, 0.8690,
         0.8661],
        [0.8994, 0.8922, 0.8905, 0.8740, 0.8709, 0.8670, 0.8649, 0.8602, 0.8522,
         0.8478],
        [0.9050, 0.8837, 0.8813, 0.8767, 0.8696, 0.8639, 0.8593, 0.8537, 0.8516,
         0.8510],
        [0.8725, 0.8489, 0.8489, 0.8415, 0.8347, 0.8334, 0.8256, 0.8227, 0.8098,
         0.8080],
        [0.8404, 0.8261, 0.8128, 0.7960, 0.7885, 0.7825, 0.7818, 0.7665, 0.7656,
         0.7631],
        [0.9161, 0.9135, 0.9068, 0.9022, 0.9016, 0.8992, 0.8951, 0.8936, 0.8924,
         0.8905],
        [0.8276, 0.8045, 0.7986, 0.7968, 0.7871, 0.7838, 0.7784, 0.7661, 0.7593,
         0.7518],
        [0.8637, 0.8477, 0.8246, 0.8223, 0.8195, 0.8172, 0.8109, 0.8094, 0.8045,
         0.7999],
        [0.9070, 0.8793, 0.8717, 0.8701, 0.8509, 0.8480, 0.8451, 0.8440, 0.8396,
         0.8366],
        [0.8989, 0.8622, 0.8609, 0.8587, 0.8470, 0.8376, 0.8313, 0.8234, 0.8225,
         0.8214],
        [0.9022, 0.8960, 0.8637, 0.8481, 0.8457, 0.8410, 0.8296, 0.8258, 0.8252,
         0.8225],
        [0.9383, 0.9229, 0.9186, 0.9075, 0.8859, 0.8807, 0.8732, 0.8690, 0.8651,
         0.8600],
        [0.9236, 0.8811, 0.8797, 0.8742, 0.8709, 0.8676, 0.8648, 0.8571, 0.8558,
         0.8507],
        [0.9398, 0.9208, 0.9170, 0.9156, 0.9101, 0.9100, 0.9100, 0.9069, 0.9053,
         0.9032],
        [0.9178, 0.8939, 0.8874, 0.8867, 0.8842, 0.8834, 0.8829, 0.8823, 0.8803,
         0.8794],
        [0.9273, 0.8934, 0.8862, 0.8834, 0.8775, 0.8665, 0.8533, 0.8466, 0.8462,
         0.8421],
        [0.9443, 0.8956, 0.8843, 0.8831, 0.8709, 0.8599, 0.8567, 0.8554, 0.8526,
         0.8472],
        [0.9230, 0.8725, 0.8716, 0.8649, 0.8535, 0.8513, 0.8482, 0.8470, 0.8420,
         0.8408],
        [0.9088, 0.8352, 0.8350, 0.8290, 0.8191, 0.8140, 0.8138, 0.8118, 0.8093,
         0.8086],
        [0.8854, 0.8842, 0.8762, 0.8749, 0.8733, 0.8715, 0.8702, 0.8679, 0.8675,
         0.8673],
        [0.8611, 0.7955, 0.7944, 0.7891, 0.7841, 0.7572, 0.7542, 0.7517, 0.7508,
         0.7432],
        [0.8932, 0.8685, 0.8685, 0.8435, 0.8369, 0.8358, 0.8305, 0.8251, 0.8243,
         0.8215],
        [0.8467, 0.8342, 0.8221, 0.8054, 0.8047, 0.7875, 0.7863, 0.7853, 0.7847,
         0.7830]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 0, 1, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],
        [1, 1, 0, 1, 0, 0, 1, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 1, 0, 0, 1],
        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 1, 1, 0, 0, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 362649.5625,  202654.4062,  190585.6406,  185022.3594,  184856.4062,
          174689.1875,  172007.4531,  161658.7969,  160726.9219,  149880.5625],
        [ 778411.0625,  708898.0625,  691118.7500,  689292.1250,  676725.6250,
          637219.9375,  628576.1875,  627605.8125,  620236.0000,  615491.4375],
        [ 454855.8125,  430344.5938,  386737.8438,  303214.6250,  273596.9062,
          228518.3281,  202909.6875,  187248.7188,  184114.2812,  167363.0625],
        [ 826579.6250,  437344.5938,  395720.7188,  370026.9062,  348158.1875,
          332347.9688,  321407.0938,  316185.5000,  306194.2812,  305735.5938],
        [ 199939.9688,  183970.8906,  168038.7656,  133693.7031,  130231.1484,
          118169.9609,  102262.9922,   99145.6250,   94426.3125,   93751.5391],
        [ 219862.2344,  134332.7188,  131868.9844,  110415.7578,  101072.4609,
           98994.4531,   88649.9141,   87050.1719,   79642.5000,   78360.5859],
        [ 339815.5938,  338338.7500,  297984.3750,  279879.9688,  259033.8594,
          245791.7812,  245734.3750,  245474.1250,  231161.5312,  225927.7344],
        [ 470346.0625,  359018.1250,  329260.7188,  251074.5156,  226547.6094,
          220467.7812,  157586.9219,  154747.4375,  152571.7969,  142515.8594],
        [ 495919.6250,  441566.1250,  416443.2812,  411518.9688,  389852.8750,
          352041.1875,  348862.8125,  348778.9688,  339105.3125,  334968.6562],
        [ 823666.5000,  366777.4375,  366462.4062,  355978.6875,  341240.5938,
          319869.6250,  318980.6875,  309432.7812,  265179.1875,  256282.0469],
        [ 813069.6250,  477836.3125,  460078.3750,  445899.4375,  434945.4688,
          421737.7188,  412682.2812,  379615.5312,  377978.3750,  375240.5000],
        [ 439950.7812,  357656.3125,  259838.7031,  252628.0312,  196855.2031,
          147657.6719,  146286.3125,  109128.1250,  108553.5000,  106490.7422],
        [ 678757.0625,  489439.9062,  471184.2500,  453184.9062,  423124.7812,
          362175.7188,  331802.2812,  323342.4375,  272379.5312,  257611.4062],
        [1008074.0000,  914561.6250,  908970.5625,  892621.2500,  880212.5000,
          868531.0625,  833775.5000,  813304.6250,  792037.5625,  788517.9375],
        [ 737930.0625,  643043.8125,  634594.5625,  634118.4375,  629200.5625,
          570141.1250,  560533.7500,  554886.9375,  552256.2500,  518641.3750],
        [ 595658.8750,  567607.5625,  565034.6250,  516402.1562,  490576.8750,
          488049.0625,  475630.8125,  459302.0000,  458779.2812,  445975.1250],
        [ 951263.9375,  876399.6875,  801823.3125,  737306.1250,  647486.1875,
          641005.5000,  636090.0000,  622458.1250,  584659.9375,  578485.8750],
        [1011275.6875,  927957.0625,  857667.7500,  716800.3125,  709861.4375,
          708478.3750,  693303.1250,  693014.2500,  680884.8750,  653963.6875],
        [ 640299.2500,  611444.0625,  537969.2500,  415043.6875,  407666.3438,
          406149.7812,  399614.3750,  397021.8125,  393274.5312,  389704.1875],
        [ 477446.8438,  430179.6562,  428747.3750,  422885.5312,  422727.0938,
          416798.5000,  408061.1250,  400297.1562,  390901.2500,  390596.0312],
        [ 881393.5000,  768051.3750,  743197.2500,  735787.4375,  716645.1875,
          695237.0625,  681864.1250,  675768.5625,  655785.5000,  655219.7500],
        [ 625166.7500,  541616.9375,  532211.5000,  499665.2500,  486664.5000,
          457274.5000,  456574.2188,  441561.0938,  425797.6875,  425678.7188],
        [1096701.8750,  785772.6875,  758318.2500,  744173.1875,  683042.8125,
          680177.5000,  619833.8750,  534980.8750,  530993.8125,  518284.3750],
        [ 769486.8750,  711684.8125,  651558.8750,  612473.0000,  602057.0000,
          591640.8750,  583577.6250,  577294.8750,  571959.0000,  556869.0625],
        [ 933406.7500,  437748.5312,  433623.8750,  333768.0938,  322870.6562,
          313255.5938,  294472.9062,  293141.1250,  290973.3438,  280036.4375],
        [ 735679.3750,  699932.5625,  676441.7500,  618333.6875,  574428.1250,
          574153.7500,  528767.8125,  513244.2500,  479902.7812,  461006.8750],
        [ 651833.6250,  644755.1875,  628272.9375,  574364.0625,  514463.9688,
          465661.9375,  462974.9688,  450263.2188,  435713.5312,  425748.9375],
        [ 963141.8125,  818137.0625,  790370.0000,  669219.2500,  619338.7500,
          588808.5000,  574547.5625,  568410.9375,  561357.5625,  555027.1250],
        [ 817827.3750,  737484.6875,  674605.6875,  653111.1250,  609461.8125,
          604827.9375,  599109.5625,  598204.0625,  576130.0000,  560278.3125],
        [ 689887.3125,  645925.7500,  612963.2500,  588961.2500,  586996.4375,
          582323.3750,  571196.3750,  529221.3750,  525278.6875,  518532.0938],
        [ 868216.4375,  811737.8125,  785951.0000,  782178.0000,  781691.0000,
          775940.7500,  698970.0000,  697874.3125,  680618.6875,  653903.2500],
        [ 767200.7500,  566927.5625,  561207.7500,  514883.1250,  474589.1562,
          452144.5312,  442391.4688,  421390.7812,  418819.0312,  405031.1562],
        [ 323803.7500,  284297.7812,  261411.0312,  240544.0312,  233888.8594,
          227266.9062,  224360.7656,  221620.8906,  218722.0469,  216148.5156],
        [ 516938.2812,  430634.8438,  392170.4062,  392152.4375,  377058.1562,
          355815.7500,  344019.0000,  342337.7812,  330359.3438,  329619.8125],
        [ 595860.5625,  570586.6250,  543373.4375,  536926.1875,  458721.0938,
          439158.5000,  415345.8125,  412752.7500,  383270.6875,  381182.7500],
        [ 467044.6562,  443714.8125,  402173.6875,  366124.2500,  321061.5312,
          316481.7812,  304438.2500,  286325.4062,  261153.8750,  258587.8438],
        [ 769754.8125,  262957.5000,  226941.6094,  215263.5312,  213101.6562,
          213003.1094,  183816.9062,  175805.2656,  167613.2031,  152889.0156],
        [ 521682.7812,  427222.0938,  407148.8125,  378197.9688,  292363.2812,
          286780.6875,  284571.4688,  281918.8438,  265928.5625,  261213.6562],
        [ 418040.0938,  403937.5938,  341802.4375,  282154.7500,  266159.9375,
          234208.9375,  229216.1250,  226383.6875,  199081.0938,  195388.1094],
        [ 751989.3750,  747296.7500,  710622.7500,  701247.5000,  691921.9375,
          691580.2500,  634923.2500,  633584.0625,  625994.8125,  620775.1250],
        [ 352700.1875,  344610.3750,  325928.4375,  307013.8750,  254403.3281,
          251932.4688,  242978.0312,  203951.2969,  178757.3125,  164154.4531],
        [ 458985.4062,  286629.1875,  278717.0625,  277590.7188,  273117.7500,
          260645.2969,  247851.1719,  247796.5781,  246442.1875,  236370.6875],
        [ 380464.7188,  342950.4688,  334999.0000,  264560.5938,  253137.8594,
          239452.7344,  232448.3594,  217124.0781,  193739.2969,  182007.2969],
        [ 412106.5000,  303713.2812,  293456.6562,  274942.9062,  248284.0938,
          228991.0781,  214284.6719,  197925.9375,  192149.3281,  190462.8125],
        [ 258903.2031,  184857.8125,  184731.6250,  166301.6875,  150817.2812,
          148187.7969,  132427.2812,  127205.8594,  105739.5391,  102999.2109],
        [ 163714.6719,  133355.8594,  110278.5312,   86819.2734,   77958.9688,
           71574.4531,   70849.8281,   56949.4766,   56261.4688,   54241.7422],
        [ 482787.4688,  465165.2500,  422722.6562,  395681.4688,  392305.8125,
          379177.0312,  357347.7500,  349823.9688,  344194.2500,  334942.4688],
        [ 136422.3906,   97953.1641,   90046.5469,   87817.3828,   76460.4375,
           72930.4688,   67469.6016,   56593.1172,   51353.2852,   46143.7695],
        [ 228219.2969,  181758.5625,  130689.3828,  126312.7578,  121446.9141,
          117537.5156,  107426.5156,  105131.5078,   98027.1719,   91773.0000],
        [ 423750.6875,  285408.0000,  256009.9062,  250246.9375,  190095.8906,
          182572.4688,  174944.0938,  172429.2188,  161770.7656,  155064.9062],
        [ 377471.1875,  223472.8438,  219448.9375,  212671.6562,  179977.2812,
          157165.7656,  143730.0781,  128346.3047,  126666.4375,  124866.4062],
        [ 395863.4062,  362328.7812,  228417.6719,  182701.3594,  176456.3438,
          165167.7812,  140258.3125,  132911.5938,  131753.3438,  126688.3047],
        [ 663256.9375,  532089.6875,  500448.7500,  426870.6250,  313717.1875,
          291226.2500,  261344.2344,  246365.8125,  233022.5625,  216546.9219],
        [ 537648.6250,  292612.4062,  286945.9062,  265214.0938,  253090.5312,
          241267.2656,  232016.4844,  207867.0000,  203973.4688,  189703.2656],
        [ 677233.0625,  516252.5000,  488599.0625,  479335.1562,  443141.3438,
          442135.0000,  442135.0000,  423046.9062,  413412.2188,  401673.0625],
        [ 494364.6562,  351542.9688,  320520.6250,  317379.4688,  306032.5625,
          302701.2188,  300517.7812,  297817.6250,  289248.5938,  285800.2188],
        [ 566093.3750,  348969.9375,  314716.2500,  302487.4062,  278005.0625,
          237600.6094,  196695.3125,  178829.5938,  177709.7344,  167806.8906],
        [ 721701.9375,  360296.8125,  306368.0938,  301163.9062,  253166.8438,
          216384.0469,  206517.2344,  202751.0625,  194818.3594,  180513.6094],
        [ 533030.3125,  258824.4531,  255632.7344,  232242.0781,  197483.4219,
          191304.0312,  182978.9531,  179909.1562,  167466.0312,  164614.4062],
        [ 435018.0625,  152027.2656,  151626.7656,  139140.0156,  120829.8750,
          112231.1641,  111893.0156,  108736.6797,  105021.0859,  103987.5859],
        [ 311471.8125,  306188.7500,  273064.8750,  267869.6562,  262059.0312,
          255124.6875,  250459.2031,  242372.1406,  241150.6250,  240534.3906],
        [ 219912.1406,   86187.1953,   84898.8203,   78675.7578,   73256.8359,
           49899.5938,   47779.3242,   46090.1992,   45541.5469,   40845.8477],
        [ 347903.5938,  244709.3438,  244412.6719,  171192.6406,  155781.2812,
          153222.2500,  142076.9844,  131503.6406,  130018.8125,  124987.6875],
        [ 179026.1719,  149760.9531,  125965.8203,   99266.5312,   98259.1094,
           76918.7188,   75597.3984,   74509.2422,   73845.4219,   72127.7734]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[362649.5625,      0.0000],
         [202654.4062,      0.0000],
         [190585.6406,      0.0000],
         ...,
         [     0.0000, 161658.7969],
         [160726.9219,      0.0000],
         [149880.5625,      0.0000]],

        [[778411.0625,      0.0000],
         [708898.0625,      0.0000],
         [691118.7500,      0.0000],
         ...,
         [627605.8125,      0.0000],
         [620236.0000,      0.0000],
         [615491.4375,      0.0000]],

        [[454855.8125,      0.0000],
         [430344.5938,      0.0000],
         [386737.8438,      0.0000],
         ...,
         [187248.7188,      0.0000],
         [184114.2812,      0.0000],
         [     0.0000, 167363.0625]],

        ...,

        [[     0.0000, 219912.1406],
         [ 86187.1953,      0.0000],
         [ 84898.8203,      0.0000],
         ...,
         [ 46090.1992,      0.0000],
         [ 45541.5469,      0.0000],
         [ 40845.8477,      0.0000]],

        [[347903.5938,      0.0000],
         [244709.3438,      0.0000],
         [     0.0000, 244412.6719],
         ...,
         [131503.6406,      0.0000],
         [130018.8125,      0.0000],
         [124987.6875,      0.0000]],

        [[     0.0000, 179026.1719],
         [     0.0000, 149760.9531],
         [     0.0000, 125965.8203],
         ...,
         [ 74509.2422,      0.0000],
         [ 73845.4219,      0.0000],
         [     0.0000,  72127.7734]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1783072.5000,  161658.7969],
        [6673575.0000,       0.0000],
        [2651540.7500,  167363.0625],
        [3959700.0000,       0.0000],
        [ 551248.3125,  772382.5625],
        [ 998380.8125,  131868.9844],
        [2152124.0000,  557018.2500],
        [2464136.7500,       0.0000],
        [3467539.0000,  411518.9688],
        [3723870.0000,       0.0000],
        [4599083.5000,       0.0000],
        [2125045.5000,       0.0000],
        [4063002.5000,       0.0000],
        [8700607.0000,       0.0000],
        [6035347.0000,       0.0000],
        [5063016.5000,       0.0000],
        [7076978.0000,       0.0000],
        [7653206.5000,       0.0000],
        [4598187.0000,       0.0000],
        [4188640.7500,       0.0000],
        [7208950.0000,       0.0000],
        [4892211.0000,       0.0000],
        [6952279.0000,       0.0000],
        [6228602.0000,       0.0000],
        [3933297.2500,       0.0000],
        [5861891.0000,       0.0000],
        [5254052.0000,       0.0000],
        [6708358.0000,       0.0000],
        [6431041.0000,       0.0000],
        [5851286.0000,       0.0000],
        [7537081.0000,       0.0000],
        [5024585.0000,       0.0000],
        [2452064.7500,       0.0000],
        [3026783.0000,  784322.8750],
        [2568636.7500, 2168541.7500],
        [1279938.3750, 2147167.7500],
        [1247744.2500, 1333402.2500],
        [1795353.8750, 1611674.3750],
        [1137216.5000, 1659156.2500],
        [6809935.5000,       0.0000],
        [2626429.7500,       0.0000],
        [2814146.0000,       0.0000],
        [2640884.5000,       0.0000],
        [2556317.2500,       0.0000],
        [ 383762.6875, 1178408.7500],
        [ 418393.7500,  463610.5312],
        [3441360.5000,  482787.4688],
        [ 515030.5000,  268159.6562],
        [ 568455.8750,  739866.7500],
        [ 740586.3750, 1511706.5000],
        [1611784.7500,  282032.1875],
        [1055937.0000,  986609.8750],
        [1971976.5000, 1712912.3750],
        [2445125.0000,  265214.0938],
        [4726963.0000,       0.0000],
        [3265925.5000,       0.0000],
        [1438540.5000, 1330373.7500],
        [2023850.2500,  919831.7500],
        [1300891.5000, 1062594.1250],
        [ 645113.5625,  895397.8750],
        [1791484.0000,  858811.1875],
        [ 553175.0625,  219912.1406],
        [1276981.3750,  568827.5625],
        [ 247621.2031,  777656.0000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 62.5
Top1 accuracy for validation set is 62.5 size is torch.Size([64, 1])
Epoch 301/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<28:00, 57.94s/it]  7%|▋         | 2/30 [01:01<12:03, 25.84s/it] 10%|█         | 3/30 [01:02<06:28, 14.38s/it] 13%|█▎        | 4/30 [01:02<03:53,  9.00s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.02s/it] 20%|██        | 6/30 [01:04<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.5296322822570803
Epoch 302/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:08, 58.24s/it]  7%|▋         | 2/30 [00:59<11:30, 24.65s/it] 10%|█         | 3/30 [01:01<06:27, 14.36s/it] 13%|█▎        | 4/30 [01:02<03:53,  8.98s/it] 17%|█▋        | 5/30 [01:02<02:30,  6.01s/it] 20%|██        | 6/30 [01:03<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.09s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.5301257610321044
Epoch 303/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:03, 62.18s/it]  7%|▋         | 2/30 [01:03<12:22, 26.53s/it] 10%|█         | 3/30 [01:04<06:38, 14.76s/it] 13%|█▎        | 4/30 [01:05<03:59,  9.23s/it] 17%|█▋        | 5/30 [01:05<02:34,  6.17s/it] 20%|██        | 6/30 [01:06<01:43,  4.32s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:08<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.33it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Epoch loss is 2.506271465619405
Epoch 304/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:40, 59.34s/it]  7%|▋         | 2/30 [01:00<11:48, 25.30s/it] 10%|█         | 3/30 [01:01<06:20, 14.09s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.82s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.91s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.53599849541982
Epoch 305/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:30, 56.92s/it]  7%|▋         | 2/30 [00:57<11:08, 23.88s/it] 10%|█         | 3/30 [00:59<06:10, 13.72s/it] 13%|█▎        | 4/30 [01:00<03:43,  8.60s/it] 17%|█▋        | 5/30 [01:00<02:24,  5.77s/it] 20%|██        | 6/30 [01:01<01:37,  4.06s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.27s/it] 30%|███       | 9/30 [01:03<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.66s/it]
Epoch loss is 2.539166808128357
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0248,  0.0026,  0.0095,  ..., -0.0001,  0.0061,  0.0062],
        [-0.0008,  0.0173,  0.0238,  ...,  0.0141, -0.0042, -0.0163],
        [-0.0319, -0.0366,  0.0208,  ...,  0.0794, -0.0131, -0.0193],
        ...,
        [ 0.0145, -0.0066,  0.0019,  ..., -0.0232, -0.0164, -0.0120],
        [-0.0291,  0.0035, -0.0078,  ...,  0.0007,  0.0128, -0.0129],
        [-0.0372, -0.0111,  0.0150,  ...,  0.0361,  0.0300, -0.0395]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8950, 0.8552, 0.8503, 0.8485, 0.8478, 0.8456, 0.8427, 0.8388, 0.8386,
         0.8333],
        [0.9491, 0.9419, 0.9405, 0.9397, 0.9388, 0.9345, 0.9337, 0.9332, 0.9329,
         0.9325],
        [0.9117, 0.9074, 0.8992, 0.8829, 0.8751, 0.8628, 0.8556, 0.8495, 0.8481,
         0.8409],
        [0.9539, 0.9077, 0.9014, 0.8961, 0.8925, 0.8884, 0.8863, 0.8854, 0.8837,
         0.8829],
        [0.8533, 0.8487, 0.8416, 0.8253, 0.8239, 0.8174, 0.8072, 0.8059, 0.8016,
         0.7997],
        [0.8607, 0.8260, 0.8222, 0.8128, 0.8053, 0.8051, 0.7959, 0.7933, 0.7899,
         0.7861],
        [0.8895, 0.8891, 0.8793, 0.8765, 0.8705, 0.8679, 0.8665, 0.8663, 0.8635,
         0.8626],
        [0.9130, 0.8937, 0.8877, 0.8696, 0.8624, 0.8593, 0.8369, 0.8361, 0.8331,
         0.8287],
        [0.9176, 0.9094, 0.9046, 0.9025, 0.8998, 0.8932, 0.8927, 0.8925, 0.8908,
         0.8898],
        [0.9532, 0.8961, 0.8956, 0.8943, 0.8907, 0.8860, 0.8858, 0.8843, 0.8733,
         0.8713],
        [0.9518, 0.9140, 0.9115, 0.9097, 0.9078, 0.9046, 0.9042, 0.8980, 0.8980,
         0.8972],
        [0.9082, 0.8937, 0.8717, 0.8697, 0.8513, 0.8308, 0.8301, 0.8104, 0.8094,
         0.8086],
        [0.9383, 0.9165, 0.9141, 0.9104, 0.9054, 0.8950, 0.8885, 0.8865, 0.8756,
         0.8702],
        [0.9671, 0.9607, 0.9600, 0.9584, 0.9576, 0.9571, 0.9538, 0.9524, 0.9502,
         0.9494],
        [0.9458, 0.9354, 0.9348, 0.9345, 0.9344, 0.9270, 0.9260, 0.9251, 0.9246,
         0.9205],
        [0.9304, 0.9270, 0.9268, 0.9201, 0.9168, 0.9166, 0.9148, 0.9118, 0.9114,
         0.9106],
        [0.9626, 0.9570, 0.9505, 0.9450, 0.9359, 0.9349, 0.9339, 0.9328, 0.9286,
         0.9278],
        [0.9676, 0.9616, 0.9559, 0.9434, 0.9425, 0.9422, 0.9412, 0.9409, 0.9393,
         0.9365],
        [0.9351, 0.9314, 0.9222, 0.9051, 0.9033, 0.9032, 0.9024, 0.9013, 0.9000,
         0.8998],
        [0.9145, 0.9064, 0.9063, 0.9061, 0.9060, 0.9052, 0.9036, 0.9021, 0.9011,
         0.9004],
        [0.9573, 0.9475, 0.9454, 0.9448, 0.9424, 0.9407, 0.9395, 0.9389, 0.9372,
         0.9365],
        [0.9333, 0.9228, 0.9219, 0.9181, 0.9163, 0.9123, 0.9115, 0.9095, 0.9060,
         0.9055],
        [0.9732, 0.9501, 0.9471, 0.9461, 0.9399, 0.9392, 0.9330, 0.9230, 0.9220,
         0.9204],
        [0.9481, 0.9426, 0.9364, 0.9317, 0.9310, 0.9299, 0.9289, 0.9280, 0.9275,
         0.9255],
        [0.9617, 0.9083, 0.9080, 0.8897, 0.8864, 0.8836, 0.8802, 0.8800, 0.8790,
         0.8768],
        [0.9449, 0.9414, 0.9392, 0.9327, 0.9278, 0.9274, 0.9223, 0.9199, 0.9148,
         0.9120],
        [0.9360, 0.9354, 0.9341, 0.9274, 0.9196, 0.9126, 0.9124, 0.9100, 0.9083,
         0.9061],
        [0.9641, 0.9526, 0.9501, 0.9381, 0.9330, 0.9295, 0.9274, 0.9262, 0.9260,
         0.9243],
        [0.9525, 0.9451, 0.9383, 0.9367, 0.9320, 0.9309, 0.9306, 0.9302, 0.9284,
         0.9257],
        [0.9403, 0.9357, 0.9322, 0.9293, 0.9292, 0.9289, 0.9273, 0.9222, 0.9212,
         0.9205],
        [0.9564, 0.9518, 0.9497, 0.9497, 0.9496, 0.9489, 0.9416, 0.9412, 0.9391,
         0.9370],
        [0.9480, 0.9270, 0.9263, 0.9201, 0.9138, 0.9103, 0.9091, 0.9063, 0.9057,
         0.9028],
        [0.8875, 0.8785, 0.8735, 0.8662, 0.8646, 0.8637, 0.8617, 0.8608, 0.8596,
         0.8593],
        [0.9199, 0.9070, 0.9007, 0.9001, 0.8976, 0.8944, 0.8913, 0.8912, 0.8891,
         0.8887],
        [0.9302, 0.9266, 0.9238, 0.9230, 0.9119, 0.9087, 0.9045, 0.9037, 0.8994,
         0.8988],
        [0.9132, 0.9095, 0.9029, 0.8958, 0.8877, 0.8858, 0.8835, 0.8798, 0.8723,
         0.8701],
        [0.9486, 0.8731, 0.8642, 0.8589, 0.8587, 0.8569, 0.8467, 0.8435, 0.8395,
         0.8338],
        [0.9200, 0.9059, 0.9031, 0.8977, 0.8802, 0.8791, 0.8785, 0.8775, 0.8727,
         0.8716],
        [0.9050, 0.9032, 0.8908, 0.8778, 0.8729, 0.8636, 0.8626, 0.8625, 0.8528,
         0.8508],
        [0.9465, 0.9454, 0.9422, 0.9414, 0.9406, 0.9404, 0.9346, 0.9345, 0.9338,
         0.9328],
        [0.8935, 0.8916, 0.8879, 0.8833, 0.8701, 0.8700, 0.8674, 0.8549, 0.8458,
         0.8399],
        [0.9116, 0.8785, 0.8760, 0.8759, 0.8752, 0.8715, 0.8686, 0.8683, 0.8677,
         0.8647],
        [0.8985, 0.8914, 0.8892, 0.8735, 0.8699, 0.8651, 0.8640, 0.8592, 0.8511,
         0.8464],
        [0.9036, 0.8831, 0.8793, 0.8753, 0.8682, 0.8630, 0.8581, 0.8533, 0.8503,
         0.8501],
        [0.8713, 0.8476, 0.8471, 0.8405, 0.8329, 0.8319, 0.8235, 0.8202, 0.8071,
         0.8053],
        [0.8400, 0.8244, 0.8117, 0.7953, 0.7883, 0.7825, 0.7819, 0.7658, 0.7641,
         0.7632],
        [0.9151, 0.9115, 0.9047, 0.9007, 0.9003, 0.8979, 0.8932, 0.8918, 0.8906,
         0.8892],
        [0.8263, 0.8025, 0.7979, 0.7953, 0.7841, 0.7810, 0.7775, 0.7656, 0.7570,
         0.7494],
        [0.8613, 0.8469, 0.8235, 0.8216, 0.8191, 0.8161, 0.8107, 0.8083, 0.8031,
         0.8000],
        [0.9063, 0.8785, 0.8691, 0.8684, 0.8494, 0.8471, 0.8435, 0.8424, 0.8374,
         0.8351],
        [0.8978, 0.8622, 0.8608, 0.8585, 0.8467, 0.8372, 0.8303, 0.8238, 0.8211,
         0.8206],
        [0.9013, 0.8956, 0.8627, 0.8458, 0.8439, 0.8399, 0.8272, 0.8238, 0.8232,
         0.8216],
        [0.9375, 0.9218, 0.9179, 0.9067, 0.8844, 0.8789, 0.8724, 0.8686, 0.8634,
         0.8577],
        [0.9228, 0.8804, 0.8795, 0.8727, 0.8702, 0.8663, 0.8643, 0.8562, 0.8546,
         0.8500],
        [0.9389, 0.9197, 0.9157, 0.9144, 0.9088, 0.9088, 0.9085, 0.9059, 0.9045,
         0.9020],
        [0.9169, 0.8932, 0.8866, 0.8861, 0.8826, 0.8821, 0.8817, 0.8816, 0.8794,
         0.8779],
        [0.9260, 0.8917, 0.8856, 0.8822, 0.8764, 0.8661, 0.8520, 0.8456, 0.8445,
         0.8424],
        [0.9441, 0.8945, 0.8828, 0.8828, 0.8701, 0.8596, 0.8552, 0.8541, 0.8517,
         0.8453],
        [0.9221, 0.8711, 0.8707, 0.8636, 0.8518, 0.8489, 0.8462, 0.8458, 0.8412,
         0.8395],
        [0.9079, 0.8342, 0.8340, 0.8274, 0.8177, 0.8130, 0.8126, 0.8105, 0.8070,
         0.8067],
        [0.8844, 0.8842, 0.8752, 0.8731, 0.8723, 0.8688, 0.8675, 0.8667, 0.8663,
         0.8658],
        [0.8609, 0.7946, 0.7939, 0.7878, 0.7831, 0.7553, 0.7516, 0.7489, 0.7478,
         0.7419],
        [0.8927, 0.8675, 0.8669, 0.8410, 0.8344, 0.8336, 0.8297, 0.8225, 0.8219,
         0.8198],
        [0.8465, 0.8320, 0.8198, 0.8035, 0.8027, 0.7873, 0.7832, 0.7825, 0.7818,
         0.7818]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 1, 0],
        [1, 0, 1, 1, 1, 0, 1, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],
        [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 1, 0, 0, 1],
        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 1, 1],
        [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 1, 1, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 357274.1562,  202222.3438,  188491.2188,  183699.3281,  181944.4688,
          176246.7812,  169037.0781,  160071.1406,  159560.4062,  147968.3438],
        [ 773207.6250,  697511.0625,  683806.6875,  676003.8125,  667236.6875,
          627623.1250,  621108.5000,  616190.3125,  613280.7500,  610072.3750],
        [ 453247.1562,  426273.8438,  379011.4375,  300214.4062,  268850.3750,
          225456.1562,  203460.0000,  186404.5312,  182724.0156,  164824.9219],
        [ 828724.9375,  428021.8125,  391120.8750,  362839.1562,  344534.4688,
          324834.3438,  315134.6250,  311451.5938,  303941.3438,  300604.3438],
        [ 196861.3906,  184210.8906,  166508.4688,  131900.8125,  129334.1875,
          117870.0000,  101810.7031,  100007.1250,   94082.9375,   91571.0469],
        [ 218621.7344,  133207.6406,  126248.5625,  110331.7578,   99205.4844,
           98803.8359,   86740.9062,   83548.1562,   79588.6641,   75372.7266],
        [ 329966.3750,  328195.4062,  285279.3125,  273968.6875,  251703.3594,
          242537.7031,  237795.7812,  236951.8594,  227780.2969,  224831.7812],
        [ 461461.6875,  350707.8438,  321742.9062,  248528.3438,  224060.9844,
          214310.6250,  155605.3281,  153894.2812,  147484.1406,  138416.5938],
        [ 492819.7188,  438795.5625,  409682.0625,  397319.9062,  382533.7812,
          347886.3438,  345768.8438,  344720.5000,  336316.8750,  331378.2500],
        [ 819678.6875,  362830.5312,  360095.5312,  353770.4375,  335636.0312,
          314140.5000,  313241.5312,  306670.0625,  261978.5781,  254358.4531],
        [ 803397.0625,  468153.2500,  451727.3125,  440257.5938,  429006.6875,
          409714.0938,  407038.5625,  372958.2188,  372952.5625,  368257.9062],
        [ 431279.2812,  350366.8750,  255929.5938,  248762.8594,  191335.9531,
          142804.9531,  141203.7031,  106573.0391,  105076.8828,  103934.0469],
        [ 662912.9375,  485298.6562,  469147.6875,  445187.7500,  414466.2188,
          357203.6250,  325612.5312,  316252.7812,  270477.4688,  250622.3750],
        [1000913.3125,  912222.7500,  903273.2500,  883024.9375,  872695.7500,
          867108.3750,  826944.6875,  810415.1250,  785673.0000,  776343.4375],
        [ 737285.0000,  636158.5000,  630813.1250,  627392.1875,  627346.6875,
          564072.5000,  555860.9375,  549039.7500,  544801.9375,  514042.6875],
        [ 592024.1250,  563868.0625,  562480.8125,  511048.7812,  487869.8750,
          486067.0625,  473879.5312,  454067.4375,  451130.1875,  445929.6250],
        [ 938235.9375,  866243.0000,  788882.7500,  729063.0625,  640703.0000,
          631541.4375,  622077.6875,  612731.8125,  576850.7500,  570660.6250],
        [1006780.8125,  924754.6250,  852923.8125,  712453.5000,  703367.9375,
          701176.5625,  690663.3750,  688225.3750,  672178.1875,  646445.3125],
        [ 632807.5625,  600198.4375,  526990.2500,  412485.9375,  401916.0000,
          401659.6562,  396915.4375,  390724.1875,  383399.4062,  382673.1875],
        [ 471574.0000,  420444.2500,  419465.0312,  418718.4062,  418127.4375,
          413024.8125,  403845.5312,  395461.1562,  389385.0625,  385903.7500],
        [ 869849.8750,  756030.6250,  733438.3750,  727371.3125,  703052.1250,
          686308.9375,  674396.6250,  668458.9375,  652693.9375,  645806.9375],
        [ 617508.6875,  531237.3750,  524474.3125,  497004.7812,  484066.3438,
          457392.6562,  452218.6875,  439342.4062,  417954.0000,  414792.4375],
        [1091329.6250,  784505.0625,  752089.8125,  740647.2500,  678546.7500,
          671321.6250,  614356.6875,  532572.0000,  525389.9375,  513639.9062],
        [ 762795.6875,  704643.6250,  645052.8750,  603035.0000,  597175.1875,
          587830.0000,  579724.6250,  571859.2500,  567778.1250,  552207.8125],
        [ 926354.0000,  432063.9375,  430080.7812,  331084.1562,  315851.3125,
          303256.8438,  289149.8438,  288410.1250,  284140.8125,  275136.2188],
        [ 728756.5000,  692568.2500,  671066.8750,  612133.1250,  570533.3125,
          567193.0625,  527542.8750,  509307.9375,  473715.0625,  455162.1562],
        [ 641717.5000,  636236.1875,  624064.1875,  567143.3125,  507718.7188,
          459411.5000,  457932.1562,  442453.0625,  431981.9375,  418353.1875],
        [ 958800.6250,  812890.5625,  784846.2500,  660859.5625,  614922.3750,
          584722.4375,  567542.0625,  557393.5000,  556260.2500,  542811.4375],
        [ 811935.1875,  730955.3125,  663129.8125,  648025.4375,  605654.5000,
          596522.9375,  593743.5625,  590531.0000,  575326.1875,  553339.6250],
        [ 682353.3125,  638569.8125,  607436.1250,  582485.5625,  581837.6875,
          579891.0625,  566137.6250,  526582.3125,  519403.6250,  513988.2812],
        [ 858997.9375,  803411.6250,  780418.7500,  779962.6875,  778849.9375,
          771111.8750,  694508.1875,  690995.4375,  670780.8750,  650551.8750],
        [ 761340.6875,  563738.0000,  558227.0625,  511438.8438,  467312.0000,
          444471.6562,  436503.3438,  419343.8125,  416022.5312,  399055.7188],
        [ 320592.1875,  282026.4375,  262487.2500,  236475.5312,  231455.1406,
          228353.1875,  221946.3906,  219175.1562,  215349.5781,  214516.9531],
        [ 509420.6562,  423747.0625,  387527.1875,  384283.7812,  370393.0312,
          353857.8125,  338561.1875,  338178.4375,  328008.6250,  326144.2500],
        [ 590426.2500,  560413.5000,  539181.8750,  532849.9375,  454817.2188,
          434351.9062,  409023.8750,  404207.3438,  380319.6250,  376845.6875],
        [ 463404.3125,  439000.6250,  399967.4688,  361078.0000,  321604.8750,
          313277.9688,  302860.0312,  287330.3750,  258094.1250,  250335.4844],
        [ 768206.6875,  261229.8438,  230129.7031,  213329.1875,  212735.1406,
          207103.4062,  179013.0312,  171018.6875,  161664.5000,  148877.0781],
        [ 510371.2812,  417065.6875,  400739.0625,  370933.5312,  289227.9062,
          284511.5000,  281973.4375,  278028.4062,  259748.7812,  255628.8281],
        [ 411802.8125,  401533.6562,  336274.5312,  279117.4062,  260348.6875,
          228097.4531,  224803.0312,  224592.3906,  195366.4844,  189990.4062],
        [ 745245.6250,  734005.8750,  701223.3750,  693280.0000,  685145.5000,
          683184.8750,  628731.4375,  628219.6250,  621928.2500,  612471.8125],
        [ 349349.8750,  340385.7812,  322727.8125,  302245.4375,  250023.8906,
          249949.5156,  240814.4062,  201385.5469,  176764.2188,  162468.2188],
        [ 452568.1562,  281940.9062,  272185.3125,  271713.2812,  269156.6562,
          255282.1719,  244976.7031,  243972.2969,  241935.4375,  231749.3594],
        [ 375324.5938,  339011.5312,  328884.4062,  262536.3125,  249463.4531,
          233012.5469,  229403.5312,  214087.1406,  190652.3594,  178449.8594],
        [ 403824.7188,  301143.7812,  285240.9375,  269644.2812,  243630.5156,
          226164.8750,  210896.1250,  196702.2500,  188554.7031,  187923.3281],
        [ 254658.2188,  181446.3125,  180243.1719,  163946.3750,  147059.4219,
          145027.5156,  128629.9766,  122608.8984,  101781.9609,   99123.3125],
        [ 162832.2656,  130216.0000,  108633.0391,   85971.8672,   77809.8281,
           71580.8047,   70926.1562,   56415.2891,   55021.6289,   54369.8203],
        [ 476169.5625,  452077.2500,  410058.0938,  387333.5938,  385119.5625,
          372250.0312,  348072.8438,  341259.4688,  335419.7188,  328480.0625],
        [ 133839.2500,   95274.7891,   89137.5391,   85908.0234,   73286.8203,
           70068.2891,   66646.8750,   56229.9805,   49700.1680,   44619.8281],
        [ 220787.8125,  179641.0156,  128514.3516,  125211.8672,  120684.0859,
          115646.3203,  107019.3359,  103514.3281,   96119.3359,   91935.3281],
        [ 419491.0312,  281939.8125,  246770.9688,  244091.9062,  186104.8750,
          180157.6094,  171057.8438,  168446.1406,  156808.7031,  151824.0000],
        [ 371471.9688,  223489.2656,  219141.0938,  212105.1250,  179127.4531,
          156382.9375,  141684.0469,  129090.0859,  124227.3828,  123289.7969],
        [ 390956.7812,  359915.9688,  224945.0156,  176900.6562,  172086.0469,
          162455.1875,  135468.0469,  129121.6016,  128115.9141,  125057.6797],
        [ 655231.0000,  523395.0625,  495583.0000,  421963.0000,  307120.7812,
          283713.8125,  258419.9531,  244817.1875,  227224.6562,  209584.8438],
        [ 531089.5000,  290033.3125,  285990.2812,  259734.8906,  250644.3594,
          236831.4531,  230161.3125,  205062.5781,  200422.7656,  187751.1875],
        [ 668940.4375,  507949.7812,  480056.5938,  471300.2188,  434776.2500,
          434776.2500,  433197.3125,  417052.5625,  409096.4062,  394380.9688],
        [ 488096.0938,  347832.2812,  316736.9062,  314269.0625,  299149.5312,
          296825.7812,  295187.3438,  294677.9688,  285834.5625,  279552.4062],
        [ 556377.5000,  340633.2188,  312272.1562,  297418.8438,  273849.8438,
          236307.3594,  193172.3594,  176204.4219,  173629.6875,  168332.9219],
        [ 720294.4375,  354566.8750,  300113.3750,  299901.3750,  250079.4531,
          215198.8750,  202351.7812,  199180.4219,  192280.9219,  175582.4219],
        [ 525743.8125,  253958.2812,  252488.0938,  228007.8594,  192574.0000,
          184897.4844,  177927.8125,  176934.9062,  165453.2969,  161633.3594],
        [ 429642.1562,  149882.7031,  149450.3594,  135943.0156,  118327.2734,
          110655.9922,  110087.3906,  106752.3672,  101538.5156,  101207.5938],
        [ 307121.0938,  306010.6875,  269251.6562,  261333.5156,  258137.1875,
          245751.9375,  240961.4219,  238313.8594,  236971.5312,  235396.1719],
        [ 219262.5469,   85085.0859,   84232.7578,   77225.2188,   72167.4688,
           48516.3281,   46022.1211,   44274.9570,   43623.5547,   40065.5977],
        [ 345769.5000,  240937.5312,  239154.2031,  165147.3125,  150295.6406,
          148593.1094,  140554.7656,  126737.8516,  125737.2969,  121962.0078],
        [ 178594.9219,  145214.2188,  121935.8359,   96615.8906,   95546.9375,
           76658.3047,   72265.9609,   71576.1562,   70900.1172,   70896.1250]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[357274.1562,      0.0000],
         [202222.3438,      0.0000],
         [188491.2188,      0.0000],
         ...,
         [160071.1406,      0.0000],
         [     0.0000, 159560.4062],
         [147968.3438,      0.0000]],

        [[773207.6250,      0.0000],
         [697511.0625,      0.0000],
         [683806.6875,      0.0000],
         ...,
         [616190.3125,      0.0000],
         [613280.7500,      0.0000],
         [610072.3750,      0.0000]],

        [[453247.1562,      0.0000],
         [426273.8438,      0.0000],
         [379011.4375,      0.0000],
         ...,
         [186404.5312,      0.0000],
         [182724.0156,      0.0000],
         [     0.0000, 164824.9219]],

        ...,

        [[     0.0000, 219262.5469],
         [ 85085.0859,      0.0000],
         [ 84232.7578,      0.0000],
         ...,
         [ 44274.9570,      0.0000],
         [ 43623.5547,      0.0000],
         [ 40065.5977,      0.0000]],

        [[345769.5000,      0.0000],
         [     0.0000, 240937.5312],
         [239154.2031,      0.0000],
         ...,
         [126737.8516,      0.0000],
         [125737.2969,      0.0000],
         [121962.0078,      0.0000]],

        [[     0.0000, 178594.9219],
         [     0.0000, 145214.2188],
         [     0.0000, 121935.8359],
         ...,
         [ 71576.1562,      0.0000],
         [     0.0000,  70900.1172],
         [ 70896.1250,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1766955.0000,  159560.4062],
        [6586041.0000,       0.0000],
        [2625642.0000,  164824.9219],
        [3911207.5000,       0.0000],
        [ 546896.1250,  767261.4375],
        [ 985420.8750,  126248.5625],
        [2102027.7500,  536982.6875],
        [2416213.0000,       0.0000],
        [3429902.0000,  397319.9062],
        [3682400.2500,       0.0000],
        [4523463.5000,       0.0000],
        [2077267.1250,       0.0000],
        [3997182.0000,       0.0000],
        [8638614.0000,       0.0000],
        [5986813.5000,       0.0000],
        [5028366.0000,       0.0000],
        [6976990.5000,       0.0000],
        [7598970.0000,       0.0000],
        [4529770.0000,       0.0000],
        [4135949.2500,       0.0000],
        [7117408.0000,       0.0000],
        [4835991.5000,       0.0000],
        [6904399.0000,       0.0000],
        [6172102.0000,       0.0000],
        [3875527.7500,       0.0000],
        [5807979.0000,       0.0000],
        [5187012.0000,       0.0000],
        [6641049.0000,       0.0000],
        [6369164.0000,       0.0000],
        [5798685.5000,       0.0000],
        [7479589.0000,       0.0000],
        [4977453.5000,       0.0000],
        [2432377.7500,       0.0000],
        [2988311.2500,  771811.0000],
        [2157278.7500, 2525158.5000],
        [1260708.2500, 2136245.0000],
        [1227080.7500, 1326226.5000],
        [1765346.3750, 1582882.0000],
        [1117164.2500, 1634762.7500],
        [6733436.5000,       0.0000],
        [2596114.7500,       0.0000],
        [2765480.2500,       0.0000],
        [2600825.7500,       0.0000],
        [2513725.5000,       0.0000],
        [ 371450.2812, 1153074.8750],
        [ 415680.1250,  458096.5625],
        [3360070.5000,  476169.5625],
        [ 501940.0938,  262771.4688],
        [ 561175.9375,  727897.8750],
        [ 728123.7500, 1478569.2500],
        [1600336.5000,  279672.7500],
        [1029205.1875,  975817.7500],
        [1940148.7500, 1686904.3750],
        [2417986.5000,  259734.8906],
        [4651527.0000,       0.0000],
        [3218162.0000,       0.0000],
        [1252917.6250, 1475280.7500],
        [2004790.2500,  904759.6875],
        [1274214.3750, 1045404.5000],
        [ 634500.1250,  878987.2500],
        [1747803.5000,  851445.6250],
        [ 541213.0625,  219262.5469],
        [1250211.2500,  554677.9375],
        [ 239088.1719,  761116.3125]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 306/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:52, 59.75s/it]  7%|▋         | 2/30 [01:00<11:41, 25.04s/it] 10%|█         | 3/30 [01:01<06:16, 13.95s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.5380501985549926
Epoch 307/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:02, 60.09s/it]  7%|▋         | 2/30 [01:00<11:45, 25.18s/it] 10%|█         | 3/30 [01:01<06:18, 14.03s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.501437862714132
Epoch 308/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:07<32:23, 67.03s/it]  7%|▋         | 2/30 [01:08<13:20, 28.57s/it] 10%|█         | 3/30 [01:11<07:33, 16.79s/it] 13%|█▎        | 4/30 [01:12<04:31, 10.46s/it] 17%|█▋        | 5/30 [01:12<02:53,  6.96s/it] 20%|██        | 6/30 [01:13<01:56,  4.84s/it] 23%|██▎       | 7/30 [01:14<01:20,  3.50s/it] 27%|██▋       | 8/30 [01:15<00:57,  2.63s/it] 30%|███       | 9/30 [01:15<00:42,  2.04s/it] 33%|███▎      | 10/30 [01:16<00:32,  1.64s/it] 37%|███▋      | 11/30 [01:17<00:25,  1.37s/it] 40%|████      | 12/30 [01:18<00:21,  1.18s/it] 43%|████▎     | 13/30 [01:18<00:17,  1.05s/it] 47%|████▋     | 14/30 [01:19<00:15,  1.04it/s] 50%|█████     | 15/30 [01:20<00:13,  1.12it/s] 53%|█████▎    | 16/30 [01:21<00:11,  1.18it/s] 57%|█████▋    | 17/30 [01:21<00:10,  1.22it/s] 60%|██████    | 18/30 [01:22<00:09,  1.25it/s] 63%|██████▎   | 19/30 [01:23<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:24<00:07,  1.30it/s] 70%|███████   | 21/30 [01:24<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:25<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:26<00:05,  1.32it/s] 80%|████████  | 24/30 [01:27<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:27<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:28<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:29<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:30<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:30<00:00,  1.34it/s]100%|██████████| 30/30 [01:31<00:00,  1.34it/s]100%|██████████| 30/30 [01:31<00:00,  3.06s/it]
Epoch loss is 2.520176951090495
Epoch 309/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:06<32:16, 66.79s/it]  7%|▋         | 2/30 [01:07<13:02, 27.94s/it] 10%|█         | 3/30 [01:08<06:59, 15.53s/it] 13%|█▎        | 4/30 [01:09<04:11,  9.69s/it] 17%|█▋        | 5/30 [01:09<02:41,  6.47s/it] 20%|██        | 6/30 [01:10<01:48,  4.52s/it] 23%|██▎       | 7/30 [01:11<01:15,  3.29s/it] 27%|██▋       | 8/30 [01:12<00:54,  2.48s/it] 30%|███       | 9/30 [01:12<00:40,  1.94s/it] 33%|███▎      | 10/30 [01:13<00:31,  1.57s/it] 37%|███▋      | 11/30 [01:14<00:25,  1.32s/it] 40%|████      | 12/30 [01:15<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:15<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:16<00:15,  1.06it/s] 50%|█████     | 15/30 [01:17<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:18<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:18<00:10,  1.23it/s] 60%|██████    | 18/30 [01:19<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:20<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:20<00:07,  1.30it/s] 70%|███████   | 21/30 [01:21<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:22<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:23<00:05,  1.32it/s] 80%|████████  | 24/30 [01:23<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:24<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:25<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:26<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:26<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:27<00:00,  1.34it/s]100%|██████████| 30/30 [01:28<00:00,  1.34it/s]100%|██████████| 30/30 [01:28<00:00,  2.95s/it]
Epoch loss is 2.4921812772750855
Epoch 310/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:09<33:35, 69.50s/it]  7%|▋         | 2/30 [01:11<14:00, 30.00s/it] 10%|█         | 3/30 [01:12<07:29, 16.64s/it] 13%|█▎        | 4/30 [01:13<04:29, 10.37s/it] 17%|█▋        | 5/30 [01:14<02:52,  6.90s/it] 20%|██        | 6/30 [01:14<01:55,  4.81s/it] 23%|██▎       | 7/30 [01:15<01:20,  3.48s/it] 27%|██▋       | 8/30 [01:16<00:57,  2.61s/it] 30%|███       | 9/30 [01:17<00:42,  2.03s/it] 33%|███▎      | 10/30 [01:17<00:32,  1.63s/it] 37%|███▋      | 11/30 [01:18<00:25,  1.36s/it] 40%|████      | 12/30 [01:19<00:21,  1.17s/it] 43%|████▎     | 13/30 [01:20<00:17,  1.05s/it] 47%|████▋     | 14/30 [01:20<00:15,  1.05it/s] 50%|█████     | 15/30 [01:21<00:13,  1.12it/s] 53%|█████▎    | 16/30 [01:22<00:11,  1.18it/s] 57%|█████▋    | 17/30 [01:23<00:10,  1.22it/s] 60%|██████    | 18/30 [01:23<00:09,  1.25it/s] 63%|██████▎   | 19/30 [01:24<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:25<00:07,  1.30it/s] 70%|███████   | 21/30 [01:26<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:26<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:27<00:05,  1.32it/s] 80%|████████  | 24/30 [01:28<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:29<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:29<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:30<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:31<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:32<00:00,  1.34it/s]100%|██████████| 30/30 [01:32<00:00,  1.34it/s]100%|██████████| 30/30 [01:32<00:00,  3.10s/it]
Epoch loss is 2.5089972972869874
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0249,  0.0025,  0.0097,  ..., -0.0006,  0.0070,  0.0057],
        [-0.0009,  0.0168,  0.0243,  ...,  0.0142, -0.0035, -0.0163],
        [-0.0316, -0.0368,  0.0213,  ...,  0.0793, -0.0124, -0.0189],
        ...,
        [ 0.0139, -0.0061,  0.0024,  ..., -0.0229, -0.0154, -0.0116],
        [-0.0293,  0.0042, -0.0074,  ...,  0.0011,  0.0133, -0.0128],
        [-0.0369, -0.0112,  0.0154,  ...,  0.0367,  0.0306, -0.0386]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8940, 0.8547, 0.8494, 0.8479, 0.8465, 0.8464, 0.8414, 0.8385, 0.8371,
         0.8324],
        [0.9487, 0.9408, 0.9398, 0.9384, 0.9378, 0.9334, 0.9330, 0.9326, 0.9320,
         0.9316],
        [0.9113, 0.9069, 0.8977, 0.8822, 0.8741, 0.8619, 0.8558, 0.8493, 0.8476,
         0.8396],
        [0.9541, 0.9061, 0.9005, 0.8948, 0.8916, 0.8867, 0.8848, 0.8842, 0.8834,
         0.8815],
        [0.8520, 0.8487, 0.8409, 0.8236, 0.8232, 0.8172, 0.8064, 0.8063, 0.8013,
         0.7981],
        [0.8603, 0.8254, 0.8196, 0.8127, 0.8052, 0.8034, 0.7944, 0.7905, 0.7897,
         0.7836],
        [0.8875, 0.8870, 0.8763, 0.8749, 0.8684, 0.8667, 0.8644, 0.8640, 0.8624,
         0.8621],
        [0.9116, 0.8919, 0.8860, 0.8691, 0.8615, 0.8573, 0.8358, 0.8355, 0.8309,
         0.8268],
        [0.9172, 0.9090, 0.9033, 0.9002, 0.8987, 0.8924, 0.8923, 0.8916, 0.8902,
         0.8890],
        [0.9528, 0.8955, 0.8945, 0.8938, 0.8895, 0.8849, 0.8845, 0.8837, 0.8724,
         0.8707],
        [0.9511, 0.9125, 0.9101, 0.9089, 0.9068, 0.9031, 0.9027, 0.8970, 0.8969,
         0.8958],
        [0.9069, 0.8920, 0.8704, 0.8682, 0.8492, 0.8288, 0.8278, 0.8088, 0.8086,
         0.8057],
        [0.9366, 0.9159, 0.9138, 0.9090, 0.9038, 0.8939, 0.8872, 0.8851, 0.8750,
         0.8683],
        [0.9666, 0.9603, 0.9595, 0.9576, 0.9570, 0.9570, 0.9532, 0.9520, 0.9497,
         0.9483],
        [0.9457, 0.9347, 0.9345, 0.9342, 0.9338, 0.9264, 0.9254, 0.9243, 0.9238,
         0.9201],
        [0.9301, 0.9266, 0.9265, 0.9195, 0.9165, 0.9163, 0.9144, 0.9112, 0.9104,
         0.9102],
        [0.9617, 0.9563, 0.9492, 0.9443, 0.9353, 0.9345, 0.9318, 0.9317, 0.9277,
         0.9268],
        [0.9673, 0.9613, 0.9556, 0.9430, 0.9420, 0.9415, 0.9410, 0.9405, 0.9385,
         0.9358],
        [0.9343, 0.9300, 0.9208, 0.9046, 0.9025, 0.9021, 0.9018, 0.9002, 0.8989,
         0.8978],
        [0.9136, 0.9056, 0.9051, 0.9050, 0.9045, 0.9042, 0.9029, 0.9014, 0.9008,
         0.8997],
        [0.9563, 0.9465, 0.9445, 0.9439, 0.9411, 0.9398, 0.9389, 0.9381, 0.9369,
         0.9356],
        [0.9325, 0.9215, 0.9209, 0.9177, 0.9158, 0.9123, 0.9110, 0.9090, 0.9047,
         0.9038],
        [0.9730, 0.9500, 0.9466, 0.9458, 0.9395, 0.9384, 0.9325, 0.9227, 0.9214,
         0.9199],
        [0.9476, 0.9420, 0.9359, 0.9309, 0.9307, 0.9295, 0.9285, 0.9277, 0.9270,
         0.9251],
        [0.9612, 0.9081, 0.9068, 0.8891, 0.8849, 0.8812, 0.8794, 0.8786, 0.8771,
         0.8756],
        [0.9444, 0.9407, 0.9386, 0.9321, 0.9274, 0.9268, 0.9221, 0.9194, 0.9139,
         0.9111],
        [0.9349, 0.9346, 0.9336, 0.9264, 0.9189, 0.9116, 0.9116, 0.9088, 0.9078,
         0.9052],
        [0.9639, 0.9522, 0.9497, 0.9373, 0.9327, 0.9292, 0.9267, 0.9255, 0.9249,
         0.9235],
        [0.9520, 0.9445, 0.9372, 0.9361, 0.9315, 0.9305, 0.9292, 0.9292, 0.9282,
         0.9248],
        [0.9397, 0.9350, 0.9317, 0.9288, 0.9287, 0.9286, 0.9266, 0.9220, 0.9204,
         0.9201],
        [0.9557, 0.9511, 0.9495, 0.9494, 0.9490, 0.9485, 0.9411, 0.9405, 0.9383,
         0.9367],
        [0.9474, 0.9265, 0.9258, 0.9197, 0.9128, 0.9092, 0.9081, 0.9059, 0.9051,
         0.9018],
        [0.8867, 0.8778, 0.8735, 0.8650, 0.8638, 0.8637, 0.8609, 0.8600, 0.8587,
         0.8587],
        [0.9187, 0.9059, 0.8997, 0.8987, 0.8964, 0.8938, 0.8903, 0.8901, 0.8885,
         0.8879],
        [0.9295, 0.9251, 0.9233, 0.9224, 0.9114, 0.9079, 0.9035, 0.9022, 0.8994,
         0.8979],
        [0.9128, 0.9084, 0.9026, 0.8949, 0.8878, 0.8850, 0.8831, 0.8799, 0.8713,
         0.8693],
        [0.9485, 0.8725, 0.8650, 0.8584, 0.8584, 0.8551, 0.8447, 0.8418, 0.8370,
         0.8320],
        [0.9184, 0.9040, 0.9020, 0.8963, 0.8796, 0.8790, 0.8772, 0.8765, 0.8709,
         0.8702],
        [0.9038, 0.9025, 0.8894, 0.8769, 0.8712, 0.8618, 0.8616, 0.8611, 0.8513,
         0.8490],
        [0.9457, 0.9443, 0.9412, 0.9407, 0.9400, 0.9395, 0.9341, 0.9338, 0.9334,
         0.9319],
        [0.8929, 0.8909, 0.8873, 0.8823, 0.8697, 0.8690, 0.8669, 0.8541, 0.8453,
         0.8392],
        [0.9108, 0.8774, 0.8747, 0.8742, 0.8740, 0.8701, 0.8679, 0.8679, 0.8661,
         0.8634],
        [0.8976, 0.8904, 0.8882, 0.8727, 0.8688, 0.8634, 0.8630, 0.8580, 0.8500,
         0.8455],
        [0.9021, 0.8825, 0.8775, 0.8742, 0.8669, 0.8621, 0.8571, 0.8529, 0.8496,
         0.8485],
        [0.8704, 0.8466, 0.8455, 0.8395, 0.8311, 0.8305, 0.8217, 0.8177, 0.8047,
         0.8028],
        [0.8395, 0.8231, 0.8104, 0.7947, 0.7880, 0.7824, 0.7817, 0.7659, 0.7631,
         0.7617],
        [0.9141, 0.9095, 0.9025, 0.8993, 0.8989, 0.8965, 0.8916, 0.8899, 0.8887,
         0.8876],
        [0.8249, 0.8004, 0.7972, 0.7938, 0.7813, 0.7783, 0.7767, 0.7651, 0.7550,
         0.7474],
        [0.8591, 0.8458, 0.8224, 0.8209, 0.8187, 0.8151, 0.8104, 0.8070, 0.8014,
         0.8005],
        [0.9055, 0.8774, 0.8668, 0.8666, 0.8481, 0.8462, 0.8421, 0.8409, 0.8351,
         0.8336],
        [0.8964, 0.8620, 0.8606, 0.8582, 0.8464, 0.8368, 0.8292, 0.8239, 0.8198,
         0.8198],
        [0.9006, 0.8951, 0.8618, 0.8435, 0.8423, 0.8383, 0.8248, 0.8219, 0.8214,
         0.8205],
        [0.9367, 0.9206, 0.9174, 0.9058, 0.8831, 0.8771, 0.8716, 0.8682, 0.8616,
         0.8557],
        [0.9221, 0.8799, 0.8792, 0.8713, 0.8696, 0.8650, 0.8637, 0.8554, 0.8537,
         0.8499],
        [0.9380, 0.9185, 0.9145, 0.9133, 0.9075, 0.9075, 0.9071, 0.9049, 0.9039,
         0.9008],
        [0.9157, 0.8924, 0.8864, 0.8847, 0.8811, 0.8807, 0.8807, 0.8804, 0.8784,
         0.8763],
        [0.9249, 0.8901, 0.8852, 0.8812, 0.8753, 0.8658, 0.8507, 0.8446, 0.8431,
         0.8427],
        [0.9440, 0.8934, 0.8825, 0.8814, 0.8692, 0.8592, 0.8536, 0.8528, 0.8507,
         0.8433],
        [0.9213, 0.8702, 0.8700, 0.8625, 0.8502, 0.8466, 0.8450, 0.8445, 0.8405,
         0.8384],
        [0.9073, 0.8335, 0.8330, 0.8259, 0.8163, 0.8121, 0.8116, 0.8089, 0.8051,
         0.8048],
        [0.8846, 0.8830, 0.8743, 0.8713, 0.8713, 0.8661, 0.8659, 0.8647, 0.8646,
         0.8643],
        [0.8605, 0.7937, 0.7932, 0.7868, 0.7818, 0.7532, 0.7489, 0.7465, 0.7452,
         0.7407],
        [0.8923, 0.8664, 0.8653, 0.8387, 0.8321, 0.8317, 0.8293, 0.8200, 0.8200,
         0.8183],
        [0.8460, 0.8301, 0.8177, 0.8015, 0.8010, 0.7868, 0.7807, 0.7806, 0.7805,
         0.7786]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 1, 0],
        [1, 0, 1, 1, 1, 0, 1, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],
        [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 1, 0, 0, 1],
        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 1, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 1, 0, 1, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 351911.9375,  200884.3281,  186042.0469,  182150.5469,  178568.6875,
          178428.2500,  165986.7031,  159395.7031,  156121.5625,  146077.3281],
        [ 769404.7500,  687375.2500,  677648.5000,  663591.0000,  658369.7500,
          617927.5625,  614448.1250,  611187.5625,  606097.1250,  602627.3750],
        [ 450775.7812,  423398.4688,  371232.1875,  297221.7812,  264768.0625,
          222415.0938,  203875.8438,  185801.9688,  181340.7812,  161865.2031],
        [ 830810.1250,  418476.0938,  386117.2500,  356225.5938,  340049.9688,
          317046.9688,  308483.1250,  305946.1875,  302762.7188,  294389.5000],
        [ 193080.2656,  184218.4531,  164761.4219,  128794.9531,  128085.6172,
          117581.9141,  100742.4844,  100527.2109,   93566.7422,   89459.8828],
        [ 217448.5781,  132022.2500,  121608.1250,  110166.1562,   99054.8906,
           96529.2266,   84892.5078,   80284.0625,   79385.0547,   72671.8438],
        [ 320858.5938,  318357.6875,  273389.0000,  267905.6875,  244360.6875,
          238372.0625,  230748.3281,  229350.8281,  224047.9531,  223172.7656],
        [ 452616.0625,  341687.0625,  314152.1875,  246512.0000,  221267.5781,
          208441.2812,  153357.7812,  152644.1250,  142981.0312,  134791.1094],
        [ 490577.3438,  436190.8125,  401800.2500,  384365.5312,  376390.9688,
          344227.4062,  343434.8750,  340020.1250,  333344.6875,  327649.4062],
        [ 815619.3125,  359705.9375,  354723.7812,  350997.2812,  330307.6875,
          309257.2500,  307518.8125,  303743.4062,  258423.9062,  252349.4219],
        [ 795857.4375,  458388.7500,  442955.0625,  435563.9688,  422913.3750,
          400676.0312,  398666.9375,  367684.1562,  367008.0000,  361398.3750],
        [ 423291.0625,  342142.5938,  251274.7812,  243565.4531,  185730.2344,
          138641.3125,  136753.5156,  104192.4688,  103911.7422,   99762.5000],
        [ 647236.1250,  481400.3750,  466799.7500,  436313.5625,  405196.1250,
          351725.7500,  319610.7188,  309769.0625,  268213.1875,  243768.7969],
        [ 993573.1250,  907750.8125,  897368.6250,  873736.6875,  866135.6250,
          865691.3750,  819620.8750,  805914.0625,  780232.7500,  765116.1250],
        [ 736325.1875,  629631.5000,  627541.8125,  625511.4375,  621168.3750,
          559116.3125,  551071.4375,  542300.8125,  538540.5625,  511011.7500],
        [ 589214.6250,  560955.1250,  560380.3750,  506555.0938,  485206.0938,
          484117.1250,  471124.5000,  449756.4062,  445215.3438,  443884.9375],
        [ 926504.1875,  857202.4375,  774570.7500,  722132.9375,  634749.5625,
          627557.3750,  603914.4375,  603064.3750,  570003.0625,  562814.5625],
        [1002451.3125,  921267.0625,  848844.5625,  708531.7500,  698817.3750,
          693734.3750,  688441.3125,  684438.8750,  665104.6250,  639793.8750],
        [ 625764.4375,  588986.5000,  516479.4688,  409413.3438,  397425.2500,
          395342.7500,  393713.5938,  384356.3438,  377513.6562,  371612.9688],
        [ 465650.3750,  415717.9375,  412673.2500,  411703.0625,  408964.5938,
          407257.5625,  400003.6875,  391269.3438,  387807.4375,  381782.2812],
        [ 857162.3750,  744761.7500,  724449.1875,  718283.1250,  689732.6875,
          676854.7500,  668107.1250,  660577.2500,  649450.3125,  637355.3750],
        [ 610495.5625,  521539.0000,  517212.9375,  493646.6562,  480417.9688,
          457040.7812,  448660.1250,  436123.8438,  410200.8438,  405162.1562],
        [1087625.5000,  783454.5625,  746500.3750,  737304.0000,  673948.4375,
          663792.2500,  610244.1250,  530703.1875,  520902.2188,  509564.9688],
        [ 756744.0625,  698365.6875,  640425.6250,  596487.1250,  594241.5000,
          584906.4375,  576619.1875,  569299.0000,  563918.6250,  549207.3125],
        [ 918777.8750,  430507.1562,  422910.5625,  328093.4062,  308996.0312,
          293200.4062,  285570.5625,  282322.4375,  276477.6562,  270551.0000],
        [ 723195.6875,  685803.8125,  665843.9375,  606846.1250,  566927.0625,
          562299.5000,  526226.8750,  506031.6875,  468056.4062,  449238.5312],
        [ 631580.6250,  628615.7500,  620040.8125,  559245.3750,  502046.8125,
          452772.3438,  452525.4062,  435199.4062,  428707.3125,  413054.7500],
        [ 955416.1250,  808355.6875,  779651.8125,  653365.2500,  611640.6250,
          582129.6250,  561597.5000,  551805.0625,  547555.3125,  536233.3125],
        [ 806350.0000,  724682.1250,  652599.3125,  642285.0625,  601307.0000,
          593156.6250,  582390.0625,  581968.0625,  573635.5000,  546279.5000],
        [ 676040.5625,  632238.6875,  602809.6250,  578551.0000,  577500.8125,
          577098.3750,  560979.7500,  524905.1250,  513328.4375,  510915.7500],
        [ 849678.7500,  796256.0625,  778370.3125,  777199.7500,  772605.4375,
          767126.0625,  690361.1875,  684290.7500,  662772.5625,  647496.0000],
        [ 754904.5625,  559886.2500,  554395.5000,  508343.2188,  460684.7188,
          437416.7812,  430452.9375,  417341.4062,  412698.0312,  393401.6562],
        [ 317234.2188,  279316.8438,  262646.9688,  232532.3906,  228656.9688,
          228454.2812,  219521.3594,  216551.8750,  212687.8750,  212480.0781],
        [ 501208.6562,  417387.2188,  382137.4375,  376699.4375,  364054.6875,
          351207.8750,  334016.1250,  332755.1875,  325590.1562,  322740.4375],
        [ 584648.2500,  548932.4375,  535087.5000,  527972.6875,  451162.0312,
          429375.0625,  403346.3125,  395707.1562,  380450.9375,  372240.0938],
        [ 460196.8438,  432667.5938,  397894.3750,  356532.8125,  322404.2188,
          309685.7500,  301338.2812,  287956.9062,  254638.5469,  247457.4688],
        [ 766429.1875,  259084.0000,  232752.4844,  211773.8281,  211693.0781,
          202033.6250,  173966.4844,  167055.9375,  155871.7812,  145087.8281],
        [ 498781.5625,  406065.3750,  394578.1250,  363592.5312,  286453.5000,
          284123.7500,  277079.7500,  274111.4062,  253103.3281,  250721.1094],
        [ 405133.5625,  397626.1875,  329738.0000,  275827.9688,  254281.3281,
          222189.7188,  221587.2812,  220088.1875,  191226.5000,  185056.0781],
        [ 737120.5000,  722534.5000,  690428.3125,  686132.1875,  678755.7500,
          674560.6250,  623847.5625,  621420.1875,  617952.3125,  605044.3125],
        [ 346494.7188,  336812.1250,  319842.1562,  297784.6875,  248720.1719,
          246239.9062,  239140.0781,  198991.8750,  175506.9219,  160798.2188],
        [ 447575.9062,  277739.2812,  267162.9375,  265175.9062,  264423.3750,
          250034.1406,  242514.1094,  242413.2969,  236462.9062,  227356.2344],
        [ 370797.6875,  334375.9688,  323811.7812,  259770.8125,  245459.3906,
          227263.4531,  226211.4688,  210517.1406,  187733.2656,  175991.8125],
        [ 395164.4375,  298874.3438,  277984.4062,  265450.6875,  239034.9688,
          223165.7344,  207743.5312,  195757.3906,  186742.7812,  183728.0625],
        [ 251103.7344,  178903.9688,  176090.1875,  161592.2188,  143374.8125,
          142128.7500,  125260.8359,  118303.9141,   98274.8516,   95679.0625],
        [ 161643.3906,  127840.3203,  106580.7578,   85193.0703,   77398.5547,
           71431.3906,   70809.1641,   56496.9141,   54216.3477,   53211.6875],
        [ 468877.5312,  439326.9062,  397335.4688,  379441.4375,  377561.5312,
          364774.7812,  339949.7812,  331971.9375,  326327.5000,  321161.9688],
        [ 131229.0312,   92450.1953,   88292.6797,   84098.5469,   70335.9609,
           67366.6641,   65912.6406,   55796.5977,   48325.7969,   43325.0820],
        [ 213706.9062,  176824.5781,  126558.9766,  123901.7734,  120108.9609,
          113960.3828,  106624.9844,  101567.4688,   93825.6875,   92597.0156],
        [ 414864.8125,  277675.4688,  238622.2500,  238042.6562,  182609.9062,
          177848.0938,  167821.4531,  164894.4062,  151812.6875,  148636.1875],
        [ 364266.1875,  222900.7188,  218575.8750,  211150.0781,  178371.4219,
          155589.2969,  139450.4688,  129380.2031,  122012.6172,  121983.5234],
        [ 386892.7812,  357624.9062,  222065.5938,  171156.8906,  168203.4219,
          158912.5781,  131012.4609,  125725.3125,  124774.5078,  123211.0391],
        [ 647784.5000,  514690.2188,  491761.5000,  416669.7500,  301153.5625,
          276610.5938,  255755.1562,  243373.2188,  221586.6406,  203577.0312],
        [ 526038.6875,  287961.5625,  285013.3438,  254542.1562,  248520.0469,
          232631.9844,  228303.7656,  202938.7031,  197962.0000,  187536.9688],
        [ 659798.4375,  499574.7188,  472075.2812,  464100.0000,  426790.0000,
          426790.0000,  424307.1250,  411043.5938,  405250.6562,  387838.8750],
        [ 480131.6875,  343973.0625,  315778.7188,  308341.6562,  292661.5000,
          291070.2188,  290937.8438,  289801.3750,  281850.3125,  273209.4375],
        [ 547635.7500,  333176.5312,  310539.5938,  293179.4062,  269484.6562,
          235356.6719,  189643.5781,  173718.7969,  170068.3750,  169212.8750],
        [ 718780.0000,  348818.8750,  298650.1250,  294148.1562,  246929.6406,
          214080.0000,  197723.5156,  195348.5938,  189612.2969,  170497.0781],
        [ 519972.1250,  250504.3438,  249720.0625,  224506.0938,  188376.5781,
          178941.6875,  174894.5469,  173504.5625,  163910.2656,  159057.5312],
        [ 425413.2812,  148231.7500,  147174.0469,  133137.9219,  116089.9766,
          109222.2422,  108415.6875,  104433.1094,   98786.6875,   98393.0156],
        [ 307661.9375,  301028.0625,  265642.8750,  254592.1719,  254464.0000,
          236313.8906,  235592.9062,  231539.7031,  231319.6562,  230212.9062],
        [ 218047.2500,   84007.4844,   83370.2656,   76120.8125,   70910.4609,
           47123.0312,   44314.6680,   42796.5664,   41989.1875,   39424.0898],
        [ 343604.5312,  237468.3125,  233603.7344,  159685.3906,  145350.6875,
          144490.3594,  139622.4062,  122265.0234,  122229.3594,  119326.4297],
        [ 177443.3594,  141298.6719,  118360.2266,   93891.5703,   93172.8047,
           76144.6328,   69728.7266,   69683.0547,   69532.8281,   67713.2578]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[351911.9375,      0.0000],
         [200884.3281,      0.0000],
         [186042.0469,      0.0000],
         ...,
         [159395.7031,      0.0000],
         [     0.0000, 156121.5625],
         [146077.3281,      0.0000]],

        [[769404.7500,      0.0000],
         [687375.2500,      0.0000],
         [677648.5000,      0.0000],
         ...,
         [611187.5625,      0.0000],
         [606097.1250,      0.0000],
         [602627.3750,      0.0000]],

        [[450775.7812,      0.0000],
         [423398.4688,      0.0000],
         [371232.1875,      0.0000],
         ...,
         [185801.9688,      0.0000],
         [181340.7812,      0.0000],
         [     0.0000, 161865.2031]],

        ...,

        [[     0.0000, 218047.2500],
         [ 84007.4844,      0.0000],
         [ 83370.2656,      0.0000],
         ...,
         [ 42796.5664,      0.0000],
         [ 41989.1875,      0.0000],
         [ 39424.0898,      0.0000]],

        [[343604.5312,      0.0000],
         [     0.0000, 237468.3125],
         [233603.7344,      0.0000],
         ...,
         [122265.0234,      0.0000],
         [122229.3594,      0.0000],
         [119326.4297,      0.0000]],

        [[     0.0000, 177443.3594],
         [     0.0000, 141298.6719],
         [     0.0000, 118360.2266],
         ...,
         [     0.0000,  69683.0547],
         [     0.0000,  69532.8281],
         [ 67713.2578,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1749445.5000,  156121.5625],
        [6508677.0000,       0.0000],
        [2600830.0000,  161865.2031],
        [3860307.7500,       0.0000],
        [ 540434.3750,  760384.5000],
        [ 972454.6250,  121608.1250],
        [2052813.8750,  517749.6875],
        [2368450.2500,       0.0000],
        [3393635.5000,  384365.5312],
        [3642646.7500,       0.0000],
        [4451112.0000,       0.0000],
        [2029265.7500,       0.0000],
        [3930033.5000,       0.0000],
        [8575140.0000,       0.0000],
        [5942219.0000,       0.0000],
        [4996410.0000,       0.0000],
        [6882513.0000,       0.0000],
        [7551425.5000,       0.0000],
        [4460608.0000,       0.0000],
        [4082829.5000,       0.0000],
        [7026733.5000,       0.0000],
        [4780500.0000,       0.0000],
        [6864040.0000,       0.0000],
        [6130214.0000,       0.0000],
        [3817407.0000,       0.0000],
        [5760470.0000,       0.0000],
        [5123788.5000,       0.0000],
        [6587750.0000,       0.0000],
        [6304653.0000,       0.0000],
        [5754368.0000,       0.0000],
        [7426157.0000,       0.0000],
        [4929525.0000,       0.0000],
        [2410082.7500,       0.0000],
        [2948960.2500,  758836.8750],
        [2128641.5000, 2500281.2500],
        [1244449.3750, 2126323.5000],
        [1207512.2500, 1318236.0000],
        [1736047.1250, 1552563.2500],
        [1095887.6250, 1606867.2500],
        [6657796.5000,       0.0000],
        [2570330.7500,       0.0000],
        [2720858.0000,       0.0000],
        [2561932.7500,       0.0000],
        [2473646.2500,       0.0000],
        [ 359953.5625, 1130758.7500],
        [ 412260.2188,  452561.3750],
        [3277851.5000,  468877.5312],
        [ 489648.6875,  257484.5156],
        [ 553364.2500,  716312.5000],
        [ 715313.6875, 1447514.2500],
        [1586078.5000,  277601.9062],
        [1002996.1875,  966583.2500],
        [1910578.5000, 1662383.6250],
        [2396907.2500,  254542.1562],
        [4577568.5000,       0.0000],
        [3167755.7500,       0.0000],
        [1236566.0000, 1455450.2500],
        [1984691.6250,  889896.6250],
        [1252071.2500, 1031316.5000],
        [ 624344.5625,  864953.1875],
        [1704085.2500,  844282.8750],
        [ 530056.5625,  218047.2500],
        [1226002.1250,  541644.0625],
        [ 231333.5625,  745635.6250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 311/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:35, 59.17s/it]  7%|▋         | 2/30 [00:59<11:34, 24.80s/it] 10%|█         | 3/30 [01:00<06:16, 13.95s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.534326505661011
Epoch 312/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:50, 57.60s/it]  7%|▋         | 2/30 [00:59<11:31, 24.69s/it] 10%|█         | 3/30 [01:00<06:16, 13.96s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.86s/it] 20%|██        | 6/30 [01:02<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:04<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.5278887271881105
Epoch 313/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:47, 63.71s/it]  7%|▋         | 2/30 [01:04<12:32, 26.87s/it] 10%|█         | 3/30 [01:06<06:50, 15.20s/it] 13%|█▎        | 4/30 [01:06<04:06,  9.49s/it] 17%|█▋        | 5/30 [01:07<02:38,  6.34s/it] 20%|██        | 6/30 [01:08<01:46,  4.44s/it] 23%|██▎       | 7/30 [01:09<01:14,  3.23s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.44s/it] 30%|███       | 9/30 [01:10<00:40,  1.91s/it] 33%|███▎      | 10/30 [01:11<00:31,  1.55s/it] 37%|███▋      | 11/30 [01:12<00:24,  1.31s/it] 40%|████      | 12/30 [01:12<00:20,  1.14s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:14,  1.07it/s] 50%|█████     | 15/30 [01:15<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:18<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:21<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:23<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:24<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.88s/it]
Epoch loss is 2.4963082949320476
Epoch 314/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:01, 62.13s/it]  7%|▋         | 2/30 [01:02<12:08, 26.02s/it] 10%|█         | 3/30 [01:03<06:30, 14.48s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 2.4847481489181518
Epoch 315/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<28:01, 57.99s/it]  7%|▋         | 2/30 [01:01<12:00, 25.73s/it] 10%|█         | 3/30 [01:01<06:26, 14.32s/it] 13%|█▎        | 4/30 [01:02<03:53,  8.96s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.00s/it] 20%|██        | 6/30 [01:04<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.5276511907577515
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0250,  0.0024,  0.0099,  ..., -0.0010,  0.0077,  0.0054],
        [-0.0010,  0.0162,  0.0247,  ...,  0.0143, -0.0030, -0.0163],
        [-0.0313, -0.0371,  0.0218,  ...,  0.0794, -0.0116, -0.0185],
        ...,
        [ 0.0134, -0.0056,  0.0029,  ..., -0.0225, -0.0145, -0.0111],
        [-0.0296,  0.0048, -0.0072,  ...,  0.0016,  0.0138, -0.0127],
        [-0.0367, -0.0113,  0.0157,  ...,  0.0373,  0.0312, -0.0376]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8928, 0.8540, 0.8482, 0.8473, 0.8470, 0.8452, 0.8401, 0.8382, 0.8356,
         0.8315],
        [0.9483, 0.9399, 0.9391, 0.9371, 0.9369, 0.9323, 0.9323, 0.9321, 0.9309,
         0.9308],
        [0.9109, 0.9065, 0.8962, 0.8814, 0.8728, 0.8607, 0.8557, 0.8488, 0.8468,
         0.8382],
        [0.9542, 0.9047, 0.8995, 0.8934, 0.8906, 0.8852, 0.8833, 0.8830, 0.8829,
         0.8798],
        [0.8506, 0.8485, 0.8401, 0.8227, 0.8224, 0.8168, 0.8063, 0.8059, 0.8007,
         0.7964],
        [0.8600, 0.8249, 0.8171, 0.8126, 0.8050, 0.8018, 0.7931, 0.7898, 0.7880,
         0.7811],
        [0.8857, 0.8850, 0.8734, 0.8734, 0.8664, 0.8656, 0.8625, 0.8620, 0.8616,
         0.8612],
        [0.9102, 0.8902, 0.8844, 0.8684, 0.8608, 0.8554, 0.8348, 0.8348, 0.8287,
         0.8249],
        [0.9168, 0.9086, 0.9020, 0.8977, 0.8975, 0.8918, 0.8916, 0.8906, 0.8895,
         0.8881],
        [0.9525, 0.8949, 0.8935, 0.8932, 0.8885, 0.8838, 0.8833, 0.8828, 0.8716,
         0.8701],
        [0.9503, 0.9111, 0.9085, 0.9083, 0.9058, 0.9019, 0.9009, 0.8959, 0.8957,
         0.8946],
        [0.9054, 0.8904, 0.8691, 0.8667, 0.8472, 0.8266, 0.8255, 0.8081, 0.8073,
         0.8028],
        [0.9350, 0.9152, 0.9134, 0.9077, 0.9021, 0.8930, 0.8860, 0.8836, 0.8743,
         0.8660],
        [0.9661, 0.9600, 0.9591, 0.9570, 0.9569, 0.9565, 0.9526, 0.9515, 0.9492,
         0.9473],
        [0.9455, 0.9341, 0.9340, 0.9340, 0.9331, 0.9258, 0.9247, 0.9234, 0.9230,
         0.9197],
        [0.9298, 0.9263, 0.9263, 0.9190, 0.9161, 0.9159, 0.9140, 0.9107, 0.9103,
         0.9091],
        [0.9610, 0.9557, 0.9480, 0.9436, 0.9347, 0.9340, 0.9308, 0.9296, 0.9270,
         0.9260],
        [0.9669, 0.9611, 0.9553, 0.9427, 0.9415, 0.9407, 0.9407, 0.9403, 0.9378,
         0.9350],
        [0.9335, 0.9288, 0.9194, 0.9042, 0.9017, 0.9013, 0.9011, 0.8992, 0.8977,
         0.8959],
        [0.9129, 0.9051, 0.9043, 0.9039, 0.9035, 0.9024, 0.9023, 0.9007, 0.9006,
         0.8989],
        [0.9553, 0.9455, 0.9437, 0.9431, 0.9397, 0.9390, 0.9382, 0.9372, 0.9365,
         0.9346],
        [0.9319, 0.9203, 0.9200, 0.9174, 0.9151, 0.9122, 0.9105, 0.9085, 0.9034,
         0.9023],
        [0.9727, 0.9499, 0.9462, 0.9456, 0.9389, 0.9376, 0.9321, 0.9225, 0.9209,
         0.9193],
        [0.9471, 0.9413, 0.9354, 0.9303, 0.9300, 0.9293, 0.9282, 0.9273, 0.9266,
         0.9247],
        [0.9607, 0.9078, 0.9057, 0.8885, 0.8833, 0.8792, 0.8784, 0.8771, 0.8750,
         0.8745],
        [0.9439, 0.9400, 0.9381, 0.9315, 0.9270, 0.9262, 0.9220, 0.9190, 0.9131,
         0.9100],
        [0.9338, 0.9338, 0.9331, 0.9255, 0.9181, 0.9110, 0.9107, 0.9077, 0.9072,
         0.9044],
        [0.9638, 0.9519, 0.9491, 0.9366, 0.9322, 0.9289, 0.9260, 0.9248, 0.9237,
         0.9234],
        [0.9515, 0.9439, 0.9362, 0.9355, 0.9310, 0.9301, 0.9281, 0.9280, 0.9280,
         0.9239],
        [0.9391, 0.9344, 0.9311, 0.9287, 0.9281, 0.9280, 0.9257, 0.9217, 0.9197,
         0.9196],
        [0.9550, 0.9505, 0.9493, 0.9492, 0.9485, 0.9482, 0.9407, 0.9398, 0.9375,
         0.9362],
        [0.9467, 0.9260, 0.9253, 0.9192, 0.9120, 0.9081, 0.9071, 0.9057, 0.9045,
         0.9007],
        [0.8859, 0.8771, 0.8736, 0.8639, 0.8638, 0.8628, 0.8601, 0.8592, 0.8588,
         0.8579],
        [0.9175, 0.9049, 0.8987, 0.8974, 0.8952, 0.8933, 0.8893, 0.8889, 0.8879,
         0.8872],
        [0.9289, 0.9239, 0.9228, 0.9218, 0.9107, 0.9072, 0.9026, 0.9008, 0.8995,
         0.8971],
        [0.9122, 0.9075, 0.9022, 0.8941, 0.8881, 0.8840, 0.8828, 0.8803, 0.8705,
         0.8685],
        [0.9483, 0.8719, 0.8656, 0.8581, 0.8576, 0.8534, 0.8426, 0.8402, 0.8345,
         0.8302],
        [0.9168, 0.9022, 0.9007, 0.8947, 0.8789, 0.8787, 0.8759, 0.8754, 0.8692,
         0.8689],
        [0.9027, 0.9019, 0.8880, 0.8756, 0.8695, 0.8608, 0.8597, 0.8594, 0.8498,
         0.8472],
        [0.9450, 0.9432, 0.9402, 0.9400, 0.9393, 0.9387, 0.9335, 0.9330, 0.9329,
         0.9311],
        [0.8922, 0.8901, 0.8866, 0.8812, 0.8693, 0.8679, 0.8663, 0.8532, 0.8445,
         0.8383],
        [0.9101, 0.8764, 0.8733, 0.8728, 0.8726, 0.8686, 0.8675, 0.8671, 0.8646,
         0.8622],
        [0.8968, 0.8895, 0.8870, 0.8720, 0.8677, 0.8620, 0.8617, 0.8568, 0.8488,
         0.8444],
        [0.9005, 0.8821, 0.8758, 0.8733, 0.8656, 0.8613, 0.8562, 0.8526, 0.8491,
         0.8470],
        [0.8693, 0.8457, 0.8439, 0.8383, 0.8293, 0.8291, 0.8197, 0.8152, 0.8021,
         0.8003],
        [0.8390, 0.8218, 0.8090, 0.7940, 0.7876, 0.7823, 0.7816, 0.7661, 0.7628,
         0.7593],
        [0.9131, 0.9076, 0.9004, 0.8978, 0.8975, 0.8950, 0.8897, 0.8879, 0.8868,
         0.8859],
        [0.8237, 0.7987, 0.7966, 0.7922, 0.7785, 0.7760, 0.7756, 0.7648, 0.7531,
         0.7455],
        [0.8571, 0.8450, 0.8214, 0.8203, 0.8184, 0.8140, 0.8103, 0.8058, 0.8010,
         0.8001],
        [0.9045, 0.8762, 0.8654, 0.8640, 0.8469, 0.8452, 0.8409, 0.8396, 0.8331,
         0.8327],
        [0.8951, 0.8617, 0.8603, 0.8579, 0.8461, 0.8361, 0.8284, 0.8242, 0.8189,
         0.8189],
        [0.8998, 0.8946, 0.8609, 0.8412, 0.8407, 0.8368, 0.8225, 0.8202, 0.8196,
         0.8194],
        [0.9359, 0.9194, 0.9169, 0.9048, 0.8818, 0.8755, 0.8708, 0.8675, 0.8598,
         0.8537],
        [0.9213, 0.8794, 0.8789, 0.8700, 0.8689, 0.8637, 0.8630, 0.8546, 0.8528,
         0.8496],
        [0.9371, 0.9174, 0.9134, 0.9123, 0.9063, 0.9063, 0.9056, 0.9038, 0.9033,
         0.9002],
        [0.9147, 0.8918, 0.8862, 0.8833, 0.8799, 0.8798, 0.8793, 0.8791, 0.8776,
         0.8748],
        [0.9238, 0.8884, 0.8848, 0.8801, 0.8739, 0.8655, 0.8493, 0.8437, 0.8429,
         0.8416],
        [0.9438, 0.8922, 0.8820, 0.8798, 0.8682, 0.8588, 0.8520, 0.8515, 0.8496,
         0.8416],
        [0.9205, 0.8693, 0.8692, 0.8614, 0.8487, 0.8443, 0.8442, 0.8426, 0.8398,
         0.8374],
        [0.9064, 0.8325, 0.8317, 0.8242, 0.8148, 0.8110, 0.8103, 0.8074, 0.8028,
         0.8027],
        [0.8847, 0.8819, 0.8732, 0.8702, 0.8694, 0.8649, 0.8633, 0.8628, 0.8626,
         0.8621],
        [0.8599, 0.7925, 0.7921, 0.7855, 0.7805, 0.7512, 0.7465, 0.7445, 0.7426,
         0.7398],
        [0.8918, 0.8654, 0.8638, 0.8365, 0.8297, 0.8296, 0.8286, 0.8178, 0.8175,
         0.8164],
        [0.8456, 0.8282, 0.8158, 0.7993, 0.7992, 0.7864, 0.7795, 0.7785, 0.7777,
         0.7754]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 1, 0],
        [1, 0, 1, 1, 1, 0, 1, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],
        [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 1, 0, 1, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 0, 1],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 1, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 1, 1, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 346131.1250,  198923.7656,  183064.1406,  180542.3438,  179872.0938,
          175391.4688,  162900.1406,  158665.1406,  152944.2969,  144123.0469],
        [ 764095.2500,  677835.9375,  670716.2500,  651259.5000,  649402.0000,
          608649.2500,  608085.8750,  607019.7500,  596351.1250,  595429.3750],
        [ 447907.6875,  420929.2812,  363277.8125,  293983.2500,  260210.6875,
          218690.1406,  203633.5312,  184484.0938,  179385.7656,  158519.6406],
        [ 831987.5000,  409875.8750,  380914.5312,  349173.3438,  335185.6250,
          310309.5625,  302015.2500,  300662.8438,  300412.0312,  287298.8750],
        [ 189493.5156,  183780.4531,  162968.9688,  127097.6953,  126536.1641,
          116911.4844,  100616.7969,   99954.9609,   92784.1562,   87285.9297],
        [ 216446.1719,  131201.8906,  117366.7031,  109962.0000,   98709.5625,
           94257.9688,   83261.4922,   79456.7031,   77418.4844,   70155.1406],
        [ 312612.1562,  309680.4375,  262294.0625,  262114.2656,  237150.3438,
          234616.2344,  224536.0781,  222898.3906,  221614.1094,  220188.7344],
        [ 443405.1562,  333224.8438,  306779.7500,  244137.0781,  218987.5469,
          202770.5781,  151125.1094,  151067.6094,  138472.4375,  131121.2031],
        [ 487397.4062,  433845.5938,  394355.7812,  371156.8125,  370259.1875,
          341244.5312,  340311.1250,  335171.8750,  330317.7812,  323734.5938],
        [ 811805.8750,  356497.4688,  349340.2188,  348124.0000,  325233.9062,
          304158.5000,  302231.0312,  300086.1875,  255599.5781,  250294.9062],
        [ 786794.7500,  449160.5938,  433268.3750,  431548.7500,  416769.8750,
          393860.4062,  388510.7500,  361678.6875,  360902.4062,  354827.6562],
        [ 414464.6250,  334585.5312,  246657.0938,  238402.9844,  180494.1562,
          134414.4688,  132335.7500,  103115.5859,  102040.1953,   95632.1797],
        [ 632248.3750,  476656.5938,  464257.5938,  427914.0625,  394980.5938,
          346784.9688,  313965.3125,  303203.6562,  265711.0625,  236006.4531],
        [ 985868.3750,  903412.8125,  892296.9375,  865475.0625,  865096.3125,
          859129.0000,  812503.0000,  800560.3125,  774092.9375,  753852.0000],
        [ 734889.1250,  623878.5000,  623628.6250,  623205.9375,  615489.1250,
          554354.8125,  545833.6875,  535706.8750,  532674.6250,  508122.7188],
        [ 586747.3750,  558779.9375,  558239.3125,  503396.4062,  482685.6875,
          481415.5312,  468250.1562,  446641.6562,  444600.1250,  436928.9688],
        [ 916725.5000,  849755.7500,  761507.6875,  715018.3125,  629854.9375,
          623031.8750,  595482.7500,  585117.3750,  564096.1875,  556248.0625],
        [ 997823.3750,  917927.5000,  844995.1250,  705750.6250,  694239.9375,
          686365.8750,  686014.4375,  681619.0625,  657998.7500,  632253.8125],
        [ 619251.3125,  578732.0000,  505860.4062,  406964.0312,  393098.6562,
          390917.2812,  389550.3438,  379080.4688,  371261.2500,  361511.7812],
        [ 461019.1875,  412248.4062,  407615.0312,  405592.0312,  403235.9375,
          396630.9062,  396584.7500,  387634.7500,  386853.3125,  377631.0625],
        [ 845022.5000,  735086.8125,  716300.0625,  709986.0000,  676378.5000,
          669189.2500,  662000.6875,  652583.7500,  646190.6875,  628598.3750],
        [ 604754.7500,  512257.4375,  510100.7188,  491953.8125,  476020.1562,
          456672.1875,  445606.1250,  433313.4375,  402401.5312,  396402.5000],
        [1082987.6250,  782497.2500,  741573.8125,  735262.7500,  668729.9375,
          656674.8125,  606583.4375,  528969.0625,  516850.0312,  505616.8750],
        [ 751247.5625,  692020.3125,  636039.0000,  591574.8750,  588892.1875,
          582936.2500,  574015.8125,  566738.8750,  560948.1875,  546133.1250],
        [ 912787.5000,  428660.3125,  415890.0312,  325366.9688,  302319.5312,
          284912.8125,  281781.2500,  276629.3125,  268446.3438,  266537.4062],
        [ 718075.6250,  679367.7500,  660884.8125,  601697.6875,  564153.7500,
          557706.1250,  524806.0000,  503432.8750,  462602.0000,  442439.9688],
        [ 621891.4375,  621641.2500,  615349.4375,  551677.1875,  496619.5938,
          448626.7812,  446641.6562,  428218.2188,  425226.7188,  408394.4062],
        [ 953493.6875,  804880.2500,  773767.5000,  646736.3750,  607812.1875,
          579518.4375,  555665.8750,  546769.9375,  538434.7500,  535443.3125],
        [ 800728.3125,  718024.9375,  643537.6875,  636484.4375,  597172.9375,
          589911.2500,  573237.8750,  572410.8125,  571784.5000,  539491.5000],
        [ 670293.0000,  626473.8125,  598227.4375,  577563.6250,  572808.9375,
          572511.8125,  553755.0625,  522943.0000,  507937.6250,  507263.3125],
        [ 841165.5625,  789554.1250,  775090.2500,  774208.8750,  766721.6250,
          763211.8125,  686083.1250,  677605.8750,  655590.3750,  643073.2500],
        [ 747227.6250,  556065.6250,  550562.4375,  504428.1875,  455376.1875,
          430482.9375,  424297.8438,  415992.7500,  408966.5312,  387589.6562],
        [ 313738.1250,  276409.6250,  262983.8438,  228986.0625,  228621.2031,
          225570.1406,  216866.6406,  214065.7031,  213031.5625,  210103.3594],
        [ 492737.4688,  411421.6562,  376531.3438,  369288.3750,  358224.0000,
          348682.8438,  329200.4375,  327452.5938,  322862.6250,  319240.2812],
        [ 579627.8750,  539300.1875,  531156.3125,  523636.1875,  447084.4688,
          424762.5938,  397763.0938,  388048.6250,  380553.2500,  368100.2500],
        [ 456444.5000,  427111.6875,  395657.3125,  352612.7188,  323390.2188,
          305168.1562,  299827.3125,  289460.7812,  251510.9219,  244609.9375],
        [ 764759.3750,  256719.4219,  234594.7656,  210695.9062,  209360.5000,
          197157.8750,  168922.9688,  163236.5156,  150401.6094,  141460.0625],
        [ 487849.4375,  395565.6562,  387364.9688,  355531.1875,  283676.2188,
          282736.2812,  271805.5312,  269783.4375,  247156.7500,  245814.5156],
        [ 398819.0625,  394083.5938,  323063.7812,  270721.3438,  248151.5312,
          219090.9375,  215670.7969,  214827.9219,  187188.5312,  180448.7188],
        [ 729184.0000,  710570.5625,  681312.9375,  678994.0625,  672704.0000,
          666335.0000,  618890.5625,  614229.5625,  613229.8750,  597707.3750],
        [ 343007.3750,  332827.2188,  316733.8750,  293018.6875,  247271.5781,
          242340.2344,  237144.2344,  196497.6875,  173465.5156,  158823.7969],
        [ 442893.3750,  273639.1562,  261923.3594,  260022.6406,  259343.3438,
          244948.4219,  241055.8906,  239665.1875,  231182.0469,  223538.2969],
        [ 366208.7500,  329876.4375,  318549.6250,  256987.6406,  241598.1250,
          222991.7188,  221924.5938,  206860.6250,  184606.4062,  173233.0781],
        [ 386343.4375,  296985.4688,  271384.6562,  261935.0938,  234695.9062,
          220485.0312,  205106.5938,  194768.3906,  185267.2656,  179763.7188],
        [ 247210.5156,  176460.7031,  172092.6094,  158944.2500,  139704.0469,
          139336.2812,  121773.0391,  114167.1797,   94677.1641,   92270.4219],
        [ 160506.0469,  125472.4453,  104559.1719,   84379.2422,   77029.7109,
           71348.9453,   70660.2188,   56593.6016,   53995.6602,   51412.4336],
        [ 462135.9375,  427518.4062,  385608.7188,  371912.2500,  370290.2500,
          357169.9062,  331187.7188,  322470.3125,  317831.0625,  313430.3750],
        [ 129000.6094,   90206.7578,   87502.2969,   82277.8203,   67587.3906,
           65200.7383,   64857.1719,   55605.4727,   47060.9219,   42201.9141],
        [ 207770.2812,  174873.7031,  124718.1094,  122760.4219,  119500.3281,
          112192.9531,  106451.0391,   99782.6797,   93243.6484,   92056.0469],
        [ 409003.9688,  272944.5625,  233886.1875,  229191.2031,  179514.4531,
          175362.2031,  164873.6562,  161767.8438,  147474.2969,  146563.5000],
        [ 357827.5625,  222050.3438,  217422.8750,  210158.6719,  177577.6094,
          153966.9531,  137910.7500,  129849.9219,  120391.7578,  120382.5703],
        [ 382490.7500,  354813.7812,  219335.1094,  165562.5312,  164275.0469,
          155584.2656,  126833.3594,  122653.1094,  121621.9219,  121218.9609],
        [ 640412.8125,  506149.9375,  487967.6250,  410848.0000,  295695.6250,
          270118.1250,  252776.9531,  241135.6875,  215951.5312,  198022.4219],
        [ 520286.0938,  285587.9688,  283703.7812,  249828.2031,  246105.3750,
          228318.5781,  225996.0625,  200550.0938,  195499.5469,  186671.9219],
        [ 651547.7500,  491412.6875,  464564.0938,  457223.0312,  419858.8125,
          419858.8125,  415539.9688,  405173.3438,  401796.0312,  384397.4062],
        [ 473401.1562,  340895.4688,  314839.9062,  302288.0938,  287857.2500,
          287282.6875,  285294.2812,  284452.0625,  278466.0000,  267578.8438],
        [ 539116.0625,  325065.5312,  308469.3125,  288701.5625,  264290.5000,
          234412.2656,  185816.1562,  171509.4844,  169716.6250,  166497.5000],
        [ 716824.1875,  342851.7188,  296477.5312,  287452.3438,  243550.6094,
          212925.5312,  193226.1562,  191701.2500,  186783.0312,  166454.9531],
        [ 513977.0000,  247209.7969,  247018.2031,  220948.1094,  184228.1094,
          173008.5312,  172942.0625,  168872.7188,  162237.5156,  156843.7031],
        [ 420283.8750,  146159.9688,  144595.8125,  129934.1641,  113504.9453,
          107507.9922,  106482.7188,  102226.1328,   95638.8281,   95581.3906],
        [ 308068.5938,  295986.2500,  261398.3281,  250459.4219,  247556.5938,
          232383.2031,  226970.1719,  225517.6719,  224845.5000,  223044.0469],
        [ 216285.8281,   82557.6328,   82079.6094,   74738.8281,   69518.2422,
           45743.3398,   42804.4453,   41572.4062,   40505.9531,   38876.7812],
        [ 340930.2812,  234100.6250,  228656.7656,  154831.7344,  140497.1406,
          140381.0312,  138347.1719,  118458.8125,  118044.5938,  116173.9297],
        [ 176340.7656,  137501.1406,  115157.7812,   91006.0391,   90844.0547,
           75726.5625,   68594.6719,   67604.4766,   66842.9844,   64720.4336]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[346131.1250,      0.0000],
         [198923.7656,      0.0000],
         [183064.1406,      0.0000],
         ...,
         [158665.1406,      0.0000],
         [     0.0000, 152944.2969],
         [144123.0469,      0.0000]],

        [[764095.2500,      0.0000],
         [677835.9375,      0.0000],
         [670716.2500,      0.0000],
         ...,
         [607019.7500,      0.0000],
         [596351.1250,      0.0000],
         [595429.3750,      0.0000]],

        [[447907.6875,      0.0000],
         [420929.2812,      0.0000],
         [363277.8125,      0.0000],
         ...,
         [184484.0938,      0.0000],
         [179385.7656,      0.0000],
         [     0.0000, 158519.6406]],

        ...,

        [[     0.0000, 216285.8281],
         [ 82557.6328,      0.0000],
         [ 82079.6094,      0.0000],
         ...,
         [ 41572.4062,      0.0000],
         [ 40505.9531,      0.0000],
         [ 38876.7812,      0.0000]],

        [[340930.2812,      0.0000],
         [     0.0000, 234100.6250],
         [228656.7656,      0.0000],
         ...,
         [118458.8125,      0.0000],
         [118044.5938,      0.0000],
         [116173.9297,      0.0000]],

        [[     0.0000, 176340.7656],
         [     0.0000, 137501.1406],
         [     0.0000, 115157.7812],
         ...,
         [ 67604.4766,      0.0000],
         [     0.0000,  66842.9844],
         [ 64720.4336,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1729613.3750,  152944.2969],
        [6428844.0000,       0.0000],
        [2572502.2500,  158519.6406],
        [3807835.5000,       0.0000],
        [ 534320.7500,  753109.3125],
        [ 960869.4375,  117366.7031],
        [2008260.5000,  499444.4062],
        [2321091.2500,       0.0000],
        [3356638.0000,  371156.8125],
        [3603371.7500,       0.0000],
        [4377322.0000,       0.0000],
        [1982142.5000,       0.0000],
        [3861728.7500,       0.0000],
        [8512287.0000,       0.0000],
        [5897784.0000,       0.0000],
        [4967685.0000,       0.0000],
        [6796839.0000,       0.0000],
        [7504988.0000,       0.0000],
        [4396227.5000,       0.0000],
        [4035045.5000,       0.0000],
        [6941337.0000,       0.0000],
        [4729483.0000,       0.0000],
        [6825746.0000,       0.0000],
        [6090546.0000,       0.0000],
        [3763331.5000,       0.0000],
        [5715166.5000,       0.0000],
        [5064286.5000,       0.0000],
        [6542522.0000,       0.0000],
        [6242784.0000,       0.0000],
        [5709777.5000,       0.0000],
        [7372305.0000,       0.0000],
        [4880989.5000,       0.0000],
        [2390376.2500,       0.0000],
        [2909822.0000,  745819.7500],
        [2102310.7500, 2477722.0000],
        [1228400.7500, 2117393.0000],
        [1187571.8750, 1309737.2500],
        [1705601.5000, 1521682.5000],
        [1073120.3750, 1578945.8750],
        [6583157.5000,       0.0000],
        [2541130.2500,       0.0000],
        [2678211.7500,       0.0000],
        [2522837.0000,       0.0000],
        [2436735.5000,       0.0000],
        [ 348548.4062, 1108087.8750],
        [ 408826.2188,  447131.2812],
        [3197419.0000,  462135.9375],
        [ 478286.1875,  253214.8750],
        [ 546292.3750,  707056.7500],
        [ 556478.2500, 1564103.5000],
        [1573189.5000,  274349.5312],
        [ 977749.1875,  956639.6250],
        [1881416.5000, 1637662.2500],
        [2372719.2500,  249828.2031],
        [4511372.0000,       0.0000],
        [3122355.5000,       0.0000],
        [1218787.0000, 1434808.0000],
        [1964392.7500,  873854.6250],
        [1229999.3750, 1017286.3125],
        [ 612969.5625,  848946.2500],
        [1659791.7500,  836438.0000],
        [ 518397.2500,  216285.8281],
        [1201108.7500,  529313.3750],
        [ 223330.9531,  731007.9375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 316/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:59, 59.98s/it]  7%|▋         | 2/30 [01:00<11:43, 25.14s/it] 10%|█         | 3/30 [01:02<06:35, 14.66s/it] 13%|█▎        | 4/30 [01:03<03:58,  9.16s/it] 17%|█▋        | 5/30 [01:04<02:33,  6.13s/it] 20%|██        | 6/30 [01:05<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:05<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.38s/it] 30%|███       | 9/30 [01:07<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 2.482736261685689
Epoch 317/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:09, 58.25s/it]  7%|▋         | 2/30 [00:58<11:23, 24.43s/it] 10%|█         | 3/30 [01:01<06:25, 14.29s/it] 13%|█▎        | 4/30 [01:01<03:52,  8.94s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.99s/it] 20%|██        | 6/30 [01:03<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.34s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.495572336514791
Epoch 318/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:32, 61.12s/it]  7%|▋         | 2/30 [01:02<11:59, 25.70s/it] 10%|█         | 3/30 [01:02<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.99s/it] 20%|██        | 6/30 [01:05<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.5275935967763266
Epoch 319/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:52, 59.74s/it]  7%|▋         | 2/30 [01:01<12:01, 25.76s/it] 10%|█         | 3/30 [01:02<06:27, 14.34s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.97s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.01s/it] 20%|██        | 6/30 [01:04<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.476514991124471
Epoch 320/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:55, 61.90s/it]  7%|▋         | 2/30 [01:03<12:17, 26.32s/it] 10%|█         | 3/30 [01:04<06:35, 14.65s/it] 13%|█▎        | 4/30 [01:04<03:58,  9.16s/it] 17%|█▋        | 5/30 [01:05<02:33,  6.13s/it] 20%|██        | 6/30 [01:06<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.38s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 2.516847825050354
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0249,  0.0023,  0.0101,  ..., -0.0013,  0.0083,  0.0053],
        [-0.0012,  0.0157,  0.0250,  ...,  0.0144, -0.0026, -0.0164],
        [-0.0311, -0.0373,  0.0221,  ...,  0.0796, -0.0111, -0.0182],
        ...,
        [ 0.0128, -0.0052,  0.0035,  ..., -0.0224, -0.0137, -0.0107],
        [-0.0299,  0.0052, -0.0069,  ...,  0.0019,  0.0140, -0.0126],
        [-0.0364, -0.0114,  0.0160,  ...,  0.0378,  0.0318, -0.0368]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8918, 0.8534, 0.8476, 0.8470, 0.8467, 0.8442, 0.8389, 0.8380, 0.8345,
         0.8307],
        [0.9478, 0.9389, 0.9384, 0.9359, 0.9358, 0.9321, 0.9313, 0.9313, 0.9299,
         0.9298],
        [0.9104, 0.9060, 0.8947, 0.8805, 0.8715, 0.8596, 0.8554, 0.8484, 0.8461,
         0.8369],
        [0.9545, 0.9032, 0.8985, 0.8921, 0.8896, 0.8836, 0.8829, 0.8815, 0.8812,
         0.8783],
        [0.8495, 0.8484, 0.8394, 0.8220, 0.8212, 0.8164, 0.8066, 0.8056, 0.8000,
         0.7948],
        [0.8597, 0.8243, 0.8150, 0.8123, 0.8046, 0.8002, 0.7919, 0.7900, 0.7857,
         0.7804],
        [0.8841, 0.8834, 0.8721, 0.8708, 0.8648, 0.8646, 0.8610, 0.8608, 0.8603,
         0.8601],
        [0.9088, 0.8887, 0.8827, 0.8678, 0.8599, 0.8535, 0.8341, 0.8337, 0.8265,
         0.8230],
        [0.9163, 0.9081, 0.9008, 0.8965, 0.8957, 0.8913, 0.8911, 0.8896, 0.8890,
         0.8873],
        [0.9522, 0.8944, 0.8927, 0.8925, 0.8876, 0.8827, 0.8823, 0.8823, 0.8711,
         0.8697],
        [0.9496, 0.9096, 0.9077, 0.9071, 0.9048, 0.9008, 0.8991, 0.8948, 0.8946,
         0.8933],
        [0.9042, 0.8890, 0.8678, 0.8652, 0.8455, 0.8243, 0.8231, 0.8075, 0.8059,
         0.7998],
        [0.9336, 0.9146, 0.9131, 0.9065, 0.9004, 0.8918, 0.8847, 0.8822, 0.8737,
         0.8638],
        [0.9655, 0.9596, 0.9587, 0.9569, 0.9563, 0.9559, 0.9520, 0.9510, 0.9486,
         0.9463],
        [0.9453, 0.9337, 0.9336, 0.9334, 0.9325, 0.9252, 0.9241, 0.9226, 0.9222,
         0.9193],
        [0.9295, 0.9261, 0.9260, 0.9186, 0.9158, 0.9156, 0.9136, 0.9103, 0.9102,
         0.9080],
        [0.9604, 0.9551, 0.9469, 0.9429, 0.9342, 0.9335, 0.9299, 0.9276, 0.9263,
         0.9252],
        [0.9666, 0.9607, 0.9550, 0.9425, 0.9412, 0.9406, 0.9401, 0.9398, 0.9371,
         0.9343],
        [0.9329, 0.9276, 0.9182, 0.9037, 0.9011, 0.9008, 0.9003, 0.8984, 0.8966,
         0.8942],
        [0.9123, 0.9045, 0.9036, 0.9034, 0.9022, 0.9019, 0.9005, 0.9005, 0.9003,
         0.8982],
        [0.9544, 0.9447, 0.9430, 0.9424, 0.9384, 0.9380, 0.9376, 0.9364, 0.9362,
         0.9336],
        [0.9312, 0.9192, 0.9190, 0.9171, 0.9144, 0.9122, 0.9099, 0.9082, 0.9020,
         0.9008],
        [0.9724, 0.9499, 0.9456, 0.9454, 0.9384, 0.9369, 0.9317, 0.9222, 0.9202,
         0.9187],
        [0.9466, 0.9408, 0.9351, 0.9299, 0.9293, 0.9291, 0.9280, 0.9271, 0.9263,
         0.9243],
        [0.9603, 0.9075, 0.9046, 0.8880, 0.8819, 0.8776, 0.8775, 0.8758, 0.8736,
         0.8730],
        [0.9435, 0.9395, 0.9377, 0.9311, 0.9266, 0.9257, 0.9217, 0.9185, 0.9125,
         0.9093],
        [0.9331, 0.9327, 0.9324, 0.9245, 0.9173, 0.9101, 0.9097, 0.9066, 0.9065,
         0.9038],
        [0.9635, 0.9516, 0.9486, 0.9359, 0.9317, 0.9286, 0.9251, 0.9242, 0.9232,
         0.9226],
        [0.9510, 0.9433, 0.9353, 0.9349, 0.9305, 0.9298, 0.9280, 0.9272, 0.9267,
         0.9231],
        [0.9386, 0.9337, 0.9307, 0.9286, 0.9276, 0.9276, 0.9250, 0.9214, 0.9193,
         0.9188],
        [0.9543, 0.9500, 0.9491, 0.9489, 0.9480, 0.9479, 0.9403, 0.9392, 0.9367,
         0.9358],
        [0.9460, 0.9256, 0.9248, 0.9186, 0.9114, 0.9072, 0.9062, 0.9056, 0.9041,
         0.9002],
        [0.8852, 0.8764, 0.8736, 0.8639, 0.8627, 0.8618, 0.8593, 0.8589, 0.8584,
         0.8571],
        [0.9164, 0.9040, 0.8978, 0.8961, 0.8941, 0.8927, 0.8882, 0.8880, 0.8873,
         0.8862],
        [0.9284, 0.9226, 0.9222, 0.9212, 0.9101, 0.9064, 0.9016, 0.8996, 0.8995,
         0.8963],
        [0.9117, 0.9068, 0.9019, 0.8933, 0.8882, 0.8831, 0.8824, 0.8807, 0.8695,
         0.8677],
        [0.9483, 0.8713, 0.8661, 0.8578, 0.8569, 0.8518, 0.8408, 0.8388, 0.8321,
         0.8295],
        [0.9155, 0.9005, 0.8995, 0.8934, 0.8786, 0.8781, 0.8746, 0.8742, 0.8677,
         0.8676],
        [0.9018, 0.9013, 0.8866, 0.8746, 0.8680, 0.8600, 0.8581, 0.8579, 0.8485,
         0.8457],
        [0.9443, 0.9420, 0.9392, 0.9392, 0.9387, 0.9378, 0.9330, 0.9324, 0.9322,
         0.9302],
        [0.8916, 0.8893, 0.8861, 0.8802, 0.8689, 0.8668, 0.8658, 0.8524, 0.8437,
         0.8374],
        [0.9094, 0.8754, 0.8719, 0.8719, 0.8710, 0.8674, 0.8672, 0.8663, 0.8631,
         0.8610],
        [0.8959, 0.8886, 0.8859, 0.8713, 0.8666, 0.8612, 0.8601, 0.8556, 0.8477,
         0.8433],
        [0.8991, 0.8818, 0.8743, 0.8725, 0.8644, 0.8604, 0.8554, 0.8524, 0.8485,
         0.8456],
        [0.8682, 0.8451, 0.8423, 0.8371, 0.8278, 0.8278, 0.8179, 0.8129, 0.7999,
         0.7981],
        [0.8386, 0.8205, 0.8079, 0.7935, 0.7874, 0.7820, 0.7814, 0.7661, 0.7625,
         0.7571],
        [0.9120, 0.9059, 0.8986, 0.8965, 0.8965, 0.8938, 0.8880, 0.8860, 0.8852,
         0.8843],
        [0.8224, 0.7974, 0.7958, 0.7907, 0.7758, 0.7753, 0.7730, 0.7647, 0.7513,
         0.7438],
        [0.8555, 0.8444, 0.8202, 0.8197, 0.8179, 0.8131, 0.8100, 0.8045, 0.8012,
         0.7988],
        [0.9034, 0.8752, 0.8640, 0.8615, 0.8457, 0.8444, 0.8394, 0.8382, 0.8317,
         0.8307],
        [0.8939, 0.8613, 0.8598, 0.8573, 0.8456, 0.8357, 0.8274, 0.8243, 0.8183,
         0.8179],
        [0.8991, 0.8940, 0.8599, 0.8389, 0.8389, 0.8353, 0.8203, 0.8183, 0.8183,
         0.8180],
        [0.9352, 0.9183, 0.9164, 0.9041, 0.8807, 0.8740, 0.8700, 0.8670, 0.8584,
         0.8522],
        [0.9206, 0.8786, 0.8786, 0.8688, 0.8683, 0.8624, 0.8624, 0.8537, 0.8519,
         0.8494],
        [0.9362, 0.9162, 0.9123, 0.9113, 0.9052, 0.9052, 0.9041, 0.9030, 0.9027,
         0.8997],
        [0.9137, 0.8911, 0.8860, 0.8817, 0.8791, 0.8786, 0.8780, 0.8776, 0.8768,
         0.8736],
        [0.9229, 0.8869, 0.8845, 0.8793, 0.8727, 0.8652, 0.8479, 0.8429, 0.8428,
         0.8403],
        [0.9436, 0.8910, 0.8815, 0.8784, 0.8673, 0.8585, 0.8505, 0.8502, 0.8485,
         0.8401],
        [0.9197, 0.8685, 0.8683, 0.8601, 0.8471, 0.8437, 0.8420, 0.8406, 0.8392,
         0.8366],
        [0.9059, 0.8315, 0.8306, 0.8227, 0.8133, 0.8098, 0.8092, 0.8061, 0.8012,
         0.8007],
        [0.8848, 0.8808, 0.8721, 0.8690, 0.8675, 0.8638, 0.8611, 0.8610, 0.8606,
         0.8605],
        [0.8596, 0.7913, 0.7911, 0.7843, 0.7793, 0.7492, 0.7445, 0.7426, 0.7401,
         0.7390],
        [0.8912, 0.8646, 0.8623, 0.8346, 0.8281, 0.8279, 0.8272, 0.8157, 0.8151,
         0.8148],
        [0.8451, 0.8264, 0.8140, 0.7976, 0.7975, 0.7860, 0.7786, 0.7765, 0.7751,
         0.7728]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 1, 0],
        [1, 0, 1, 1, 1, 0, 1, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 1, 0, 1],
        [1, 1, 1, 0, 0, 0, 1, 0, 1, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 341140.0625,  196995.8594,  181492.5156,  179917.5625,  179161.9688,
          172811.6562,  160307.4844,  158249.1094,  150424.2812,  142538.4062],
        [ 759169.1875,  668912.3750,  663712.5000,  640119.1250,  639269.9375,
          606732.1250,  599993.0000,  599509.6250,  588166.4375,  587058.5625],
        [ 444981.4375,  417973.1250,  355592.5625,  290336.6562,  255290.2031,
          215375.8594,  202881.6094,  183419.9375,  177544.9219,  155799.8438],
        [ 834980.1875,  401624.8125,  375505.7500,  342768.3438,  330687.8125,
          303631.6250,  300565.9375,  294540.0000,  293314.2188,  281284.5312],
        [ 186408.2656,  183608.4219,  161480.6875,  125893.6328,  124401.5391,
          116196.5234,  100948.4844,   99504.6172,   91916.6484,   85338.9609],
        [ 215573.5469,  130037.1797,  113864.8984,  109529.3359,   98177.8984,
           92212.5391,   81907.6562,   79675.7734,   74961.1875,   69497.2266],
        [ 305751.9375,  302548.2500,  257375.1719,  252837.2344,  231894.5938,
          231322.7344,  219724.1250,  219148.8281,  217562.4531,  217013.7344],
        [ 435106.4375,  326361.4062,  299581.7500,  242221.9375,  216391.6719,
          197399.4375,  149677.7188,  148689.7812,  134145.2969,  127718.2188],
        [ 484089.8750,  430678.7812,  387915.8125,  364775.5000,  360412.2812,
          338561.1875,  337806.1562,  330733.2188,  327810.6562,  319932.7812],
        [ 808165.2500,  354242.4062,  345767.5312,  344329.5000,  321181.9062,
          299625.7812,  297923.5938,  297792.9062,  253872.3281,  248814.3438],
        [ 778904.8750,  439660.5312,  428179.4062,  424241.5625,  410526.4688,
          387911.7500,  378626.6875,  356150.8438,  355083.9062,  348456.7812],
        [ 407054.8438,  327724.0625,  242170.6719,  233135.9219,  176067.1875,
          130129.7266,  127931.7969,  102295.9609,  100005.3047,   91655.2734],
        [ 619711.0000,  472386.0000,  462452.9062,  421111.9688,  385901.1875,
          341299.1875,  308001.5938,  297571.4688,  263286.7500,  228792.2188],
        [ 977925.8125,  898012.3750,  887086.5000,  864602.2500,  856628.7500,
          852552.1875,  806535.3750,  794232.6250,  768216.1875,  743163.9375],
        [ 732991.5625,  620766.8125,  619726.3750,  618305.3750,  609761.1875,
          549890.7500,  541140.4375,  529371.3125,  526761.5625,  504984.0938],
        [ 584413.0000,  556787.8125,  556161.0625,  500433.0000,  480749.7812,
          479545.0625,  465685.9062,  444017.9062,  443980.1875,  429777.3438],
        [ 908614.3750,  842404.3125,  749137.0000,  708287.1875,  624973.0000,
          618989.1875,  587828.8125,  569155.6875,  558536.4375,  549396.5000],
        [ 993202.6875,  913417.1250,  841119.0625,  704155.2500,  690640.3125,
          684717.6875,  680549.8750,  677581.3125,  651846.0625,  626088.0000],
        [ 613758.2500,  568555.6875,  497230.4688,  404477.2500,  389511.3438,
          388180.4062,  384950.6562,  375011.1875,  365204.6562,  353064.6250],
        [ 457345.5938,  408729.8750,  403690.7500,  402810.0625,  395555.4688,
          394135.4688,  386360.3750,  386225.9062,  385047.5938,  373966.9062],
        [ 834142.1250,  726286.5000,  708633.8125,  702607.6875,  663508.0625,
          660183.0000,  656604.6875,  645129.8125,  643074.5000,  619588.6875],
        [ 599165.5625,  504299.7500,  503322.0000,  489863.0000,  471431.4688,
          456643.9062,  441826.4688,  431191.2812,  394632.3125,  388061.9688],
        [1078970.2500,  782170.5000,  735825.3125,  733455.8750,  663884.0625,
          650061.9375,  603338.1875,  527013.3750,  511977.5938,  500757.1562],
        [ 745782.4375,  687154.4375,  632778.5625,  587493.7500,  582650.0000,
          581204.8750,  571867.3750,  564553.0625,  558112.6250,  542938.3125],
        [ 907711.0000,  427121.4375,  409540.2500,  323304.8125,  296065.2812,
          278670.3125,  278196.8438,  271560.9375,  263002.9062,  260634.8750],
        [ 714316.3125,  673878.4375,  657360.3125,  598113.3750,  561097.4375,
          553671.1250,  523269.2500,  499732.8750,  458302.6250,  438208.4375],
        [ 615059.5625,  612166.4375,  609053.9375,  543909.5000,  491189.1875,
          443158.2812,  440507.4688,  421167.8125,  421046.5312,  404806.0312],
        [ 949434.0000,  801025.4375,  768326.1250,  640060.5000,  603413.5625,
          577113.2500,  548844.5000,  542227.8125,  534373.5625,  529632.8125],
        [ 794546.9375,  712303.4375,  635356.3750,  631225.3750,  592914.5625,
          586656.1250,  571902.8125,  565849.3750,  561227.5000,  533111.1875],
        [ 666009.6875,  620698.7500,  594280.5625,  577064.2500,  568637.0000,
          568532.9375,  548288.9375,  520794.9375,  505535.8750,  501676.3750],
        [ 833455.8750,  783291.6875,  773066.8125,  771197.1875,  761487.3750,
          760350.1875,  682190.0000,  671171.8750,  647967.3750,  639263.8125],
        [ 739783.9375,  553063.6875,  546944.1250,  500496.4688,  451230.0000,
          424937.2500,  418964.0625,  415461.0938,  406684.6875,  384609.7500],
        [ 310309.2500,  273729.2188,  262982.3438,  228962.4688,  225023.5469,
          222366.9531,  214277.7188,  213189.0625,  211830.0000,  207816.8438],
        [ 484782.9062,  406143.6250,  371745.1875,  362876.8750,  352644.0312,
          345552.9062,  324181.0312,  323124.4688,  320060.3125,  315016.2188],
        [ 575236.7500,  529728.3125,  526787.6875,  518924.9062,  443017.9688,
          420278.6562,  392489.5625,  381274.7188,  380963.9688,  363997.7500],
        [ 453008.5938,  422553.7812,  394005.4375,  348690.5000,  323813.0312,
          301148.9375,  298370.5625,  291174.3438,  248169.0469,  241772.6094],
        [ 764567.5625,  254674.0000,  236302.3906,  209958.1562,  207297.4375,
          192510.6562,  164577.3594,  159909.7188,  145468.0156,  140138.3750],
        [ 478481.1250,  386506.6562,  380880.7812,  349127.7500,  282299.0000,
          280506.8750,  266811.5625,  265193.8750,  241810.1875,  241289.8125],
        [ 393469.6250,  390461.2188,  316905.1875,  266908.5312,  242895.5469,
          216687.8125,  210646.4688,  210167.6875,  183753.4688,  176482.5781],
        [ 722247.9375,  698591.5000,  671739.2500,  671286.4375,  666532.6250,
          658215.9375,  614097.8125,  608877.3750,  607162.1875,  590498.3125],
        [ 339934.8750,  329325.3750,  314371.2500,  288892.1562,  246101.1719,
          238606.7812,  235316.2656,  194214.3594,  171621.5625,  156847.1406],
        [ 438419.5000,  270047.3125,  256844.5625,  256671.4531,  253416.6094,
          240593.1250,  239989.7500,  237054.4688,  226373.3281,  219596.7344],
        [ 361949.2188,  325673.3750,  313730.0312,  254678.1406,  237917.8281,
          220233.0469,  216896.6406,  203514.7188,  181665.6719,  170728.9531],
        [ 378509.0000,  295514.3750,  265539.8125,  258890.8594,  230629.5312,
          217929.1875,  202729.2031,  194190.8281,  183790.4531,  176243.7500],
        [ 243490.9062,  174936.4219,  168294.5781,  156189.0312,  136820.1719,
          136757.9375,  118770.1094,  110496.1250,   91814.8516,   89507.9297],
        [ 159630.4219,  123247.4766,  102837.4688,   83778.5859,   76733.9375,
           71075.1172,   70456.6641,   56662.9453,   53783.5625,   49797.9961],
        [ 455485.2188,  417268.2188,  375709.5625,  365061.1875,  364934.1250,
          351214.5625,  323207.6875,  314041.6562,  310211.3125,  306347.6562],
        [ 126527.7109,   88605.0312,   86505.4609,   80501.8750,   65055.3359,
           64592.7344,   62513.3672,   55479.7734,   45854.1523,   41197.1914],
        [ 203124.2031,  173204.3281,  122709.7344,  121775.1172,  118701.6094,
          110767.1719,  106088.0156,   98067.7578,   93560.1406,   90347.1797],
        [ 402828.1250,  268927.2812,  229158.8594,  221134.4531,  176629.5781,
          173222.1719,  161435.4062,  158608.5625,  144481.2656,  142512.3125],
        [ 351393.4688,  220792.0312,  215938.5625,  208339.5312,  176287.6250,
          152963.9844,  136038.7344,  130033.9453,  119371.0469,  118715.1953],
        [ 378349.1250,  351930.0938,  216388.3750,  160235.1875,  160183.2344,
          152242.4375,  122811.2422,  119399.5156,  119291.1562,  118876.6406],
        [ 634506.8750,  497894.2812,  485094.5938,  406713.7500,  291125.1875,
          264463.2188,  249827.7344,  239466.4219,  211534.8438,  193816.5469],
        [ 514414.4062,  282640.3438,  282403.5000,  245687.9531,  243737.8750,
          224203.1250,  224055.2031,  198036.5781,  192914.0625,  186091.3906],
        [ 643016.8125,  483155.9375,  457404.4688,  450426.0000,  412838.5625,
          412838.5625,  406701.3750,  400059.3750,  398843.7812,  381866.4375],
        [ 466447.3125,  337556.5625,  313975.4688,  295357.7188,  284566.0312,
          282498.5938,  280238.6875,  278355.8125,  275258.2500,  262853.9688],
        [ 531973.5000,  317872.9062,  307200.1875,  285195.5000,  259823.8438,
          233191.2969,  182304.3438,  169741.7188,  169500.5156,  163429.3438],
        [ 715332.7500,  337278.2188,  294558.2812,  281784.7188,  240290.4375,
          211938.7031,  189223.9062,  188276.1719,  183849.3594,  162969.5938],
        [ 508122.7188,  244655.9062,  243734.8594,  217023.8906,  180166.5312,
          171486.2656,  167392.4375,  164216.9219,  160933.3750,  154940.8906],
        [ 416953.1562,  144084.5625,  142391.6875,  127196.3906,  111147.7969,
          105704.4531,  104843.3594,  100269.5547,   93498.0547,   92799.4688],
        [ 308753.5938,  291426.2812,  257339.3438,  246454.6406,  240964.1875,
          228683.5781,  219950.5312,  219805.2188,  218588.5938,  218269.2500],
        [ 215378.9375,   81153.4609,   80916.8203,   73449.9922,   68375.5469,
           44502.6641,   41599.3359,   40488.8086,   39086.2656,   38434.9922],
        [ 338418.8125,  231428.2188,  223963.1406,  150667.7812,  137379.9062,
          136838.1719,  135559.7969,  114992.1797,  114037.1484,  113574.0312],
        [ 175068.9375,  134081.7344,  112282.0156,   88837.1172,   88638.9297,
           75273.8906,   67677.1094,   65689.5547,   64388.0508,   62303.5078]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[341140.0625,      0.0000],
         [196995.8594,      0.0000],
         [181492.5156,      0.0000],
         ...,
         [158249.1094,      0.0000],
         [     0.0000, 150424.2812],
         [142538.4062,      0.0000]],

        [[759169.1875,      0.0000],
         [668912.3750,      0.0000],
         [663712.5000,      0.0000],
         ...,
         [599509.6250,      0.0000],
         [588166.4375,      0.0000],
         [587058.5625,      0.0000]],

        [[444981.4375,      0.0000],
         [417973.1250,      0.0000],
         [355592.5625,      0.0000],
         ...,
         [183419.9375,      0.0000],
         [177544.9219,      0.0000],
         [     0.0000, 155799.8438]],

        ...,

        [[     0.0000, 215378.9375],
         [ 81153.4609,      0.0000],
         [ 80916.8203,      0.0000],
         ...,
         [ 40488.8086,      0.0000],
         [ 39086.2656,      0.0000],
         [ 38434.9922,      0.0000]],

        [[338418.8125,      0.0000],
         [     0.0000, 231428.2188],
         [223963.1406,      0.0000],
         ...,
         [114992.1797,      0.0000],
         [114037.1484,      0.0000],
         [113574.0312,      0.0000]],

        [[     0.0000, 175068.9375],
         [     0.0000, 134081.7344],
         [     0.0000, 112282.0156],
         ...,
         [ 65689.5547,      0.0000],
         [     0.0000,  64388.0508],
         [ 62303.5078,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1712614.7500,  150424.2812],
        [6352643.0000,       0.0000],
        [2543396.2500,  155799.8438],
        [3758903.5000,       0.0000],
        [ 528499.2500,  747198.5000],
        [ 882075.1250,  183362.1250],
        [1971019.1250,  484159.9688],
        [2277293.7500,       0.0000],
        [3322304.0000,  360412.2812],
        [3571715.5000,       0.0000],
        [4307743.0000,       0.0000],
        [1938170.7500,       0.0000],
        [3800514.5000,       0.0000],
        [8448956.0000,       0.0000],
        [5853700.0000,       0.0000],
        [4941551.0000,       0.0000],
        [6717322.5000,       0.0000],
        [7463317.5000,       0.0000],
        [4339944.5000,       0.0000],
        [3993868.0000,       0.0000],
        [6859759.0000,       0.0000],
        [4680438.0000,       0.0000],
        [6787454.0000,       0.0000],
        [6054535.0000,       0.0000],
        [3715808.7500,       0.0000],
        [5677950.0000,       0.0000],
        [5002065.0000,       0.0000],
        [6494451.0000,       0.0000],
        [6185094.0000,       0.0000],
        [5671519.0000,       0.0000],
        [7323442.0000,       0.0000],
        [4842175.5000,       0.0000],
        [2370487.2500,       0.0000],
        [2871505.5000,  734622.0625],
        [2076965.6250, 2455734.7500],
        [1213644.3750, 2109062.5000],
        [1309956.2500, 1165447.2500],
        [1676493.1250, 1496414.5000],
        [1054478.8750, 1553899.2500],
        [6509249.0000,       0.0000],
        [2515231.0000,       0.0000],
        [2639007.0000,       0.0000],
        [2486987.5000,       0.0000],
        [2403967.0000,       0.0000],
        [ 339068.9062, 1088009.1250],
        [ 405625.8750,  442378.3125],
        [3127996.0000,  455485.2188],
        [ 466957.9062,  249874.7188],
        [ 539658.8125,  698686.4375],
        [ 545340.4375, 1533597.5000],
        [1557539.2500,  272335.0312],
        [ 953039.3750,  946667.6250],
        [1856809.2500, 1617634.1250],
        [2348496.5000,  245687.9531],
        [4447151.5000,       0.0000],
        [3077108.5000,       0.0000],
        [1204024.3750, 1416208.7500],
        [1946148.8750,  859353.3750],
        [1208510.2500, 1004163.5000],
        [ 696235.0000,  742653.4375],
        [1621371.7500,  828863.5000],
        [ 508007.8750,  215378.9375],
        [1177924.8750,  518934.1562],
        [ 216632.0000,  717608.8750]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 64.0625
Top1 accuracy for validation set is 64.0625 size is torch.Size([64, 1])
Epoch 321/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:25, 56.73s/it]  7%|▋         | 2/30 [00:59<11:37, 24.92s/it] 10%|█         | 3/30 [01:00<06:15, 13.89s/it] 13%|█▎        | 4/30 [01:00<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.83s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.29s/it] 30%|███       | 9/30 [01:04<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.5409366846084596
Epoch 322/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:30, 58.99s/it]  7%|▋         | 2/30 [00:59<11:32, 24.73s/it] 10%|█         | 3/30 [01:00<06:12, 13.78s/it] 13%|█▎        | 4/30 [01:01<03:44,  8.64s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.79s/it] 20%|██        | 6/30 [01:02<01:37,  4.08s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 2.5275318304697674
Epoch 323/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:18, 56.51s/it]  7%|▋         | 2/30 [00:57<11:03, 23.71s/it] 10%|█         | 3/30 [00:58<05:57, 13.23s/it] 13%|█▎        | 4/30 [01:00<03:54,  9.02s/it] 17%|█▋        | 5/30 [01:01<02:30,  6.04s/it] 20%|██        | 6/30 [01:02<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:02<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:03<00:51,  2.35s/it] 30%|███       | 9/30 [01:04<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:05<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:05<00:24,  1.28s/it] 40%|████      | 12/30 [01:06<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:07<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 2.4824772675832114
Epoch 324/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:52, 61.81s/it]  7%|▋         | 2/30 [01:02<12:04, 25.89s/it] 10%|█         | 3/30 [01:03<06:32, 14.54s/it] 13%|█▎        | 4/30 [01:04<03:56,  9.10s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.09s/it] 20%|██        | 6/30 [01:05<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.36s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.488234869639079
Epoch 325/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:46, 59.55s/it]  7%|▋         | 2/30 [01:00<11:38, 24.96s/it] 10%|█         | 3/30 [01:01<06:15, 13.90s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.4785168886184694
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0249,  0.0024,  0.0103,  ..., -0.0016,  0.0090,  0.0051],
        [-0.0013,  0.0153,  0.0253,  ...,  0.0146, -0.0022, -0.0164],
        [-0.0308, -0.0374,  0.0224,  ...,  0.0797, -0.0106, -0.0179],
        ...,
        [ 0.0122, -0.0046,  0.0041,  ..., -0.0225, -0.0128, -0.0102],
        [-0.0301,  0.0057, -0.0068,  ...,  0.0023,  0.0144, -0.0125],
        [-0.0361, -0.0115,  0.0162,  ...,  0.0382,  0.0321, -0.0360]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8907, 0.8526, 0.8480, 0.8460, 0.8456, 0.8428, 0.8378, 0.8376, 0.8332,
         0.8298],
        [0.9473, 0.9380, 0.9377, 0.9348, 0.9347, 0.9318, 0.9306, 0.9303, 0.9290,
         0.9289],
        [0.9099, 0.9054, 0.8935, 0.8797, 0.8703, 0.8587, 0.8552, 0.8480, 0.8454,
         0.8358],
        [0.9548, 0.9020, 0.8975, 0.8910, 0.8888, 0.8826, 0.8821, 0.8803, 0.8798,
         0.8768],
        [0.8481, 0.8481, 0.8390, 0.8214, 0.8204, 0.8159, 0.8064, 0.8048, 0.7993,
         0.7931],
        [0.8594, 0.8236, 0.8129, 0.8121, 0.8044, 0.7989, 0.7906, 0.7902, 0.7835,
         0.7802],
        [0.8826, 0.8817, 0.8706, 0.8681, 0.8639, 0.8626, 0.8605, 0.8591, 0.8589,
         0.8583],
        [0.9077, 0.8874, 0.8813, 0.8672, 0.8594, 0.8518, 0.8336, 0.8325, 0.8245,
         0.8214],
        [0.9158, 0.9076, 0.8998, 0.8956, 0.8936, 0.8908, 0.8907, 0.8888, 0.8885,
         0.8866],
        [0.9519, 0.8939, 0.8922, 0.8915, 0.8866, 0.8818, 0.8816, 0.8813, 0.8704,
         0.8692],
        [0.9490, 0.9081, 0.9073, 0.9058, 0.9038, 0.8998, 0.8974, 0.8940, 0.8934,
         0.8921],
        [0.9031, 0.8877, 0.8667, 0.8637, 0.8439, 0.8222, 0.8209, 0.8070, 0.8045,
         0.7971],
        [0.9321, 0.9140, 0.9128, 0.9054, 0.8989, 0.8908, 0.8835, 0.8811, 0.8730,
         0.8629],
        [0.9650, 0.9591, 0.9582, 0.9568, 0.9556, 0.9554, 0.9515, 0.9504, 0.9480,
         0.9453],
        [0.9451, 0.9333, 0.9332, 0.9328, 0.9317, 0.9246, 0.9236, 0.9217, 0.9215,
         0.9188],
        [0.9292, 0.9257, 0.9256, 0.9182, 0.9154, 0.9154, 0.9132, 0.9102, 0.9098,
         0.9068],
        [0.9599, 0.9545, 0.9459, 0.9423, 0.9337, 0.9330, 0.9291, 0.9258, 0.9258,
         0.9243],
        [0.9663, 0.9604, 0.9547, 0.9424, 0.9408, 0.9405, 0.9395, 0.9393, 0.9365,
         0.9336],
        [0.9323, 0.9264, 0.9168, 0.9033, 0.9004, 0.9002, 0.8994, 0.8978, 0.8955,
         0.8925],
        [0.9118, 0.9039, 0.9029, 0.9029, 0.9014, 0.9008, 0.9005, 0.8998, 0.8985,
         0.8975],
        [0.9534, 0.9439, 0.9422, 0.9416, 0.9372, 0.9370, 0.9370, 0.9357, 0.9355,
         0.9325],
        [0.9306, 0.9182, 0.9180, 0.9167, 0.9139, 0.9121, 0.9095, 0.9078, 0.9007,
         0.8995],
        [0.9721, 0.9498, 0.9452, 0.9450, 0.9379, 0.9361, 0.9314, 0.9219, 0.9195,
         0.9180],
        [0.9460, 0.9402, 0.9347, 0.9293, 0.9289, 0.9285, 0.9276, 0.9267, 0.9260,
         0.9239],
        [0.9598, 0.9072, 0.9035, 0.8874, 0.8804, 0.8765, 0.8761, 0.8745, 0.8726,
         0.8710],
        [0.9431, 0.9388, 0.9373, 0.9306, 0.9263, 0.9252, 0.9215, 0.9180, 0.9118,
         0.9088],
        [0.9322, 0.9316, 0.9316, 0.9236, 0.9165, 0.9093, 0.9088, 0.9057, 0.9053,
         0.9031],
        [0.9632, 0.9513, 0.9482, 0.9352, 0.9312, 0.9282, 0.9242, 0.9237, 0.9230,
         0.9220],
        [0.9504, 0.9427, 0.9344, 0.9342, 0.9300, 0.9292, 0.9279, 0.9263, 0.9255,
         0.9222],
        [0.9382, 0.9330, 0.9302, 0.9284, 0.9271, 0.9271, 0.9244, 0.9212, 0.9189,
         0.9184],
        [0.9536, 0.9494, 0.9488, 0.9485, 0.9476, 0.9476, 0.9399, 0.9386, 0.9358,
         0.9354],
        [0.9453, 0.9252, 0.9244, 0.9180, 0.9105, 0.9062, 0.9053, 0.9052, 0.9035,
         0.8999],
        [0.8845, 0.8759, 0.8736, 0.8639, 0.8616, 0.8610, 0.8591, 0.8585, 0.8578,
         0.8565],
        [0.9152, 0.9031, 0.8969, 0.8949, 0.8931, 0.8920, 0.8873, 0.8871, 0.8867,
         0.8853],
        [0.9277, 0.9217, 0.9215, 0.9206, 0.9095, 0.9057, 0.9008, 0.8997, 0.8986,
         0.8956],
        [0.9111, 0.9060, 0.9015, 0.8925, 0.8881, 0.8823, 0.8820, 0.8809, 0.8688,
         0.8669],
        [0.9483, 0.8707, 0.8664, 0.8573, 0.8563, 0.8502, 0.8388, 0.8375, 0.8298,
         0.8288],
        [0.9145, 0.8991, 0.8982, 0.8921, 0.8785, 0.8774, 0.8733, 0.8729, 0.8664,
         0.8662],
        [0.9008, 0.9007, 0.8854, 0.8737, 0.8664, 0.8593, 0.8566, 0.8565, 0.8474,
         0.8441],
        [0.9436, 0.9409, 0.9385, 0.9382, 0.9381, 0.9371, 0.9324, 0.9319, 0.9315,
         0.9294],
        [0.8908, 0.8884, 0.8856, 0.8792, 0.8685, 0.8658, 0.8652, 0.8515, 0.8430,
         0.8365],
        [0.9086, 0.8743, 0.8708, 0.8703, 0.8692, 0.8668, 0.8659, 0.8653, 0.8615,
         0.8597],
        [0.8952, 0.8875, 0.8848, 0.8705, 0.8654, 0.8601, 0.8585, 0.8544, 0.8464,
         0.8423],
        [0.8975, 0.8811, 0.8728, 0.8715, 0.8630, 0.8595, 0.8543, 0.8519, 0.8479,
         0.8439],
        [0.8670, 0.8444, 0.8406, 0.8359, 0.8264, 0.8263, 0.8162, 0.8105, 0.7979,
         0.7960],
        [0.8381, 0.8194, 0.8068, 0.7930, 0.7870, 0.7820, 0.7813, 0.7662, 0.7623,
         0.7573],
        [0.9111, 0.9040, 0.8968, 0.8954, 0.8951, 0.8926, 0.8860, 0.8841, 0.8835,
         0.8827],
        [0.8211, 0.7960, 0.7949, 0.7895, 0.7744, 0.7733, 0.7706, 0.7646, 0.7497,
         0.7421],
        [0.8538, 0.8436, 0.8190, 0.8188, 0.8175, 0.8121, 0.8097, 0.8033, 0.8015,
         0.7973],
        [0.9025, 0.8742, 0.8625, 0.8591, 0.8445, 0.8436, 0.8380, 0.8370, 0.8305,
         0.8287],
        [0.8926, 0.8608, 0.8592, 0.8566, 0.8452, 0.8349, 0.8264, 0.8243, 0.8176,
         0.8168],
        [0.8983, 0.8934, 0.8590, 0.8372, 0.8366, 0.8338, 0.8181, 0.8171, 0.8163,
         0.8162],
        [0.9345, 0.9173, 0.9159, 0.9033, 0.8794, 0.8726, 0.8692, 0.8664, 0.8569,
         0.8512],
        [0.9198, 0.8784, 0.8780, 0.8676, 0.8675, 0.8618, 0.8611, 0.8529, 0.8511,
         0.8490],
        [0.9353, 0.9150, 0.9113, 0.9102, 0.9042, 0.9042, 0.9027, 0.9022, 0.9019,
         0.8991],
        [0.9126, 0.8903, 0.8856, 0.8802, 0.8782, 0.8774, 0.8766, 0.8761, 0.8760,
         0.8723],
        [0.9218, 0.8852, 0.8841, 0.8783, 0.8716, 0.8648, 0.8466, 0.8431, 0.8419,
         0.8389],
        [0.9435, 0.8897, 0.8810, 0.8771, 0.8665, 0.8580, 0.8491, 0.8489, 0.8473,
         0.8385],
        [0.9188, 0.8680, 0.8672, 0.8589, 0.8455, 0.8428, 0.8397, 0.8386, 0.8386,
         0.8356],
        [0.9054, 0.8303, 0.8295, 0.8214, 0.8118, 0.8086, 0.8079, 0.8047, 0.7999,
         0.7986],
        [0.8850, 0.8797, 0.8711, 0.8681, 0.8656, 0.8628, 0.8596, 0.8593, 0.8591,
         0.8581],
        [0.8592, 0.7902, 0.7899, 0.7833, 0.7782, 0.7474, 0.7424, 0.7408, 0.7382,
         0.7376],
        [0.8907, 0.8639, 0.8609, 0.8328, 0.8276, 0.8264, 0.8248, 0.8139, 0.8131,
         0.8127],
        [0.8446, 0.8248, 0.8123, 0.7962, 0.7957, 0.7856, 0.7777, 0.7746, 0.7728,
         0.7700]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [1, 0, 1, 1, 1, 0, 1, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 1, 0, 1],
        [1, 1, 0, 1, 0, 0, 1, 0, 1, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 0, 1, 0, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 335895.0625,  194766.1562,  182571.5938,  177254.6094,  176224.0938,
          169394.1875,  157729.7500,  157268.6406,  147725.4062,  140739.7500],
        [ 753695.3125,  659883.4375,  657257.5000,  630412.5625,  629264.7500,
          603902.3125,  593409.5625,  591124.8750,  580434.3750,  579496.3125],
        [ 441847.0938,  414483.5938,  349483.1875,  286896.9375,  250873.9375,
          212577.7656,  202210.7656,  182333.0469,  175877.7031,  153327.6406],
        [ 839334.5625,  394380.9688,  370196.6875,  337294.3125,  326899.0625,
          299289.6250,  297149.5000,  289509.0938,  287294.2188,  275289.2188],
        [ 182647.7031,  182626.7969,  160368.9531,  124696.8203,  123043.4766,
          115315.9219,  100650.0000,   98370.7734,   90977.0547,   83236.3281],
        [ 214601.2500,  128832.0469,  110585.1016,  109194.8516,   97886.3984,
           90422.0859,   80407.4297,   79958.3281,   72612.6172,   69310.5703],
        [ 299099.6250,  295343.0625,  252106.9531,  243219.6094,  229011.1719,
          224813.1250,  218243.6406,  213761.7344,  213109.5938,  211335.2188],
        [ 427881.8125,  320436.5938,  293509.2500,  240133.7344,  214606.7812,
          192650.7969,  148626.2656,  146276.2656,  130384.8750,  124748.5703],
        [ 480551.7500,  427678.6562,  382441.8750,  359899.5000,  350163.4375,
          336245.0312,  335616.8125,  326903.7500,  325202.8750,  316879.5000],
        [ 805291.7500,  351758.6250,  343038.4688,  339668.1562,  316691.0000,
          295849.3750,  294888.8125,  293789.3125,  251292.7344,  246854.9844],
        [ 772433.7500,  430527.6875,  425615.7812,  416436.1250,  405087.1875,
          382543.2812,  369702.7812,  351938.8125,  349164.6875,  342446.8438],
        [ 400977.6250,  321761.6250,  238175.7188,  228240.8594,  172112.6406,
          126240.1328,  123971.2734,  101617.9531,   97984.0938,   88176.3047],
        [ 607059.6875,  468481.0938,  460190.2812,  414227.5312,  377539.9375,
          336421.1562,  303013.7500,  292671.8125,  260929.3438,  225729.6094],
        [ 969979.6875,  892152.3125,  881154.0000,  863293.0000,  848731.2500,
          846474.3750,  800417.5625,  787975.9375,  761783.6875,  732807.7500],
        [ 730058.6875,  617516.9375,  616206.8125,  613108.8125,  603100.5625,
          545285.8750,  537192.0000,  522789.9375,  521253.0938,  501616.0625],
        [ 582223.4375,  553746.1250,  553113.8125,  497406.8750,  477870.4688,
          477587.5625,  463056.2188,  443462.2500,  441122.0938,  422732.3438],
        [ 902234.0625,  835166.5625,  738481.3125,  702300.8750,  620840.8125,
          614771.0625,  581073.5625,  554799.0625,  554265.4375,  542823.8750],
        [ 988629.8750,  908642.9375,  838103.5625,  702730.3125,  686764.6250,
          683535.4375,  674621.1250,  672776.5000,  645601.2500,  620200.5000],
        [ 608464.0625,  559125.3750,  487848.5312,  402052.0938,  385879.8438,
          384691.9062,  380264.1250,  371474.4688,  359780.7188,  344551.9062],
        [ 454090.8125,  405279.2500,  399752.0000,  399576.6875,  391533.2500,
          388140.0625,  386033.3438,  382521.0312,  375502.5312,  370048.7812],
        [ 822508.6250,  717559.5000,  700856.3750,  695140.3125,  652080.5000,
          650643.0625,  650495.4375,  638790.9375,  637085.6250,  609772.2500],
        [ 593868.1250,  497043.1875,  496201.5312,  486670.0625,  467439.9062,
          456038.0938,  439172.3125,  428488.2188,  387376.7812,  380736.2188],
        [1074243.1250,  781610.5000,  731303.9375,  729690.5000,  659254.3750,
          642603.0625,  600256.2500,  524504.8125,  506653.6562,  495645.8438],
        [ 740249.6875,  681526.7500,  629324.7500,  582548.9375,  579951.3750,
          576390.5000,  568989.6250,  561778.0000,  555715.1875,  539498.7500],
        [ 901425.6250,  425198.7188,  403128.2812,  320513.6250,  289997.0938,
          274061.7188,  272734.8750,  266373.5000,  259326.0312,  253292.4062],
        [ 709991.4375,  667928.7500,  653590.2500,  593958.8125,  558117.3750,
          549466.6875,  521069.1562,  496034.0938,  453926.7188,  434782.9062],
        [ 607744.3750,  602760.7500,  602671.6875,  537339.5625,  485576.8750,
          437739.7812,  434814.0312,  416304.3125,  413763.6250,  400664.5625],
        [ 945704.5625,  797717.6250,  763177.6250,  633792.5625,  599106.6875,
          573953.3750,  542231.0000,  538290.5000,  532838.2500,  524873.1250],
        [ 787378.7500,  706287.9375,  626655.4375,  625086.2500,  588676.5625,
          582198.4375,  571236.6875,  558039.6875,  552191.4375,  526738.0000],
        [ 661567.6875,  614829.6875,  590487.0625,  575338.8125,  564464.2500,
          564450.2500,  543495.7500,  519062.9688,  502748.7188,  498607.5000],
        [ 825025.0000,  776866.3125,  770142.5000,  767098.3125,  757021.2500,
          756485.7500,  678040.2500,  665326.0000,  639608.3750,  635778.2500],
        [ 732846.1250,  549364.5000,  543429.9375,  495695.0000,  445681.3438,
          418952.0938,  413957.0625,  413361.3438,  403452.8750,  382870.2812],
        [ 307537.2812,  271630.0938,  263013.7188,  228968.7969,  221691.0469,
          219811.2969,  213685.1094,  211909.5938,  209922.1094,  205975.3438],
        [ 476538.4062,  400954.3125,  367120.0312,  356450.8750,  347388.4062,
          341954.0312,  319764.0625,  318905.2812,  317278.3750,  310715.2812],
        [ 570060.6875,  522889.6562,  521435.0625,  514596.4375,  438997.6875,
          416063.7812,  387699.4375,  381807.4375,  376138.0000,  360098.2500],
        [ 449179.8438,  417842.0000,  391795.8438,  344447.7500,  323534.9062,
          297682.1875,  296630.5312,  291842.9062,  245461.0312,  239058.9062],
        [ 764336.5000,  252297.4531,  237232.6875,  208489.5938,  205417.0625,
          188386.6406,  160049.4688,  157085.0000,  140748.7344,  138788.4219],
        [ 471506.5312,  378659.5625,  373595.8125,  342700.0312,  281956.2500,
          277576.1875,  261814.4688,  260227.2969,  237260.2812,  236710.8750],
        [ 387790.0625,  387336.9062,  311149.6875,  263466.8438,  237376.3906,
          214551.5156,  206451.2812,  205871.2656,  180933.2656,  172451.4375],
        [ 715481.5000,  687628.3125,  665020.2500,  661819.5000,  660978.6875,
          651115.4375,  609264.1875,  604548.8125,  601407.9375,  583572.6250],
        [ 336282.5625,  325179.6250,  312183.1250,  284912.2500,  244564.6875,
          235197.5938,  233290.9531,  191771.1094,  169873.6875,  154907.2031],
        [ 433675.5625,  265654.5312,  252745.3906,  251028.5469,  246904.6875,
          238553.5312,  235661.6719,  233736.5625,  221149.2188,  215734.7812],
        [ 358134.8125,  320594.3125,  308866.0938,  251751.3906,  233979.2031,
          216810.3906,  212054.9688,  199962.4688,  178301.1875,  168178.5625],
        [ 370115.5000,  292959.4688,  259995.3438,  255116.9062,  226150.6406,
          214977.7500,  199651.1094,  192866.4219,  182223.3594,  172066.6719],
        [ 239255.7188,  173276.2031,  164170.4375,  153552.1250,  133979.4688,
          133863.2500,  115850.5312,  106813.9766,   89244.4609,   86828.1328],
        [ 158440.7344,  121328.3672,  101358.8594,   83216.3281,   76292.4766,
           71031.2734,   70365.2734,   56707.4922,   53652.5664,   49944.8672],
        [ 449454.5312,  406163.0000,  366288.4062,  359310.2812,  357630.3750,
          344871.7812,  313954.5000,  305745.5000,  302866.6875,  299393.5625],
        [ 124199.9062,   86752.3203,   85396.7578,   79134.8516,   63773.4492,
           62797.1328,   60416.6953,   55442.4336,   44814.8906,   40206.3008],
        [ 198296.0625,  171236.3906,  120601.4766,  120295.4688,  117946.3594,
          109203.7031,  105529.6953,   96336.4688,   93834.2812,   88461.2500],
        [ 397680.0312,  265249.5000,  224589.8438,  213677.3594,  173673.9062,
          171293.3906,  158232.3594,  155958.7656,  142021.9844,  138406.5625],
        [ 344961.2188,  219141.9219,  214006.9219,  206345.1875,  175216.0938,
          151387.2031,  134059.3594,  129979.1484,  118178.0781,  116899.5547],
        [ 374168.4375,  348892.4062,  213510.9375,  156285.5781,  155036.6719,
          148968.1094,  118989.2656,  117268.4688,  115997.4609,  115888.7656],
        [ 628075.2500,  490705.0938,  481429.7188,  401871.9062,  285678.9688,
          259134.4219,  246870.7812,  237288.5781,  207227.2812,  190865.0156],
        [ 508618.6875,  281613.5938,  280168.6562,  241397.5156,  241218.2500,
          222131.2656,  219976.1250,  195571.1562,  190644.3594,  185129.1562],
        [ 634588.5625,  475140.2812,  450603.4375,  443643.7188,  407187.2500,
          407187.2500,  398761.6250,  396003.0938,  393808.5938,  378513.6875],
        [ 459234.0938,  334012.9375,  312050.9688,  289077.3438,  280929.8750,
          277762.3125,  274557.9688,  272781.9375,  272014.2812,  258011.4375],
        [ 523741.0625,  310559.4375,  305509.4375,  281161.4062,  255481.6250,
          232079.7812,  178956.7031,  170128.3906,  167308.4844,  160290.9688],
        [ 713647.6250,  331202.2500,  292166.5312,  276608.7500,  237510.9062,
          210540.0156,  185450.7656,  184720.1719,  180762.0156,  159395.5469],
        [ 502011.3750,  242686.4688,  239976.7031,  213328.7969,  176006.2344,
          169334.1094,  162005.2812,  159635.9062,  159513.0938,  152930.0000],
        [ 414063.2500,  141775.0156,  140093.4844,  124831.5156,  108862.5391,
          103917.1953,  102838.3516,   98286.5703,   91748.4141,   90133.1562],
        [ 309332.4375,  286870.6562,  253680.3906,  243064.2344,  234627.4219,
          225516.1562,  215286.9219,  214482.3750,  213957.1250,  210812.4688],
        [ 214157.5938,   79946.6562,   79533.7344,   72443.6328,   67359.1562,
           43331.7773,   40360.8906,   39450.4922,   38019.2891,   37713.8164],
        [ 335684.3750,  229069.0625,  219320.0469,  146865.0312,  136324.1875,
          134001.8281,  130908.5469,  112047.9609,  110859.0078,  110221.8594],
        [ 173720.4531,  130981.7188,  109526.0938,   87033.3203,   86416.6562,
           74843.0391,   66817.5547,   63981.6719,   62274.3398,   59871.8047]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[335895.0625,      0.0000],
         [194766.1562,      0.0000],
         [182571.5938,      0.0000],
         ...,
         [157268.6406,      0.0000],
         [     0.0000, 147725.4062],
         [140739.7500,      0.0000]],

        [[753695.3125,      0.0000],
         [659883.4375,      0.0000],
         [657257.5000,      0.0000],
         ...,
         [591124.8750,      0.0000],
         [580434.3750,      0.0000],
         [579496.3125,      0.0000]],

        [[441847.0938,      0.0000],
         [414483.5938,      0.0000],
         [349483.1875,      0.0000],
         ...,
         [182333.0469,      0.0000],
         [175877.7031,      0.0000],
         [     0.0000, 153327.6406]],

        ...,

        [[     0.0000, 214157.5938],
         [ 79946.6562,      0.0000],
         [ 79533.7344,      0.0000],
         ...,
         [ 39450.4922,      0.0000],
         [ 38019.2891,      0.0000],
         [ 37713.8164,      0.0000]],

        [[335684.3750,      0.0000],
         [     0.0000, 229069.0625],
         [219320.0469,      0.0000],
         ...,
         [112047.9609,      0.0000],
         [110859.0078,      0.0000],
         [110221.8594,      0.0000]],

        [[     0.0000, 173720.4531],
         [     0.0000, 130981.7188],
         [     0.0000, 109526.0938],
         ...,
         [ 63981.6719,      0.0000],
         [     0.0000,  62274.3398],
         [ 59871.8047,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1691843.8750,  147725.4062],
        [6278881.0000,       0.0000],
        [2516584.0000,  153327.6406],
        [3716637.5000,       0.0000],
        [ 522603.6875,  739330.1250],
        [ 873915.0000,  179895.6719],
        [1932011.0000,  468032.7500],
        [2239255.0000,       0.0000],
        [3291419.7500,  350163.4375],
        [3539123.2500,       0.0000],
        [4245897.0000,       0.0000],
        [1899258.2500,       0.0000],
        [3746264.5000,       0.0000],
        [8384769.5000,       0.0000],
        [5808129.0000,       0.0000],
        [4912321.0000,       0.0000],
        [6646756.5000,       0.0000],
        [7421607.0000,       0.0000],
        [4284133.0000,       0.0000],
        [3952477.7500,       0.0000],
        [6774932.0000,       0.0000],
        [4633034.0000,       0.0000],
        [6745766.0000,       0.0000],
        [6015973.5000,       0.0000],
        [3666051.5000,       0.0000],
        [5638866.5000,       0.0000],
        [4939379.5000,       0.0000],
        [6451685.0000,       0.0000],
        [6124489.0000,       0.0000],
        [5635053.0000,       0.0000],
        [7271392.0000,       0.0000],
        [4799610.5000,       0.0000],
        [2354144.5000,       0.0000],
        [2833498.2500,  723570.8750],
        [2054596.0000, 2435190.5000],
        [1200044.1250, 2097431.7500],
        [1291212.8750, 1161618.6250],
        [1647184.8750, 1474822.2500],
        [1037274.3125, 1530104.3750],
        [6440837.0000,       0.0000],
        [2488163.0000,       0.0000],
        [2594844.5000,       0.0000],
        [2448633.5000,       0.0000],
        [2366123.0000,       0.0000],
        [ 329921.6875, 1066912.6250],
        [ 354557.9375,  487780.3125],
        [3056223.7500,  449454.5312],
        [ 456760.2188,  246174.5156],
        [ 532549.2500,  689191.8750],
        [ 536086.6250, 1504697.1250],
        [1540609.3750,  269565.2812],
        [ 928434.3125,  936571.8125],
        [1832613.0000, 1596534.1250],
        [2325071.2500,  241397.5156],
        [4385437.5000,       0.0000],
        [3030433.2500,       0.0000],
        [1188417.6250, 1396799.6250],
        [1926682.6250,  845321.8750],
        [1186844.3750,  990583.5000],
        [ 683604.3750,  732945.1250],
        [1585910.8750,  821719.2500],
        [ 498159.4375,  214157.5938],
        [1155366.0000,  509935.9062],
        [ 210270.1406,  705196.5625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 326/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:20, 60.71s/it]  7%|▋         | 2/30 [01:01<11:56, 25.61s/it] 10%|█         | 3/30 [01:02<06:24, 14.25s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.92s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.97s/it] 20%|██        | 6/30 [01:04<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.4990699609120686
Epoch 327/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:00, 60.03s/it]  7%|▋         | 2/30 [01:04<12:42, 27.24s/it] 10%|█         | 3/30 [01:05<06:48, 15.14s/it] 13%|█▎        | 4/30 [01:05<04:05,  9.46s/it] 17%|█▋        | 5/30 [01:06<02:37,  6.32s/it] 20%|██        | 6/30 [01:07<01:46,  4.42s/it] 23%|██▎       | 7/30 [01:08<01:14,  3.22s/it] 27%|██▋       | 8/30 [01:08<00:53,  2.44s/it] 30%|███       | 9/30 [01:09<00:40,  1.91s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.55s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.30s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.85s/it]
Epoch loss is 2.482188622156779
Epoch 328/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:15, 60.53s/it]  7%|▋         | 2/30 [01:01<11:50, 25.36s/it] 10%|█         | 3/30 [01:02<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.5427469571431476
Epoch 329/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:15, 58.46s/it]  7%|▋         | 2/30 [01:00<11:54, 25.52s/it] 10%|█         | 3/30 [01:01<06:23, 14.21s/it] 13%|█▎        | 4/30 [01:02<03:51,  8.90s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.96s/it] 20%|██        | 6/30 [01:03<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.73s/it]
Epoch loss is 2.4898425261179606
Epoch 330/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:45, 59.48s/it]  7%|▋         | 2/30 [01:00<11:38, 24.93s/it] 10%|█         | 3/30 [01:00<06:15, 13.89s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.83s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.503534428278605
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0248,  0.0024,  0.0105,  ..., -0.0018,  0.0097,  0.0049],
        [-0.0016,  0.0148,  0.0257,  ...,  0.0149, -0.0018, -0.0164],
        [-0.0306, -0.0377,  0.0228,  ...,  0.0799, -0.0099, -0.0177],
        ...,
        [ 0.0116, -0.0044,  0.0047,  ..., -0.0227, -0.0121, -0.0098],
        [-0.0304,  0.0060, -0.0065,  ...,  0.0025,  0.0146, -0.0124],
        [-0.0360, -0.0117,  0.0164,  ...,  0.0384,  0.0325, -0.0354]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8896, 0.8519, 0.8484, 0.8452, 0.8442, 0.8414, 0.8370, 0.8367, 0.8318,
         0.8291],
        [0.9469, 0.9371, 0.9371, 0.9339, 0.9336, 0.9316, 0.9298, 0.9293, 0.9287,
         0.9280],
        [0.9096, 0.9047, 0.8923, 0.8789, 0.8691, 0.8577, 0.8549, 0.8474, 0.8448,
         0.8348],
        [0.9550, 0.9007, 0.8964, 0.8897, 0.8877, 0.8824, 0.8805, 0.8791, 0.8783,
         0.8754],
        [0.8477, 0.8467, 0.8385, 0.8208, 0.8196, 0.8153, 0.8061, 0.8040, 0.7985,
         0.7915],
        [0.8591, 0.8229, 0.8117, 0.8110, 0.8040, 0.7973, 0.7904, 0.7895, 0.7812,
         0.7799],
        [0.8813, 0.8802, 0.8694, 0.8656, 0.8629, 0.8609, 0.8600, 0.8583, 0.8573,
         0.8566],
        [0.9063, 0.8860, 0.8799, 0.8665, 0.8587, 0.8500, 0.8330, 0.8313, 0.8226,
         0.8196],
        [0.9153, 0.9072, 0.8988, 0.8946, 0.8917, 0.8903, 0.8900, 0.8879, 0.8878,
         0.8864],
        [0.9517, 0.8933, 0.8916, 0.8905, 0.8854, 0.8811, 0.8806, 0.8803, 0.8696,
         0.8685],
        [0.9484, 0.9067, 0.9066, 0.9045, 0.9029, 0.8988, 0.8957, 0.8929, 0.8921,
         0.8908],
        [0.9019, 0.8862, 0.8656, 0.8623, 0.8423, 0.8202, 0.8188, 0.8066, 0.8029,
         0.7943],
        [0.9306, 0.9134, 0.9124, 0.9043, 0.8975, 0.8898, 0.8823, 0.8798, 0.8723,
         0.8622],
        [0.9644, 0.9586, 0.9578, 0.9567, 0.9550, 0.9549, 0.9509, 0.9499, 0.9474,
         0.9443],
        [0.9449, 0.9330, 0.9327, 0.9323, 0.9310, 0.9240, 0.9231, 0.9209, 0.9209,
         0.9185],
        [0.9289, 0.9254, 0.9253, 0.9178, 0.9151, 0.9150, 0.9128, 0.9100, 0.9093,
         0.9057],
        [0.9593, 0.9538, 0.9448, 0.9419, 0.9332, 0.9325, 0.9282, 0.9252, 0.9240,
         0.9236],
        [0.9660, 0.9601, 0.9544, 0.9422, 0.9404, 0.9403, 0.9390, 0.9388, 0.9358,
         0.9334],
        [0.9317, 0.9253, 0.9155, 0.9028, 0.8998, 0.8997, 0.8985, 0.8971, 0.8946,
         0.8909],
        [0.9114, 0.9033, 0.9023, 0.9022, 0.9010, 0.9003, 0.8996, 0.8993, 0.8968,
         0.8966],
        [0.9525, 0.9430, 0.9415, 0.9408, 0.9364, 0.9363, 0.9356, 0.9353, 0.9347,
         0.9314],
        [0.9299, 0.9171, 0.9171, 0.9162, 0.9133, 0.9120, 0.9090, 0.9073, 0.8993,
         0.8985],
        [0.9718, 0.9498, 0.9450, 0.9446, 0.9376, 0.9354, 0.9310, 0.9216, 0.9188,
         0.9175],
        [0.9455, 0.9396, 0.9342, 0.9287, 0.9287, 0.9278, 0.9273, 0.9264, 0.9258,
         0.9235],
        [0.9593, 0.9067, 0.9023, 0.8866, 0.8789, 0.8755, 0.8745, 0.8729, 0.8715,
         0.8688],
        [0.9426, 0.9382, 0.9369, 0.9301, 0.9260, 0.9247, 0.9211, 0.9174, 0.9111,
         0.9083],
        [0.9315, 0.9310, 0.9307, 0.9229, 0.9159, 0.9084, 0.9079, 0.9051, 0.9043,
         0.9025],
        [0.9629, 0.9510, 0.9478, 0.9344, 0.9308, 0.9279, 0.9234, 0.9231, 0.9228,
         0.9215],
        [0.9498, 0.9423, 0.9335, 0.9335, 0.9295, 0.9287, 0.9277, 0.9253, 0.9244,
         0.9216],
        [0.9377, 0.9325, 0.9298, 0.9282, 0.9266, 0.9265, 0.9238, 0.9210, 0.9186,
         0.9179],
        [0.9529, 0.9488, 0.9485, 0.9482, 0.9473, 0.9471, 0.9394, 0.9380, 0.9350,
         0.9349],
        [0.9447, 0.9248, 0.9241, 0.9174, 0.9097, 0.9053, 0.9050, 0.9044, 0.9031,
         0.8996],
        [0.8838, 0.8752, 0.8735, 0.8638, 0.8608, 0.8601, 0.8591, 0.8578, 0.8572,
         0.8558],
        [0.9141, 0.9021, 0.8961, 0.8937, 0.8920, 0.8913, 0.8864, 0.8862, 0.8861,
         0.8843],
        [0.9271, 0.9212, 0.9204, 0.9200, 0.9089, 0.9050, 0.8999, 0.8998, 0.8977,
         0.8948],
        [0.9106, 0.9052, 0.9009, 0.8916, 0.8879, 0.8817, 0.8814, 0.8808, 0.8679,
         0.8666],
        [0.9482, 0.8701, 0.8667, 0.8569, 0.8557, 0.8487, 0.8369, 0.8363, 0.8281,
         0.8276],
        [0.9135, 0.8978, 0.8969, 0.8907, 0.8784, 0.8767, 0.8721, 0.8715, 0.8653,
         0.8649],
        [0.9001, 0.8998, 0.8841, 0.8727, 0.8647, 0.8585, 0.8556, 0.8548, 0.8462,
         0.8423],
        [0.9430, 0.9398, 0.9378, 0.9376, 0.9372, 0.9363, 0.9320, 0.9314, 0.9308,
         0.9286],
        [0.8900, 0.8876, 0.8850, 0.8781, 0.8680, 0.8649, 0.8646, 0.8507, 0.8422,
         0.8356],
        [0.9078, 0.8731, 0.8698, 0.8689, 0.8673, 0.8663, 0.8644, 0.8643, 0.8598,
         0.8585],
        [0.8945, 0.8864, 0.8838, 0.8698, 0.8644, 0.8592, 0.8570, 0.8534, 0.8452,
         0.8413],
        [0.8960, 0.8806, 0.8712, 0.8705, 0.8616, 0.8585, 0.8533, 0.8514, 0.8474,
         0.8424],
        [0.8658, 0.8438, 0.8390, 0.8349, 0.8251, 0.8250, 0.8147, 0.8082, 0.7959,
         0.7940],
        [0.8375, 0.8186, 0.8058, 0.7926, 0.7866, 0.7819, 0.7810, 0.7662, 0.7622,
         0.7576],
        [0.9102, 0.9022, 0.8950, 0.8943, 0.8937, 0.8914, 0.8842, 0.8823, 0.8818,
         0.8812],
        [0.8196, 0.7947, 0.7937, 0.7882, 0.7737, 0.7710, 0.7683, 0.7645, 0.7484,
         0.7406],
        [0.8520, 0.8426, 0.8183, 0.8176, 0.8169, 0.8111, 0.8092, 0.8019, 0.8016,
         0.7958],
        [0.9017, 0.8732, 0.8611, 0.8568, 0.8434, 0.8428, 0.8366, 0.8357, 0.8292,
         0.8269],
        [0.8913, 0.8603, 0.8585, 0.8561, 0.8446, 0.8343, 0.8252, 0.8242, 0.8168,
         0.8157],
        [0.8974, 0.8927, 0.8581, 0.8355, 0.8344, 0.8323, 0.8159, 0.8158, 0.8146,
         0.8142],
        [0.9339, 0.9162, 0.9152, 0.9025, 0.8781, 0.8710, 0.8684, 0.8657, 0.8556,
         0.8502],
        [0.9190, 0.8782, 0.8774, 0.8670, 0.8664, 0.8612, 0.8599, 0.8520, 0.8503,
         0.8486],
        [0.9343, 0.9138, 0.9103, 0.9092, 0.9032, 0.9032, 0.9018, 0.9014, 0.9008,
         0.8984],
        [0.9116, 0.8895, 0.8852, 0.8787, 0.8774, 0.8764, 0.8753, 0.8752, 0.8748,
         0.8712],
        [0.9207, 0.8838, 0.8837, 0.8774, 0.8704, 0.8645, 0.8454, 0.8433, 0.8412,
         0.8377],
        [0.9434, 0.8887, 0.8803, 0.8760, 0.8656, 0.8575, 0.8478, 0.8475, 0.8464,
         0.8370],
        [0.9180, 0.8673, 0.8661, 0.8577, 0.8437, 0.8419, 0.8380, 0.8374, 0.8367,
         0.8346],
        [0.9048, 0.8293, 0.8284, 0.8199, 0.8102, 0.8074, 0.8065, 0.8034, 0.7987,
         0.7966],
        [0.8849, 0.8786, 0.8699, 0.8670, 0.8636, 0.8618, 0.8581, 0.8577, 0.8575,
         0.8568],
        [0.8589, 0.7893, 0.7890, 0.7825, 0.7772, 0.7457, 0.7406, 0.7391, 0.7376,
         0.7354],
        [0.8900, 0.8633, 0.8596, 0.8311, 0.8271, 0.8251, 0.8224, 0.8121, 0.8117,
         0.8105],
        [0.8442, 0.8233, 0.8105, 0.7948, 0.7940, 0.7854, 0.7769, 0.7728, 0.7704,
         0.7671]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [1, 0, 1, 1, 1, 1, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 1, 0, 1],
        [1, 1, 0, 1, 0, 0, 1, 0, 1, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 0, 0, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 330554.4375,  192858.8906,  183607.3750,  175281.7812,  172801.9219,
          166036.5625,  155837.7344,  155221.7500,  144861.6406,  139343.0469],
        [ 749407.8125,  651717.3750,  651639.6875,  622631.5000,  619839.8125,
          602048.9375,  586692.5000,  583092.5000,  578296.6875,  572368.2500],
        [ 439609.3750,  410327.6250,  343702.8750,  283872.9688,  246747.1875,
          209680.6094,  201385.9219,  180898.7656,  174298.9531,  151066.8906],
        [ 841018.8125,  387122.0000,  364418.3750,  331120.4688,  321806.4375,
          298099.2188,  290088.3750,  284384.0000,  281165.7188,  269808.6562],
        [ 181597.6094,  179099.0938,  159244.6719,  123661.1953,  121533.6875,
          114410.8984,  100275.8672,   97382.2891,   89978.8984,   81450.9609],
        [ 213840.6562,  127393.5469,  108587.0391,  107561.1094,   97356.0156,
           88384.2578,   80133.5938,   79162.0234,   70265.8984,   68988.1719],
        [ 293437.5938,  288935.1250,  247826.1250,  234778.7500,  225642.0156,
          219247.9062,  216727.6875,  211448.3281,  208290.0625,  206399.3125],
        [ 419920.8750,  314004.2188,  287636.6250,  237532.6406,  212476.4219,
          187692.1094,  147171.0938,  143827.2969,  126884.4219,  121593.3906],
        [ 476890.7500,  425096.9375,  376820.1562,  354952.1875,  340521.8438,
          334045.1250,  332384.7188,  322620.1250,  322220.6875,  315595.0625],
        [ 802284.5625,  348383.0312,  340139.8125,  335041.1875,  311461.6875,
          292944.9375,  290647.5000,  289627.0312,  248413.6719,  244537.1719],
        [ 765784.8125,  422326.1562,  421433.3750,  408750.9062,  399624.6875,
          377017.1562,  360830.8125,  346689.7188,  342380.8438,  336398.6875],
        [ 394176.0625,  314919.1875,  234495.4531,  223662.3906,  168171.6719,
          122732.4375,  120261.8594,  100988.1562,   95824.5312,   84781.3438],
        [ 593475.8125,  464308.0938,  457816.4375,  408096.1562,  370226.3438,
          331615.0312,  297752.5938,  287433.1250,  258190.3594,  223576.8750],
        [ 962443.1250,  885810.0000,  875429.0625,  862017.0625,  840916.9375,
          840066.5000,  793346.8750,  781888.6250,  754751.2500,  722095.0625],
        [ 728018.1250,  614449.8750,  612119.7500,  608053.9375,  596911.0000,
          540496.8125,  533253.0625,  516855.9375,  516803.1875,  499235.5938],
        [ 579875.5625,  550909.6250,  550296.3125,  494773.5938,  475655.7812,
          475267.1562,  460181.5000,  442229.8750,  438121.5000,  416283.2812],
        [ 895165.0625,  827521.3125,  727274.1875,  697511.6875,  616393.7500,
          610510.6875,  574087.5000,  549693.0625,  540135.0000,  536994.8125],
        [ 984526.6875,  904476.6250,  834478.6875,  701232.7500,  682847.3750,
          682529.0625,  669542.9375,  667794.3750,  639738.3125,  617677.6875],
        [ 603177.0625,  550339.3750,  478802.0000,  399355.7188,  382682.3125,
          381912.3125,  375553.0312,  367732.5312,  354823.9375,  336555.9062],
        [ 451164.1562,  402039.8438,  396478.0938,  395696.9375,  388827.3125,
          385240.0312,  381125.6562,  379758.9375,  366307.9688,  365592.5000],
        [ 811447.5625,  708874.3750,  693855.4375,  686921.8125,  645047.9375,
          643923.1875,  638080.3750,  634678.1250,  629885.5625,  600934.3750],
        [ 588194.5000,  489653.7500,  489576.7188,  483413.0938,  463476.8125,
          455278.4688,  436392.1875,  425892.2812,  379830.6562,  375175.0000],
        [1069891.8750,  780908.6250,  729202.1250,  724896.3125,  656215.9375,
          635590.9375,  597166.1250,  521957.9375,  501646.1875,  492416.1875],
        [ 734975.3125,  675639.0625,  625509.0625,  578162.1250,  577786.7500,
          570437.5625,  566173.3125,  558829.0000,  554112.1875,  536551.0000],
        [ 894887.6875,  422129.6250,  396539.3750,  316686.7812,  283704.8750,
          270344.4062,  266494.7188,  260543.4062,  255335.4844,  245698.0469],
        [ 705010.6250,  661772.1875,  649807.7500,  589573.8125,  555638.3125,
          545825.4375,  518699.2812,  491846.3750,  449615.2812,  431526.1250],
        [ 601060.5000,  597607.6250,  594721.6875,  531847.2500,  481537.6562,
          432628.8125,  429071.7188,  412495.3750,  407900.8438,  397426.7812],
        [ 942141.6250,  794175.0625,  758818.8125,  627255.7500,  595197.1250,
          571256.8750,  535637.3750,  533738.4375,  531522.1875,  521714.5938],
        [ 780869.1875,  701355.1250,  618718.8750,  618469.3750,  584234.6875,
          577555.3750,  569986.7500,  550264.2500,  543093.1875,  522369.7812],
        [ 657479.4375,  609743.1875,  587055.1875,  573655.1250,  560562.0625,
          560006.3750,  538863.6875,  517944.9375,  500336.1562,  495521.5625],
        [ 816506.5000,  770554.6875,  767013.4375,  763832.9375,  753690.2500,
          751528.4375,  673412.6875,  660132.0000,  632173.6250,  631121.8125],
        [ 726139.0000,  546255.0000,  540868.5625,  491748.8750,  440591.5000,
          413585.3125,  412185.1250,  408556.8438,  400844.9375,  381302.0000],
        [ 304425.4688,  268998.8438,  262661.7500,  228613.3750,  218931.7812,
          216981.2500,  213883.6875,  209882.8750,  208085.7656,  203855.0312],
        [ 469170.0625,  395477.3750,  362919.0938,  350707.1562,  342132.8125,
          338474.6250,  315591.1562,  314832.0938,  314526.0000,  306662.1562],
        [ 564813.1875,  519392.2500,  513412.1562,  510689.2188,  435321.4375,
          411811.4688,  382873.5625,  382185.1875,  371318.9688,  355935.2500],
        [ 445994.2812,  413174.9375,  388218.5312,  340012.6875,  322519.8125,
          295095.0312,  293897.7500,  291359.5938,  242605.4844,  237893.7656],
        [ 764025.3125,  250245.5000,  238437.2969,  207251.9844,  203559.9531,
          184406.1719,  155760.6250,  154343.4688,  137352.9219,  136266.3438],
        [ 464831.7812,  371442.9062,  366728.0938,  336043.4062,  281571.4375,
          275106.2812,  257585.3594,  255112.5156,  233493.9375,  232181.8281],
        [ 383900.6562,  382669.5312,  305759.2188,  259536.8125,  231737.8438,
          211925.9688,  203266.8125,  200967.4844,  177856.0469,  168310.6250],
        [ 709013.6875,  677365.5000,  657956.1250,  655972.5000,  652375.9375,
          644237.1250,  605535.5000,  600275.6875,  595418.6250,  576683.0000],
        [ 332666.9688,  321245.9062,  309671.5938,  280359.5000,  242942.5781,
          232444.1562,  231216.6562,  189686.4375,  167979.3281,  152947.5000],
        [ 428537.2812,  261220.8906,  248953.6719,  245917.6875,  240297.2969,
          236819.7031,  230629.3125,  230302.7188,  215873.0625,  211992.0625],
        [ 354723.7812,  315996.2188,  304383.0938,  249000.2188,  230614.8125,
          213972.0156,  207443.7969,  196990.9844,  175315.8750,  165860.0938],
        [ 362072.4688,  290561.5625,  253985.1719,  251546.6719,  221447.4219,
          212013.1094,  196766.4062,  191413.1562,  181021.4688,  168504.9531],
        [ 235286.4219,  171881.5156,  160415.0000,  151325.1406,  131536.2656,
          131416.2656,  113317.6172,  103360.7422,   86666.0703,   84390.8281],
        [ 157061.3438,  119846.1406,   99895.6016,   82665.6406,   75852.0391,
           70934.2031,   70075.5078,   56732.4258,   53590.0234,   50165.1250],
        [ 443816.7812,  395863.0000,  357150.8438,  353432.1875,  350710.1875,
          339048.0938,  305902.1250,  297911.0938,  295724.4062,  293148.9375],
        [ 121558.9531,   85187.2266,   84001.0781,   77678.1484,   63085.3125,
           60776.4414,   58426.9492,   55366.7148,   43955.3398,   39324.9570],
        [ 193190.6094,  168831.3281,  119417.9688,  118261.7344,  117020.6875,
          107631.9219,  104797.7734,   94438.3750,   93987.8828,   86575.6953],
        [ 392677.1562,  261383.1094,  220130.5781,  207049.6875,  170858.9219,
          169465.2812,  155061.3594,  153086.4219,  139436.3750,  135033.3750],
        [ 338467.2188,  217660.6094,  212011.0781,  204710.0938,  173805.4688,
          149945.3125,  131698.1875,  129908.2578,  116865.7812,  115059.5312],
        [ 369732.3750,  345683.1250,  210915.4219,  152716.6406,  150341.5312,
          145707.7969,  115377.8516,  115242.9297,  113270.9531,  112535.7578],
        [ 622139.4375,  483300.1562,  476830.2812,  397496.5312,  280539.2188,
          253530.6875,  244358.3750,  234931.0469,  203307.3281,  188156.6406],
        [ 503131.9688,  280746.9375,  277801.2812,  239347.6875,  237272.7344,
          220184.1250,  216158.8125,  193071.9844,  188490.8594,  184128.6875],
        [ 626458.8750,  467075.8438,  444313.5938,  437284.5625,  401447.5000,
          401447.5000,  393366.4375,  391476.8750,  387879.5625,  374633.6875],
        [ 452563.8125,  330063.0312,  310608.0312,  282751.6562,  277761.5312,
          273885.8750,  269365.1875,  269230.5938,  267640.3438,  254195.2656],
        [ 515842.0312,  304323.8750,  303671.5625,  277791.1875,  251248.1875,
          231064.7812,  175794.3750,  170500.0156,  165520.0469,  157527.5625],
        [ 712697.5000,  326492.8125,  289523.7500,  272060.2188,  234545.3438,
          209128.0312,  181958.8750,  181094.0000,  178308.5000,  155932.2969],
        [ 495947.9688,  240357.5781,  236366.6250,  209474.3438,  171539.9219,
          167138.6406,  158176.2344,  156930.7812,  155277.2656,  150748.1094],
        [ 410583.2188,  139782.1406,  137849.7344,  122141.0312,  106385.7812,
          102180.3203,  100891.4141,   96547.1797,   90264.5859,   87599.8125],
        [ 309220.6562,  282538.7188,  249576.5000,  239275.8125,  228082.2344,
          222110.0781,  210742.9062,  209533.8750,  208886.4531,  206766.9219],
        [ 213098.6094,   78932.3281,   78516.7031,   71555.5469,   66366.5312,
           42320.1250,   39331.9336,   38488.5469,   37677.2539,   36548.3555],
        [ 332587.0312,  227047.9062,  215282.4219,  143232.1406,  135313.7656,
          131561.3594,  126574.5391,  109184.2344,  108667.8516,  106773.4453],
        [ 172799.6094,  128164.7969,  106769.5703,   85354.6641,   84356.1484,
           74579.2031,   66114.4141,   62296.3164,   60223.6445,   57446.6797]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[330554.4375,      0.0000],
         [192858.8906,      0.0000],
         [183607.3750,      0.0000],
         ...,
         [155221.7500,      0.0000],
         [     0.0000, 144861.6406],
         [139343.0469,      0.0000]],

        [[749407.8125,      0.0000],
         [651717.3750,      0.0000],
         [651639.6875,      0.0000],
         ...,
         [583092.5000,      0.0000],
         [578296.6875,      0.0000],
         [572368.2500,      0.0000]],

        [[439609.3750,      0.0000],
         [410327.6250,      0.0000],
         [343702.8750,      0.0000],
         ...,
         [180898.7656,      0.0000],
         [174298.9531,      0.0000],
         [     0.0000, 151066.8906]],

        ...,

        [[     0.0000, 213098.6094],
         [ 78932.3281,      0.0000],
         [ 78516.7031,      0.0000],
         ...,
         [ 38488.5469,      0.0000],
         [ 37677.2539,      0.0000],
         [ 36548.3555,      0.0000]],

        [[332587.0312,      0.0000],
         [     0.0000, 227047.9062],
         [215282.4219,      0.0000],
         ...,
         [109184.2344,      0.0000],
         [108667.8516,      0.0000],
         [106773.4453,      0.0000]],

        [[     0.0000, 172799.6094],
         [     0.0000, 128164.7969],
         [     0.0000, 106769.5703],
         ...,
         [ 62296.3164,      0.0000],
         [     0.0000,  60223.6445],
         [ 57446.6797,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1671543.5000,  144861.6406],
        [6217735.0000,       0.0000],
        [2490524.2500,  151066.8906],
        [3669032.2500,       0.0000],
        [ 516900.6250,  731734.6250],
        [ 865123.0000,  176549.2812],
        [1898706.3750,  454026.6562],
        [2198739.2500,       0.0000],
        [3260625.7500,  340521.8438],
        [3503480.5000,       0.0000],
        [4181237.5000,       0.0000],
        [1860013.0000,       0.0000],
        [3692490.7500,       0.0000],
        [8318765.0000,       0.0000],
        [5766197.0000,       0.0000],
        [4883594.0000,       0.0000],
        [6575287.0000,       0.0000],
        [7384844.5000,       0.0000],
        [4230934.0000,       0.0000],
        [3912231.5000,       0.0000],
        [6693649.0000,       0.0000],
        [4586883.5000,       0.0000],
        [6709892.0000,       0.0000],
        [5978175.5000,       0.0000],
        [3612364.2500,       0.0000],
        [5599315.0000,       0.0000],
        [4886298.0000,       0.0000],
        [6411458.0000,       0.0000],
        [6066916.5000,       0.0000],
        [5601167.5000,       0.0000],
        [7219966.5000,       0.0000],
        [4762077.0000,       0.0000],
        [2336319.7500,       0.0000],
        [2796866.2500,  713626.2500],
        [2032628.5000, 2415124.2500],
        [1187572.0000, 2083199.8750],
        [1273426.2500, 1158223.2500],
        [1620208.0000, 1453889.5000],
        [1018596.9375, 1507334.0000],
        [6374834.0000,       0.0000],
        [2461160.7500,       0.0000],
        [2550543.7500,       0.0000],
        [2414301.0000,       0.0000],
        [2329332.2500,       0.0000],
        [ 321563.0938, 1048032.8125],
        [ 353117.4375,  483700.6562],
        [2988891.0000,  443816.7812],
        [ 446396.9062,  242964.2188],
        [ 525084.6250,  679069.3125],
        [ 527710.5000, 1476471.7500],
        [1523320.3750,  266811.0938],
        [ 905193.5000,  926330.9375],
        [1809066.5000, 1575523.1250],
        [2303062.2500,  237272.7344],
        [4325384.5000,       0.0000],
        [2988065.2500,       0.0000],
        [1174025.3750, 1379258.2500],
        [1908642.8750,  833098.3750],
        [1165545.2500,  976412.2500],
        [ 671720.8125,  722504.3750],
        [1552864.7500,  813869.5000],
        [ 489737.3438,  213098.6094],
        [1134383.3750,  501841.3750],
        [ 204099.1406,  694005.9375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 331/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:03, 58.06s/it]  7%|▋         | 2/30 [01:00<11:53, 25.48s/it] 10%|█         | 3/30 [01:01<06:23, 14.19s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.88s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.95s/it] 20%|██        | 6/30 [01:03<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.5222233295440675
Epoch 332/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:41, 57.28s/it]  7%|▋         | 2/30 [01:00<11:53, 25.49s/it] 10%|█         | 3/30 [01:01<06:23, 14.19s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.88s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.95s/it] 20%|██        | 6/30 [01:03<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.462969668706258
Epoch 333/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:55<27:01, 55.90s/it]  7%|▋         | 2/30 [00:57<11:11, 23.97s/it] 10%|█         | 3/30 [00:58<06:01, 13.39s/it] 13%|█▎        | 4/30 [00:59<03:38,  8.40s/it] 17%|█▋        | 5/30 [00:59<02:21,  5.64s/it] 20%|██        | 6/30 [01:00<01:35,  3.98s/it] 23%|██▎       | 7/30 [01:01<01:07,  2.92s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.23s/it] 30%|███       | 9/30 [01:02<00:37,  1.77s/it] 33%|███▎      | 10/30 [01:03<00:29,  1.45s/it] 37%|███▋      | 11/30 [01:04<00:23,  1.24s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:05<00:16,  1.02it/s] 47%|████▋     | 14/30 [01:06<00:14,  1.09it/s] 50%|█████     | 15/30 [01:07<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:08<00:10,  1.24it/s] 60%|██████    | 18/30 [01:09<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:10<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:11<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:12<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:13<00:05,  1.33it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:14<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:15<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:16<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:17<00:00,  1.34it/s]100%|██████████| 30/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:18<00:00,  2.62s/it]
Epoch loss is 2.477030118306478
Epoch 334/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:18, 56.50s/it]  7%|▋         | 2/30 [00:58<11:22, 24.39s/it] 10%|█         | 3/30 [01:00<06:23, 14.21s/it] 13%|█▎        | 4/30 [01:01<03:52,  8.94s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.99s/it] 20%|██        | 6/30 [01:02<01:40,  4.20s/it] 23%|██▎       | 7/30 [01:03<01:10,  3.07s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.33s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:05<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.70s/it]
Epoch loss is 2.4626532951990763
Epoch 335/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:16, 60.57s/it]  7%|▋         | 2/30 [01:01<11:51, 25.41s/it] 10%|█         | 3/30 [01:02<06:22, 14.15s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.86s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.4952073891957602
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0247,  0.0024,  0.0108,  ..., -0.0021,  0.0105,  0.0047],
        [-0.0017,  0.0143,  0.0260,  ...,  0.0152, -0.0012, -0.0164],
        [-0.0305, -0.0379,  0.0232,  ...,  0.0802, -0.0092, -0.0174],
        ...,
        [ 0.0110, -0.0042,  0.0053,  ..., -0.0228, -0.0115, -0.0094],
        [-0.0307,  0.0064, -0.0062,  ...,  0.0030,  0.0149, -0.0123],
        [-0.0358, -0.0119,  0.0166,  ...,  0.0388,  0.0328, -0.0347]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8887, 0.8510, 0.8487, 0.8446, 0.8429, 0.8401, 0.8366, 0.8356, 0.8306,
         0.8285],
        [0.9465, 0.9365, 0.9364, 0.9330, 0.9326, 0.9313, 0.9290, 0.9285, 0.9284,
         0.9272],
        [0.9091, 0.9041, 0.8909, 0.8782, 0.8680, 0.8569, 0.8546, 0.8467, 0.8439,
         0.8337],
        [0.9552, 0.8994, 0.8954, 0.8886, 0.8869, 0.8823, 0.8789, 0.8778, 0.8770,
         0.8740],
        [0.8474, 0.8454, 0.8380, 0.8201, 0.8185, 0.8148, 0.8060, 0.8037, 0.7978,
         0.7902],
        [0.8588, 0.8222, 0.8115, 0.8090, 0.8036, 0.7957, 0.7906, 0.7883, 0.7797,
         0.7790],
        [0.8799, 0.8787, 0.8682, 0.8634, 0.8618, 0.8595, 0.8590, 0.8574, 0.8556,
         0.8556],
        [0.9051, 0.8848, 0.8784, 0.8656, 0.8578, 0.8482, 0.8323, 0.8303, 0.8206,
         0.8184],
        [0.9148, 0.9068, 0.8977, 0.8938, 0.8899, 0.8897, 0.8894, 0.8873, 0.8871,
         0.8862],
        [0.9514, 0.8927, 0.8910, 0.8896, 0.8845, 0.8805, 0.8796, 0.8792, 0.8690,
         0.8678],
        [0.9480, 0.9064, 0.9052, 0.9033, 0.9021, 0.8979, 0.8941, 0.8921, 0.8908,
         0.8897],
        [0.9006, 0.8848, 0.8645, 0.8608, 0.8407, 0.8181, 0.8165, 0.8061, 0.8012,
         0.7919],
        [0.9291, 0.9127, 0.9120, 0.9032, 0.8961, 0.8887, 0.8811, 0.8785, 0.8716,
         0.8616],
        [0.9639, 0.9582, 0.9573, 0.9566, 0.9544, 0.9543, 0.9502, 0.9493, 0.9468,
         0.9433],
        [0.9446, 0.9326, 0.9322, 0.9316, 0.9302, 0.9233, 0.9226, 0.9202, 0.9201,
         0.9180],
        [0.9286, 0.9250, 0.9249, 0.9174, 0.9147, 0.9146, 0.9123, 0.9097, 0.9088,
         0.9053],
        [0.9587, 0.9532, 0.9438, 0.9413, 0.9327, 0.9320, 0.9274, 0.9246, 0.9228,
         0.9222],
        [0.9657, 0.9597, 0.9541, 0.9421, 0.9402, 0.9400, 0.9385, 0.9383, 0.9353,
         0.9332],
        [0.9310, 0.9242, 0.9143, 0.9023, 0.8993, 0.8992, 0.8976, 0.8963, 0.8937,
         0.8893],
        [0.9109, 0.9027, 0.9020, 0.9017, 0.9004, 0.9002, 0.8988, 0.8985, 0.8961,
         0.8949],
        [0.9515, 0.9422, 0.9409, 0.9399, 0.9358, 0.9354, 0.9348, 0.9343, 0.9340,
         0.9305],
        [0.9294, 0.9163, 0.9161, 0.9158, 0.9127, 0.9119, 0.9087, 0.9070, 0.8980,
         0.8975],
        [0.9715, 0.9497, 0.9448, 0.9441, 0.9373, 0.9347, 0.9306, 0.9213, 0.9182,
         0.9171],
        [0.9450, 0.9390, 0.9338, 0.9284, 0.9282, 0.9270, 0.9269, 0.9260, 0.9254,
         0.9231],
        [0.9589, 0.9062, 0.9012, 0.8860, 0.8774, 0.8745, 0.8729, 0.8716, 0.8705,
         0.8669],
        [0.9421, 0.9376, 0.9365, 0.9296, 0.9257, 0.9242, 0.9207, 0.9168, 0.9105,
         0.9078],
        [0.9307, 0.9304, 0.9297, 0.9221, 0.9153, 0.9076, 0.9068, 0.9044, 0.9033,
         0.9019],
        [0.9626, 0.9507, 0.9473, 0.9337, 0.9303, 0.9276, 0.9226, 0.9226, 0.9226,
         0.9211],
        [0.9491, 0.9417, 0.9327, 0.9325, 0.9287, 0.9280, 0.9276, 0.9242, 0.9231,
         0.9212],
        [0.9372, 0.9319, 0.9293, 0.9279, 0.9260, 0.9259, 0.9231, 0.9207, 0.9182,
         0.9174],
        [0.9521, 0.9483, 0.9482, 0.9479, 0.9470, 0.9466, 0.9390, 0.9373, 0.9346,
         0.9339],
        [0.9440, 0.9244, 0.9237, 0.9168, 0.9089, 0.9047, 0.9044, 0.9037, 0.9025,
         0.8993],
        [0.8831, 0.8745, 0.8733, 0.8636, 0.8597, 0.8593, 0.8591, 0.8571, 0.8566,
         0.8552],
        [0.9130, 0.9013, 0.8953, 0.8927, 0.8911, 0.8907, 0.8856, 0.8854, 0.8853,
         0.8836],
        [0.9265, 0.9207, 0.9195, 0.9193, 0.9082, 0.9044, 0.8998, 0.8991, 0.8968,
         0.8941],
        [0.9101, 0.9046, 0.9002, 0.8908, 0.8877, 0.8814, 0.8807, 0.8805, 0.8671,
         0.8661],
        [0.9481, 0.8696, 0.8670, 0.8565, 0.8549, 0.8473, 0.8352, 0.8350, 0.8275,
         0.8255],
        [0.9125, 0.8965, 0.8957, 0.8895, 0.8784, 0.8761, 0.8710, 0.8701, 0.8641,
         0.8635],
        [0.8995, 0.8990, 0.8829, 0.8718, 0.8634, 0.8577, 0.8544, 0.8534, 0.8452,
         0.8409],
        [0.9423, 0.9387, 0.9371, 0.9370, 0.9363, 0.9356, 0.9314, 0.9308, 0.9301,
         0.9278],
        [0.8893, 0.8868, 0.8844, 0.8770, 0.8676, 0.8641, 0.8639, 0.8499, 0.8413,
         0.8348],
        [0.9070, 0.8719, 0.8684, 0.8674, 0.8656, 0.8654, 0.8631, 0.8628, 0.8581,
         0.8573],
        [0.8937, 0.8854, 0.8828, 0.8689, 0.8631, 0.8582, 0.8555, 0.8521, 0.8440,
         0.8406],
        [0.8943, 0.8798, 0.8695, 0.8694, 0.8601, 0.8576, 0.8522, 0.8508, 0.8468,
         0.8409],
        [0.8646, 0.8431, 0.8373, 0.8338, 0.8238, 0.8236, 0.8132, 0.8059, 0.7939,
         0.7920],
        [0.8370, 0.8175, 0.8049, 0.7922, 0.7864, 0.7819, 0.7809, 0.7664, 0.7622,
         0.7580],
        [0.9093, 0.9005, 0.8933, 0.8930, 0.8924, 0.8901, 0.8824, 0.8806, 0.8802,
         0.8799],
        [0.8183, 0.7935, 0.7928, 0.7871, 0.7728, 0.7688, 0.7660, 0.7645, 0.7470,
         0.7390],
        [0.8503, 0.8417, 0.8177, 0.8166, 0.8164, 0.8100, 0.8088, 0.8019, 0.8007,
         0.7951],
        [0.9008, 0.8721, 0.8597, 0.8548, 0.8422, 0.8420, 0.8353, 0.8343, 0.8278,
         0.8253],
        [0.8901, 0.8600, 0.8580, 0.8556, 0.8441, 0.8335, 0.8243, 0.8242, 0.8162,
         0.8147],
        [0.8966, 0.8920, 0.8571, 0.8338, 0.8322, 0.8308, 0.8148, 0.8134, 0.8129,
         0.8123],
        [0.9332, 0.9151, 0.9147, 0.9017, 0.8770, 0.8694, 0.8676, 0.8650, 0.8541,
         0.8492],
        [0.9183, 0.8780, 0.8768, 0.8665, 0.8652, 0.8607, 0.8586, 0.8509, 0.8493,
         0.8483],
        [0.9335, 0.9126, 0.9092, 0.9082, 0.9023, 0.9023, 0.9013, 0.9002, 0.8997,
         0.8976],
        [0.9106, 0.8887, 0.8847, 0.8771, 0.8765, 0.8755, 0.8745, 0.8739, 0.8734,
         0.8706],
        [0.9196, 0.8831, 0.8824, 0.8766, 0.8692, 0.8642, 0.8439, 0.8433, 0.8404,
         0.8364],
        [0.9432, 0.8878, 0.8796, 0.8748, 0.8646, 0.8570, 0.8465, 0.8461, 0.8455,
         0.8354],
        [0.9171, 0.8665, 0.8650, 0.8564, 0.8419, 0.8408, 0.8373, 0.8352, 0.8348,
         0.8336],
        [0.9042, 0.8284, 0.8273, 0.8184, 0.8087, 0.8063, 0.8051, 0.8022, 0.7975,
         0.7948],
        [0.8850, 0.8776, 0.8688, 0.8659, 0.8617, 0.8607, 0.8566, 0.8563, 0.8556,
         0.8555],
        [0.8585, 0.7883, 0.7880, 0.7817, 0.7763, 0.7442, 0.7388, 0.7377, 0.7373,
         0.7335],
        [0.8894, 0.8627, 0.8582, 0.8293, 0.8266, 0.8238, 0.8200, 0.8103, 0.8102,
         0.8082],
        [0.8437, 0.8217, 0.8088, 0.7935, 0.7920, 0.7851, 0.7762, 0.7710, 0.7680,
         0.7641]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 0, 1, 0, 1],
        [1, 1, 0, 1, 0, 0, 1, 1, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 0, 1, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 0, 0, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 326117.8125,  190482.7969,  184269.5781,  173821.8750,  169753.3750,
          163095.2188,  155045.9844,  152872.4062,  142299.9062,  138075.9219],
        [ 744879.6875,  646212.9375,  645062.7500,  614907.1250,  611304.7500,
          600006.1250,  580741.1250,  576449.3125,  575417.2500,  565785.1875],
        [ 436992.7188,  406621.0938,  336789.3438,  280825.9062,  242716.0938,
          207075.1562,  200527.9219,  179178.7188,  172132.4844,  148837.7500],
        [ 843448.5000,  380326.5000,  359139.6875,  325752.2500,  317938.6875,
          297652.6562,  283684.0625,  279320.0312,  276012.1562,  264639.3125],
        [ 180876.8594,  175872.1875,  158148.0312,  122449.3984,  119682.2422,
          113488.2812,  100124.1250,   96887.9844,   89023.6172,   79867.8594],
        [ 212994.7812,  126251.8125,  108314.9219,  104588.5938,   96792.7734,
           86473.1328,   80382.4297,   77730.2500,   68744.7031,   68086.3125],
        [ 287728.7812,  283027.6562,  243549.9062,  227280.7812,  222327.5000,
          215000.7188,  213481.2031,  208666.2344,  203493.9375,  203276.5156],
        [ 412493.0312,  308470.7812,  281861.8750,  234483.3906,  209731.6094,
          182980.3438,  145873.2500,  141615.6875,  123406.2500,  119487.3359],
        [ 473773.7812,  422808.5312,  371353.6562,  350775.7500,  331967.1875,
          331254.6875,  329779.5312,  319687.5312,  319094.1875,  314919.5000],
        [ 799063.7500,  345449.4688,  337319.0625,  330754.0312,  307141.0000,
          290336.3438,  286505.4062,  285015.0000,  246143.8750,  242255.4531],
        [ 761074.9375,  420514.8125,  412821.2188,  402133.4062,  395140.3438,
          372286.9375,  352462.1250,  342553.6562,  336402.8438,  330824.3750],
        [ 386821.5938,  308684.1250,  230887.4531,  219128.5469,  164344.6250,
          119061.9141,  116387.0703,  100243.6406,   93503.9375,   81835.5156],
        [ 580933.8750,  459598.5938,  455270.2500,  401511.0625,  362993.1562,
          326487.1875,  292606.5312,  282051.4375,  255726.3750,  221566.5625],
        [ 954936.0000,  880386.3125,  869461.8125,  860960.5000,  834271.8125,
          833328.7500,  786217.1875,  775569.3125,  747951.9375,  712206.2500],
        [ 725294.6250,  611130.5000,  607878.3125,  602571.0625,  590555.7500,
          535329.4375,  529351.6250,  511817.4688,  511160.4062,  496238.9375],
        [ 577002.0625,  547754.3125,  547473.2500,  491658.8125,  472919.2500,
          472752.4375,  456883.0312,  440508.7500,  434717.4062,  413408.2500],
        [ 887672.1250,  820266.0000,  716515.9375,  692220.3125,  611747.3750,
          605552.8750,  567431.1250,  545025.8750,  531184.1875,  526789.2500],
        [ 979905.8125,  900472.8125,  830200.2500,  699516.1875,  681431.1875,
          678904.6875,  665145.8125,  662835.8125,  634740.4375,  615955.3750],
        [ 597413.8750,  541795.7500,  470547.5000,  396322.3438,  379578.2812,
          378980.7188,  370701.5312,  363794.4062,  350300.6875,  329055.4062],
        [ 447882.0625,  398793.5938,  394588.6562,  392838.2188,  385638.1250,
          384482.0938,  376795.7500,  375218.2812,  362784.1562,  356401.5938],
        [ 800940.6250,  700444.1250,  687895.9375,  678643.1875,  639763.3750,
          636074.2500,  630470.9375,  626043.7500,  623232.6875,  592781.1875],
        [ 583584.2500,  483751.1562,  482437.6562,  480854.8125,  460075.3125,
          454711.3750,  434144.4375,  423753.5312,  372525.2812,  370131.4062],
        [1065068.3750,  780249.8125,  727368.5625,  720553.4375,  653275.5000,
          629174.6875,  593779.7500,  519877.9375,  497107.1875,  489495.9375],
        [ 729521.3750,  669782.4375,  621593.2500,  575796.5625,  573749.8125,
          564270.4375,  563308.0625,  555976.5000,  551674.6250,  533508.9375],
        [ 889124.3125,  419182.6875,  390420.6250,  313889.2500,  277623.0312,
          266551.1250,  260548.3750,  255568.6406,  251534.6719,  238988.2344],
        [ 699847.8125,  655940.0000,  645831.5625,  585336.1250,  553391.3125,
          541824.6875,  515241.7188,  487457.8750,  445692.0000,  428818.5312],
        [ 594464.8125,  592195.8125,  586004.1250,  526004.6250,  476942.6250,
          427215.5625,  422822.2188,  408441.5000,  402129.5312,  393989.6562],
        [ 938134.7500,  790896.2500,  753697.5000,  620786.9375,  591141.7500,
          568711.8750,  529959.2500,  529562.6250,  529538.8750,  518119.8438],
        [ 773765.3125,  695381.0000,  611687.3125,  609982.1875,  578256.4375,
          572536.4375,  569097.6250,  542147.6875,  533102.0625,  519514.1250],
        [ 652552.6250,  604939.3125,  582526.6875,  571572.9375,  556290.0000,
          555125.0625,  533793.4375,  515772.6875,  497554.9062,  491731.9688],
        [ 807464.1875,  764398.5000,  764006.3750,  760187.8125,  750245.8750,
          745818.0000,  669130.5625,  653539.7500,  628404.1250,  622610.1250],
        [ 718594.9375,  543186.3750,  537783.0000,  487626.6250,  435264.9688,
          410272.8438,  408520.5938,  404048.9062,  397478.3438,  379868.3125],
        [ 301317.8750,  266426.0938,  262083.5156,  227888.5000,  215683.5625,
          214359.4844,  213772.1406,  207755.4219,  206231.6562,  202140.5781],
        [ 462046.0312,  390734.6562,  358529.8750,  345364.5000,  337757.1875,
          335662.5938,  312305.8125,  311431.7188,  310973.7500,  303277.4062],
        [ 559657.2500,  515706.7500,  506887.5625,  505348.8125,  431450.4375,
          408473.4688,  382526.8750,  378542.5625,  366132.2812,  352669.5938],
        [ 443356.9688,  409340.3438,  384664.3750,  336295.7188,  321679.3750,
          294146.7500,  291079.0938,  290289.8438,  239709.3125,  236397.5000],
        [ 762586.8750,  248557.2656,  239369.8438,  206048.4375,  201337.7188,
          180567.3281,  151878.7188,  151526.2969,  136144.7656,  132222.9688],
        [ 458244.0625,  364845.4062,  360796.0625,  330230.5312,  281633.2188,
          272771.5312,  253366.3281,  250277.2500,  229785.1719,  227741.8594],
        [ 380879.6562,  378022.7188,  300528.9688,  256240.2500,  227208.8281,
          209492.9219,  200052.5000,  197186.0781,  175290.9688,  164954.4844],
        [ 702205.7500,  666903.3750,  651644.0625,  650828.6250,  644094.5625,
          637550.0000,  600617.5625,  595896.9375,  589295.0000,  570193.9375],
        [ 329288.6562,  317486.6250,  307028.8438,  276233.5938,  241250.7031,
          229592.6406,  229148.1562,  187374.6562,  165877.3438,  151024.8281],
        [ 423565.2500,  256678.7812,  244199.2500,  240597.0156,  234771.3594,
          234104.8750,  226352.5938,  225488.8438,  210849.6719,  208298.4219],
        [ 350688.4375,  311473.0000,  299913.0938,  246098.8125,  226516.7344,
          211048.0156,  203081.5938,  193483.9375,  172295.0938,  164157.5781],
        [ 353580.8750,  287326.2500,  247907.4219,  247843.8594,  217013.3281,
          209218.3906,  193837.6094,  189995.6719,  179329.1406,  164936.5469],
        [ 231266.0469,  170155.8125,  156663.8594,  148964.2812,  129165.9375,
          128774.3203,  111029.1484,  100002.1562,   84255.6484,   81953.0625],
        [ 155929.9062,  117979.8906,   98605.3047,   82249.3359,   75627.6172,
           71010.8906,   69995.8203,   56852.0703,   53573.9805,   50470.1250],
        [ 437760.6250,  386362.5938,  348645.2812,  347254.9062,  343968.8125,
          333164.1562,  298388.2188,  290609.2500,  289200.8750,  287604.7812],
        [ 119343.7266,   83774.9141,   82936.6562,   76484.2891,   62331.8555,
           58830.1914,   56539.8164,   55349.8711,   43135.4336,   38452.1875],
        [ 188469.2969,  166744.6094,  118377.8438,  116528.4453,  116211.9375,
          106099.8594,  104237.9844,   94390.2109,   92785.6562,   85756.0156],
        [ 388188.9375,  257395.2969,  215604.5938,  200975.3438,  167851.0625,
          167538.2500,  152079.6094,  149973.2031,  136663.0312,  131865.9688],
        [ 332995.1562,  216608.6719,  210398.1094,  203405.4688,  172627.9844,
          148361.4531,  130008.2734,  129784.3047,  115837.1562,  113468.6953],
        [ 365183.4062,  342203.5938,  207804.7500,  148916.5625,  145620.7031,
          142696.0469,  113628.7422,  111330.0547,  110474.2188,  109489.7422],
        [ 616091.6250,  475625.8438,  472816.8750,  393121.1250,  275922.1562,
          247853.5469,  241451.6250,  232558.3281,  199191.6250,  185674.2656],
        [ 497937.9688,  279971.8125,  275238.0312,  237539.2031,  233248.6875,
          218633.8438,  212170.2500,  190186.5625,  185998.5938,  183123.5000],
        [ 618576.1250,  459181.5625,  437204.9062,  430927.7500,  396210.8750,
          396210.8750,  390612.0312,  384477.3438,  381736.4062,  370586.6562],
        [ 446371.7188,  326353.6562,  308379.2812,  276616.1250,  274078.4688,
          270451.6875,  266376.8125,  263984.9688,  262196.2500,  251882.7500],
        [ 507309.2500,  301344.3125,  298434.5938,  274405.3750,  247108.6875,
          230020.0000,  172177.1562,  170578.4062,  163633.1875,  154492.7812],
        [ 710960.3125,  322095.3438,  286742.3750,  267499.2500,  231440.5625,
          207477.8125,  178646.2031,  177569.8125,  176088.3438,  152471.2812],
        [ 489549.1562,  237745.8906,  232696.1094,  205677.1719,  167131.6250,
          164624.4688,  156570.0625,  152075.5469,  151101.0312,  148521.4219],
        [ 407491.0312,  137890.1094,  135686.9531,  119621.8750,  104128.9922,
          100551.1875,   98899.8047,   94847.5156,   88696.9297,   85362.7266],
        [ 309324.1875,  278422.1875,  245693.1094,  235668.2031,  221903.4219,
          218902.1406,  206356.5938,  205559.7188,  203344.0000,  203021.9531],
        [ 211936.6875,   77793.4297,   77377.2266,   70799.2422,   65485.8320,
           41420.2461,   38341.8906,   37723.2383,   37514.7617,   35539.1719],
        [ 329653.1250,  224963.9062,  210954.8594,  139767.2031,  134391.2812,
          129055.7344,  122370.9531,  106501.1016,  106369.4531,  103363.4062],
        [ 171603.2500,  125289.5078,  104177.8672,   83782.0234,   81948.6875,
           74241.3359,   65377.3828,   60720.9375,   58214.3203,   55055.5312]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[326117.8125,      0.0000],
         [190482.7969,      0.0000],
         [184269.5781,      0.0000],
         ...,
         [152872.4062,      0.0000],
         [     0.0000, 142299.9062],
         [138075.9219,      0.0000]],

        [[744879.6875,      0.0000],
         [646212.9375,      0.0000],
         [645062.7500,      0.0000],
         ...,
         [576449.3125,      0.0000],
         [575417.2500,      0.0000],
         [565785.1875,      0.0000]],

        [[436992.7188,      0.0000],
         [406621.0938,      0.0000],
         [336789.3438,      0.0000],
         ...,
         [179178.7188,      0.0000],
         [172132.4844,      0.0000],
         [     0.0000, 148837.7500]],

        ...,

        [[     0.0000, 211936.6875],
         [ 77793.4297,      0.0000],
         [ 77377.2266,      0.0000],
         ...,
         [ 37723.2383,      0.0000],
         [ 37514.7617,      0.0000],
         [ 35539.1719,      0.0000]],

        [[329653.1250,      0.0000],
         [     0.0000, 224963.9062],
         [210954.8594,      0.0000],
         ...,
         [106501.1016,      0.0000],
         [106369.4531,      0.0000],
         [103363.4062,      0.0000]],

        [[     0.0000, 171603.2500],
         [     0.0000, 125289.5078],
         [     0.0000, 104177.8672],
         ...,
         [ 60720.9375,      0.0000],
         [     0.0000,  58214.3203],
         [ 55055.5312,      0.0000]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1653535.0000,  142299.9062],
        [6160766.5000,       0.0000],
        [2462859.5000,  148837.7500],
        [3627913.7500,       0.0000],
        [ 511147.2500,  725273.3750],
        [ 857026.3750,  173333.2969],
        [1867071.2500,  440762.0000],
        [2160403.5000,       0.0000],
        [3234159.7500,  331254.6875],
        [3469983.5000,       0.0000],
        [4126214.5000,       0.0000],
        [1820898.5000,       0.0000],
        [3638745.0000,       0.0000],
        [8255290.0000,       0.0000],
        [5721328.5000,       0.0000],
        [4855077.5000,       0.0000],
        [6504405.0000,       0.0000],
        [7349108.5000,       0.0000],
        [4178490.2500,       0.0000],
        [3875422.5000,       0.0000],
        [6616290.0000,       0.0000],
        [4545969.5000,       0.0000],
        [6675951.5000,       0.0000],
        [5939182.0000,       0.0000],
        [3563431.0000,       0.0000],
        [5559382.0000,       0.0000],
        [4830210.5000,       0.0000],
        [6370549.5000,       0.0000],
        [6005470.5000,       0.0000],
        [5561859.5000,       0.0000],
        [7165805.0000,       0.0000],
        [4722645.0000,       0.0000],
        [2317658.7500,       0.0000],
        [2764189.0000,  703894.3750],
        [2012705.5000, 2394690.2500],
        [1175737.0000, 2071222.3750],
        [1256404.7500, 1153835.5000],
        [1594738.2500, 1434953.1250],
        [1003164.6875, 1486692.7500],
        [6309229.5000,       0.0000],
        [2434306.0000,       0.0000],
        [2504906.0000,       0.0000],
        [2378756.2500,       0.0000],
        [2290989.0000,       0.0000],
        [ 313423.7500, 1028806.5000],
        [ 352457.6562,  479837.3125],
        [2925199.0000,  437760.6250],
        [ 437270.0938,  239908.8281],
        [ 519231.3125,  670370.5000],
        [ 520054.9062, 1448080.2500],
        [1509296.6250,  264198.6250],
        [ 882156.0625,  915191.7500],
        [1785872.2500, 1554434.8750],
        [2280799.7500,  233248.6875],
        [4265724.5000,       0.0000],
        [2946691.5000,       0.0000],
        [1158668.7500, 1360835.0000],
        [1889956.0000,  821035.1250],
        [1143920.2500,  961772.2500],
        [ 660150.2500,  713026.8125],
        [1521547.0000,  806648.5000],
        [ 481995.0625,  211936.6875],
        [1113604.1250,  493786.8125],
        [ 197725.1562,  682685.6250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 336/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:53, 59.78s/it]  7%|▋         | 2/30 [01:00<11:41, 25.06s/it] 10%|█         | 3/30 [01:01<06:16, 13.96s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.4522419770558677
Epoch 337/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:30, 56.92s/it]  7%|▋         | 2/30 [00:57<11:08, 23.88s/it] 10%|█         | 3/30 [00:58<05:59, 13.32s/it] 13%|█▎        | 4/30 [00:59<03:37,  8.36s/it] 17%|█▋        | 5/30 [00:59<02:20,  5.62s/it] 20%|██        | 6/30 [01:00<01:36,  4.01s/it] 23%|██▎       | 7/30 [01:01<01:07,  2.95s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.25s/it] 30%|███       | 9/30 [01:03<00:37,  1.78s/it] 33%|███▎      | 10/30 [01:03<00:29,  1.46s/it] 37%|███▋      | 11/30 [01:04<00:23,  1.25s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:06<00:14,  1.09it/s] 50%|█████     | 15/30 [01:07<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:09<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:10<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:12<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:13<00:05,  1.32it/s] 80%|████████  | 24/30 [01:14<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:15<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:16<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  2.63s/it]
Epoch loss is 2.513072872161865
Epoch 338/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:05, 60.19s/it]  7%|▋         | 2/30 [01:03<12:24, 26.59s/it] 10%|█         | 3/30 [01:04<06:39, 14.79s/it] 13%|█▎        | 4/30 [01:04<04:00,  9.25s/it] 17%|█▋        | 5/30 [01:05<02:34,  6.19s/it] 20%|██        | 6/30 [01:06<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.17s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.40s/it] 30%|███       | 9/30 [01:08<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 2.456993754704793
Epoch 339/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:19, 56.55s/it]  7%|▋         | 2/30 [00:59<11:33, 24.76s/it] 10%|█         | 3/30 [00:59<06:12, 13.80s/it] 13%|█▎        | 4/30 [01:00<03:44,  8.65s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.80s/it] 20%|██        | 6/30 [01:02<01:38,  4.08s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.28s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 2.465095551808675
Epoch 340/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:46, 59.54s/it]  7%|▋         | 2/30 [01:00<11:38, 24.96s/it] 10%|█         | 3/30 [01:01<06:15, 13.90s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.4561007976531983
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0246,  0.0025,  0.0110,  ..., -0.0024,  0.0111,  0.0045],
        [-0.0019,  0.0140,  0.0263,  ...,  0.0154, -0.0008, -0.0164],
        [-0.0304, -0.0379,  0.0234,  ...,  0.0804, -0.0088, -0.0171],
        ...,
        [ 0.0103, -0.0041,  0.0058,  ..., -0.0228, -0.0108, -0.0089],
        [-0.0311,  0.0067, -0.0060,  ...,  0.0033,  0.0152, -0.0122],
        [-0.0358, -0.0122,  0.0169,  ...,  0.0390,  0.0332, -0.0342]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8877, 0.8501, 0.8490, 0.8439, 0.8417, 0.8391, 0.8363, 0.8346, 0.8293,
         0.8278],
        [0.9461, 0.9359, 0.9357, 0.9322, 0.9317, 0.9311, 0.9284, 0.9283, 0.9275,
         0.9264],
        [0.9088, 0.9034, 0.8895, 0.8776, 0.8669, 0.8561, 0.8542, 0.8462, 0.8432,
         0.8325],
        [0.9553, 0.8981, 0.8944, 0.8876, 0.8860, 0.8822, 0.8773, 0.8766, 0.8758,
         0.8727],
        [0.8471, 0.8442, 0.8374, 0.8196, 0.8173, 0.8142, 0.8057, 0.8032, 0.7970,
         0.7888],
        [0.8583, 0.8218, 0.8114, 0.8070, 0.8031, 0.7944, 0.7909, 0.7865, 0.7793,
         0.7769],
        [0.8786, 0.8774, 0.8671, 0.8613, 0.8607, 0.8590, 0.8570, 0.8565, 0.8549,
         0.8539],
        [0.9040, 0.8836, 0.8773, 0.8648, 0.8570, 0.8466, 0.8318, 0.8292, 0.8190,
         0.8172],
        [0.9144, 0.9065, 0.8969, 0.8929, 0.8895, 0.8889, 0.8880, 0.8867, 0.8864,
         0.8861],
        [0.9511, 0.8921, 0.8905, 0.8887, 0.8836, 0.8799, 0.8785, 0.8781, 0.8683,
         0.8672],
        [0.9476, 0.9061, 0.9037, 0.9022, 0.9015, 0.8970, 0.8926, 0.8913, 0.8896,
         0.8885],
        [0.8994, 0.8836, 0.8636, 0.8596, 0.8392, 0.8161, 0.8144, 0.8056, 0.7995,
         0.7905],
        [0.9277, 0.9120, 0.9116, 0.9022, 0.8947, 0.8877, 0.8799, 0.8772, 0.8710,
         0.8610],
        [0.9633, 0.9577, 0.9568, 0.9565, 0.9540, 0.9537, 0.9497, 0.9487, 0.9461,
         0.9424],
        [0.9444, 0.9322, 0.9318, 0.9310, 0.9295, 0.9228, 0.9221, 0.9196, 0.9193,
         0.9176],
        [0.9283, 0.9246, 0.9246, 0.9170, 0.9144, 0.9143, 0.9119, 0.9094, 0.9082,
         0.9051],
        [0.9582, 0.9527, 0.9427, 0.9408, 0.9321, 0.9314, 0.9267, 0.9240, 0.9221,
         0.9214],
        [0.9654, 0.9595, 0.9538, 0.9418, 0.9401, 0.9397, 0.9381, 0.9379, 0.9348,
         0.9330],
        [0.9305, 0.9232, 0.9131, 0.9018, 0.8987, 0.8986, 0.8968, 0.8957, 0.8929,
         0.8878],
        [0.9104, 0.9022, 0.9017, 0.9013, 0.9000, 0.8998, 0.8983, 0.8975, 0.8954,
         0.8940],
        [0.9507, 0.9413, 0.9403, 0.9392, 0.9354, 0.9346, 0.9345, 0.9333, 0.9332,
         0.9301],
        [0.9290, 0.9155, 0.9154, 0.9151, 0.9122, 0.9118, 0.9083, 0.9066, 0.8967,
         0.8967],
        [0.9712, 0.9497, 0.9447, 0.9437, 0.9369, 0.9339, 0.9301, 0.9211, 0.9175,
         0.9167],
        [0.9446, 0.9385, 0.9334, 0.9283, 0.9276, 0.9265, 0.9263, 0.9256, 0.9252,
         0.9228],
        [0.9584, 0.9058, 0.9003, 0.8854, 0.8761, 0.8737, 0.8716, 0.8704, 0.8696,
         0.8650],
        [0.9415, 0.9369, 0.9360, 0.9291, 0.9254, 0.9237, 0.9202, 0.9161, 0.9098,
         0.9074],
        [0.9299, 0.9298, 0.9288, 0.9214, 0.9146, 0.9068, 0.9059, 0.9036, 0.9023,
         0.9013],
        [0.9624, 0.9504, 0.9469, 0.9330, 0.9298, 0.9272, 0.9224, 0.9221, 0.9218,
         0.9205],
        [0.9485, 0.9411, 0.9320, 0.9316, 0.9281, 0.9275, 0.9274, 0.9233, 0.9219,
         0.9209],
        [0.9367, 0.9313, 0.9288, 0.9277, 0.9256, 0.9253, 0.9225, 0.9204, 0.9178,
         0.9169],
        [0.9514, 0.9480, 0.9477, 0.9476, 0.9467, 0.9460, 0.9385, 0.9366, 0.9341,
         0.9330],
        [0.9432, 0.9240, 0.9232, 0.9162, 0.9080, 0.9044, 0.9036, 0.9028, 0.9020,
         0.8990],
        [0.8824, 0.8737, 0.8732, 0.8633, 0.8590, 0.8588, 0.8585, 0.8564, 0.8560,
         0.8546],
        [0.9120, 0.9004, 0.8944, 0.8915, 0.8901, 0.8900, 0.8851, 0.8846, 0.8844,
         0.8828],
        [0.9258, 0.9203, 0.9190, 0.9182, 0.9077, 0.9037, 0.8999, 0.8982, 0.8958,
         0.8934],
        [0.9098, 0.9039, 0.8996, 0.8901, 0.8875, 0.8811, 0.8807, 0.8796, 0.8662,
         0.8656],
        [0.9480, 0.8692, 0.8672, 0.8561, 0.8541, 0.8459, 0.8338, 0.8336, 0.8268,
         0.8235],
        [0.9115, 0.8953, 0.8945, 0.8883, 0.8783, 0.8755, 0.8698, 0.8690, 0.8629,
         0.8626],
        [0.8989, 0.8982, 0.8818, 0.8709, 0.8619, 0.8569, 0.8535, 0.8520, 0.8443,
         0.8400],
        [0.9417, 0.9377, 0.9366, 0.9364, 0.9355, 0.9350, 0.9309, 0.9304, 0.9294,
         0.9270],
        [0.8885, 0.8859, 0.8838, 0.8760, 0.8669, 0.8633, 0.8631, 0.8490, 0.8405,
         0.8338],
        [0.9062, 0.8708, 0.8671, 0.8659, 0.8651, 0.8637, 0.8620, 0.8613, 0.8566,
         0.8561],
        [0.8930, 0.8843, 0.8817, 0.8681, 0.8620, 0.8574, 0.8542, 0.8509, 0.8427,
         0.8401],
        [0.8928, 0.8791, 0.8684, 0.8679, 0.8588, 0.8568, 0.8513, 0.8502, 0.8461,
         0.8395],
        [0.8635, 0.8425, 0.8357, 0.8328, 0.8224, 0.8223, 0.8119, 0.8036, 0.7919,
         0.7901],
        [0.8364, 0.8163, 0.8039, 0.7918, 0.7859, 0.7819, 0.7806, 0.7663, 0.7621,
         0.7584],
        [0.9084, 0.8990, 0.8919, 0.8918, 0.8911, 0.8890, 0.8808, 0.8789, 0.8788,
         0.8785],
        [0.8171, 0.7921, 0.7919, 0.7858, 0.7719, 0.7663, 0.7644, 0.7636, 0.7456,
         0.7373],
        [0.8485, 0.8407, 0.8172, 0.8160, 0.8156, 0.8090, 0.8082, 0.8022, 0.7993,
         0.7949],
        [0.9001, 0.8712, 0.8582, 0.8528, 0.8412, 0.8409, 0.8339, 0.8330, 0.8265,
         0.8236],
        [0.8891, 0.8596, 0.8573, 0.8550, 0.8436, 0.8330, 0.8242, 0.8232, 0.8156,
         0.8138],
        [0.8957, 0.8913, 0.8560, 0.8321, 0.8300, 0.8293, 0.8138, 0.8112, 0.8111,
         0.8104],
        [0.9325, 0.9140, 0.9140, 0.9009, 0.8759, 0.8678, 0.8667, 0.8643, 0.8527,
         0.8487],
        [0.9176, 0.8777, 0.8761, 0.8659, 0.8642, 0.8601, 0.8572, 0.8498, 0.8483,
         0.8479],
        [0.9326, 0.9115, 0.9081, 0.9072, 0.9013, 0.9013, 0.9008, 0.8990, 0.8988,
         0.8968],
        [0.9096, 0.8879, 0.8843, 0.8757, 0.8757, 0.8746, 0.8737, 0.8726, 0.8720,
         0.8699],
        [0.9183, 0.8827, 0.8811, 0.8756, 0.8680, 0.8639, 0.8433, 0.8426, 0.8395,
         0.8351],
        [0.9430, 0.8868, 0.8791, 0.8736, 0.8638, 0.8565, 0.8452, 0.8448, 0.8446,
         0.8339],
        [0.9162, 0.8659, 0.8640, 0.8552, 0.8403, 0.8398, 0.8367, 0.8332, 0.8330,
         0.8326],
        [0.9037, 0.8275, 0.8262, 0.8170, 0.8073, 0.8052, 0.8038, 0.8010, 0.7963,
         0.7931],
        [0.8849, 0.8766, 0.8677, 0.8648, 0.8598, 0.8597, 0.8552, 0.8548, 0.8541,
         0.8538],
        [0.8582, 0.7873, 0.7870, 0.7812, 0.7753, 0.7427, 0.7372, 0.7370, 0.7363,
         0.7316],
        [0.8889, 0.8620, 0.8566, 0.8278, 0.8261, 0.8226, 0.8177, 0.8090, 0.8086,
         0.8062],
        [0.8432, 0.8202, 0.8074, 0.7921, 0.7900, 0.7846, 0.7754, 0.7692, 0.7657,
         0.7619]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 0, 1, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 1, 1, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 0, 0, 1],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 321676.0312,  187931.0312,  185203.3125,  172035.8281,  166860.2344,
          160597.1406,  154403.8281,  150688.4688,  139779.2031,  136763.6875],
        [ 740561.1250,  640950.5000,  638235.5625,  607614.6250,  602960.8750,
          598291.9375,  575305.3125,  574955.3750,  568103.0625,  559545.6875],
        [ 434751.8125,  402828.9062,  330234.0000,  278469.4375,  239100.1719,
          204871.6094,  199408.1094,  177719.2188,  170464.4219,  146315.8906],
        [ 845097.4375,  373373.2188,  354009.6875,  321117.2500,  314166.8750,
          297210.7188,  277373.7188,  274661.6875,  271416.2188,  259763.1406],
        [ 180136.6406,  172762.8594,  156886.0469,  121532.9922,  117606.8125,
          112508.8281,   99741.7656,   96206.0938,   88111.0781,   78275.8125],
        [ 211412.4375,  125482.8516,  108137.5000,  101647.4219,   96096.6953,
           84859.1562,   80667.9531,   75837.7188,   68404.7031,   66031.0469],
        [ 282669.7188,  277777.4375,  239741.0781,  220520.7812,  218782.3281,
          213425.0312,  207605.8906,  206163.0312,  201374.2188,  198580.2969],
        [ 406101.0000,  303496.7188,  277119.3750,  231967.5938,  207458.8281,
          178834.0312,  144698.7188,  139451.7969,  120517.5391,  117543.7969],
        [ 470885.5312,  420643.5625,  366748.0625,  346571.7188,  329950.6562,
          327051.8438,  323023.1250,  317029.7500,  315585.1250,  314314.0000],
        [ 795946.2500,  342351.8125,  334644.2500,  326271.7812,  303273.6562,
          287668.1875,  282115.2188,  280371.5312,  243825.0469,  240131.2188],
        [ 756951.1875,  418525.5625,  404336.8750,  395646.3750,  391548.9375,
          367501.5000,  344996.1250,  338552.1250,  330617.7812,  325595.7188],
        [ 380096.2500,  303503.3750,  228053.9688,  215484.5312,  160978.3438,
          115738.6641,  112908.5703,   99580.6641,   91297.2500,   80234.1562],
        [ 569970.9375,  455280.6562,  452870.3750,  395540.7500,  355750.2812,
          321607.9062,  287655.8125,  276989.9062,  253337.0938,  219618.9531],
        [ 947665.5000,  874418.5625,  863765.7500,  859926.5625,  829200.0625,
          826500.8125,  780211.1250,  769469.2500,  741436.6250,  702725.0000],
        [ 722815.0625,  607885.8125,  603722.6875,  597454.8750,  584875.7500,
          531010.4375,  525527.7500,  507144.3438,  505583.5625,  492979.0625],
        [ 574464.8750,  544705.8125,  544650.2500,  488962.6562,  470843.7500,
          470140.6562,  454644.5938,  438632.7812,  431353.7188,  412363.2188],
        [ 880236.8750,  813784.0625,  706018.5625,  686923.1250,  607062.0000,
          601035.8750,  561215.1875,  540193.2500,  525594.8750,  521009.0625],
        [ 975708.6250,  896837.2500,  826907.5625,  697314.8125,  680119.1250,
          675874.9375,  660654.7500,  658778.6250,  630229.8750,  614516.6875],
        [ 592548.2500,  534257.3750,  462633.7812,  393750.4062,  376505.5000,
          376117.1875,  366325.0938,  360465.5625,  346479.1875,  322401.7500],
        [ 444842.6875,  395764.5000,  393157.5312,  390433.6562,  383595.7812,
          382442.9688,  374132.7812,  370154.6562,  359341.4688,  352188.2812],
        [ 791023.7500,  692100.1250,  681929.1875,  671024.6250,  635557.5625,
          628784.8125,  627672.2500,  616788.9375,  616076.9375,  589226.9375],
        [ 580119.5000,  478591.5625,  478053.7188,  475658.5000,  456294.3125,
          454254.0938,  431605.9688,  421662.5312,  365876.4375,  365628.4375],
        [1060907.0000,  779685.2500,  725869.0000,  716252.9375,  649268.1875,
          622866.0625,  589916.2500,  518459.9062,  492655.2500,  486635.2812],
        [ 724850.0000,  664396.5000,  617863.3125,  574506.5000,  568662.0000,
          560106.2500,  558244.1250,  552921.8125,  549513.8125,  531436.0000],
        [ 883393.8750,  416604.5625,  385087.2188,  311384.4688,  272621.4688,
          263273.9375,  255687.1094,  251266.6250,  248443.0469,  232605.3594],
        [ 693846.8125,  649408.8125,  641546.7500,  581248.6875,  551182.3750,
          538107.7500,  511541.7500,  483009.8750,  441393.5312,  426519.0625],
        [ 587769.4375,  586895.6250,  578574.6875,  520722.9375,  472458.0938,
          422364.4062,  417177.8750,  403929.4688,  396543.5312,  390588.5938],
        [ 934931.0625,  787586.7500,  749014.1250,  614283.4375,  586998.6250,
          565665.9375,  527822.6250,  525781.8750,  523806.0000,  514341.3438],
        [ 767134.8750,  689987.3125,  605807.0625,  602317.7500,  573001.7500,
          568034.3125,  567287.1875,  534685.0000,  524333.2500,  516833.2500],
        [ 647751.1250,  600066.7500,  578989.2500,  569813.3750,  552572.3750,
          550661.6875,  529014.9375,  513346.5312,  494665.5312,  488341.0000],
        [ 798937.3125,  761541.0625,  758378.3125,  757280.4375,  747060.1875,
          740056.3125,  664599.2500,  647179.9375,  624578.5625,  614845.5625],
        [ 711363.8125,  539982.5625,  534568.7500,  483257.3125,  429743.3438,
          408161.1875,  403945.6562,  399296.3125,  394469.0000,  377981.6250],
        [ 298122.2500,  263431.6562,  261413.7812,  226992.7031,  213573.2500,
          212843.7188,  211910.8281,  205758.2031,  204516.7031,  200368.5000],
        [ 455389.6562,  385678.9688,  354100.8750,  339743.3438,  333115.8750,
          332691.0625,  309812.7812,  307649.3438,  306916.7188,  300134.8438],
        [ 554449.4375,  512237.4062,  503060.0000,  497472.8125,  427911.5938,
          404257.4375,  383201.2500,  373662.4688,  360949.2188,  349156.3438],
        [ 441157.4375,  405714.2812,  381413.6562,  332893.5625,  320878.5000,
          292563.0000,  291077.1562,  286381.0938,  236489.2812,  234546.0000],
        [ 761719.7500,  247044.1094,  240172.6719,  204838.5781,  198937.9844,
          176956.6562,  148979.1875,  148493.7969,  134833.0156,  128542.2969],
        [ 452197.5625,  358610.5625,  354740.0312,  324641.0938,  281192.0000,
          270404.2188,  249252.7812,  246202.0938,  225723.5781,  224683.8750],
        [ 377653.3750,  373650.6875,  295917.0938,  253073.1562,  222614.5625,
          207269.1875,  197329.9844,  193283.8438,  173042.5312,  162801.9844],
        [ 695530.2500,  656822.0000,  646909.6875,  644772.4375,  636770.9375,
          632664.5000,  595960.0000,  592099.7500,  583310.5625,  564236.5625],
        [ 325457.3125,  313358.9688,  304225.5000,  272257.2188,  239155.1250,
          226892.9219,  226326.0625,  185188.4844,  163820.7188,  148991.7031],
        [ 419015.6250,  252701.5156,  239836.6719,  235696.0625,  233073.2344,
          228432.0469,  222718.1875,  220686.1250,  206165.1875,  204784.0938],
        [ 347070.1562,  306580.0000,  295470.4062,  243131.5000,  222949.1875,
          208668.2188,  199245.0156,  190211.7656,  169264.3594,  163053.2344],
        [ 346182.2812,  284366.3750,  244221.6094,  242283.6406,  212793.1719,
          206820.7656,  191235.2656,  188310.8438,  177612.3281,  161652.9375],
        [ 227711.2344,  168609.6094,  153071.9688,  146870.3438,  126605.8125,
          126327.0859,  108938.3516,   96741.4688,   81923.2891,   79735.2109],
        [ 154612.3125,  116091.0859,   97150.3906,   81717.7500,   75183.2031,
           70997.6172,   69697.4141,   56793.3789,   53450.5312,   50738.7422],
        [ 432206.1250,  377952.4375,  341573.6875,  341003.4375,  337614.5000,
          327526.0000,  291455.2188,  283662.1562,  283274.4688,  282016.4688],
        [ 117386.5156,   82137.7969,   81830.1250,   75052.0312,   61497.4297,
           56808.5508,   55232.2344,   54615.5234,   42251.0469,   37528.8281],
        [ 183889.1562,  164441.0312,  117476.4453,  115433.5469,  114804.2578,
          104583.6094,  103393.9688,   94865.0625,   91060.1250,   85484.1875],
        [ 384121.1250,  254010.8594,  211215.5312,  195474.3750,  165624.5781,
          164977.6094,  149219.3594,  147265.3125,  134200.0469,  128767.8125],
        [ 328006.1250,  215263.9375,  208432.5312,  201788.6875,  171314.6406,
          147175.7344,  129936.0234,  127946.4375,  114858.0312,  111892.2656],
        [ 360738.6250,  338906.1875,  204456.4531,  145429.1719,  141145.1250,
          139652.2188,  111966.3516,  107842.3438,  107649.6797,  106635.3594],
        [ 609725.7500,  468414.0625,  468258.1875,  388653.0625,  271700.0625,
          242191.4531,  238348.8750,  230233.0938,  195090.5469,  184259.2031],
        [ 493077.3750,  279062.0312,  272429.4062,  235542.8281,  229854.8750,
          216813.5000,  208138.3594,  187246.3906,  183337.0312,  182069.4531],
        [ 610591.0625,  452236.8125,  430788.4688,  425180.4688,  390808.4375,
          390808.4375,  388038.2812,  377907.3750,  376815.1562,  366632.6250],
        [ 440057.7812,  322660.4375,  306468.9062,  271161.6250,  271149.2188,
          266714.3750,  263324.4062,  259168.5312,  256943.5312,  249358.1094],
        [ 498375.9688,  299334.4375,  292963.9375,  270788.2188,  242879.1094,
          229046.3281,  170580.3594,  168804.6094,  161685.4688,  151682.7344],
        [ 708510.1875,  317758.3125,  284469.1562,  263187.8438,  228814.9062,
          206129.4219,  175343.2969,  174213.3594,  173824.7031,  149224.9062],
        [ 483080.3438,  235605.5000,  229457.5781,  202150.9844,  163377.7656,
          162381.6250,  155248.7031,  147739.5000,  147340.0469,  146420.2969],
        [ 404118.3125,  136063.2500,  133612.0000,  117207.5391,  102075.7266,
           98943.7656,   97058.5234,   93271.7578,   87135.0547,   83307.0078],
        [ 309291.7500,  274691.0312,  241602.9531,  231878.2344,  216061.1250,
          215506.1094,  202232.7500,  201205.8594,  198947.2969,  198084.3594],
        [ 210998.1250,   76618.4688,   76344.5156,   70216.3906,   64574.8672,
           40545.6484,   37472.4258,   37360.4883,   36999.9922,   34607.3164],
        [ 327194.4375,  222865.6406,  206456.0000,  136715.8125,  133465.1562,
          126876.1875,  118412.8438,  104595.3828,  103946.0391,  100422.8594],
        [ 170304.2031,  122742.3906,  102150.7109,   82123.9297,   79716.8906,
           73724.8047,   64633.2148,   59165.2461,   56274.2930,   53338.3477]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[321676.0312,      0.0000],
         [187931.0312,      0.0000],
         [185203.3125,      0.0000],
         ...,
         [150688.4688,      0.0000],
         [     0.0000, 139779.2031],
         [136763.6875,      0.0000]],

        [[740561.1250,      0.0000],
         [640950.5000,      0.0000],
         [638235.5625,      0.0000],
         ...,
         [574955.3750,      0.0000],
         [568103.0625,      0.0000],
         [559545.6875,      0.0000]],

        [[434751.8125,      0.0000],
         [402828.9062,      0.0000],
         [330234.0000,      0.0000],
         ...,
         [177719.2188,      0.0000],
         [170464.4219,      0.0000],
         [     0.0000, 146315.8906]],

        ...,

        [[     0.0000, 210998.1250],
         [ 76618.4688,      0.0000],
         [ 76344.5156,      0.0000],
         ...,
         [ 37360.4883,      0.0000],
         [ 36999.9922,      0.0000],
         [ 34607.3164,      0.0000]],

        [[327194.4375,      0.0000],
         [     0.0000, 222865.6406],
         [206456.0000,      0.0000],
         ...,
         [104595.3828,      0.0000],
         [103946.0391,      0.0000],
         [100422.8594,      0.0000]],

        [[     0.0000, 170304.2031],
         [     0.0000, 122742.3906],
         [     0.0000, 102150.7109],
         ...,
         [ 59165.2461,      0.0000],
         [     0.0000,  56274.2930],
         [     0.0000,  53338.3477]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1636159.5000,  139779.2031],
        [6106524.0000,       0.0000],
        [2437847.7500,  146315.8906],
        [3588190.0000,       0.0000],
        [ 505268.4375,  718500.5000],
        [ 848525.3750,  170052.1250],
        [1838513.1250,  428126.6875],
        [2127189.5000,       0.0000],
        [3208780.0000,  323023.1250],
        [3436598.7500,       0.0000],
        [4074272.0000,       0.0000],
        [1787875.7500,       0.0000],
        [3588623.0000,       0.0000],
        [8195319.0000,       0.0000],
        [5678999.0000,       0.0000],
        [4830762.5000,       0.0000],
        [6443073.0000,       0.0000],
        [7316942.5000,       0.0000],
        [4131484.0000,       0.0000],
        [3846054.2500,       0.0000],
        [6550185.5000,       0.0000],
        [4507745.0000,       0.0000],
        [6642515.0000,       0.0000],
        [5902500.0000,       0.0000],
        [3520367.5000,       0.0000],
        [5517805.5000,       0.0000],
        [4777024.5000,       0.0000],
        [6330232.0000,       0.0000],
        [5949422.0000,       0.0000],
        [5525223.0000,       0.0000],
        [7114457.0000,       0.0000],
        [4682769.5000,       0.0000],
        [2298931.5000,       0.0000],
        [2731389.0000,  693844.2500],
        [1991085.5000, 2375272.5000],
        [1163130.6250, 2059983.2500],
        [1240131.8750, 1150386.2500],
        [1571006.6250, 1416641.2500],
        [ 989470.7500, 1467165.7500],
        [6249077.0000,       0.0000],
        [2405674.0000,       0.0000],
        [2463108.7500,       0.0000],
        [2345644.0000,       0.0000],
        [2255479.0000,       0.0000],
        [ 305270.5625, 1011263.7500],
        [ 351046.5000,  475385.8750],
        [2866078.5000,  432206.1250],
        [ 427943.8125,  236396.2969],
        [ 514037.9375,  661393.4375],
        [ 512888.9375, 1421987.7500],
        [1494580.6250,  262033.7656],
        [ 860320.2500,  904101.2500],
        [1764332.1250, 1532542.0000],
        [2257716.2500,  229854.8750],
        [4209807.0000,       0.0000],
        [2907007.0000,       0.0000],
        [1143491.7500, 1342649.2500],
        [1871715.1250,  809761.1250],
        [1124675.2500,  948127.1250],
        [ 649216.3125,  703576.6875],
        [1490012.6250,  799488.8750],
        [ 474740.1250,  210998.1250],
        [1094492.7500,  486457.6250],
        [ 138882.1406,  725291.8750]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 341/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:41, 59.38s/it]  7%|▋         | 2/30 [01:00<11:36, 24.89s/it] 10%|█         | 3/30 [01:00<06:14, 13.87s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.69s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.83s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.45603183110555
Epoch 342/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:41, 59.37s/it]  7%|▋         | 2/30 [01:00<11:43, 25.13s/it] 10%|█         | 3/30 [01:01<06:17, 14.00s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.77s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.438576833407084
Epoch 343/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:57, 57.84s/it]  7%|▋         | 2/30 [01:00<11:46, 25.25s/it] 10%|█         | 3/30 [01:01<06:19, 14.06s/it] 13%|█▎        | 4/30 [01:01<03:48,  8.80s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.90s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.4648765405019124
Epoch 344/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:29, 56.90s/it]  7%|▋         | 2/30 [00:58<11:15, 24.13s/it] 10%|█         | 3/30 [00:58<06:03, 13.46s/it] 13%|█▎        | 4/30 [00:59<03:39,  8.44s/it] 17%|█▋        | 5/30 [01:00<02:21,  5.67s/it] 20%|██        | 6/30 [01:01<01:35,  3.99s/it] 23%|██▎       | 7/30 [01:01<01:07,  2.93s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.24s/it] 30%|███       | 9/30 [01:03<00:37,  1.77s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.45s/it] 37%|███▋      | 11/30 [01:04<00:23,  1.24s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:07<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:10<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:13<00:05,  1.33it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:16<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.64s/it]
Epoch loss is 2.4859104712804156
Epoch 345/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:47, 61.64s/it]  7%|▋         | 2/30 [01:02<12:03, 25.82s/it] 10%|█         | 3/30 [01:03<06:28, 14.37s/it] 13%|█▎        | 4/30 [01:03<03:53,  9.00s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.02s/it] 20%|██        | 6/30 [01:05<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 2.4567232688268024
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0245,  0.0024,  0.0113,  ..., -0.0028,  0.0118,  0.0043],
        [-0.0020,  0.0134,  0.0266,  ...,  0.0155, -0.0004, -0.0164],
        [-0.0302, -0.0380,  0.0237,  ...,  0.0805, -0.0084, -0.0169],
        ...,
        [ 0.0098, -0.0039,  0.0064,  ..., -0.0229, -0.0102, -0.0085],
        [-0.0314,  0.0070, -0.0057,  ...,  0.0035,  0.0154, -0.0121],
        [-0.0357, -0.0125,  0.0172,  ...,  0.0393,  0.0335, -0.0337]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8867, 0.8493, 0.8492, 0.8434, 0.8405, 0.8380, 0.8360, 0.8337, 0.8282,
         0.8274],
        [0.9456, 0.9354, 0.9350, 0.9314, 0.9309, 0.9308, 0.9281, 0.9278, 0.9266,
         0.9257],
        [0.9084, 0.9029, 0.8882, 0.8770, 0.8660, 0.8555, 0.8538, 0.8455, 0.8426,
         0.8313],
        [0.9554, 0.8969, 0.8934, 0.8866, 0.8853, 0.8820, 0.8759, 0.8756, 0.8746,
         0.8714],
        [0.8467, 0.8431, 0.8368, 0.8191, 0.8164, 0.8135, 0.8052, 0.8028, 0.7963,
         0.7874],
        [0.8577, 0.8214, 0.8111, 0.8049, 0.8027, 0.7932, 0.7910, 0.7848, 0.7791,
         0.7749],
        [0.8773, 0.8760, 0.8660, 0.8595, 0.8591, 0.8583, 0.8558, 0.8551, 0.8541,
         0.8524],
        [0.9030, 0.8826, 0.8762, 0.8641, 0.8561, 0.8452, 0.8313, 0.8283, 0.8175,
         0.8162],
        [0.9139, 0.9061, 0.8960, 0.8921, 0.8890, 0.8882, 0.8863, 0.8861, 0.8859,
         0.8855],
        [0.9508, 0.8915, 0.8899, 0.8879, 0.8827, 0.8793, 0.8775, 0.8771, 0.8678,
         0.8666],
        [0.9472, 0.9059, 0.9024, 0.9009, 0.9008, 0.8962, 0.8911, 0.8906, 0.8885,
         0.8880],
        [0.8982, 0.8827, 0.8631, 0.8588, 0.8381, 0.8144, 0.8126, 0.8052, 0.7980,
         0.7895],
        [0.9265, 0.9112, 0.9111, 0.9011, 0.8935, 0.8868, 0.8787, 0.8760, 0.8702,
         0.8602],
        [0.9628, 0.9572, 0.9565, 0.9564, 0.9535, 0.9532, 0.9492, 0.9483, 0.9456,
         0.9415],
        [0.9441, 0.9319, 0.9313, 0.9305, 0.9289, 0.9223, 0.9217, 0.9191, 0.9186,
         0.9172],
        [0.9280, 0.9243, 0.9241, 0.9166, 0.9141, 0.9139, 0.9116, 0.9091, 0.9078,
         0.9048],
        [0.9576, 0.9521, 0.9418, 0.9403, 0.9317, 0.9309, 0.9259, 0.9234, 0.9214,
         0.9207],
        [0.9651, 0.9592, 0.9536, 0.9417, 0.9401, 0.9393, 0.9377, 0.9374, 0.9343,
         0.9329],
        [0.9300, 0.9223, 0.9121, 0.9015, 0.8982, 0.8981, 0.8960, 0.8951, 0.8922,
         0.8864],
        [0.9098, 0.9016, 0.9015, 0.9007, 0.8998, 0.8994, 0.8978, 0.8966, 0.8948,
         0.8935],
        [0.9498, 0.9405, 0.9397, 0.9384, 0.9349, 0.9342, 0.9338, 0.9326, 0.9321,
         0.9297],
        [0.9286, 0.9150, 0.9149, 0.9141, 0.9118, 0.9116, 0.9079, 0.9064, 0.8960,
         0.8954],
        [0.9709, 0.9496, 0.9445, 0.9433, 0.9365, 0.9333, 0.9298, 0.9209, 0.9170,
         0.9163],
        [0.9441, 0.9380, 0.9330, 0.9281, 0.9271, 0.9261, 0.9256, 0.9252, 0.9250,
         0.9225],
        [0.9580, 0.9053, 0.8993, 0.8848, 0.8748, 0.8729, 0.8704, 0.8691, 0.8689,
         0.8638],
        [0.9410, 0.9362, 0.9356, 0.9287, 0.9251, 0.9233, 0.9196, 0.9156, 0.9093,
         0.9071],
        [0.9292, 0.9292, 0.9279, 0.9208, 0.9140, 0.9060, 0.9050, 0.9030, 0.9015,
         0.9008],
        [0.9622, 0.9502, 0.9465, 0.9323, 0.9294, 0.9269, 0.9221, 0.9217, 0.9212,
         0.9201],
        [0.9480, 0.9407, 0.9314, 0.9308, 0.9275, 0.9274, 0.9269, 0.9224, 0.9208,
         0.9206],
        [0.9362, 0.9308, 0.9283, 0.9275, 0.9251, 0.9247, 0.9219, 0.9201, 0.9174,
         0.9164],
        [0.9507, 0.9478, 0.9474, 0.9472, 0.9464, 0.9455, 0.9381, 0.9360, 0.9338,
         0.9321],
        [0.9426, 0.9236, 0.9229, 0.9156, 0.9073, 0.9040, 0.9029, 0.9021, 0.9015,
         0.8987],
        [0.8817, 0.8730, 0.8729, 0.8629, 0.8590, 0.8579, 0.8577, 0.8557, 0.8554,
         0.8540],
        [0.9111, 0.8996, 0.8936, 0.8905, 0.8895, 0.8891, 0.8846, 0.8838, 0.8835,
         0.8822],
        [0.9251, 0.9198, 0.9185, 0.9172, 0.9071, 0.9030, 0.9001, 0.8973, 0.8949,
         0.8928],
        [0.9095, 0.9033, 0.8991, 0.8894, 0.8874, 0.8806, 0.8805, 0.8788, 0.8654,
         0.8652],
        [0.9480, 0.8688, 0.8674, 0.8557, 0.8533, 0.8445, 0.8326, 0.8320, 0.8261,
         0.8216],
        [0.9106, 0.8941, 0.8933, 0.8872, 0.8780, 0.8750, 0.8686, 0.8679, 0.8618,
         0.8617],
        [0.8983, 0.8974, 0.8806, 0.8701, 0.8605, 0.8562, 0.8525, 0.8508, 0.8434,
         0.8393],
        [0.9411, 0.9367, 0.9361, 0.9357, 0.9347, 0.9345, 0.9304, 0.9300, 0.9288,
         0.9264],
        [0.8878, 0.8851, 0.8832, 0.8750, 0.8665, 0.8627, 0.8622, 0.8483, 0.8397,
         0.8330],
        [0.9055, 0.8699, 0.8662, 0.8647, 0.8647, 0.8622, 0.8610, 0.8601, 0.8552,
         0.8550],
        [0.8923, 0.8833, 0.8808, 0.8674, 0.8610, 0.8566, 0.8528, 0.8499, 0.8417,
         0.8397],
        [0.8914, 0.8784, 0.8674, 0.8664, 0.8574, 0.8560, 0.8505, 0.8498, 0.8456,
         0.8383],
        [0.8625, 0.8419, 0.8344, 0.8318, 0.8212, 0.8211, 0.8106, 0.8016, 0.7902,
         0.7883],
        [0.8358, 0.8154, 0.8028, 0.7912, 0.7854, 0.7820, 0.7803, 0.7663, 0.7619,
         0.7587],
        [0.9076, 0.8976, 0.8910, 0.8904, 0.8898, 0.8879, 0.8792, 0.8774, 0.8773,
         0.8771],
        [0.8162, 0.7911, 0.7908, 0.7847, 0.7707, 0.7644, 0.7640, 0.7613, 0.7440,
         0.7357],
        [0.8469, 0.8397, 0.8166, 0.8154, 0.8144, 0.8081, 0.8077, 0.8025, 0.7980,
         0.7945],
        [0.8994, 0.8702, 0.8568, 0.8509, 0.8405, 0.8398, 0.8326, 0.8317, 0.8254,
         0.8220],
        [0.8880, 0.8591, 0.8565, 0.8543, 0.8430, 0.8323, 0.8242, 0.8221, 0.8150,
         0.8129],
        [0.8949, 0.8908, 0.8549, 0.8306, 0.8280, 0.8280, 0.8129, 0.8095, 0.8093,
         0.8089],
        [0.9317, 0.9134, 0.9129, 0.9002, 0.8749, 0.8662, 0.8658, 0.8636, 0.8513,
         0.8482],
        [0.9169, 0.8775, 0.8754, 0.8653, 0.8630, 0.8594, 0.8560, 0.8488, 0.8475,
         0.8475],
        [0.9317, 0.9104, 0.9071, 0.9063, 0.9004, 0.9004, 0.9003, 0.8978, 0.8978,
         0.8961],
        [0.9087, 0.8871, 0.8838, 0.8749, 0.8744, 0.8737, 0.8728, 0.8713, 0.8707,
         0.8692],
        [0.9171, 0.8822, 0.8800, 0.8747, 0.8669, 0.8636, 0.8433, 0.8412, 0.8388,
         0.8340],
        [0.9428, 0.8861, 0.8785, 0.8726, 0.8632, 0.8562, 0.8441, 0.8438, 0.8436,
         0.8326],
        [0.9153, 0.8654, 0.8631, 0.8540, 0.8389, 0.8387, 0.8362, 0.8318, 0.8314,
         0.8313],
        [0.9032, 0.8266, 0.8251, 0.8156, 0.8060, 0.8041, 0.8027, 0.7999, 0.7950,
         0.7915],
        [0.8850, 0.8758, 0.8665, 0.8637, 0.8586, 0.8580, 0.8539, 0.8534, 0.8527,
         0.8520],
        [0.8578, 0.7863, 0.7861, 0.7806, 0.7743, 0.7413, 0.7368, 0.7356, 0.7350,
         0.7297],
        [0.8885, 0.8615, 0.8552, 0.8263, 0.8258, 0.8214, 0.8156, 0.8077, 0.8069,
         0.8043],
        [0.8428, 0.8189, 0.8061, 0.7908, 0.7883, 0.7842, 0.7747, 0.7676, 0.7636,
         0.7610]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 1, 1, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 317218.1562,  185862.4062,  185614.5938,  170803.5312,  163972.8125,
          158082.5781,  153821.3594,  148755.8750,  137429.2969,  135914.7500],
        [ 735992.3750,  635620.0000,  631996.9375,  600442.3125,  596244.8125,
          595206.2500,  573078.8125,  570167.8125,  560680.2500,  553763.5625],
        [ 432712.5938,  399786.6875,  324066.6562,  276023.4688,  235889.9062,
          203017.0938,  198325.7500,  176087.5000,  168830.8438,  143776.2812],
        [ 846358.8750,  367137.5312,  349030.5312,  316479.9688,  310928.0938,
          296653.1562,  271876.8438,  270591.0000,  266785.8750,  254793.5312],
        [ 179186.3906,  170038.7031,  155478.9375,  120812.5938,  116094.7422,
          111496.1250,   99036.0938,   95609.2891,   87126.4141,   76765.0391],
        [ 209547.8750,  124816.1562,  107773.2500,   98529.0781,   95465.4219,
           83456.3438,   80813.6328,   73955.5000,   68156.8594,   64179.5469],
        [ 277492.5625,  272323.6875,  235957.6250,  215032.2812,  213942.2344,
          211493.8906,  203862.4375,  201978.9062,  198993.2031,  194261.7812],
        [ 400578.5938,  299006.3438,  272956.0312,  229771.3750,  204921.2500,
          175222.7812,  143818.3750,  137757.4844,  118092.6719,  115840.9219],
        [ 468020.6562,  418405.4375,  362361.9375,  342433.7500,  327621.5938,
          324153.8125,  315397.0625,  314277.1250,  313565.8438,  311629.8750],
        [ 792812.1250,  339840.5312,  331999.5000,  322662.8750,  299749.2500,
          285200.6875,  278239.3125,  276493.2188,  242086.6250,  237946.4219],
        [ 752986.9375,  417250.6875,  396751.5625,  388710.9062,  387930.9688,
          363228.3125,  337916.0000,  335201.5938,  325271.1250,  323242.5312],
        [ 373808.5938,  299458.3750,  226303.6094,  213036.4375,  158339.6875,
          112975.9922,  110075.3125,   98957.3516,   89266.4219,   79090.4922],
        [ 559639.0625,  450297.1562,  449704.0938,  389718.3125,  349571.5000,
          317537.1875,  283056.0000,  272278.2188,  250397.5781,  217283.7812],
        [ 940685.3125,  868860.8125,  859404.3125,  858391.1250,  823544.7500,
          819988.2500,  774770.2500,  764156.5000,  736056.2500,  693848.8125],
        [ 719616.0625,  605069.1250,  600064.5000,  592946.2500,  579641.6875,
          527269.7500,  522725.6250,  503564.9375,  499937.4062,  490080.7500],
        [ 572011.3750,  542883.9375,  541447.6250,  486471.4375,  469081.5000,
          467777.4688,  452604.0000,  436838.5625,  428699.9375,  410877.4062],
        [ 873656.6875,  807650.5625,  696393.6875,  681625.5000,  602812.5000,
          596356.8750,  555187.0625,  536037.5000,  520633.0625,  515277.5625],
        [ 972528.6875,  893265.0000,  824399.6875,  695516.2500,  679694.3750,
          672677.6875,  656993.0625,  654465.9375,  626335.2500,  613854.8125],
        [ 588627.1250,  527369.8125,  455692.5000,  391760.3438,  373702.0000,
          373493.9062,  362045.5312,  357835.0938,  343098.0000,  315996.5312],
        [ 441303.0312,  392568.5625,  391641.1875,  387332.4688,  382654.5625,
          380286.5938,  371552.0312,  365169.1250,  355855.8125,  349693.8750],
        [ 781433.1250,  684484.5625,  676282.3750,  663585.3125,  631613.7500,
          625044.5000,  622007.6875,  610822.8125,  606237.5625,  585946.5625],
        [ 577205.6875,  475344.1875,  474281.9062,  469083.2812,  454103.3750,
          452605.7188,  429478.2500,  420046.6562,  362170.5625,  359050.3125],
        [1056158.3750,  779379.7500,  724590.8750,  712210.3750,  645934.4375,
          617122.5000,  586933.6875,  516798.7500,  488739.8125,  484118.5000],
        [ 720410.5000,  659975.8750,  614448.6875,  573019.2500,  564587.5000,
          557048.0625,  552888.6250,  550067.5000,  548425.9375,  529061.8750],
        [ 877746.3750,  413688.6562,  379752.7812,  308715.3438,  267544.1250,
          260363.3438,  251098.4688,  246813.8125,  245768.1094,  228541.6562],
        [ 689135.0000,  643610.1250,  637942.2500,  577596.6875,  549150.2500,
          534947.1875,  507776.8438,  479327.4062,  438060.5000,  424745.1875],
        [ 582338.9375,  581813.2500,  571449.7500,  516061.0000,  468301.9688,
          417830.8750,  411951.6875,  400354.7812,  391745.0312,  387658.4062],
        [ 932573.8750,  785273.7500,  745325.9375,  608478.6250,  583608.7500,
          563210.2500,  525760.3750,  522942.5000,  518880.3438,  511128.2188],
        [ 761353.6875,  685472.3125,  600322.0625,  595896.9375,  568294.3750,
          567160.0625,  563105.5625,  528225.0000,  516290.3750,  514692.6250],
        [ 643316.1250,  595567.3750,  574811.7500,  567928.1875,  549019.3750,
          545490.7500,  524465.3125,  511042.9375,  491942.5625,  484439.9688],
        [ 791611.6875,  759440.0000,  754474.1875,  752550.4375,  744203.0000,
          734781.1875,  660544.5000,  641409.1250,  621146.4375,  607041.1875],
        [ 705129.0000,  537006.6250,  531606.3125,  479139.5938,  425624.2812,
          406184.2500,  399631.9375,  395319.3750,  391624.7500,  376251.3750],
        [ 295301.3750,  260687.0625,  260487.0156,  225813.5781,  213438.8750,
          210059.8906,  209552.2656,  203666.7344,  202684.5469,  198741.1562],
        [ 449268.5312,  381061.3438,  349978.1250,  334999.9688,  330315.5625,
          328400.8125,  307594.4688,  304244.9688,  303073.8750,  297376.5938],
        [ 549207.3125,  508785.0938,  499840.6250,  490274.2500,  424260.1875,
          400339.5312,  383900.6562,  369113.3750,  356403.2812,  345959.8438],
        [ 439041.6562,  402050.9375,  378413.7188,  329793.6875,  320131.1562,
          290615.3438,  290308.4062,  283149.6875,  233822.6094,  233243.3438],
        [ 761436.5000,  245669.9219,  240568.5781,  203746.7969,  196791.3750,
          173539.6406,  146524.7812,  145266.3125,  133466.1562,  125207.9219],
        [ 446007.9062,  352276.6250,  348726.0625,  319276.5312,  280233.8750,
          268263.3438,  245054.5000,  242289.4062,  222150.3281,  221804.0000],
        [ 374563.3125,  369336.6250,  290769.7188,  250067.7656,  218237.8125,
          205075.1094,  194452.8750,  189902.3750,  170911.8906,  161088.4688],
        [ 690251.8750,  648217.0625,  642782.6250,  639071.8125,  629900.0000,
          628026.1250,  591853.6250,  588681.0000,  578407.0000,  559431.5000],
        [ 322086.7500,  309817.2188,  301679.8750,  268507.7812,  237571.6094,
          225102.7500,  223438.3281,  183209.2656,  162158.6250,  147324.2969],
        [ 414746.1562,  249469.8906,  236479.5938,  231698.7500,  231595.1406,
          223419.7812,  219572.6562,  216957.4531,  202150.4219,  201725.0000],
        [ 343385.0938,  302173.4062,  291322.9375,  240850.9219,  219724.9531,
          206171.2969,  195441.7500,  187372.6875,  166874.4062,  162119.9531],
        [ 339100.1562,  281844.4062,  240645.8906,  237192.6406,  208808.9688,
          204474.9844,  189010.3594,  187156.5781,  176357.5781,  158847.7188],
        [ 224363.1250,  167260.2969,  150267.2656,  144673.7500,  124467.5234,
          124325.1562,  106941.3828,   93984.2891,   79856.8203,   77786.8281],
        [ 153178.8594,  114522.5703,   95600.5312,   81020.9141,   74647.8672,
           71024.5078,   69391.9219,   56775.0781,   53369.5938,   50912.2188],
        [ 427775.7500,  370461.2188,  337240.9062,  334380.4375,  331574.2500,
          322837.4062,  285128.0938,  277734.7812,  277469.7812,  276659.9062],
        [ 115812.5391,   80883.1016,   80608.2031,   73866.8281,   60482.3047,
           55248.5664,   54918.2500,   52862.1836,   41302.1445,   36680.3125],
        [ 179721.3750,  162052.1094,  116443.6875,  114506.6250,  112905.7656,
          103182.5703,  102551.2891,   95185.6016,   89377.6719,   84978.6875],
        [ 380237.3125,  250724.2188,  207024.0156,  190276.3594,  163933.0781,
          162232.7188,  146466.1094,  144659.5469,  132102.4688,  125870.9531],
        [ 323319.6250,  213668.7969,  205974.7500,  199778.7188,  169950.8281,
          145710.0156,  129816.4922,  126087.2109,  113856.2109,  110519.0000],
        [ 356683.8125,  336398.3438,  201264.3906,  142327.1875,  137113.5469,
          137068.0469,  110496.0234,  105306.6094,  104978.2266,  104387.2109],
        [ 603237.5000,  464596.0000,  461020.0625,  384377.9688,  267792.0000,
          236714.4688,  235350.8438,  228055.4688,  191212.4531,  182942.6562],
        [ 488522.1875,  277976.1875,  269901.5938,  233750.1562,  226214.7031,
          214729.0000,  204454.7031,  184634.0469,  181117.8281,  181083.9688],
        [ 603536.7500,  445070.1562,  424638.2812,  419800.3750,  385850.3750,
          385850.3750,  385342.5312,  371876.4062,  371530.0938,  362864.0625],
        [ 434084.3750,  318923.5312,  304225.8125,  267924.8438,  265919.9375,
          263228.7500,  260102.2500,  254657.2500,  252209.6562,  246892.9062],
        [ 489742.9375,  297545.9375,  288067.8750,  267323.7812,  239071.2031,
          227959.7969,  170556.9375,  165644.0156,  159889.1250,  149306.9062],
        [ 706417.9375,  314262.4688,  282024.5312,  259283.7188,  226580.0312,
          205100.9062,  172579.9219,  171877.0938,  171296.5156,  146398.9375],
        [ 477199.2188,  233970.9531,  226395.7969,  198764.0938,  160263.4531,
          159858.9375,  154088.4375,  144757.3906,  143927.8594,  143792.0469],
        [ 401508.3750,  134409.7344,  131566.7500,  114922.8828,  100170.0625,
           97483.7578,   95554.9609,   91760.0547,   85588.8516,   81368.9766],
        [ 309370.5000,  271441.0625,  237713.4844,  228272.4062,  212283.0000,
          210613.5312,  198391.5781,  197070.0781,  195088.8750,  193279.9688],
        [ 209965.3594,   75576.0703,   75309.9375,   69686.7734,   63660.6094,
           39754.8672,   37278.1289,   36647.6562,   36338.2305,   33673.7461],
        [ 325349.0000,  221113.1562,  202142.9062,  133837.7188,  132853.9375,
          124818.4219,  114932.7500,  102595.4141,  101496.5000,   97749.0781],
        [ 169457.5312,  120326.7969,  100329.1484,   80647.3359,   77776.4375,
           73314.9141,   64015.2969,   57834.3828,   54626.0469,   52616.4375]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[317218.1562,      0.0000],
         [185862.4062,      0.0000],
         [185614.5938,      0.0000],
         ...,
         [148755.8750,      0.0000],
         [     0.0000, 137429.2969],
         [135914.7500,      0.0000]],

        [[735992.3750,      0.0000],
         [635620.0000,      0.0000],
         [631996.9375,      0.0000],
         ...,
         [570167.8125,      0.0000],
         [560680.2500,      0.0000],
         [553763.5625,      0.0000]],

        [[432712.5938,      0.0000],
         [399786.6875,      0.0000],
         [324066.6562,      0.0000],
         ...,
         [176087.5000,      0.0000],
         [168830.8438,      0.0000],
         [     0.0000, 143776.2812]],

        ...,

        [[     0.0000, 209965.3594],
         [ 75576.0703,      0.0000],
         [ 75309.9375,      0.0000],
         ...,
         [ 36647.6562,      0.0000],
         [ 36338.2305,      0.0000],
         [ 33673.7461,      0.0000]],

        [[325349.0000,      0.0000],
         [     0.0000, 221113.1562],
         [202142.9062,      0.0000],
         ...,
         [102595.4141,      0.0000],
         [101496.5000,      0.0000],
         [ 97749.0781,      0.0000]],

        [[     0.0000, 169457.5312],
         [     0.0000, 120326.7969],
         [     0.0000, 100329.1484],
         ...,
         [ 57834.3828,      0.0000],
         [     0.0000,  54626.0469],
         [     0.0000,  52616.4375]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1620046.1250,  137429.2969],
        [6053193.0000,       0.0000],
        [2414740.5000,  143776.2812],
        [3550635.5000,       0.0000],
        [ 499834.8750,  711809.4375],
        [ 840007.7500,  166685.9375],
        [1809417.5000,  415921.1250],
        [2097966.0000,       0.0000],
        [3182470.0000,  315397.0625],
        [3407030.5000,       0.0000],
        [4028490.5000,       0.0000],
        [1761312.2500,       0.0000],
        [3539483.0000,       0.0000],
        [8139706.0000,       0.0000],
        [5640916.0000,       0.0000],
        [4808693.0000,       0.0000],
        [6385631.0000,       0.0000],
        [7289731.0000,       0.0000],
        [4089621.0000,       0.0000],
        [3818057.5000,       0.0000],
        [6487458.0000,       0.0000],
        [4473370.0000,       0.0000],
        [6611987.0000,       0.0000],
        [5869934.0000,       0.0000],
        [3480032.5000,       0.0000],
        [5482291.5000,       0.0000],
        [4729506.0000,       0.0000],
        [6297182.5000,       0.0000],
        [5900813.0000,       0.0000],
        [5488024.0000,       0.0000],
        [7067202.5000,       0.0000],
        [4647517.5000,       0.0000],
        [2280432.5000,       0.0000],
        [2701336.2500,  684978.1250],
        [1971656.6250, 2356427.5000],
        [1152266.5000, 2048304.0000],
        [1224946.5000, 1147271.3750],
        [1548287.6250, 1397794.8750],
        [ 815957.1250, 1608448.8750],
        [6196623.0000,       0.0000],
        [2380896.5000,       0.0000],
        [2427815.0000,       0.0000],
        [2315437.5000,       0.0000],
        [2223439.2500,       0.0000],
        [ 298308.6250,  995617.7500],
        [ 349454.7812,  470989.2500],
        [2813486.7500,  427775.7500],
        [ 419645.0625,  233019.3906],
        [ 508489.2500,  652416.1875],
        [ 506108.2500, 1397418.5000],
        [1479115.5000,  259566.2188],
        [ 841676.8750,  894346.5000],
        [1743047.7500, 1512251.7500],
        [2236169.7500,  226214.7031],
        [4156359.2500,       0.0000],
        [2868169.2500,       0.0000],
        [1129474.0000, 1325634.5000],
        [1855695.8750,  800126.2500],
        [1107114.2500,  935903.9375],
        [ 639050.4375,  695284.0000],
        [1460430.0000,  793094.5625],
        [ 467926.0312,  209965.3594],
        [1077119.5000,  479769.2812],
        [ 135610.8125,  715333.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 346/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:10, 58.28s/it]  7%|▋         | 2/30 [00:59<11:24, 24.44s/it] 10%|█         | 3/30 [01:00<06:11, 13.76s/it] 13%|█▎        | 4/30 [01:00<03:44,  8.63s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.79s/it] 20%|██        | 6/30 [01:02<01:37,  4.07s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 2.402681493759155
Epoch 347/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:14, 56.35s/it]  7%|▋         | 2/30 [01:01<12:11, 26.11s/it] 10%|█         | 3/30 [01:02<06:32, 14.53s/it] 13%|█▎        | 4/30 [01:02<03:56,  9.09s/it] 17%|█▋        | 5/30 [01:03<02:32,  6.08s/it] 20%|██        | 6/30 [01:04<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.36s/it] 30%|███       | 9/30 [01:06<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.4329183022181193
Epoch 348/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:00, 60.02s/it]  7%|▋         | 2/30 [01:00<11:44, 25.15s/it] 10%|█         | 3/30 [01:01<06:18, 14.01s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.77s/it] 17%|█▋        | 5/30 [01:03<02:26,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.458974011739095
Epoch 349/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<28:00, 57.94s/it]  7%|▋         | 2/30 [01:00<11:43, 25.13s/it] 10%|█         | 3/30 [01:00<06:17, 14.00s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.77s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.4355053345362347
Epoch 350/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<30:57, 64.04s/it]  7%|▋         | 2/30 [01:04<12:32, 26.88s/it] 10%|█         | 3/30 [01:05<06:43, 14.95s/it] 13%|█▎        | 4/30 [01:06<04:02,  9.34s/it] 17%|█▋        | 5/30 [01:07<02:36,  6.24s/it] 20%|██        | 6/30 [01:07<01:44,  4.37s/it] 23%|██▎       | 7/30 [01:08<01:13,  3.19s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.41s/it] 30%|███       | 9/30 [01:10<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.87s/it]
Epoch loss is 2.429253896077474
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 2.4378e-02,  2.3916e-03,  1.1483e-02,  ..., -3.0325e-03,
          1.2376e-02,  4.2047e-03],
        [-2.0686e-03,  1.2985e-02,  2.6815e-02,  ...,  1.5698e-02,
          5.2868e-05, -1.6261e-02],
        [-3.0043e-02, -3.8113e-02,  2.3985e-02,  ...,  8.0582e-02,
         -7.9451e-03, -1.6537e-02],
        ...,
        [ 9.3782e-03, -3.7680e-03,  6.8442e-03,  ..., -2.2937e-02,
         -9.6567e-03, -8.0127e-03],
        [-3.1525e-02,  7.2614e-03, -5.4746e-03,  ...,  3.7230e-03,
          1.5597e-02, -1.1982e-02],
        [-3.5511e-02, -1.2687e-02,  1.7417e-02,  ...,  3.9587e-02,
          3.3791e-02, -3.3114e-02]], device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8858, 0.8495, 0.8484, 0.8430, 0.8394, 0.8370, 0.8358, 0.8327, 0.8272,
         0.8270],
        [0.9453, 0.9349, 0.9343, 0.9307, 0.9305, 0.9300, 0.9280, 0.9272, 0.9258,
         0.9251],
        [0.9081, 0.9025, 0.8870, 0.8765, 0.8651, 0.8548, 0.8534, 0.8449, 0.8420,
         0.8307],
        [0.9555, 0.8959, 0.8924, 0.8855, 0.8847, 0.8818, 0.8746, 0.8746, 0.8737,
         0.8701],
        [0.8464, 0.8419, 0.8361, 0.8188, 0.8152, 0.8129, 0.8050, 0.8022, 0.7956,
         0.7860],
        [0.8572, 0.8212, 0.8111, 0.8030, 0.8023, 0.7920, 0.7913, 0.7832, 0.7790,
         0.7731],
        [0.8761, 0.8747, 0.8649, 0.8585, 0.8578, 0.8572, 0.8549, 0.8534, 0.8533,
         0.8509],
        [0.9021, 0.8816, 0.8752, 0.8634, 0.8554, 0.8437, 0.8308, 0.8273, 0.8161,
         0.8152],
        [0.9136, 0.9057, 0.8952, 0.8913, 0.8885, 0.8877, 0.8858, 0.8854, 0.8848,
         0.8847],
        [0.9506, 0.8909, 0.8893, 0.8871, 0.8819, 0.8786, 0.8766, 0.8762, 0.8672,
         0.8658],
        [0.9469, 0.9056, 0.9012, 0.9001, 0.8999, 0.8953, 0.8899, 0.8898, 0.8876,
         0.8874],
        [0.8972, 0.8819, 0.8626, 0.8581, 0.8369, 0.8129, 0.8110, 0.8046, 0.7966,
         0.7886],
        [0.9252, 0.9106, 0.9104, 0.9001, 0.8924, 0.8859, 0.8777, 0.8749, 0.8694,
         0.8595],
        [0.9623, 0.9568, 0.9564, 0.9560, 0.9531, 0.9526, 0.9487, 0.9477, 0.9451,
         0.9407],
        [0.9438, 0.9315, 0.9308, 0.9300, 0.9283, 0.9218, 0.9213, 0.9186, 0.9178,
         0.9168],
        [0.9278, 0.9241, 0.9238, 0.9164, 0.9138, 0.9136, 0.9113, 0.9088, 0.9074,
         0.9046],
        [0.9571, 0.9516, 0.9409, 0.9398, 0.9312, 0.9304, 0.9251, 0.9230, 0.9207,
         0.9199],
        [0.9649, 0.9589, 0.9534, 0.9415, 0.9400, 0.9389, 0.9373, 0.9370, 0.9339,
         0.9329],
        [0.9296, 0.9214, 0.9111, 0.9011, 0.8978, 0.8977, 0.8952, 0.8947, 0.8916,
         0.8852],
        [0.9092, 0.9011, 0.9011, 0.9001, 0.8997, 0.8990, 0.8973, 0.8957, 0.8941,
         0.8931],
        [0.9490, 0.9398, 0.9391, 0.9376, 0.9345, 0.9339, 0.9331, 0.9319, 0.9310,
         0.9293],
        [0.9282, 0.9146, 0.9143, 0.9131, 0.9118, 0.9109, 0.9077, 0.9061, 0.8952,
         0.8940],
        [0.9706, 0.9496, 0.9444, 0.9429, 0.9361, 0.9327, 0.9294, 0.9206, 0.9164,
         0.9160],
        [0.9437, 0.9376, 0.9327, 0.9280, 0.9266, 0.9258, 0.9250, 0.9250, 0.9249,
         0.9223],
        [0.9576, 0.9048, 0.8983, 0.8843, 0.8736, 0.8721, 0.8692, 0.8680, 0.8679,
         0.8627],
        [0.9405, 0.9356, 0.9352, 0.9282, 0.9248, 0.9229, 0.9192, 0.9151, 0.9087,
         0.9069],
        [0.9287, 0.9284, 0.9271, 0.9202, 0.9134, 0.9053, 0.9042, 0.9024, 0.9007,
         0.9003],
        [0.9620, 0.9499, 0.9462, 0.9316, 0.9290, 0.9266, 0.9218, 0.9213, 0.9205,
         0.9197],
        [0.9475, 0.9402, 0.9308, 0.9301, 0.9273, 0.9270, 0.9264, 0.9216, 0.9203,
         0.9198],
        [0.9357, 0.9303, 0.9279, 0.9272, 0.9247, 0.9240, 0.9214, 0.9198, 0.9170,
         0.9158],
        [0.9501, 0.9476, 0.9471, 0.9467, 0.9462, 0.9451, 0.9377, 0.9354, 0.9334,
         0.9314],
        [0.9420, 0.9233, 0.9225, 0.9150, 0.9067, 0.9038, 0.9022, 0.9014, 0.9010,
         0.8983],
        [0.8809, 0.8726, 0.8721, 0.8625, 0.8589, 0.8569, 0.8568, 0.8549, 0.8547,
         0.8533],
        [0.9102, 0.8987, 0.8927, 0.8895, 0.8891, 0.8882, 0.8841, 0.8830, 0.8828,
         0.8817],
        [0.9246, 0.9193, 0.9182, 0.9163, 0.9064, 0.9024, 0.9002, 0.8966, 0.8941,
         0.8922],
        [0.9092, 0.9027, 0.8985, 0.8887, 0.8872, 0.8804, 0.8802, 0.8779, 0.8648,
         0.8647],
        [0.9479, 0.8684, 0.8674, 0.8554, 0.8526, 0.8431, 0.8315, 0.8305, 0.8254,
         0.8209],
        [0.9096, 0.8928, 0.8921, 0.8861, 0.8778, 0.8745, 0.8675, 0.8668, 0.8610,
         0.8610],
        [0.8978, 0.8967, 0.8795, 0.8693, 0.8592, 0.8555, 0.8516, 0.8496, 0.8427,
         0.8389],
        [0.9406, 0.9358, 0.9357, 0.9351, 0.9341, 0.9340, 0.9299, 0.9296, 0.9282,
         0.9258],
        [0.8870, 0.8843, 0.8826, 0.8740, 0.8660, 0.8622, 0.8613, 0.8475, 0.8390,
         0.8322],
        [0.9048, 0.8691, 0.8652, 0.8644, 0.8634, 0.8607, 0.8601, 0.8590, 0.8542,
         0.8539],
        [0.8915, 0.8823, 0.8797, 0.8668, 0.8600, 0.8556, 0.8516, 0.8488, 0.8407,
         0.8393],
        [0.8900, 0.8779, 0.8664, 0.8650, 0.8561, 0.8552, 0.8497, 0.8493, 0.8451,
         0.8370],
        [0.8614, 0.8414, 0.8330, 0.8307, 0.8202, 0.8201, 0.8093, 0.7997, 0.7885,
         0.7865],
        [0.8351, 0.8142, 0.8017, 0.7906, 0.7849, 0.7820, 0.7801, 0.7662, 0.7618,
         0.7587],
        [0.9069, 0.8963, 0.8899, 0.8891, 0.8886, 0.8868, 0.8778, 0.8761, 0.8758,
         0.8758],
        [0.8154, 0.7903, 0.7896, 0.7836, 0.7697, 0.7644, 0.7617, 0.7590, 0.7427,
         0.7343],
        [0.8456, 0.8388, 0.8158, 0.8149, 0.8133, 0.8072, 0.8071, 0.8025, 0.7968,
         0.7941],
        [0.8986, 0.8694, 0.8555, 0.8491, 0.8397, 0.8386, 0.8313, 0.8306, 0.8244,
         0.8204],
        [0.8870, 0.8585, 0.8557, 0.8537, 0.8425, 0.8316, 0.8240, 0.8213, 0.8145,
         0.8122],
        [0.8942, 0.8904, 0.8539, 0.8293, 0.8266, 0.8261, 0.8121, 0.8084, 0.8081,
         0.8071],
        [0.9310, 0.9128, 0.9118, 0.8994, 0.8738, 0.8651, 0.8647, 0.8629, 0.8500,
         0.8475],
        [0.9163, 0.8773, 0.8748, 0.8648, 0.8620, 0.8588, 0.8547, 0.8478, 0.8470,
         0.8466],
        [0.9310, 0.9094, 0.9062, 0.9054, 0.8999, 0.8997, 0.8997, 0.8970, 0.8967,
         0.8955],
        [0.9078, 0.8864, 0.8834, 0.8742, 0.8731, 0.8730, 0.8721, 0.8702, 0.8695,
         0.8685],
        [0.9159, 0.8818, 0.8787, 0.8738, 0.8658, 0.8632, 0.8432, 0.8399, 0.8379,
         0.8329],
        [0.9425, 0.8852, 0.8778, 0.8715, 0.8624, 0.8557, 0.8430, 0.8430, 0.8424,
         0.8313],
        [0.9144, 0.8649, 0.8621, 0.8529, 0.8380, 0.8372, 0.8356, 0.8310, 0.8297,
         0.8296],
        [0.9028, 0.8258, 0.8241, 0.8142, 0.8047, 0.8030, 0.8016, 0.7988, 0.7939,
         0.7899],
        [0.8848, 0.8749, 0.8655, 0.8627, 0.8576, 0.8564, 0.8526, 0.8521, 0.8515,
         0.8505],
        [0.8576, 0.7854, 0.7852, 0.7803, 0.7735, 0.7402, 0.7369, 0.7343, 0.7340,
         0.7290],
        [0.8881, 0.8609, 0.8538, 0.8253, 0.8249, 0.8203, 0.8136, 0.8063, 0.8054,
         0.8026],
        [0.8424, 0.8176, 0.8050, 0.7897, 0.7866, 0.7835, 0.7740, 0.7660, 0.7616,
         0.7600]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 313142.3438,  186361.3438,  183437.4375,  169865.7500,  161369.9844,
          155998.9375,  153272.6875,  146698.5781,  135560.3125,  135158.0938],
        [ 732833.6250,  631027.9375,  625740.5625,  594811.3125,  592962.1250,
          588459.8750,  572537.5000,  565251.2500,  554246.4375,  548826.1875],
        [ 430596.2188,  397215.3438,  318597.9375,  274005.8125,  232994.7812,
          201079.0625,  196983.8281,  174520.8281,  167518.2656,  142601.3594],
        [ 847873.6875,  361741.5000,  343865.1562,  311956.6562,  308019.8125,
          295587.9375,  266832.4375,  266667.0625,  263247.5625,  250327.6094],
        [ 178228.9375,  167216.4375,  153928.6406,  120151.1172,  114192.8828,
          110567.2812,   98732.5312,   94902.8984,   86287.1172,   75252.2812],
        [ 208219.5625,  124332.9922,  107656.2422,   96003.1719,   94962.1875,
           82032.5000,   81197.1250,   72344.5000,   68049.3047,   62626.3867],
        [ 272765.2812,  267115.8125,  232198.2031,  212015.3281,  209966.7500,
          208150.2812,  201371.5312,  197200.1875,  196831.5469,  190283.9844],
        [ 395041.6250,  294958.5938,  268985.0000,  227418.9062,  202941.0156,
          171679.0312,  142739.4688,  135811.3594,  115601.7656,  114215.9766],
        [ 465646.8438,  416008.2188,  358106.1562,  338718.4375,  325485.5312,
          321779.7500,  313163.2812,  311530.9062,  308505.5000,  308192.0000],
        [ 790411.4375,  336843.6250,  329197.5938,  319142.2812,  296104.2500,
          282600.4062,  274530.2188,  273051.5938,  240020.8594,  235296.2969],
        [ 749254.8750,  415568.5000,  389907.5000,  384139.7812,  382746.5312,
          358810.0312,  332065.3438,  331529.3438,  321032.4375,  320240.7812],
        [ 368580.4375,  296094.9062,  224686.6562,  210836.7969,  155698.8438,
          110530.6953,  107567.5859,   98141.4844,   87611.0156,   78087.0234],
        [ 549627.5625,  446522.8438,  445041.2812,  383892.2188,  343823.5000,
          313617.2500,  278884.5625,  267892.4062,  247732.0625,  215083.1562],
        [ 933817.2500,  863156.3750,  858082.5000,  853067.7500,  818846.5625,
          813562.1875,  769340.1250,  757885.1875,  730369.9375,  686245.4375],
        [ 716870.6875,  601771.7500,  595807.1250,  588788.8125,  574682.9375,
          523691.5938,  519566.1562,  500037.0625,  494544.3125,  487512.7188],
        [ 570201.5000,  541487.8750,  538864.2500,  484628.5000,  467403.3438,
          465707.2188,  450742.7188,  434732.3438,  426474.2812,  409680.1250],
        [ 867493.0000,  801201.1250,  687604.0625,  676832.7500,  598668.0625,
          592183.3750,  549037.6875,  532438.9375,  515579.8750,  509591.6562],
        [ 969179.8125,  889488.9375,  822224.0000,  693444.0000,  679482.4375,
          668607.5000,  653399.5000,  650670.3750,  622695.0625,  613364.3750],
        [ 585168.1250,  520863.4688,  449536.8438,  389752.1250,  371518.7188,
          370909.1250,  358285.4688,  355757.4062,  339989.3438,  310361.3438],
        [ 437561.5625,  389738.3750,  389538.4375,  384262.9062,  381637.7500,
          377946.3125,  368829.4062,  360533.2812,  352390.1875,  347462.6250],
        [ 772293.8125,  677660.8125,  670455.3125,  655821.1250,  627714.7500,
          622458.1250,  615647.0000,  605157.4375,  596969.0625,  582853.4375],
        [ 574131.3125,  472564.8750,  470140.6562,  462285.3750,  454149.2812,
          448412.4375,  427969.1562,  418227.9062,  358344.9375,  351874.0312],
        [1051311.6250,  778666.5000,  722887.4375,  708156.8125,  642551.0000,
          611976.7500,  583782.4375,  514727.9688,  484841.1250,  481737.9062],
        [ 716441.5000,  656174.6250,  611504.7500,  571741.4375,  560548.7500,
          554146.5000,  547976.3125,  547854.0625,  547374.1250,  527089.2500],
        [ 872700.7500,  410707.7812,  374118.1562,  306399.9375,  262922.6562,
          257494.4844,  246921.3906,  242960.4219,  242525.2188,  225096.7344],
        [ 684472.8125,  637830.9375,  634254.5625,  573920.5625,  546742.8125,
          531824.3750,  504695.2812,  475685.2500,  434040.5312,  423241.4062],
        [ 577912.3750,  575602.1875,  565194.6875,  511602.7500,  464515.8125,
          413398.0000,  406974.9062,  396831.4062,  387162.9688,  385382.9688],
        [ 929800.5000,  782783.8750,  741572.4375,  602322.3125,  580058.1250,
          561049.3125,  523519.8438,  519737.1250,  514021.5938,  508188.6562],
        [ 756425.8750,  681144.6875,  595868.4375,  589921.9375,  566368.7500,
          563685.3125,  558881.1875,  522198.9375,  512465.0938,  508823.9062],
        [ 639088.2500,  591513.3750,  571286.8125,  565468.5625,  545919.6250,
          540414.3125,  520705.0312,  508642.9375,  489138.4688,  480834.1562],
        [ 784834.2500,  756811.9375,  751769.3125,  747493.4375,  741614.1250,
          730376.2500,  656796.9375,  636128.1875,  617895.7500,  600232.1875],
        [ 699278.0625,  534709.5000,  529048.8125,  475214.5938,  422030.6250,
          404845.0312,  395962.3438,  391334.2812,  388937.4688,  374577.6250],
        [ 292015.8125,  259161.8438,  257525.9219,  224405.0625,  213084.9844,
          207114.0781,  207031.7188,  201501.7812,  200778.0156,  196796.2500],
        [ 443588.7188,  376323.8750,  345748.0625,  330225.4688,  328113.4375,
          324058.6250,  305433.6875,  300868.5000,  299990.0312,  295164.8438],
        [ 544706.3125,  505606.2500,  497109.5625,  483805.1250,  420481.1250,
          396615.3750,  384457.9062,  365293.1562,  352530.6875,  342922.0000],
        [ 437493.1250,  398437.7812,  375596.7500,  326386.3438,  319626.5625,
          289724.5312,  288881.1250,  279885.3125,  231923.3594,  231516.2969],
        [ 760762.1875,  244165.0156,  240649.5625,  202664.4531,  194905.8906,
          170140.5625,  144059.1562,  142124.6875,  132161.8281,  123906.1484],
        [ 439721.7500,  346231.4688,  342676.8125,  314323.0000,  279336.8125,
          266314.5625,  241227.0000,  238834.9062,  219586.2656,  219566.3906],
        [ 371878.9062,  365675.8438,  286284.4375,  247464.0781,  214063.8750,
          203078.4844,  191958.0938,  186798.0000,  169045.2812,  160105.0312],
        [ 685050.8125,  639415.6250,  639038.8750,  633566.0000,  624095.1250,
          623305.8125,  587591.7500,  585072.1250,  573519.5000,  554502.8125],
        [ 318728.0000,  306422.1562,  298899.4375,  264714.7812,  236004.6562,
          223379.7344,  220711.6094,  181261.7656,  160478.7969,  145623.7500],
        [ 410916.5938,  246504.2344,  233427.5938,  230559.3750,  227436.2500,
          218720.3750,  216849.6875,  213586.8906,  199233.5938,  198402.3594],
        [ 339842.5000,  297669.4062,  286908.6875,  238614.7500,  216704.9688,
          203447.1875,  192175.3281,  184437.6562,  164483.8594,  161160.3750],
        [ 332317.8438,  279491.8750,  237224.7656,  232469.2031,  204875.9062,
          202299.6875,  186840.7500,  185774.1562,  175140.0781,  156015.0000],
        [ 221079.6250,  166119.8750,  147196.6406,  142550.9219,  122724.3672,
          122455.5938,  105024.0859,   91461.7812,   78015.7969,   75828.4609],
        [ 151855.4062,  112618.8594,   94172.7109,   80338.1328,   74126.1641,
           71065.3594,   69135.8359,   56749.2578,   53276.7344,   50982.8203],
        [ 423413.7812,  363722.2188,  332147.0625,  328202.6250,  326010.8438,
          317597.4375,  279302.4375,  272541.9375,  271397.0625,  271379.9688],
        [ 114515.6875,   80031.7969,   79235.6719,   72713.8594,   59576.5352,
           55265.3750,   53144.4414,   51199.9766,   40532.9688,   35935.4648],
        [ 176287.2812,  160092.8281,  115212.2734,  113755.8203,  111156.2812,
          101878.7812,  101767.2109,   95316.3203,   87751.6562,   84534.9375],
        [ 376141.2188,  247857.5469,  203159.4531,  185371.5312,  162008.9844,
          159636.0469,  143775.1719,  142236.2812,  130174.7734,  122967.4609],
        [ 318660.8438,  211924.3594,  203737.0625,  197970.5000,  168709.4844,
          144367.3594,  129543.4219,  124592.2266,  113102.0156,  109338.1406],
        [ 352833.0625,  334233.1250,  198384.9531,  139604.0156,  134486.0156,
          133451.2812,  109184.9531,  103608.1562,  103143.9141,  101700.2656],
        [ 596969.0625,  460789.7188,  454025.0000,  380256.1562,  263884.2500,
          232863.9375,  231630.0312,  225843.7188,  187619.7969,  181282.5000],
        [ 484421.9375,  277137.6250,  267439.5312,  231900.1250,  222790.2188,
          212874.1562,  200690.3438,  182010.6094,  179964.4062,  178787.1406],
        [ 596958.2500,  438467.5938,  419088.7500,  414323.1562,  382886.0000,
          381670.5312,  381670.5312,  367394.2812,  365682.8125,  359751.2500],
        [ 428749.0000,  315732.0312,  302526.6250,  265162.0000,  261000.7500,
          260596.5938,  257624.9062,  250398.5312,  247940.3125,  244625.3438],
        [ 481304.8750,  295718.7500,  283009.3125,  263958.7812,  235446.9062,
          226583.2812,  170289.9062,  162610.8281,  157947.2656,  146981.3281],
        [ 704258.0000,  310403.3750,  279238.2812,  255309.9062,  224127.0156,
          203806.8438,  169861.2188,  169813.4375,  168460.6094,  143778.3281],
        [ 470973.9688,  232237.2031,  223329.6719,  195794.7344,  158205.6562,
          156458.1250,  152855.9219,  143052.6250,  140530.7812,  140257.7812],
        [ 398962.4688,  132778.8281,  129687.9219,  112633.4688,   98270.8281,
           95965.9141,   94049.2031,   90288.4375,   84229.1328,   79566.6562],
        [ 308840.7812,  267883.4375,  234426.1406,  225248.5625,  209155.9531,
          205847.1094,  194758.3594,  193597.6406,  191912.7031,  188992.3438],
        [ 209313.5938,   74570.5938,   74345.6328,   69358.5156,   62904.0039,
           39100.3203,   37300.5391,   35979.6016,   35802.1289,   33312.9336],
        [ 323330.4062,  219267.7812,  198103.0781,  131968.8750,  131145.9688,
          122914.2422,  111574.3047,  100629.8516,   99284.6094,   95342.8672],
        [ 168357.6562,  118210.9922,   98666.0781,   79298.3359,   75915.6562,
           72642.3359,   63361.6992,   56538.0938,   53104.1680,   51883.6055]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[313142.3438,      0.0000],
         [186361.3438,      0.0000],
         [183437.4375,      0.0000],
         ...,
         [146698.5781,      0.0000],
         [     0.0000, 135560.3125],
         [135158.0938,      0.0000]],

        [[732833.6250,      0.0000],
         [631027.9375,      0.0000],
         [625740.5625,      0.0000],
         ...,
         [565251.2500,      0.0000],
         [554246.4375,      0.0000],
         [548826.1875,      0.0000]],

        [[430596.2188,      0.0000],
         [397215.3438,      0.0000],
         [318597.9375,      0.0000],
         ...,
         [174520.8281,      0.0000],
         [167518.2656,      0.0000],
         [142601.3594,      0.0000]],

        ...,

        [[     0.0000, 209313.5938],
         [ 74570.5938,      0.0000],
         [ 74345.6328,      0.0000],
         ...,
         [ 35979.6016,      0.0000],
         [ 35802.1289,      0.0000],
         [ 33312.9336,      0.0000]],

        [[323330.4062,      0.0000],
         [     0.0000, 219267.7812],
         [198103.0781,      0.0000],
         ...,
         [100629.8516,      0.0000],
         [ 99284.6094,      0.0000],
         [ 95342.8672,      0.0000]],

        [[     0.0000, 168357.6562],
         [     0.0000, 118210.9922],
         [     0.0000,  98666.0781],
         ...,
         [ 56538.0938,      0.0000],
         [     0.0000,  53104.1680],
         [     0.0000,  51883.6055]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1605305.0000,  135560.3125],
        [6006696.5000,       0.0000],
        [2536113.5000,       0.0000],
        [3516119.2500,       0.0000],
        [ 494615.9375,  704844.1875],
        [ 833371.5000,  164052.4688],
        [1782917.1250,  404981.8125],
        [2069392.7500,       0.0000],
        [3158631.0000,  308505.5000],
        [3377198.5000,       0.0000],
        [3985295.0000,       0.0000],
        [1737835.5000,       0.0000],
        [3492117.0000,       0.0000],
        [8084373.5000,       0.0000],
        [5603273.0000,       0.0000],
        [4789922.5000,       0.0000],
        [6330631.0000,       0.0000],
        [7262556.0000,       0.0000],
        [4052142.0000,       0.0000],
        [3789900.7500,       0.0000],
        [6427030.5000,       0.0000],
        [4438100.0000,       0.0000],
        [6580640.0000,       0.0000],
        [5840851.5000,       0.0000],
        [3441847.5000,       0.0000],
        [5446708.0000,       0.0000],
        [4684578.0000,       0.0000],
        [6263053.5000,       0.0000],
        [5855784.0000,       0.0000],
        [5453011.5000,       0.0000],
        [7023952.5000,       0.0000],
        [4615938.5000,       0.0000],
        [2259415.5000,       0.0000],
        [2673541.7500,  675973.5000],
        [1954470.6250, 2339056.7500],
        [1141762.7500, 2037708.5000],
        [1212003.0000, 1143536.5000],
        [1528205.8750, 1379613.0000],
        [ 806385.8125, 1589966.1250],
        [6145158.5000,       0.0000],
        [2356224.5000,       0.0000],
        [2395637.0000,       0.0000],
        [2285444.7500,       0.0000],
        [2192449.2500,       0.0000],
        [ 292201.9375,  980255.1875],
        [ 347942.2188,  466379.0625],
        [2762301.5000,  423413.7812],
        [ 412138.7500,  230013.0469],
        [ 503021.8750,  644731.5000],
        [ 499108.6875, 1374219.7500],
        [1464476.0000,  257469.3750],
        [ 825178.6250,  885451.1250],
        [1722004.5000, 1493159.6250],
        [2215226.0000,  222790.2188],
        [4107893.2500,       0.0000],
        [2834356.0000,       0.0000],
        [1115682.5000, 1308168.7500],
        [1839216.7500,  789840.2500],
        [1090355.8750,  923340.6875],
        [ 629304.3750,  687128.5000],
        [1434782.7500,  785880.1875],
        [ 462674.2500,  209313.5938],
        [1060234.0000,  473328.0000],
        [ 132453.7500,  705524.8125]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 351/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:19, 60.68s/it]  7%|▋         | 2/30 [01:01<11:51, 25.43s/it] 10%|█         | 3/30 [01:02<06:23, 14.22s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.90s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.96s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.4324368238449097
Epoch 352/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:21, 62.82s/it]  7%|▋         | 2/30 [01:03<12:16, 26.31s/it] 10%|█         | 3/30 [01:04<06:35, 14.64s/it] 13%|█▎        | 4/30 [01:05<03:58,  9.15s/it] 17%|█▋        | 5/30 [01:05<02:33,  6.12s/it] 20%|██        | 6/30 [01:06<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.38s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 2.429881048202515
Epoch 353/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<31:10, 64.51s/it]  7%|▋         | 2/30 [01:07<13:07, 28.13s/it] 10%|█         | 3/30 [01:08<07:03, 15.69s/it] 13%|█▎        | 4/30 [01:08<04:14,  9.79s/it] 17%|█▋        | 5/30 [01:09<02:43,  6.53s/it] 20%|██        | 6/30 [01:10<01:49,  4.56s/it] 23%|██▎       | 7/30 [01:11<01:16,  3.32s/it] 27%|██▋       | 8/30 [01:11<00:54,  2.50s/it] 30%|███       | 9/30 [01:12<00:40,  1.95s/it] 33%|███▎      | 10/30 [01:13<00:31,  1.58s/it] 37%|███▋      | 11/30 [01:14<00:25,  1.32s/it] 40%|████      | 12/30 [01:14<00:20,  1.15s/it] 43%|████▎     | 13/30 [01:15<00:17,  1.03s/it] 47%|████▋     | 14/30 [01:16<00:15,  1.06it/s] 50%|█████     | 15/30 [01:17<00:13,  1.13it/s] 53%|█████▎    | 16/30 [01:17<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:18<00:10,  1.23it/s] 60%|██████    | 18/30 [01:19<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:20<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:20<00:07,  1.30it/s] 70%|███████   | 21/30 [01:21<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:22<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:23<00:05,  1.32it/s] 80%|████████  | 24/30 [01:23<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:24<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:25<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:25<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:26<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:27<00:00,  1.34it/s]100%|██████████| 30/30 [01:28<00:00,  1.34it/s]100%|██████████| 30/30 [01:28<00:00,  2.95s/it]
Epoch loss is 2.395653986930847
Epoch 354/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:17, 58.54s/it]  7%|▋         | 2/30 [00:59<11:27, 24.55s/it] 10%|█         | 3/30 [01:02<06:35, 14.63s/it] 13%|█▎        | 4/30 [01:02<03:57,  9.15s/it] 17%|█▋        | 5/30 [01:03<02:33,  6.12s/it] 20%|██        | 6/30 [01:04<01:43,  4.29s/it] 23%|██▎       | 7/30 [01:05<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:05<00:52,  2.37s/it] 30%|███       | 9/30 [01:06<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.45009495417277
Epoch 355/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:24, 58.79s/it]  7%|▋         | 2/30 [01:00<11:44, 25.16s/it] 10%|█         | 3/30 [01:01<06:18, 14.01s/it] 13%|█▎        | 4/30 [01:01<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.451216705640157
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0243,  0.0023,  0.0116,  ..., -0.0033,  0.0128,  0.0041],
        [-0.0021,  0.0126,  0.0270,  ...,  0.0158,  0.0004, -0.0161],
        [-0.0299, -0.0382,  0.0242,  ...,  0.0807, -0.0076, -0.0163],
        ...,
        [ 0.0090, -0.0036,  0.0072,  ..., -0.0230, -0.0092, -0.0076],
        [-0.0317,  0.0075, -0.0053,  ...,  0.0039,  0.0158, -0.0119],
        [-0.0353, -0.0128,  0.0176,  ...,  0.0397,  0.0342, -0.0327]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8849, 0.8497, 0.8476, 0.8425, 0.8383, 0.8362, 0.8356, 0.8319, 0.8265,
         0.8263],
        [0.9450, 0.9343, 0.9336, 0.9305, 0.9297, 0.9291, 0.9279, 0.9266, 0.9250,
         0.9244],
        [0.9079, 0.9020, 0.8860, 0.8760, 0.8643, 0.8543, 0.8528, 0.8443, 0.8415,
         0.8306],
        [0.9557, 0.8949, 0.8914, 0.8846, 0.8841, 0.8815, 0.8737, 0.8733, 0.8727,
         0.8690],
        [0.8460, 0.8407, 0.8356, 0.8184, 0.8143, 0.8123, 0.8048, 0.8017, 0.7950,
         0.7849],
        [0.8569, 0.8209, 0.8108, 0.8018, 0.8014, 0.7916, 0.7911, 0.7819, 0.7788,
         0.7715],
        [0.8751, 0.8735, 0.8638, 0.8575, 0.8573, 0.8552, 0.8540, 0.8528, 0.8515,
         0.8494],
        [0.9011, 0.8806, 0.8742, 0.8628, 0.8549, 0.8423, 0.8304, 0.8264, 0.8146,
         0.8141],
        [0.9131, 0.9053, 0.8944, 0.8905, 0.8880, 0.8871, 0.8856, 0.8848, 0.8840,
         0.8836],
        [0.9504, 0.8903, 0.8887, 0.8864, 0.8811, 0.8780, 0.8757, 0.8753, 0.8666,
         0.8652],
        [0.9466, 0.9053, 0.9000, 0.8995, 0.8988, 0.8946, 0.8892, 0.8885, 0.8872,
         0.8863],
        [0.8963, 0.8811, 0.8621, 0.8573, 0.8358, 0.8115, 0.8095, 0.8039, 0.7954,
         0.7877],
        [0.9241, 0.9102, 0.9097, 0.8992, 0.8912, 0.8851, 0.8769, 0.8739, 0.8688,
         0.8589],
        [0.9618, 0.9563, 0.9562, 0.9556, 0.9527, 0.9521, 0.9482, 0.9472, 0.9446,
         0.9401],
        [0.9436, 0.9312, 0.9304, 0.9295, 0.9277, 0.9215, 0.9209, 0.9181, 0.9171,
         0.9165],
        [0.9275, 0.9239, 0.9235, 0.9161, 0.9136, 0.9133, 0.9111, 0.9085, 0.9072,
         0.9045],
        [0.9567, 0.9510, 0.9401, 0.9393, 0.9307, 0.9300, 0.9244, 0.9226, 0.9200,
         0.9192],
        [0.9646, 0.9586, 0.9532, 0.9413, 0.9400, 0.9385, 0.9369, 0.9366, 0.9336,
         0.9328],
        [0.9292, 0.9207, 0.9102, 0.9008, 0.8973, 0.8972, 0.8946, 0.8944, 0.8909,
         0.8846],
        [0.9087, 0.9009, 0.9007, 0.8996, 0.8995, 0.8986, 0.8968, 0.8948, 0.8935,
         0.8927],
        [0.9482, 0.9392, 0.9385, 0.9368, 0.9341, 0.9336, 0.9324, 0.9314, 0.9299,
         0.9289],
        [0.9279, 0.9142, 0.9137, 0.9122, 0.9118, 0.9103, 0.9075, 0.9057, 0.8946,
         0.8929],
        [0.9703, 0.9495, 0.9441, 0.9425, 0.9359, 0.9321, 0.9291, 0.9203, 0.9159,
         0.9156],
        [0.9434, 0.9372, 0.9324, 0.9279, 0.9261, 0.9254, 0.9248, 0.9247, 0.9244,
         0.9224],
        [0.9572, 0.9043, 0.8973, 0.8839, 0.8724, 0.8714, 0.8680, 0.8674, 0.8668,
         0.8616],
        [0.9401, 0.9350, 0.9349, 0.9278, 0.9246, 0.9225, 0.9188, 0.9146, 0.9081,
         0.9066],
        [0.9282, 0.9278, 0.9264, 0.9196, 0.9129, 0.9046, 0.9034, 0.9018, 0.9000,
         0.9000],
        [0.9618, 0.9498, 0.9458, 0.9310, 0.9285, 0.9264, 0.9215, 0.9209, 0.9199,
         0.9193],
        [0.9471, 0.9398, 0.9303, 0.9295, 0.9272, 0.9264, 0.9258, 0.9208, 0.9201,
         0.9189],
        [0.9353, 0.9299, 0.9275, 0.9269, 0.9244, 0.9234, 0.9209, 0.9195, 0.9167,
         0.9153],
        [0.9495, 0.9473, 0.9469, 0.9463, 0.9460, 0.9447, 0.9373, 0.9349, 0.9331,
         0.9308],
        [0.9416, 0.9231, 0.9223, 0.9146, 0.9062, 0.9036, 0.9018, 0.9009, 0.9005,
         0.8982],
        [0.8802, 0.8722, 0.8714, 0.8621, 0.8587, 0.8560, 0.8560, 0.8543, 0.8541,
         0.8526],
        [0.9094, 0.8979, 0.8921, 0.8886, 0.8886, 0.8873, 0.8836, 0.8823, 0.8821,
         0.8812],
        [0.9240, 0.9189, 0.9178, 0.9154, 0.9059, 0.9018, 0.9004, 0.8959, 0.8934,
         0.8917],
        [0.9090, 0.9021, 0.8981, 0.8881, 0.8871, 0.8802, 0.8797, 0.8772, 0.8644,
         0.8640],
        [0.9479, 0.8680, 0.8674, 0.8550, 0.8520, 0.8419, 0.8304, 0.8291, 0.8248,
         0.8202],
        [0.9087, 0.8918, 0.8910, 0.8850, 0.8776, 0.8741, 0.8666, 0.8659, 0.8604,
         0.8602],
        [0.8974, 0.8959, 0.8785, 0.8686, 0.8580, 0.8548, 0.8507, 0.8486, 0.8419,
         0.8384],
        [0.9401, 0.9353, 0.9349, 0.9346, 0.9335, 0.9335, 0.9294, 0.9291, 0.9277,
         0.9253],
        [0.8863, 0.8835, 0.8820, 0.8732, 0.8655, 0.8616, 0.8605, 0.8468, 0.8384,
         0.8313],
        [0.9043, 0.8682, 0.8644, 0.8641, 0.8623, 0.8593, 0.8592, 0.8580, 0.8534,
         0.8526],
        [0.8909, 0.8814, 0.8787, 0.8662, 0.8592, 0.8548, 0.8506, 0.8478, 0.8398,
         0.8389],
        [0.8886, 0.8772, 0.8654, 0.8637, 0.8549, 0.8546, 0.8489, 0.8488, 0.8447,
         0.8363],
        [0.8604, 0.8410, 0.8316, 0.8298, 0.8192, 0.8191, 0.8082, 0.7980, 0.7870,
         0.7849],
        [0.8346, 0.8133, 0.8006, 0.7901, 0.7845, 0.7820, 0.7798, 0.7661, 0.7617,
         0.7587],
        [0.9062, 0.8949, 0.8889, 0.8877, 0.8873, 0.8858, 0.8763, 0.8747, 0.8744,
         0.8743],
        [0.8146, 0.7898, 0.7885, 0.7827, 0.7687, 0.7644, 0.7596, 0.7570, 0.7414,
         0.7329],
        [0.8442, 0.8381, 0.8153, 0.8144, 0.8124, 0.8067, 0.8061, 0.8027, 0.7955,
         0.7939],
        [0.8979, 0.8688, 0.8544, 0.8474, 0.8389, 0.8376, 0.8302, 0.8296, 0.8235,
         0.8190],
        [0.8860, 0.8580, 0.8549, 0.8531, 0.8419, 0.8311, 0.8239, 0.8204, 0.8141,
         0.8114],
        [0.8935, 0.8899, 0.8529, 0.8280, 0.8253, 0.8243, 0.8112, 0.8075, 0.8068,
         0.8056],
        [0.9303, 0.9124, 0.9108, 0.8988, 0.8729, 0.8644, 0.8633, 0.8623, 0.8486,
         0.8470],
        [0.9159, 0.8770, 0.8742, 0.8644, 0.8611, 0.8582, 0.8534, 0.8469, 0.8465,
         0.8457],
        [0.9303, 0.9085, 0.9053, 0.9045, 0.8995, 0.8990, 0.8990, 0.8962, 0.8957,
         0.8950],
        [0.9070, 0.8858, 0.8830, 0.8735, 0.8723, 0.8718, 0.8714, 0.8691, 0.8683,
         0.8679],
        [0.9148, 0.8813, 0.8777, 0.8730, 0.8648, 0.8627, 0.8430, 0.8386, 0.8371,
         0.8318],
        [0.9423, 0.8844, 0.8771, 0.8705, 0.8617, 0.8553, 0.8422, 0.8419, 0.8413,
         0.8301],
        [0.9135, 0.8643, 0.8613, 0.8520, 0.8372, 0.8359, 0.8351, 0.8303, 0.8282,
         0.8280],
        [0.9024, 0.8250, 0.8231, 0.8130, 0.8035, 0.8020, 0.8005, 0.7978, 0.7929,
         0.7884],
        [0.8847, 0.8740, 0.8647, 0.8619, 0.8565, 0.8549, 0.8513, 0.8510, 0.8505,
         0.8489],
        [0.8574, 0.7844, 0.7843, 0.7798, 0.7726, 0.7390, 0.7368, 0.7331, 0.7330,
         0.7286],
        [0.8876, 0.8603, 0.8525, 0.8250, 0.8237, 0.8195, 0.8116, 0.8051, 0.8040,
         0.8009],
        [0.8421, 0.8166, 0.8040, 0.7887, 0.7850, 0.7830, 0.7733, 0.7645, 0.7599,
         0.7590]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 309224.1875,  186901.3438,  181385.0625,  168744.4062,  158774.5781,
          154058.6094,  152861.7500,  144868.1406,  134167.4219,  133879.8438],
        [ 729626.4375,  626160.2500,  619685.5625,  593267.5000,  585977.8750,
          581393.3750,  571370.6875,  560628.3750,  548253.9375,  543622.2500],
        [ 429073.4062,  394874.3750,  314025.1875,  272104.3438,  230157.5781,
          199619.8750,  195318.9844,  173046.6562,  166276.9531,  142315.7969],
        [ 849304.5000,  356697.0938,  338949.8125,  307628.5000,  305468.0625,
          294599.3125,  263527.1562,  262076.0312,  259831.0156,  246305.4375],
        [ 177411.0469,  164453.8906,  152921.3906,  119538.2812,  112738.3438,
          109504.7812,   98404.6562,   94148.0156,   85581.3359,   74042.7891],
        [ 207090.3750,  123883.2266,  107298.6328,   94324.7891,   93818.7969,
           81462.9219,   80895.9922,   70982.1172,   67927.3516,   61146.8320],
        [ 268603.3125,  262471.9688,  228677.0469,  208973.5156,  208519.2344,
          202221.1875,  198874.6250,  195524.1719,  191947.4844,  186257.9219],
        [ 389355.3438,  290661.0625,  265219.9062,  225460.8906,  201430.1094,
          168131.2500,  141896.4844,  133936.9375,  113218.1406,  112490.7969],
        [ 462307.8438,  413479.6250,  354103.5625,  334769.0625,  323181.1562,
          318746.5312,  312425.2500,  308709.1562,  305214.4375,  303610.7812],
        [ 788090.8750,  333975.3438,  326575.6250,  315777.8125,  292719.0000,
          280047.9062,  271021.5000,  269606.5000,  238085.5625,  233144.5938],
        [ 745976.6250,  413872.5625,  383595.7812,  380866.9688,  377154.5312,
          354961.0000,  328704.7500,  325645.0938,  319595.5000,  315484.6250],
        [ 363821.1250,  292804.4375,  223189.5781,  208343.3125,  153314.3438,
          108366.2734,  105323.9922,   97187.0938,   86038.1406,   77120.2734],
        [ 541211.6875,  443574.3438,  440219.4062,  378951.8125,  338445.2812,
          309859.4688,  275677.5312,  264047.4062,  245428.2500,  213095.7656],
        [ 927245.8750,  857114.1250,  856583.0000,  848458.5000,  813923.8125,
          807090.0625,  763638.5000,  752805.2500,  724773.3125,  679927.7500],
        [ 714931.0625,  599067.3125,  592303.0625,  584633.1875,  570044.9375,
          521523.5625,  516929.8750,  496976.8125,  489543.0938,  485268.5625],
        [ 568229.3750,  539349.0000,  536502.8750,  482608.3750,  465704.1250,
          463907.0625,  449178.1562,  433191.9375,  424971.6875,  408872.9375],
        [ 861749.8750,  795023.7500,  679617.8750,  672392.9375,  594629.8125,
          588393.6875,  543638.3125,  529378.8750,  510685.3438,  504782.8438],
        [ 965516.4375,  886317.0000,  819709.1875,  691421.3125,  678734.4375,
          664942.8750,  650032.8125,  646915.2500,  619390.1875,  613082.5000],
        [ 581823.2500,  515593.1562,  443648.3750,  387903.2500,  369171.1250,
          368672.2188,  355251.5625,  353947.5938,  336950.6250,  307615.3125],
        [ 434436.4062,  388327.0625,  387180.3125,  381485.3125,  380687.2188,
          376073.0625,  366555.0312,  355873.4375,  349524.8125,  345336.4688],
        [ 763178.4375,  671019.5625,  664588.5000,  648429.8125,  623849.3125,
          619780.7500,  608944.1875,  600262.5625,  588108.1250,  579823.6250],
        [ 571380.0000,  469942.5000,  466321.4375,  456794.1875,  453977.8125,
          444364.8438,  426853.5312,  416229.6875,  355000.9375,  346585.6250],
        [1046741.6875,  777921.3125,  720442.8125,  704059.2500,  640048.9375,
          606884.3125,  581277.5000,  512687.5312,  481528.9062,  479522.1875],
        [ 712711.7500,  652940.4375,  609271.7500,  570943.6875,  556830.8125,
          551293.2500,  546820.0000,  546022.1875,  543823.4375,  527993.8125],
        [ 867897.6250,  407830.4375,  369035.9375,  304686.3125,  258665.0469,
          254907.2656,  242753.8438,  240745.5156,  238685.0625,  221732.5156],
        [ 679973.1875,  632746.5625,  631108.5625,  570583.9375,  544897.0000,
          529001.8125,  501639.5312,  472159.4688,  430707.5312,  421510.5625],
        [ 573839.5625,  570206.3750,  558981.9375,  507435.0625,  461080.2812,
          409577.3438,  402548.9375,  393601.6875,  383333.5625,  383322.2188],
        [ 927199.8125,  780651.0000,  738184.1875,  596878.5625,  576320.6875,
          559079.0000,  521749.4062,  516892.9375,  509730.2188,  505422.0938],
        [ 751492.6250,  676837.3125,  591221.2500,  584728.5625,  565718.3125,
          559559.5625,  554527.6875,  516441.0625,  510809.5625,  502051.1250],
        [ 635351.5000,  588075.5625,  568221.2500,  563502.0625,  543455.3125,
          536135.1250,  517176.4375,  506685.5625,  486839.9375,  477513.3438],
        [ 778132.7500,  754201.5000,  749242.0000,  742748.0625,  739524.3750,
          726461.7500,  653506.1250,  631696.3125,  615237.3125,  595765.0625],
        [ 694640.6250,  533240.8750,  527285.3125,  472146.8750,  419252.6562,
          403887.5000,  393357.4375,  388517.0938,  386489.7500,  373604.0312],
        [ 289138.8125,  257881.0625,  254970.2344,  223259.8281,  212574.9375,
          204607.0312,  204525.1094,  199644.2500,  199080.7031,  194835.4531],
        [ 438405.3125,  372010.5000,  342414.8438,  325971.9688,  325948.3438,
          319729.0000,  303465.1562,  297776.7500,  297050.6250,  293099.4688],
        [ 540399.3750,  502329.8438,  494510.8125,  478162.2500,  417495.0625,
          393382.5625,  385526.7188,  361673.8438,  348875.4375,  340414.0312],
        [ 435925.0938,  395068.0000,  373115.8438,  323371.0938,  318773.3125,
          288979.7500,  287114.5312,  277072.0625,  230767.4844,  229267.9375],
        [ 760262.5000,  242691.5625,  240713.8438,  201590.9531,  193299.6875,
          167140.0781,  141848.4531,  139279.2812,  130929.7734,  122611.4766],
        [ 433968.9062,  341181.3750,  337110.6562,  309387.3438,  278616.0938,
          264934.7500,  237833.8906,  235457.4688,  217885.7500,  217244.0000],
        [ 369529.6875,  361713.5312,  282135.3750,  244826.2812,  210525.9688,
          201172.6562,  189707.2500,  183972.6406,  167208.1562,  159178.0156],
        [ 680359.7500,  635186.1250,  631084.5000,  628702.1250,  618792.6875,
          618720.0625,  583726.1875,  581454.8750,  569419.5625,  550369.2500],
        [ 315506.2812,  303023.2812,  296622.5938,  261595.3281,  234378.0938,
          221564.4531,  218241.5625,  179264.1719,  158972.5938,  143790.5312],
        [ 407993.4375,  243636.5469,  230745.9062,  229764.8125,  223766.5000,
          214546.4062,  214269.7500,  210468.7500,  197025.5469,  194784.7344],
        [ 336723.5000,  294102.4375,  283006.0625,  236729.6094,  214166.7812,
          201136.4062,  189438.0625,  182002.6250,  162270.7812,  160225.0938],
        [ 325981.0000,  276995.7188,  234086.7812,  228226.0469,  201317.9375,
          200495.7969,  184920.0469,  184568.9219,  173997.3438,  154346.8438],
        [ 217883.8906,  165098.0156,  144248.7344,  140651.3125,  120944.9375,
          120729.5625,  103314.3281,   89261.3125,   76341.8984,   74038.1328],
        [ 150705.5625,  111170.2734,   92711.4531,   79824.3828,   73614.9219,
           71064.4141,   68888.6328,   56656.7852,   53163.0469,   50928.8789],
        [ 419147.0938,  356510.0312,  327369.2188,  321555.1562,  319925.7500,
          312912.8125,  273387.7188,  267043.2188,  266043.4688,  265655.8125],
        [ 113225.7969,   79392.2422,   77949.7578,   71746.4688,   58801.4727,
           55288.1992,   51608.0938,   49746.0703,   39807.2578,   35265.0547],
        [ 172937.4375,  158302.8438,  114363.7656,  112935.9141,  109693.4531,
          101213.9609,  100299.8750,   95535.7344,   86191.4688,   84293.6641],
        [ 372216.6562,  245430.8281,  199873.4219,  180964.1562,  160229.0781,
          157265.4688,  141460.3281,  140190.6406,  128590.7344,  120600.0938],
        [ 314133.6250,  210483.4219,  201452.7812,  196317.3281,  167196.9844,
          143246.6250,  129389.3359,  122973.9141,  112355.4922,  108110.0703],
        [ 349285.5938,  331877.0000,  195728.6406,  137120.2188,  131907.3438,
          129979.3906,  107903.1406,  102339.8672,  101251.6172,   99573.7266],
        [ 591300.8125,  457876.2500,  447608.7812,  376882.3438,  260415.9688,
          230571.2500,  227074.0938,  223847.6250,  184138.5312,  179845.5156],
        [ 481201.1562,  276046.8750,  265390.9375,  230472.7656,  219915.7344,
          211106.0000,  197046.7812,  179713.5000,  178713.8438,  176604.1406],
        [ 590865.0625,  432810.7812,  413856.3750,  409042.6250,  380681.7500,
          377937.6562,  377937.6562,  363265.3750,  360524.0000,  357037.0938],
        [ 424047.0312,  313030.4062,  300906.6562,  262805.5938,  258202.1875,
          256467.1406,  254976.3281,  246739.9062,  243821.5625,  242503.2344],
        [ 474022.3750,  293757.6562,  278867.8125,  260934.3125,  232105.9062,
          225120.5625,  169899.9375,  159615.2031,  156243.5469,  144671.8125],
        [ 701346.4375,  306954.4688,  276515.3750,  251735.5312,  221766.9844,
          202614.2031,  167849.7812,  167218.5156,  165792.8906,  141336.5312],
        [ 465301.9062,  230259.0156,  220651.6250,  193254.9062,  156289.0156,
          153440.7188,  151715.0000,  141657.4375,  137495.2500,  137208.5156],
        [ 396620.3125,  131359.0000,  127866.6641,  110607.7812,   96622.8047,
           94618.0469,   92560.5625,   89035.1641,   82997.5781,   77898.4766],
        [ 308012.1875,  264520.2188,  231474.3438,  222502.9219,  206040.7656,
          201218.1406,  191397.2812,  190477.5312,  189024.2500,  184868.4062],
        [ 208694.5000,   73586.5000,   73417.6328,   68857.1641,   62134.7539,
           38427.3711,   37235.0352,   35362.4180,   35292.7773,   33130.9180],
        [ 321462.8750,  217478.2344,  194601.4844,  131447.8594,  128877.2812,
          121454.2109,  108499.8828,   98847.6562,   97336.5156,   93097.3984],
        [ 167591.7812,  116461.4609,   97249.2969,   78162.3516,   74224.9141,
           72056.1953,   62739.2461,   55362.4375,   51795.6562,   51165.8086]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[309224.1875,      0.0000],
         [186901.3438,      0.0000],
         [181385.0625,      0.0000],
         ...,
         [144868.1406,      0.0000],
         [134167.4219,      0.0000],
         [     0.0000, 133879.8438]],

        [[729626.4375,      0.0000],
         [626160.2500,      0.0000],
         [619685.5625,      0.0000],
         ...,
         [560628.3750,      0.0000],
         [548253.9375,      0.0000],
         [543622.2500,      0.0000]],

        [[429073.4062,      0.0000],
         [394874.3750,      0.0000],
         [314025.1875,      0.0000],
         ...,
         [173046.6562,      0.0000],
         [166276.9531,      0.0000],
         [142315.7969,      0.0000]],

        ...,

        [[     0.0000, 208694.5000],
         [ 73586.5000,      0.0000],
         [ 73417.6328,      0.0000],
         ...,
         [ 35362.4180,      0.0000],
         [ 35292.7773,      0.0000],
         [ 33130.9180,      0.0000]],

        [[321462.8750,      0.0000],
         [     0.0000, 217478.2344],
         [194601.4844,      0.0000],
         ...,
         [ 98847.6562,      0.0000],
         [ 97336.5156,      0.0000],
         [ 93097.3984,      0.0000]],

        [[     0.0000, 167591.7812],
         [     0.0000, 116461.4609],
         [     0.0000,  97249.2969],
         ...,
         [ 55362.4375,      0.0000],
         [     0.0000,  51795.6562],
         [     0.0000,  51165.8086]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1590985.5000,  133879.8438],
        [5959986.0000,       0.0000],
        [2516813.2500,       0.0000],
        [3484386.7500,       0.0000],
        [ 490305.4062,  698439.1250],
        [ 827084.8750,  161746.1562],
        [1757901.7500,  394168.6875],
        [2041801.0000,       0.0000],
        [3436547.5000,       0.0000],
        [3349044.7500,       0.0000],
        [3945857.5000,       0.0000],
        [1715508.6250,       0.0000],
        [3450511.0000,       0.0000],
        [8031560.0000,       0.0000],
        [5571221.5000,       0.0000],
        [4772515.5000,       0.0000],
        [6280294.0000,       0.0000],
        [7236062.0000,       0.0000],
        [4020576.5000,       0.0000],
        [3765479.2500,       0.0000],
        [6367985.0000,       0.0000],
        [4407450.5000,       0.0000],
        [6551114.0000,       0.0000],
        [5818651.0000,       0.0000],
        [3406939.5000,       0.0000],
        [5414328.5000,       0.0000],
        [4643927.0000,       0.0000],
        [6232108.0000,       0.0000],
        [5813387.0000,       0.0000],
        [5422956.5000,       0.0000],
        [6986515.5000,       0.0000],
        [4592422.0000,       0.0000],
        [2240517.5000,       0.0000],
        [2647485.2500,  668386.8125],
        [1938856.6250, 2323913.2500],
        [1132175.5000, 2027279.6250],
        [1200112.0000, 1140255.6250],
        [1510466.5000, 1363153.7500],
        [ 797179.7500, 1572789.8750],
        [6097815.0000,       0.0000],
        [2332959.0000,       0.0000],
        [2367002.5000,       0.0000],
        [2259801.5000,       0.0000],
        [2164936.5000,       0.0000],
        [ 286548.1562,  965963.9375],
        [ 346555.3750,  462172.9688],
        [2710403.5000,  419147.0938],
        [ 405525.9375,  227304.4688],
        [ 498084.6562,  637683.4375],
        [ 492816.7500, 1354004.7500],
        [1450057.5000,  255602.1250],
        [ 810075.3750,  876891.2500],
        [1703018.6250, 1476542.5000],
        [2196296.0000,  219915.7344],
        [4063958.2500,       0.0000],
        [2803500.0000,       0.0000],
        [1102656.6250, 1292582.5000],
        [1822673.6250,  780457.0000],
        [1075158.2500,  912115.0625],
        [ 620441.8125,  679744.6250],
        [1410962.8750,  778573.1250],
        [ 457444.5625,  208694.5000],
        [1045293.6250,  467809.7188],
        [ 129587.3516,  697221.8125]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 356/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:21, 56.60s/it]  7%|▋         | 2/30 [01:01<12:15, 26.26s/it] 10%|█         | 3/30 [01:02<06:34, 14.61s/it] 13%|█▎        | 4/30 [01:03<03:57,  9.14s/it] 17%|█▋        | 5/30 [01:03<02:32,  6.11s/it] 20%|██        | 6/30 [01:04<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:05<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.37s/it] 30%|███       | 9/30 [01:06<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.4125995079676312
Epoch 357/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:47, 59.56s/it]  7%|▋         | 2/30 [01:00<11:39, 24.97s/it] 10%|█         | 3/30 [01:01<06:15, 13.91s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.452325789133708
Epoch 358/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:55<27:02, 55.94s/it]  7%|▋         | 2/30 [00:56<11:00, 23.60s/it] 10%|█         | 3/30 [01:00<06:35, 14.66s/it] 13%|█▎        | 4/30 [01:01<03:58,  9.17s/it] 17%|█▋        | 5/30 [01:02<02:33,  6.13s/it] 20%|██        | 6/30 [01:03<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:03<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:04<00:52,  2.38s/it] 30%|███       | 9/30 [01:05<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.29s/it] 40%|████      | 12/30 [01:07<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.4259849309921266
Epoch 359/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:02, 62.14s/it]  7%|▋         | 2/30 [01:04<12:42, 27.24s/it] 10%|█         | 3/30 [01:05<06:48, 15.15s/it] 13%|█▎        | 4/30 [01:06<04:05,  9.46s/it] 17%|█▋        | 5/30 [01:07<02:37,  6.32s/it] 20%|██        | 6/30 [01:07<01:46,  4.42s/it] 23%|██▎       | 7/30 [01:08<01:14,  3.22s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.43s/it] 30%|███       | 9/30 [01:10<00:40,  1.91s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.55s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.87s/it]
Epoch loss is 2.420325684547424
Epoch 360/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:18, 60.65s/it]  7%|▋         | 2/30 [01:02<12:03, 25.83s/it] 10%|█         | 3/30 [01:02<06:28, 14.37s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.99s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.02s/it] 20%|██        | 6/30 [01:05<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.4091114600499473
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0242,  0.0023,  0.0117,  ..., -0.0035,  0.0132,  0.0040],
        [-0.0021,  0.0122,  0.0272,  ...,  0.0160,  0.0007, -0.0161],
        [-0.0297, -0.0382,  0.0244,  ...,  0.0809, -0.0073, -0.0161],
        ...,
        [ 0.0086, -0.0035,  0.0075,  ..., -0.0232, -0.0087, -0.0072],
        [-0.0319,  0.0077, -0.0051,  ...,  0.0040,  0.0159, -0.0118],
        [-0.0351, -0.0129,  0.0178,  ...,  0.0398,  0.0344, -0.0323]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8840, 0.8499, 0.8467, 0.8420, 0.8372, 0.8354, 0.8352, 0.8309, 0.8259,
         0.8254],
        [0.9447, 0.9339, 0.9330, 0.9304, 0.9290, 0.9284, 0.9278, 0.9260, 0.9243,
         0.9238],
        [0.9076, 0.9016, 0.8850, 0.8755, 0.8635, 0.8538, 0.8522, 0.8438, 0.8410,
         0.8305],
        [0.9558, 0.8940, 0.8905, 0.8836, 0.8836, 0.8814, 0.8729, 0.8721, 0.8718,
         0.8679],
        [0.8457, 0.8397, 0.8352, 0.8179, 0.8131, 0.8116, 0.8044, 0.8012, 0.7944,
         0.7837],
        [0.8565, 0.8206, 0.8106, 0.8013, 0.7999, 0.7918, 0.7902, 0.7806, 0.7787,
         0.7699],
        [0.8740, 0.8722, 0.8628, 0.8569, 0.8565, 0.8534, 0.8532, 0.8522, 0.8498,
         0.8484],
        [0.9001, 0.8797, 0.8734, 0.8623, 0.8543, 0.8409, 0.8301, 0.8253, 0.8133,
         0.8133],
        [0.9125, 0.9049, 0.8938, 0.8897, 0.8875, 0.8864, 0.8854, 0.8842, 0.8833,
         0.8832],
        [0.9502, 0.8897, 0.8882, 0.8858, 0.8803, 0.8774, 0.8748, 0.8745, 0.8661,
         0.8645],
        [0.9463, 0.9051, 0.8989, 0.8989, 0.8979, 0.8938, 0.8885, 0.8874, 0.8869,
         0.8853],
        [0.8955, 0.8804, 0.8616, 0.8565, 0.8348, 0.8101, 0.8081, 0.8033, 0.7941,
         0.7870],
        [0.9230, 0.9098, 0.9089, 0.8983, 0.8901, 0.8843, 0.8760, 0.8728, 0.8682,
         0.8582],
        [0.9613, 0.9562, 0.9558, 0.9552, 0.9522, 0.9515, 0.9478, 0.9467, 0.9440,
         0.9396],
        [0.9435, 0.9309, 0.9301, 0.9291, 0.9273, 0.9213, 0.9206, 0.9177, 0.9165,
         0.9162],
        [0.9274, 0.9237, 0.9232, 0.9158, 0.9134, 0.9130, 0.9108, 0.9082, 0.9069,
         0.9043],
        [0.9563, 0.9506, 0.9393, 0.9389, 0.9303, 0.9295, 0.9238, 0.9222, 0.9194,
         0.9186],
        [0.9644, 0.9584, 0.9530, 0.9411, 0.9399, 0.9382, 0.9366, 0.9362, 0.9332,
         0.9328],
        [0.9288, 0.9200, 0.9093, 0.9005, 0.8970, 0.8968, 0.8941, 0.8940, 0.8904,
         0.8846],
        [0.9082, 0.9006, 0.9003, 0.8993, 0.8991, 0.8983, 0.8964, 0.8939, 0.8929,
         0.8922],
        [0.9474, 0.9385, 0.9379, 0.9360, 0.9336, 0.9333, 0.9317, 0.9308, 0.9288,
         0.9285],
        [0.9277, 0.9139, 0.9131, 0.9117, 0.9114, 0.9098, 0.9073, 0.9054, 0.8941,
         0.8922],
        [0.9700, 0.9494, 0.9439, 0.9422, 0.9356, 0.9316, 0.9288, 0.9201, 0.9155,
         0.9153],
        [0.9431, 0.9369, 0.9322, 0.9277, 0.9257, 0.9251, 0.9248, 0.9245, 0.9240,
         0.9225],
        [0.9568, 0.9039, 0.8965, 0.8836, 0.8713, 0.8708, 0.8670, 0.8669, 0.8659,
         0.8609],
        [0.9397, 0.9345, 0.9345, 0.9274, 0.9244, 0.9221, 0.9183, 0.9141, 0.9077,
         0.9063],
        [0.9277, 0.9271, 0.9257, 0.9191, 0.9124, 0.9040, 0.9027, 0.9013, 0.8995,
         0.8993],
        [0.9616, 0.9496, 0.9455, 0.9304, 0.9281, 0.9261, 0.9213, 0.9206, 0.9193,
         0.9190],
        [0.9467, 0.9394, 0.9298, 0.9289, 0.9272, 0.9260, 0.9254, 0.9201, 0.9198,
         0.9180],
        [0.9350, 0.9296, 0.9272, 0.9267, 0.9242, 0.9230, 0.9205, 0.9193, 0.9164,
         0.9149],
        [0.9490, 0.9471, 0.9466, 0.9458, 0.9457, 0.9443, 0.9370, 0.9345, 0.9328,
         0.9306],
        [0.9411, 0.9229, 0.9220, 0.9141, 0.9058, 0.9034, 0.9013, 0.9004, 0.9001,
         0.8980],
        [0.8797, 0.8720, 0.8709, 0.8618, 0.8586, 0.8553, 0.8552, 0.8538, 0.8536,
         0.8520],
        [0.9086, 0.8972, 0.8914, 0.8882, 0.8878, 0.8865, 0.8832, 0.8817, 0.8815,
         0.8808],
        [0.9235, 0.9185, 0.9174, 0.9147, 0.9054, 0.9013, 0.9004, 0.8952, 0.8928,
         0.8911],
        [0.9087, 0.9015, 0.8975, 0.8875, 0.8868, 0.8799, 0.8793, 0.8766, 0.8640,
         0.8634],
        [0.9479, 0.8676, 0.8675, 0.8546, 0.8515, 0.8408, 0.8295, 0.8277, 0.8242,
         0.8195],
        [0.9077, 0.8907, 0.8898, 0.8838, 0.8774, 0.8737, 0.8656, 0.8649, 0.8598,
         0.8595],
        [0.8969, 0.8952, 0.8775, 0.8679, 0.8568, 0.8542, 0.8499, 0.8475, 0.8412,
         0.8380],
        [0.9397, 0.9349, 0.9340, 0.9340, 0.9330, 0.9329, 0.9291, 0.9287, 0.9272,
         0.9249],
        [0.8857, 0.8828, 0.8815, 0.8725, 0.8650, 0.8610, 0.8598, 0.8461, 0.8378,
         0.8306],
        [0.9038, 0.8674, 0.8639, 0.8637, 0.8611, 0.8583, 0.8580, 0.8570, 0.8525,
         0.8515],
        [0.8903, 0.8806, 0.8778, 0.8657, 0.8584, 0.8540, 0.8497, 0.8469, 0.8389,
         0.8385],
        [0.8873, 0.8766, 0.8646, 0.8625, 0.8540, 0.8537, 0.8483, 0.8482, 0.8442,
         0.8360],
        [0.8594, 0.8405, 0.8302, 0.8289, 0.8182, 0.8181, 0.8070, 0.7961, 0.7853,
         0.7832],
        [0.8340, 0.8125, 0.7996, 0.7897, 0.7841, 0.7820, 0.7796, 0.7660, 0.7615,
         0.7586],
        [0.9055, 0.8936, 0.8879, 0.8863, 0.8861, 0.8847, 0.8749, 0.8733, 0.8731,
         0.8728],
        [0.8137, 0.7893, 0.7873, 0.7818, 0.7677, 0.7643, 0.7577, 0.7552, 0.7402,
         0.7316],
        [0.8429, 0.8372, 0.8148, 0.8140, 0.8115, 0.8062, 0.8052, 0.8027, 0.7945,
         0.7936],
        [0.8972, 0.8682, 0.8533, 0.8458, 0.8382, 0.8367, 0.8292, 0.8286, 0.8227,
         0.8178],
        [0.8851, 0.8575, 0.8541, 0.8526, 0.8413, 0.8306, 0.8237, 0.8194, 0.8136,
         0.8106],
        [0.8928, 0.8894, 0.8521, 0.8269, 0.8241, 0.8226, 0.8105, 0.8068, 0.8056,
         0.8050],
        [0.9297, 0.9119, 0.9099, 0.8982, 0.8721, 0.8638, 0.8620, 0.8617, 0.8474,
         0.8465],
        [0.9154, 0.8767, 0.8737, 0.8640, 0.8602, 0.8577, 0.8522, 0.8461, 0.8460,
         0.8449],
        [0.9295, 0.9077, 0.9046, 0.9037, 0.8990, 0.8982, 0.8982, 0.8954, 0.8948,
         0.8944],
        [0.9063, 0.8852, 0.8827, 0.8730, 0.8716, 0.8708, 0.8708, 0.8682, 0.8673,
         0.8672],
        [0.9139, 0.8808, 0.8767, 0.8723, 0.8638, 0.8624, 0.8429, 0.8374, 0.8364,
         0.8308],
        [0.9420, 0.8838, 0.8765, 0.8696, 0.8610, 0.8549, 0.8414, 0.8409, 0.8402,
         0.8289],
        [0.9127, 0.8637, 0.8605, 0.8512, 0.8364, 0.8347, 0.8345, 0.8296, 0.8268,
         0.8265],
        [0.9020, 0.8243, 0.8222, 0.8119, 0.8024, 0.8012, 0.7995, 0.7968, 0.7918,
         0.7872],
        [0.8845, 0.8732, 0.8638, 0.8611, 0.8555, 0.8534, 0.8502, 0.8499, 0.8494,
         0.8476],
        [0.8572, 0.7837, 0.7836, 0.7794, 0.7719, 0.7379, 0.7366, 0.7322, 0.7321,
         0.7282],
        [0.8872, 0.8598, 0.8514, 0.8248, 0.8224, 0.8187, 0.8097, 0.8039, 0.8026,
         0.7991],
        [0.8417, 0.8155, 0.8030, 0.7878, 0.7835, 0.7824, 0.7726, 0.7632, 0.7582,
         0.7581]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 305149.8438,  187459.5469,  179123.8750,  167451.1875,  156375.1875,
          152382.4531,  152054.8125,  142944.3438,  133024.0781,  132169.6406],
        [ 726131.3750,  622017.1875,  614453.9375,  591836.6875,  580438.8125,
          575111.6875,  570116.6875,  556398.7500,  542400.5625,  538755.2500],
        [ 427496.3750,  392497.0312,  309604.2812,  270372.0000,  227538.6562,
          198174.5000,  193868.5000,  171880.6875,  165142.8906,  142131.5938],
        [ 850758.8125,  352019.6875,  334635.6250,  303324.5312,  303320.2188,
          294083.6562,  260481.7969,  257533.5312,  256239.5156,  242586.0469],
        [ 176457.5000,  162026.7656,  151901.4688,  118673.6484,  110868.4219,
          108420.5469,   97914.6875,   93435.8438,   84855.1953,   72782.3359],
        [ 206006.1875,  123411.7891,  106896.2109,   93698.5391,   91724.0938,
           81708.7891,   79905.0469,   69637.7422,   67841.1797,   59750.0781],
        [ 264547.7188,  257938.5938,  225404.7656,  207078.7188,  206000.8906,
          197218.9844,  196461.1562,  193820.6094,  187343.3750,  183570.6094],
        [ 384154.4375,  286849.8750,  262164.7500,  223718.0625,  199769.1875,
          164952.2969,  141197.2344,  131998.0781,  111149.7109,  111063.7734],
        [ 458443.3750,  411232.5625,  350861.7188,  330847.0938,  320865.3438,
          315846.1562,  311415.0625,  306031.4062,  302080.3438,  301489.7812],
        [ 785921.8125,  331074.3438,  323834.0000,  312908.3438,  289525.4062,
          277553.4062,  267671.4688,  266530.0625,  236217.9062,  231104.8750],
        [ 743056.2500,  412614.9688,  377708.4688,  377687.2188,  372023.6250,
          350975.8438,  325634.5625,  320189.1562,  318238.4062,  310911.7812],
        [ 359661.0000,  289859.4062,  221640.9531,  205951.9844,  151199.7812,
          106226.9219,  103133.6797,   96317.6328,   84504.9531,   76298.8828],
        [ 532706.1250,  440875.2500,  435696.0938,  374232.6875,  333085.3750,
          306486.4375,  272334.3125,  260045.9531,  243389.4531,  211177.4688],
        [ 920731.2500,  855458.8125,  851276.5625,  843463.0000,  808356.4375,
          800613.0000,  759042.5000,  747771.5000,  719500.0625,  674891.3750],
        [ 713451.0000,  596595.7500,  589216.8750,  581505.3750,  566134.4375,
          519550.3125,  514463.4688,  494186.0000,  485217.6562,  483481.8125],
        [ 567027.0625,  538025.1875,  534251.7500,  480932.2812,  464411.2812,
          461977.3125,  447648.8750,  431337.6875,  423471.1250,  407737.0938],
        [ 856787.2500,  789992.4375,  672413.4375,  668817.3125,  591230.8125,
          584833.3750,  538746.0000,  526576.7500,  506275.9375,  500413.4375],
        [ 962082.4375,  882726.8750,  817393.8750,  689855.6875,  678468.4375,
          662291.1250,  646679.6250,  643635.8750,  616379.0000,  612741.1875],
        [ 579085.3125,  510535.8125,  437954.3750,  386535.4375,  367175.3438,
          366495.9688,  352585.1562,  351889.8125,  334140.3750,  307708.5938],
        [ 431331.9375,  386754.4375,  385159.2188,  379699.5312,  378610.0625,
          374304.0938,  364148.4688,  351459.8438,  346612.4062,  343029.3125],
        [ 754834.7500,  664869.3125,  659165.7500,  641603.6875,  619958.6875,
          617109.5625,  602776.8750,  595422.5625,  579067.1250,  576302.0000],
        [ 569528.6875,  467738.6875,  462573.8125,  453418.7812,  451452.9688,
          441127.1250,  425863.0625,  414285.2500,  352306.8438,  343008.3438],
        [1042561.3125,  777088.6250,  717859.9375,  700406.0000,  637633.2500,
          602405.6250,  578395.3750,  510980.5625,  478611.6562,  477039.5000],
        [ 709937.2500,  650019.1250,  607393.2500,  570061.7500,  553818.4375,
          548918.3125,  546401.4375,  544136.2500,  540280.3125,  529047.2500],
        [ 863921.4375,  405403.3438,  364692.0000,  303514.9375,  254682.5000,
          252776.9531,  239328.7656,  238939.0156,  235605.5000,  219509.6406],
        [ 675896.1875,  628217.1875,  627967.4375,  567221.7500,  543125.8125,
          525844.5625,  497965.0312,  468869.0625,  427960.5938,  419573.0312],
        [ 569922.0625,  565100.8750,  553285.2500,  503759.5000,  457590.3438,
          406111.0625,  398631.5938,  390666.4375,  381049.7188,  379690.5000],
        [ 924336.6250,  778601.1875,  734896.8125,  592233.6250,  572877.1875,
          556892.9375,  520041.5312,  514574.3750,  505205.7188,  502788.0625],
        [ 746893.4375,  673392.0625,  587214.1875,  579659.3750,  565398.4375,
          556070.3125,  550928.5000,  510786.1562,  509258.4062,  496174.1250],
        [ 631968.0000,  585299.3125,  565694.0000,  561787.6250,  541580.8125,
          532657.3750,  514335.4375,  505323.2812,  484934.0938,  474769.7500],
        [ 772355.6250,  751318.5000,  746293.2500,  737935.6875,  737160.5625,
          722508.3750,  650569.8750,  627815.3125,  612384.8125,  593571.4375],
        [ 690249.2500,  531649.9375,  525378.3750,  469096.2500,  416608.1562,
          402763.5938,  390980.6562,  385898.2500,  384168.7188,  372675.9375],
        [ 286876.9688,  256946.2188,  252939.0000,  222273.4375,  212414.4375,
          202451.1875,  202220.9844,  198107.2344,  197761.5938,  193230.0312],
        [ 433493.6562,  368403.6875,  338991.5000,  323881.2500,  322037.9062,
          316058.5938,  301854.8438,  295284.7500,  294354.4062,  291458.2500],
        [ 536215.9375,  499218.4688,  491912.0625,  473083.0312,  414313.2812,
          390759.9688,  385765.0312,  358272.1562,  345824.5938,  337857.6875],
        [ 434243.7812,  391736.4375,  370277.5625,  320580.5625,  317740.4375,
          287677.2188,  285454.8125,  274352.5000,  229277.1250,  227262.1562],
        [ 759869.6250,  241287.9531,  240979.5781,  200496.1875,  191884.8906,
          164566.2344,  140002.9219,  136621.3281,  129924.0000,  121518.7422],
        [ 427843.8750,  335747.0938,  331749.7812,  304239.7500,  277626.2188,
          263227.7500,  234469.0625,  232289.0156,  216087.2969,  214922.6094],
        [ 367151.9062,  358322.0312,  278129.1875,  242475.4844,  206847.7812,
          199414.9531,  187549.1406,  181111.7812,  165645.9062,  158104.2969],
        [ 676002.5625,  631589.0625,  623769.0000,  623035.9375,  614802.1250,
          613679.1875,  580796.5000,  578170.3750,  565582.3750,  547136.1250],
        [ 312446.7188,  299760.9688,  294652.9688,  258935.8125,  232661.9219,
          219666.5000,  215886.2500,  177498.0156,  157612.1719,  142266.7969],
        [ 405111.5312,  240754.4688,  228919.2344,  228267.1875,  220092.3906,
          211486.6406,  210430.6250,  207447.7500,  194704.5000,  191807.6719],
        [ 334013.2812,  290854.3438,  279391.9688,  235053.8594,  211796.2656,
          198887.5312,  186876.5781,  179544.0781,  160220.8281,  159242.2500],
        [ 320004.7812,  274575.0000,  231262.5156,  224439.5156,  198690.1719,
          198028.4688,  183312.3906,  182911.9688,  172912.8594,  153807.8750],
        [ 214862.3438,  163908.0625,  141448.1875,  138871.9531,  119286.2656,
          119029.5625,  101593.1484,   86922.8359,   74535.8203,   72274.9141],
        [ 149391.7969,  109911.8906,   91336.1797,   79362.4219,   73214.8672,
           71043.6719,   68661.4297,   56524.6133,   53047.4727,   50901.3945],
        [ 414990.2812,  349887.7188,  322429.7188,  315237.4062,  314285.8438,
          308201.4062,  267773.0938,  261943.0938,  261035.1094,  259989.6562],
        [ 111708.4609,   78851.8984,   76714.8359,   70834.2891,   57954.1914,
           55202.4258,   50201.2539,   48442.8164,   39136.7305,   34601.8047],
        [ 169666.1250,  156448.8594,  113501.2734,  112185.4609,  108252.8672,
          100442.6953,   98982.1797,   95582.9375,   84949.7656,   83934.5391],
        [ 368739.7188,  243581.9531,  196883.7344,  176849.3750,  158627.7656,
          155256.2344,  139533.0781,  138259.9844,  127119.7500,  118479.9375],
        [ 309830.8125,  209068.2031,  199033.6250,  194737.0000,  165726.3281,
          142343.3438,  129004.1719,  121250.6328,  111656.3750,  106876.2344],
        [ 346184.2500,  329655.9688,  193500.3594,  134896.1562,  129651.0781,
          126857.3203,  106818.6641,  101267.6406,   99540.0234,   98729.6172],
        [ 585853.2500,  454658.5000,  441609.9688,  373580.1562,  257387.4375,
          228723.2812,  222985.1250,  221907.6562,  180937.7656,  178618.0781],
        [ 478165.4375,  274764.1250,  263574.4062,  229222.6719,  217194.9062,
          209431.5938,  193812.6562,  177693.4688,  177421.0312,  174469.9219],
        [ 584789.3750,  427866.3125,  409328.2500,  404436.0000,  378331.8125,
          373945.5000,  373945.5000,  359150.6562,  355966.7812,  354242.4062],
        [ 419731.5000,  310624.2812,  299556.3438,  260627.6562,  255791.2500,
          252604.1719,  252555.7500,  243416.6094,  240384.1719,  240136.0312],
        [ 467467.9688,  291646.7812,  274917.1875,  258006.7500,  228820.3750,
          224098.5938,  169714.8438,  156734.7031,  154557.9219,  142621.9062],
        [ 698456.9375,  304084.2812,  274150.0938,  248579.2969,  219612.6562,
          201378.6406,  165964.6875,  164867.2031,  163260.3281,  138928.9219],
        [ 459959.0625,  228424.6406,  218206.8281,  191105.6406,  154669.5312,
          150814.5469,  150548.5625,  140252.0312,  134713.7344,  134295.4375],
        [ 394470.5000,  130043.9922,  126197.8828,  108930.2500,   95134.9688,
           93493.9531,   91307.2578,   87836.1406,   81807.3438,   76580.7734],
        [ 307441.4062,  261580.6094,  228675.7344,  219900.6250,  203197.6250,
          197069.1406,  188396.1562,  187439.7031,  186211.0469,  181308.6094],
        [ 208017.7188,   72855.8828,   72764.7109,   68497.7266,   61525.8203,
           37832.2930,   37150.9297,   34909.8242,   34864.1797,   32974.0391],
        [ 319401.4062,  215849.6094,  191644.2188,  131018.3281,  126576.2344,
          120044.8359,  105601.5781,   97159.7500,   95417.9062,   90715.9297],
        [ 166743.1719,  114667.9141,   95980.6484,   77209.8984,   72628.0625,
           71482.0938,   62154.3711,   54315.6133,   50577.6680,   50507.7773]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[305149.8438,      0.0000],
         [187459.5469,      0.0000],
         [179123.8750,      0.0000],
         ...,
         [142944.3438,      0.0000],
         [133024.0781,      0.0000],
         [     0.0000, 132169.6406]],

        [[726131.3750,      0.0000],
         [622017.1875,      0.0000],
         [614453.9375,      0.0000],
         ...,
         [556398.7500,      0.0000],
         [542400.5625,      0.0000],
         [538755.2500,      0.0000]],

        [[427496.3750,      0.0000],
         [392497.0312,      0.0000],
         [309604.2812,      0.0000],
         ...,
         [171880.6875,      0.0000],
         [165142.8906,      0.0000],
         [142131.5938,      0.0000]],

        ...,

        [[     0.0000, 208017.7188],
         [ 72855.8828,      0.0000],
         [ 72764.7109,      0.0000],
         ...,
         [ 34909.8242,      0.0000],
         [ 34864.1797,      0.0000],
         [ 32974.0391,      0.0000]],

        [[319401.4062,      0.0000],
         [     0.0000, 215849.6094],
         [191644.2188,      0.0000],
         ...,
         [ 97159.7500,      0.0000],
         [ 95417.9062,      0.0000],
         [ 90715.9297,      0.0000]],

        [[     0.0000, 166743.1719],
         [     0.0000, 114667.9141],
         [     0.0000,  95980.6484],
         ...,
         [ 54315.6133,      0.0000],
         [     0.0000,  50577.6680],
         [     0.0000,  50507.7773]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1575965.2500,  132169.6406],
        [5917661.0000,       0.0000],
        [2498706.5000,       0.0000],
        [3454983.5000,       0.0000],
        [ 485094.3125,  692242.1250],
        [ 821014.3750,  159565.2812],
        [1734823.0000,  384562.3750],
        [2017017.3750,       0.0000],
        [3409112.7500,       0.0000],
        [3322341.5000,       0.0000],
        [3909040.5000,       0.0000],
        [1694795.2500,       0.0000],
        [3410029.0000,       0.0000],
        [7981104.5000,       0.0000],
        [5543803.0000,       0.0000],
        [4756819.5000,       0.0000],
        [6236087.0000,       0.0000],
        [7212254.0000,       0.0000],
        [3994106.2500,       0.0000],
        [3741109.0000,       0.0000],
        [6311110.0000,       0.0000],
        [4381303.5000,       0.0000],
        [6522981.5000,       0.0000],
        [5800013.5000,       0.0000],
        [3378374.2500,       0.0000],
        [5382641.0000,       0.0000],
        [4605807.5000,       0.0000],
        [6202448.0000,       0.0000],
        [5775775.0000,       0.0000],
        [5398350.0000,       0.0000],
        [6951913.5000,       0.0000],
        [4569469.0000,       0.0000],
        [2225221.2500,       0.0000],
        [2624789.5000,  661029.3750],
        [1924626.3750, 2308595.7500],
        [1122628.2500, 2015974.5000],
        [1189681.0000, 1137470.5000],
        [1492745.5000, 1345457.0000],
        [ 788648.1250, 1556104.2500],
        [6054563.0000,       0.0000],
        [2311388.0000,       0.0000],
        [2339022.0000,       0.0000],
        [2235881.0000,       0.0000],
        [2139945.5000,       0.0000],
        [ 280744.9375,  951988.1875],
        [ 345329.8750,  458065.8750],
        [2660783.0000,  414990.2812],
        [ 399175.4375,  224473.2500],
        [ 493553.2188,  630393.5000],
        [ 487219.6562, 1336111.8750],
        [1435527.0000,  253999.7188],
        [ 797760.5000,  869340.6250],
        [1685376.3750, 1460884.7500],
        [2178555.5000,  217194.9062],
        [4022002.5000,       0.0000],
        [2775427.7500,       0.0000],
        [1089766.5000, 1278820.5000],
        [1807006.7500,  772276.2500],
        [1061415.7500,  901574.2500],
        [ 612327.5625,  673475.5000],
        [1207692.3750,  953528.2500],
        [ 453375.4375,  208017.7188],
        [1030959.1250,  462470.6875],
        [ 126943.6719,  689323.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 361/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:56, 59.88s/it]  7%|▋         | 2/30 [01:00<11:42, 25.10s/it] 10%|█         | 3/30 [01:01<06:17, 13.98s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.76s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.435333323478699
Epoch 362/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:25, 58.81s/it]  7%|▋         | 2/30 [00:59<11:31, 24.71s/it] 10%|█         | 3/30 [01:00<06:15, 13.91s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.71s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.84s/it] 20%|██        | 6/30 [01:02<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.70s/it]
Epoch loss is 2.42426483631134
Epoch 363/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:15, 60.52s/it]  7%|▋         | 2/30 [01:01<11:50, 25.36s/it] 10%|█         | 3/30 [01:02<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.434177939097087
Epoch 364/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:29, 58.97s/it]  7%|▋         | 2/30 [01:02<12:24, 26.58s/it] 10%|█         | 3/30 [01:03<06:39, 14.79s/it] 13%|█▎        | 4/30 [01:04<04:00,  9.24s/it] 17%|█▋        | 5/30 [01:05<02:34,  6.18s/it] 20%|██        | 6/30 [01:05<01:44,  4.33s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.39s/it] 30%|███       | 9/30 [01:08<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.29s/it] 40%|████      | 12/30 [01:10<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 2.4302791039148968
Epoch 365/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:53, 59.79s/it]  7%|▋         | 2/30 [01:00<11:41, 25.07s/it] 10%|█         | 3/30 [01:01<06:17, 13.97s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.75s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.411619186401367
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0241,  0.0022,  0.0118,  ..., -0.0038,  0.0137,  0.0040],
        [-0.0021,  0.0119,  0.0273,  ...,  0.0162,  0.0010, -0.0160],
        [-0.0296, -0.0382,  0.0246,  ...,  0.0811, -0.0070, -0.0159],
        ...,
        [ 0.0082, -0.0034,  0.0078,  ..., -0.0232, -0.0082, -0.0069],
        [-0.0320,  0.0080, -0.0049,  ...,  0.0042,  0.0160, -0.0117],
        [-0.0349, -0.0130,  0.0179,  ...,  0.0400,  0.0345, -0.0319]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8831, 0.8501, 0.8459, 0.8414, 0.8362, 0.8351, 0.8344, 0.8300, 0.8253,
         0.8245],
        [0.9443, 0.9334, 0.9324, 0.9302, 0.9284, 0.9276, 0.9276, 0.9255, 0.9235,
         0.9232],
        [0.9074, 0.9012, 0.8840, 0.8751, 0.8626, 0.8533, 0.8517, 0.8433, 0.8404,
         0.8303],
        [0.9558, 0.8931, 0.8896, 0.8831, 0.8827, 0.8812, 0.8721, 0.8710, 0.8709,
         0.8670],
        [0.8454, 0.8388, 0.8348, 0.8175, 0.8121, 0.8110, 0.8041, 0.8007, 0.7939,
         0.7826],
        [0.8562, 0.8203, 0.8103, 0.8009, 0.7985, 0.7919, 0.7893, 0.7794, 0.7785,
         0.7694],
        [0.8730, 0.8711, 0.8618, 0.8565, 0.8556, 0.8523, 0.8519, 0.8518, 0.8484,
         0.8476],
        [0.8991, 0.8788, 0.8725, 0.8616, 0.8539, 0.8397, 0.8296, 0.8243, 0.8123,
         0.8120],
        [0.9120, 0.9045, 0.8931, 0.8889, 0.8870, 0.8859, 0.8853, 0.8836, 0.8827,
         0.8826],
        [0.9500, 0.8891, 0.8875, 0.8851, 0.8795, 0.8766, 0.8739, 0.8737, 0.8655,
         0.8639],
        [0.9461, 0.9049, 0.8983, 0.8979, 0.8969, 0.8930, 0.8879, 0.8866, 0.8863,
         0.8843],
        [0.8947, 0.8796, 0.8610, 0.8555, 0.8338, 0.8088, 0.8066, 0.8028, 0.7930,
         0.7861],
        [0.9219, 0.9093, 0.9083, 0.8975, 0.8891, 0.8835, 0.8752, 0.8717, 0.8676,
         0.8576],
        [0.9609, 0.9560, 0.9554, 0.9548, 0.9517, 0.9510, 0.9474, 0.9463, 0.9435,
         0.9391],
        [0.9433, 0.9306, 0.9297, 0.9288, 0.9268, 0.9210, 0.9202, 0.9174, 0.9160,
         0.9159],
        [0.9272, 0.9235, 0.9229, 0.9156, 0.9132, 0.9127, 0.9106, 0.9079, 0.9067,
         0.9041],
        [0.9558, 0.9501, 0.9385, 0.9385, 0.9298, 0.9291, 0.9231, 0.9218, 0.9189,
         0.9180],
        [0.9641, 0.9581, 0.9528, 0.9410, 0.9399, 0.9380, 0.9363, 0.9359, 0.9329,
         0.9327],
        [0.9285, 0.9194, 0.9084, 0.9003, 0.8965, 0.8964, 0.8937, 0.8936, 0.8898,
         0.8846],
        [0.9077, 0.9003, 0.9000, 0.8991, 0.8986, 0.8980, 0.8959, 0.8930, 0.8924,
         0.8918],
        [0.9467, 0.9379, 0.9373, 0.9353, 0.9332, 0.9330, 0.9310, 0.9303, 0.9280,
         0.9278],
        [0.9274, 0.9136, 0.9126, 0.9116, 0.9107, 0.9093, 0.9072, 0.9051, 0.8935,
         0.8915],
        [0.9697, 0.9494, 0.9436, 0.9418, 0.9353, 0.9311, 0.9284, 0.9198, 0.9151,
         0.9149],
        [0.9428, 0.9366, 0.9319, 0.9276, 0.9253, 0.9248, 0.9247, 0.9242, 0.9235,
         0.9227],
        [0.9565, 0.9034, 0.8956, 0.8832, 0.8702, 0.8702, 0.8662, 0.8659, 0.8650,
         0.8605],
        [0.9392, 0.9342, 0.9340, 0.9270, 0.9241, 0.9216, 0.9178, 0.9136, 0.9072,
         0.9060],
        [0.9273, 0.9265, 0.9249, 0.9185, 0.9118, 0.9034, 0.9021, 0.9007, 0.8991,
         0.8987],
        [0.9614, 0.9494, 0.9452, 0.9299, 0.9277, 0.9259, 0.9211, 0.9203, 0.9188,
         0.9186],
        [0.9462, 0.9391, 0.9294, 0.9284, 0.9271, 0.9256, 0.9249, 0.9196, 0.9193,
         0.9173],
        [0.9346, 0.9293, 0.9270, 0.9266, 0.9239, 0.9225, 0.9201, 0.9191, 0.9162,
         0.9146],
        [0.9485, 0.9468, 0.9463, 0.9455, 0.9454, 0.9440, 0.9367, 0.9341, 0.9324,
         0.9303],
        [0.9407, 0.9226, 0.9218, 0.9137, 0.9054, 0.9033, 0.9009, 0.9000, 0.8997,
         0.8978],
        [0.8791, 0.8718, 0.8704, 0.8616, 0.8585, 0.8546, 0.8544, 0.8533, 0.8532,
         0.8515],
        [0.9078, 0.8965, 0.8907, 0.8877, 0.8870, 0.8856, 0.8828, 0.8812, 0.8808,
         0.8804],
        [0.9229, 0.9180, 0.9170, 0.9140, 0.9049, 0.9008, 0.9004, 0.8946, 0.8921,
         0.8906],
        [0.9083, 0.9009, 0.8970, 0.8869, 0.8867, 0.8795, 0.8789, 0.8758, 0.8635,
         0.8627],
        [0.9478, 0.8675, 0.8672, 0.8543, 0.8510, 0.8398, 0.8286, 0.8264, 0.8237,
         0.8190],
        [0.9067, 0.8897, 0.8887, 0.8827, 0.8772, 0.8732, 0.8645, 0.8639, 0.8592,
         0.8587],
        [0.8965, 0.8946, 0.8766, 0.8672, 0.8555, 0.8536, 0.8491, 0.8463, 0.8405,
         0.8376],
        [0.9392, 0.9346, 0.9335, 0.9331, 0.9326, 0.9324, 0.9287, 0.9283, 0.9267,
         0.9245],
        [0.8850, 0.8821, 0.8811, 0.8718, 0.8646, 0.8605, 0.8591, 0.8454, 0.8371,
         0.8299],
        [0.9034, 0.8666, 0.8636, 0.8630, 0.8600, 0.8574, 0.8566, 0.8560, 0.8518,
         0.8508],
        [0.8897, 0.8799, 0.8769, 0.8652, 0.8576, 0.8532, 0.8487, 0.8459, 0.8380,
         0.8380],
        [0.8861, 0.8760, 0.8638, 0.8613, 0.8533, 0.8526, 0.8478, 0.8474, 0.8439,
         0.8358],
        [0.8584, 0.8401, 0.8289, 0.8280, 0.8173, 0.8171, 0.8060, 0.7944, 0.7838,
         0.7814],
        [0.8335, 0.8118, 0.7986, 0.7894, 0.7837, 0.7819, 0.7793, 0.7658, 0.7613,
         0.7586],
        [0.9048, 0.8922, 0.8868, 0.8849, 0.8848, 0.8837, 0.8734, 0.8720, 0.8718,
         0.8714],
        [0.8128, 0.7888, 0.7865, 0.7809, 0.7669, 0.7643, 0.7559, 0.7535, 0.7393,
         0.7305],
        [0.8418, 0.8364, 0.8142, 0.8136, 0.8105, 0.8058, 0.8044, 0.8027, 0.7935,
         0.7934],
        [0.8967, 0.8676, 0.8523, 0.8443, 0.8375, 0.8360, 0.8283, 0.8277, 0.8220,
         0.8171],
        [0.8841, 0.8571, 0.8533, 0.8521, 0.8407, 0.8302, 0.8235, 0.8184, 0.8132,
         0.8098],
        [0.8922, 0.8889, 0.8513, 0.8256, 0.8228, 0.8209, 0.8097, 0.8059, 0.8043,
         0.8043],
        [0.9290, 0.9114, 0.9090, 0.8976, 0.8712, 0.8633, 0.8611, 0.8608, 0.8462,
         0.8460],
        [0.9150, 0.8763, 0.8733, 0.8635, 0.8594, 0.8571, 0.8511, 0.8457, 0.8453,
         0.8441],
        [0.9288, 0.9069, 0.9038, 0.9030, 0.8986, 0.8975, 0.8975, 0.8946, 0.8939,
         0.8939],
        [0.9056, 0.8847, 0.8823, 0.8724, 0.8710, 0.8701, 0.8697, 0.8673, 0.8667,
         0.8662],
        [0.9129, 0.8804, 0.8757, 0.8716, 0.8630, 0.8621, 0.8429, 0.8362, 0.8356,
         0.8300],
        [0.9417, 0.8832, 0.8759, 0.8688, 0.8604, 0.8545, 0.8406, 0.8399, 0.8392,
         0.8278],
        [0.9120, 0.8633, 0.8599, 0.8506, 0.8358, 0.8342, 0.8333, 0.8290, 0.8255,
         0.8252],
        [0.9016, 0.8236, 0.8213, 0.8108, 0.8014, 0.8003, 0.7986, 0.7958, 0.7910,
         0.7861],
        [0.8844, 0.8725, 0.8630, 0.8603, 0.8546, 0.8520, 0.8492, 0.8489, 0.8485,
         0.8464],
        [0.8569, 0.7830, 0.7829, 0.7790, 0.7711, 0.7367, 0.7363, 0.7315, 0.7311,
         0.7278],
        [0.8867, 0.8592, 0.8503, 0.8245, 0.8213, 0.8179, 0.8079, 0.8026, 0.8013,
         0.7974],
        [0.8413, 0.8145, 0.8021, 0.7870, 0.7821, 0.7818, 0.7720, 0.7619, 0.7571,
         0.7567]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 1, 0],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 301321.8750,  187930.3125,  177037.8594,  166140.3125,  154181.6250,
          151654.5469,  150246.7812,  141106.0938,  131976.5625,  130384.6250],
        [ 722608.2500,  617688.9375,  609562.3750,  590078.3125,  575207.1250,
          569162.1875,  568747.6250,  552139.8750,  536586.8125,  533950.7500],
        [ 426067.0000,  389936.1562,  305263.3438,  268631.7500,  224891.4062,
          196714.2656,  192419.4219,  170626.2344,  163706.5469,  141701.6250],
        [ 851086.6875,  347519.5938,  330601.0938,  301183.4062,  299676.6250,
          293125.1875,  257390.1406,  253568.1719,  252932.7344,  239208.9531],
        [ 175900.8594,  159909.4062,  151031.4531,  117957.0469,  109233.3906,
          107566.7578,   97520.1172,   92822.2188,   84224.2344,   71657.9844],
        [ 205056.9062,  122870.7578,  106541.5312,   93104.0547,   89997.3516,
           81856.1953,   78893.6484,   68456.8438,   67603.3125,   59360.1211],
        [ 260679.6094,  253856.1094,  222359.3281,  206075.7500,  203365.5156,
          194127.5156,  192832.5781,  192522.0469,  183611.2344,  181444.9219],
        [ 378819.9062,  283224.7812,  259014.3438,  221699.3125,  198426.9688,
          161956.0156,  140313.8438,  130146.5938,  109639.6953,  109061.1172],
        [ 455148.6875,  408895.5625,  347513.3125,  327161.3438,  318362.5625,
          313605.2812,  310671.4062,  303499.3125,  299390.6875,  299211.4375],
        [ 783749.7500,  328117.1875,  320895.9375,  310145.9375,  286321.8438,
          274712.5000,  264306.1250,  263216.2188,  234231.2812,  229078.0156],
        [ 740444.6250,  411332.5938,  374424.0312,  371968.2812,  366852.9688,
          347237.3750,  322639.5000,  316852.9375,  315124.3750,  306573.2812],
        [ 355532.5312,  286695.3438,  219673.2031,  203210.4219,  148882.3281,
          104209.0703,  101049.8125,   95628.7109,   83189.3438,   75361.2266],
        [ 524764.5000,  438225.9375,  431585.3750,  369830.4062,  328016.1250,
          303014.3125,  269015.7812,  256092.9375,  241348.7188,  209335.3594],
        [ 914809.3750,  853922.4375,  846013.5625,  838478.5000,  802958.9375,
          794241.6250,  754322.3125,  743050.5625,  714443.0625,  670128.0625],
        [ 712096.9375,  593967.2500,  586568.8750,  578577.4375,  562613.3125,
          517511.4375,  511864.8125,  492076.3125,  481779.2188,  481012.5625],
        [ 566030.7500,  536615.4375,  532316.1250,  479013.0312,  463243.9375,
          460077.9375,  446061.4688,  429301.3438,  421983.1250,  406590.0312],
        [ 851555.9375,  784743.0000,  664970.1250,  664717.1875,  587171.6250,
          581000.9375,  533736.3750,  523557.7812,  502211.5312,  496280.0938],
        [ 958759.5000,  879251.8750,  815176.8125,  688452.5000,  678452.2500,
          659579.5000,  644230.3125,  640440.3125,  613349.8125,  611986.0625],
        [ 576425.6875,  505670.8438,  432579.3125,  385229.3750,  365036.5000,
          364141.8438,  350312.3750,  350050.2188,  331458.5312,  307566.8750],
        [ 428226.3438,  385389.5938,  383402.6875,  378722.3750,  375917.4375,
          372699.4062,  361566.6250,  347160.2188,  344015.0625,  341224.3438],
        [ 747321.6875,  658982.2500,  653757.3125,  635122.5000,  616115.1250,
          614473.3125,  597021.5000,  590954.6250,  572496.5000,  570806.0000],
        [ 567392.7500,  465781.8438,  458995.9062,  452916.5938,  446542.4375,
          437826.1875,  424897.9375,  412386.0312,  349609.1562,  339636.1250],
        [1038135.3125,  776378.9375,  715255.0000,  696688.6875,  635480.0000,
          597908.6250,  575249.9375,  509170.5000,  475648.0312,  474562.8750],
        [ 706962.5000,  646823.3125,  605351.9375,  569112.8125,  550683.7500,
          546662.0000,  545988.3125,  542236.1250,  536813.5625,  530047.6875],
        [ 859815.8125,  402755.5312,  360238.7500,  301708.9375,  250727.3281,
          250459.6719,  236572.7500,  235781.5000,  232530.6250,  218112.3594],
        [ 671606.5625,  625172.7500,  623441.9375,  563947.7500,  541420.7500,
          522470.4062,  494397.1875,  465576.2500,  425214.5312,  417843.5938],
        [ 566443.8750,  559926.3125,  547586.0625,  499875.9062,  454141.9062,
          402748.6250,  395061.9688,  387642.5312,  378741.8750,  376253.8750],
        [ 921980.7500,  776922.5625,  731158.1875,  588093.5000,  569626.4375,
          555005.4375,  518630.0000,  512299.9375,  501618.4688,  500396.7188],
        [ 742454.1250,  670236.0625,  583301.0625,  575292.6875,  564823.4375,
          553172.8750,  547237.3750,  507732.2812,  505224.9688,  490817.4062],
        [ 628364.0000,  582500.0000,  563730.5000,  560500.0625,  539565.1250,
          529119.9375,  511403.2188,  503778.2188,  483146.2188,  472313.0625],
        [ 766688.0000,  748188.8125,  743517.0000,  734615.1250,  733343.9375,
          719197.5625,  647779.5625,  624266.5625,  609558.8750,  591280.4375],
        [ 686245.4375,  530004.1875,  523577.2500,  466462.8750,  414162.3750,
          401898.7500,  388571.1875,  383468.5000,  381930.1250,  371659.0625],
        [ 284670.8125,  256227.5469,  251279.5781,  221525.7969,  212105.7344,
          200452.0156,  199866.7500,  196767.9219,  196533.5000,  191749.3281],
        [ 428639.4375,  364581.0625,  335772.0938,  321764.6875,  318290.5938,
          312339.1875,  300024.0938,  292998.2812,  291691.5625,  289864.0938],
        [ 532000.4375,  496018.9062,  489166.4688,  468313.5938,  411232.5625,
          388116.7500,  385766.8438,  355288.5000,  342764.0938,  335166.7500],
        [ 431905.3125,  388267.7812,  367659.9375,  317892.2812,  317180.6562,
          286118.5000,  283682.4062,  271388.2500,  227678.6562,  225222.7812],
        [ 759491.4375,  241084.4062,  240111.0781,  199554.9844,  190458.2812,
          162204.7188,  138360.5000,  133935.5312,  128997.1719,  120499.3906],
        [ 421961.8125,  330945.8750,  326530.7812,  299524.6250,  276709.5312,
          261643.5000,  231102.8906,  229072.1094,  214270.3750,  212675.9062],
        [ 364584.5312,  354905.5000,  274470.2812,  240198.7812,  203140.4688,
          197636.0469,  185375.0781,  178170.7969,  163919.7969,  157190.5156],
        [ 671155.8750,  628344.1875,  618862.2500,  615245.5625,  611051.1875,
          609139.3125,  577501.3750,  574416.6875,  561402.5625,  544180.8750],
        [ 309690.1875,  296850.9688,  292686.9062,  256192.3594,  231165.5000,
          217990.0781,  213805.9844,  175847.8594,  156095.3594,  140876.8438],
        [ 402395.7812,  237867.2344,  227891.1094,  226103.2031,  216568.8125,
          208751.4375,  206325.5000,  204486.2969,  192547.9219,  189814.2031],
        [ 331095.5000,  287617.1562,  275714.3438,  233151.4844,  209378.6719,
          196603.2188,  184281.8750,  177151.6875,  158225.2656,  158210.3281],
        [ 314501.4375,  272336.1562,  228628.2031,  220591.0312,  196841.6875,
          194891.2188,  182063.8906,  180985.7344,  172128.7188,  153248.5625],
        [ 211709.6250,  162989.0156,  138828.0000,  137047.3906,  117597.8438,
          117424.2500,  100121.9219,   84897.0391,   72958.4297,   70433.1484],
        [ 148223.4062,  108768.7344,   90043.2031,   79041.8594,   72831.3594,
           70958.4219,   68339.1719,   56385.1133,   52889.7656,   50870.3320],
        [ 410811.5625,  343303.2188,  317758.0000,  309121.5938,  308685.9062,
          303816.7188,  262212.0000,  257132.5312,  256355.3750,  254816.1250],
        [ 110326.9219,   78362.6797,   75736.2422,   69932.5703,   57246.0781,
           55157.9609,   48972.6797,   47305.7930,   38612.0742,   34053.7578],
        [ 166945.5625,  154586.2188,  112644.0938,  111551.9688,  106824.6719,
           99796.4688,   97816.8750,   95495.3828,   83743.8438,   83631.9453],
        [ 365685.2812,  241481.3281,  193962.0781,  173139.4375,  157095.1875,
          153664.6250,  137781.6562,  136446.0625,  125827.6172,  117272.2734],
        [ 305542.0625,  207813.4688,  196944.2031,  193338.9688,  164314.6875,
          141541.5625,  128494.6172,  119609.5547,  110916.4297,  105703.4453],
        [ 343259.3750,  327337.0625,  191247.8438,  132561.8438,  127352.2422,
          123812.0000,  105639.2500,   99993.1016,   97760.3516,   97733.4219],
        [ 580782.6250,  451569.2188,  435850.2500,  370427.3125,  254219.2500,
          226955.4688,  220073.6875,  219187.0781,  177801.1094,  177421.1875],
        [ 475214.5938,  273497.5000,  262063.5312,  227710.7812,  214588.3594,
          207875.5312,  190810.0625,  176635.6406,  175437.1250,  172580.7344],
        [ 579090.8125,  423321.3125,  404953.5312,  400113.9688,  376020.0000,
          369915.7812,  369915.7812,  354917.0000,  351625.4688,  351321.4375],
        [ 415722.6875,  308335.7500,  297991.5000,  258474.6719,  253403.5312,
          250330.7188,  248603.7188,  240218.7031,  238242.0625,  236793.2656],
        [ 461392.1562,  289896.7188,  271027.1875,  255599.5781,  225954.2500,
          223016.3906,  169530.7656,  154059.6406,  152920.3750,  141044.8906],
        [ 695927.0000,  301595.2812,  271766.6562,  245617.6719,  217822.1875,
          200310.2188,  164248.7188,  162572.8281,  160935.9844,  136807.3906],
        [ 455180.8438,  226884.0469,  216264.7969,  189263.4219,  153238.6094,
          149899.2812,  148000.9375,  138995.8594,  132266.2344,  131670.3125],
        [ 392225.4062,  128709.7422,  124666.0312,  107301.9062,   93703.8125,
           92295.7656,   90074.8047,   86605.4219,   80773.3359,   75324.7266],
        [ 306979.6562,  258812.3594,  226150.0000,  217522.0156,  200577.6406,
          193231.6875,  185592.6562,  184697.1094,  183695.8125,  178324.2969],
        [ 207117.2344,   72079.2891,   71987.3750,   68075.9219,   60830.6562,
           37234.4648,   36995.8984,   34533.8633,   34352.7070,   32761.1113],
        [ 317208.1875,  214057.1250,  188611.3438,  130379.7812,  124513.4609,
          118749.2812,  102929.7891,   95446.6641,   93682.7188,   88579.2656],
        [ 165716.3750,  113116.4766,   94775.9062,   76318.8906,   71202.5234,
           70913.8438,   61629.4102,   53354.3281,   49800.9414,   49494.7930]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[301321.8750,      0.0000],
         [187930.3125,      0.0000],
         [177037.8594,      0.0000],
         ...,
         [141106.0938,      0.0000],
         [131976.5625,      0.0000],
         [     0.0000, 130384.6250]],

        [[722608.2500,      0.0000],
         [617688.9375,      0.0000],
         [609562.3750,      0.0000],
         ...,
         [552139.8750,      0.0000],
         [536586.8125,      0.0000],
         [533950.7500,      0.0000]],

        [[426067.0000,      0.0000],
         [389936.1562,      0.0000],
         [305263.3438,      0.0000],
         ...,
         [170626.2344,      0.0000],
         [163706.5469,      0.0000],
         [141701.6250,      0.0000]],

        ...,

        [[     0.0000, 207117.2344],
         [ 72079.2891,      0.0000],
         [ 71987.3750,      0.0000],
         ...,
         [ 34533.8633,      0.0000],
         [ 34352.7070,      0.0000],
         [ 32761.1113,      0.0000]],

        [[317208.1875,      0.0000],
         [     0.0000, 214057.1250],
         [188611.3438,      0.0000],
         ...,
         [ 95446.6641,      0.0000],
         [ 93682.7188,      0.0000],
         [ 88579.2656,      0.0000]],

        [[     0.0000, 165716.3750],
         [     0.0000, 113116.4766],
         [     0.0000,  94775.9062],
         ...,
         [ 53354.3281,      0.0000],
         [     0.0000,  49800.9414],
         [     0.0000,  49494.7930]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1561595.8750,  130384.6250],
        [5875732.0000,       0.0000],
        [2479957.7500,       0.0000],
        [3426292.5000,       0.0000],
        [ 480592.8125,  687230.6250],
        [ 816140.0000,  157600.6562],
        [1714431.0000,  376443.8125],
        [1992302.6250,       0.0000],
        [3383459.5000,       0.0000],
        [3294774.5000,       0.0000],
        [3873450.0000,       0.0000],
        [1673432.0000,       0.0000],
        [3371229.5000,       0.0000],
        [7932368.0000,       0.0000],
        [5518068.0000,       0.0000],
        [4741233.0000,       0.0000],
        [6189944.5000,       0.0000],
        [7189679.0000,       0.0000],
        [3968471.5000,       0.0000],
        [3718324.0000,       0.0000],
        [6257050.5000,       0.0000],
        [4355985.0000,       0.0000],
        [6494478.0000,       0.0000],
        [5780682.0000,       0.0000],
        [3348703.2500,       0.0000],
        [5351091.5000,       0.0000],
        [4568423.0000,       0.0000],
        [6175732.5000,       0.0000],
        [5740292.5000,       0.0000],
        [5374420.0000,       0.0000],
        [6918436.0000,       0.0000],
        [4547980.0000,       0.0000],
        [2211179.0000,       0.0000],
        [2601902.5000,  654062.6875],
        [1910502.6250, 2293332.2500],
        [1112557.5000, 2004439.1250],
        [1180186.1250, 1134511.3750],
        [1475295.6250, 1329141.8750],
        [ 779925.3750, 1539666.3750],
        [6011300.0000,       0.0000],
        [2291202.0000,       0.0000],
        [2312751.5000,       0.0000],
        [2211429.5000,       0.0000],
        [2116216.5000,       0.0000],
        [ 275453.3125,  938553.3125],
        [ 344060.5625,  454290.8125],
        [2613201.5000,  410811.5625],
        [ 393512.7500,  222194.0312],
        [ 489388.7188,  623648.3125],
        [ 482957.5625, 1319398.0000],
        [1421761.0000,  252458.0000],
        [ 784852.1875,  861844.2500],
        [1668347.0000, 1445940.2500],
        [2161825.5000,  214588.3594],
        [3981195.0000,       0.0000],
        [2748116.5000,       0.0000],
        [1078430.6250, 1266011.3750],
        [1792568.8750,  765035.1250],
        [1049587.5000,  892076.9375],
        [ 604533.1875,  667147.7500],
        [1369213.6250,  766369.6875],
        [ 448851.2812,  207117.2344],
        [1016837.7500,  457319.8750],
        [ 124556.8516,  681766.6250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 366/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:01, 62.11s/it]  7%|▋         | 2/30 [01:02<12:08, 26.02s/it] 10%|█         | 3/30 [01:03<06:31, 14.48s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 2.418293086687724
Epoch 367/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:20, 58.65s/it]  7%|▋         | 2/30 [00:59<11:28, 24.59s/it] 10%|█         | 3/30 [01:00<06:10, 13.70s/it] 13%|█▎        | 4/30 [01:00<03:43,  8.59s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.76s/it] 20%|██        | 6/30 [01:02<01:37,  4.06s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 2.4360133091608684
Epoch 368/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:55<26:57, 55.77s/it]  7%|▋         | 2/30 [00:56<11:03, 23.68s/it] 10%|█         | 3/30 [01:00<06:34, 14.60s/it] 13%|█▎        | 4/30 [01:01<03:57,  9.13s/it] 17%|█▋        | 5/30 [01:02<02:32,  6.11s/it] 20%|██        | 6/30 [01:03<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:03<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:04<00:52,  2.37s/it] 30%|███       | 9/30 [01:05<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.28s/it] 40%|████      | 12/30 [01:07<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.413992977142334
Epoch 369/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:16, 58.51s/it]  7%|▋         | 2/30 [01:00<11:39, 24.97s/it] 10%|█         | 3/30 [01:00<06:15, 13.91s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.72s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.84s/it] 20%|██        | 6/30 [01:03<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.399450945854187
Epoch 370/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:54, 61.89s/it]  7%|▋         | 2/30 [01:02<12:05, 25.92s/it] 10%|█         | 3/30 [01:03<06:30, 14.48s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.380126619338989
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0240,  0.0022,  0.0120,  ..., -0.0040,  0.0142,  0.0040],
        [-0.0022,  0.0117,  0.0274,  ...,  0.0163,  0.0013, -0.0159],
        [-0.0296, -0.0382,  0.0247,  ...,  0.0813, -0.0066, -0.0158],
        ...,
        [ 0.0080, -0.0033,  0.0080,  ..., -0.0233, -0.0079, -0.0065],
        [-0.0321,  0.0082, -0.0047,  ...,  0.0044,  0.0161, -0.0116],
        [-0.0346, -0.0131,  0.0180,  ...,  0.0402,  0.0346, -0.0316]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8822, 0.8502, 0.8452, 0.8410, 0.8352, 0.8347, 0.8336, 0.8291, 0.8248,
         0.8236],
        [0.9441, 0.9329, 0.9319, 0.9300, 0.9277, 0.9275, 0.9270, 0.9250, 0.9229,
         0.9226],
        [0.9072, 0.9007, 0.8831, 0.8747, 0.8620, 0.8529, 0.8512, 0.8428, 0.8398,
         0.8301],
        [0.9558, 0.8923, 0.8888, 0.8826, 0.8819, 0.8809, 0.8712, 0.8700, 0.8700,
         0.8661],
        [0.8452, 0.8379, 0.8344, 0.8171, 0.8113, 0.8104, 0.8038, 0.8002, 0.7934,
         0.7817],
        [0.8558, 0.8201, 0.8101, 0.8005, 0.7973, 0.7920, 0.7886, 0.7783, 0.7782,
         0.7690],
        [0.8721, 0.8701, 0.8610, 0.8562, 0.8548, 0.8516, 0.8513, 0.8504, 0.8471,
         0.8469],
        [0.8984, 0.8781, 0.8719, 0.8611, 0.8533, 0.8386, 0.8293, 0.8236, 0.8117,
         0.8109],
        [0.9115, 0.9041, 0.8924, 0.8882, 0.8865, 0.8855, 0.8851, 0.8831, 0.8823,
         0.8820],
        [0.9499, 0.8885, 0.8870, 0.8846, 0.8788, 0.8760, 0.8731, 0.8729, 0.8650,
         0.8634],
        [0.9459, 0.9047, 0.8977, 0.8969, 0.8961, 0.8924, 0.8873, 0.8863, 0.8852,
         0.8833],
        [0.8940, 0.8788, 0.8604, 0.8546, 0.8327, 0.8074, 0.8052, 0.8022, 0.7920,
         0.7852],
        [0.9209, 0.9090, 0.9076, 0.8967, 0.8881, 0.8828, 0.8744, 0.8707, 0.8670,
         0.8570],
        [0.9604, 0.9559, 0.9550, 0.9544, 0.9514, 0.9505, 0.9470, 0.9459, 0.9431,
         0.9386],
        [0.9432, 0.9303, 0.9295, 0.9284, 0.9264, 0.9208, 0.9200, 0.9172, 0.9157,
         0.9153],
        [0.9272, 0.9234, 0.9227, 0.9154, 0.9131, 0.9125, 0.9103, 0.9076, 0.9065,
         0.9039],
        [0.9554, 0.9497, 0.9381, 0.9377, 0.9293, 0.9286, 0.9225, 0.9213, 0.9183,
         0.9175],
        [0.9639, 0.9578, 0.9526, 0.9408, 0.9400, 0.9377, 0.9360, 0.9356, 0.9327,
         0.9325],
        [0.9282, 0.9188, 0.9076, 0.9001, 0.8962, 0.8960, 0.8933, 0.8932, 0.8893,
         0.8845],
        [0.9072, 0.9001, 0.8997, 0.8989, 0.8982, 0.8977, 0.8954, 0.8923, 0.8919,
         0.8915],
        [0.9460, 0.9373, 0.9368, 0.9347, 0.9328, 0.9327, 0.9302, 0.9297, 0.9277,
         0.9269],
        [0.9272, 0.9133, 0.9121, 0.9116, 0.9099, 0.9088, 0.9070, 0.9048, 0.8930,
         0.8908],
        [0.9694, 0.9493, 0.9434, 0.9414, 0.9351, 0.9306, 0.9280, 0.9196, 0.9146,
         0.9145],
        [0.9425, 0.9363, 0.9318, 0.9275, 0.9250, 0.9247, 0.9246, 0.9240, 0.9231,
         0.9228],
        [0.9562, 0.9030, 0.8948, 0.8829, 0.8696, 0.8693, 0.8656, 0.8652, 0.8642,
         0.8601],
        [0.9388, 0.9339, 0.9335, 0.9266, 0.9239, 0.9212, 0.9173, 0.9131, 0.9068,
         0.9057],
        [0.9268, 0.9259, 0.9242, 0.9180, 0.9113, 0.9029, 0.9014, 0.9002, 0.8987,
         0.8981],
        [0.9612, 0.9492, 0.9449, 0.9294, 0.9273, 0.9256, 0.9209, 0.9200, 0.9185,
         0.9184],
        [0.9458, 0.9388, 0.9289, 0.9279, 0.9270, 0.9253, 0.9244, 0.9195, 0.9186,
         0.9166],
        [0.9342, 0.9290, 0.9268, 0.9264, 0.9236, 0.9221, 0.9198, 0.9189, 0.9159,
         0.9144],
        [0.9480, 0.9465, 0.9461, 0.9453, 0.9450, 0.9437, 0.9364, 0.9337, 0.9322,
         0.9300],
        [0.9403, 0.9225, 0.9215, 0.9134, 0.9050, 0.9032, 0.9005, 0.8996, 0.8994,
         0.8977],
        [0.8786, 0.8716, 0.8700, 0.8613, 0.8584, 0.8539, 0.8537, 0.8528, 0.8528,
         0.8509],
        [0.9071, 0.8958, 0.8901, 0.8873, 0.8862, 0.8849, 0.8824, 0.8806, 0.8802,
         0.8800],
        [0.9224, 0.9176, 0.9167, 0.9133, 0.9044, 0.9004, 0.9003, 0.8941, 0.8916,
         0.8901],
        [0.9080, 0.9003, 0.8967, 0.8866, 0.8863, 0.8791, 0.8786, 0.8751, 0.8630,
         0.8621],
        [0.9478, 0.8676, 0.8669, 0.8539, 0.8506, 0.8388, 0.8278, 0.8251, 0.8232,
         0.8183],
        [0.9058, 0.8886, 0.8878, 0.8817, 0.8770, 0.8729, 0.8636, 0.8631, 0.8587,
         0.8582],
        [0.8960, 0.8939, 0.8757, 0.8666, 0.8544, 0.8530, 0.8484, 0.8452, 0.8398,
         0.8371],
        [0.9387, 0.9341, 0.9330, 0.9323, 0.9322, 0.9319, 0.9283, 0.9278, 0.9262,
         0.9242],
        [0.8845, 0.8815, 0.8806, 0.8711, 0.8642, 0.8600, 0.8585, 0.8448, 0.8364,
         0.8293],
        [0.9029, 0.8658, 0.8633, 0.8625, 0.8590, 0.8566, 0.8554, 0.8551, 0.8511,
         0.8501],
        [0.8891, 0.8791, 0.8761, 0.8646, 0.8569, 0.8525, 0.8478, 0.8451, 0.8376,
         0.8372],
        [0.8850, 0.8755, 0.8630, 0.8601, 0.8527, 0.8516, 0.8475, 0.8468, 0.8437,
         0.8356],
        [0.8575, 0.8397, 0.8277, 0.8271, 0.8164, 0.8163, 0.8050, 0.7930, 0.7825,
         0.7798],
        [0.8329, 0.8110, 0.7976, 0.7891, 0.7833, 0.7817, 0.7789, 0.7657, 0.7611,
         0.7586],
        [0.9042, 0.8911, 0.8859, 0.8837, 0.8836, 0.8828, 0.8721, 0.8709, 0.8707,
         0.8701],
        [0.8120, 0.7885, 0.7856, 0.7801, 0.7660, 0.7642, 0.7543, 0.7520, 0.7384,
         0.7294],
        [0.8407, 0.8356, 0.8138, 0.8132, 0.8097, 0.8054, 0.8037, 0.8027, 0.7931,
         0.7925],
        [0.8961, 0.8670, 0.8513, 0.8429, 0.8370, 0.8353, 0.8276, 0.8267, 0.8214,
         0.8167],
        [0.8832, 0.8567, 0.8527, 0.8516, 0.8401, 0.8298, 0.8232, 0.8175, 0.8127,
         0.8091],
        [0.8917, 0.8884, 0.8506, 0.8245, 0.8216, 0.8192, 0.8090, 0.8050, 0.8037,
         0.8031],
        [0.9285, 0.9110, 0.9082, 0.8970, 0.8705, 0.8628, 0.8606, 0.8598, 0.8456,
         0.8452],
        [0.9146, 0.8761, 0.8730, 0.8631, 0.8585, 0.8566, 0.8501, 0.8453, 0.8445,
         0.8434],
        [0.9282, 0.9062, 0.9032, 0.9023, 0.8982, 0.8968, 0.8968, 0.8938, 0.8935,
         0.8930],
        [0.9050, 0.8842, 0.8820, 0.8718, 0.8705, 0.8696, 0.8686, 0.8663, 0.8661,
         0.8654],
        [0.9121, 0.8800, 0.8748, 0.8710, 0.8622, 0.8617, 0.8428, 0.8350, 0.8349,
         0.8293],
        [0.9415, 0.8827, 0.8753, 0.8680, 0.8599, 0.8542, 0.8400, 0.8390, 0.8383,
         0.8269],
        [0.9113, 0.8628, 0.8593, 0.8499, 0.8350, 0.8339, 0.8322, 0.8283, 0.8243,
         0.8238],
        [0.9013, 0.8229, 0.8206, 0.8099, 0.8002, 0.7994, 0.7977, 0.7950, 0.7902,
         0.7850],
        [0.8844, 0.8718, 0.8622, 0.8595, 0.8538, 0.8507, 0.8482, 0.8478, 0.8475,
         0.8461],
        [0.8565, 0.7823, 0.7820, 0.7785, 0.7703, 0.7361, 0.7358, 0.7307, 0.7302,
         0.7274],
        [0.8862, 0.8587, 0.8493, 0.8242, 0.8200, 0.8172, 0.8062, 0.8014, 0.8000,
         0.7959],
        [0.8409, 0.8137, 0.8013, 0.7862, 0.7813, 0.7809, 0.7716, 0.7608, 0.7563,
         0.7553]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 297435.8438,  188241.7031,  175268.0781,  165127.1406,  151995.9531,
          150951.9688,  148484.0312,  139373.7500,  131004.7109,  128738.0938],
        [ 719680.5625,  613809.1250,  604855.6250,  588539.0000,  569871.0000,
          568087.4375,  563936.9375,  548324.5000,  531604.3125,  529356.1250],
        [ 424789.3438,  387338.0312,  301409.8438,  267229.4688,  222789.7812,
          195646.5156,  190992.1250,  169358.3281,  162318.7500,  141317.6719],
        [ 851287.1250,  343446.6250,  326983.2500,  299204.3125,  296185.0312,
          292092.9688,  254287.1562,  249946.1719,  249777.9375,  236384.6719],
        [ 175231.1250,  158044.5938,  150288.3438,  117339.9531,  107990.4453,
          106642.2734,   97053.2500,   92232.7656,   83595.1875,   70717.3828],
        [ 204067.6406,  122490.7500,  106213.4375,   92571.2344,   88459.8906,
           81989.3281,   78092.6875,   67404.7734,   67289.6797,   59017.0352],
        [ 257306.4531,  250315.4531,  219614.5469,  205022.3125,  201144.0938,
          192029.6875,  191270.8125,  188729.9219,  180089.9062,  179688.6562],
        [ 374636.5625,  280383.3125,  256624.4531,  219957.2500,  196922.7812,
          159557.6719,  139754.5469,  128772.8438,  108602.8906,  107456.8516],
        [ 452132.0000,  406883.3125,  344218.8750,  323989.0938,  316262.7188,
          311772.8438,  310170.8125,  301168.2188,  297680.4688,  296576.5000],
        [ 782127.9375,  325269.2500,  318330.9688,  307647.8750,  283445.5312,
          272223.7188,  261249.7969,  260271.4688,  232652.1875,  227411.5312],
        [ 738464.3750,  410338.5938,  371374.9375,  366724.6250,  362535.1250,
          344212.9688,  319886.3750,  315286.1250,  310468.8125,  302216.6250],
        [ 351892.8438,  283380.6562,  217786.0312,  200547.0469,  146667.1094,
          102225.8359,   99039.0234,   94912.6641,   81927.1953,   74440.4219],
        [ 517240.0625,  435929.6562,  427792.0312,  365957.7500,  323539.5312,
          299849.0312,  266023.1875,  252402.3750,  239288.1250,  207416.2969],
        [ 909137.8750,  852502.5625,  841078.9375,  834025.1875,  798765.8750,
          788724.0000,  750345.3750,  738545.3750,  710076.0625,  665957.6250],
        [ 711132.5625,  591638.6250,  584508.3125,  575762.5625,  559247.5000,
          516067.9062,  509993.2500,  490010.6562,  480232.4375,  477397.2188],
        [ 565459.8750,  535576.6250,  530727.5000,  477742.9062,  462413.2500,
          458706.2500,  444595.0000,  427661.5312,  420991.5000,  405689.1562],
        [ 846085.3750,  779674.8750,  660763.8125,  657438.6875,  582565.5625,
          577313.0625,  528898.4375,  520127.8750,  498270.9688,  492534.5312],
        [ 955655.7500,  876119.6875,  813572.2500,  687328.7500,  678652.2500,
          656837.0625,  641607.3750,  637387.0000,  612121.4375,  610403.0000],
        [ 574114.3750,  501384.5938,  427539.1562,  384305.7812,  363192.2500,
          362232.7188,  348717.4375,  348233.2188,  329242.8125,  307514.6875],
        [ 425262.8125,  384291.1250,  381747.7188,  377800.0000,  373532.7500,
          371253.7812,  359249.3125,  343326.7812,  341463.5938,  339699.9062],
        [ 740124.0625,  653574.6250,  648534.3125,  629229.9375,  612442.6250,
          611926.5000,  590825.5625,  586573.8750,  569308.7500,  563272.5625],
        [ 565833.8125,  463670.0000,  455875.9062,  452747.7500,  441969.7500,
          434658.5625,  423690.0938,  410645.8750,  347202.5938,  336491.0938],
        [1034230.0625,  775372.6250,  712991.8750,  692576.8125,  633179.3750,
          593637.6250,  572081.1875,  507145.7500,  472722.6562,  472052.3125],
        [ 704310.3750,  644176.8750,  603658.1875,  568415.8125,  547903.1875,
          545815.5000,  544999.4375,  540730.8125,  533715.5000,  531424.3750],
        [ 855610.6250,  400289.1250,  356239.5000,  300431.2188,  248534.2656,
          247208.8594,  234764.8594,  233142.8281,  230134.0938,  216951.8594],
        [ 667384.3125,  622462.8750,  619139.7500,  560897.8750,  539599.1250,
          519138.2188,  491121.2500,  462531.4375,  422402.2812,  415994.3438],
        [ 562743.1875,  555420.0625,  542050.0000,  496210.5625,  450759.4688,
          399917.5000,  391501.9062,  384817.0000,  376579.1250,  373049.3125],
        [ 919160.0000,  774870.7500,  728166.6875,  583766.3125,  566368.7500,
          553252.5625,  517165.0938,  509992.7500,  499759.0938,  498464.4062],
        [ 738122.8750,  667504.0000,  579163.1875,  571577.8125,  563874.5625,
          550582.9375,  543441.8125,  506505.3750,  500174.4062,  485809.4062],
        [ 625321.1875,  580045.3750,  562055.0625,  559486.4375,  537560.0000,
          525921.8125,  508684.6562,  502334.6250,  481402.1875,  471296.5938],
        [ 761238.2500,  745112.0000,  741021.0000,  732452.1250,  729104.8125,
          716430.5625,  644883.1250,  621096.6250,  607101.9375,  588994.9375],
        [ 682532.3125,  528911.5625,  521762.8750,  464262.0312,  411886.4688,
          401224.3750,  386512.5625,  381443.1250,  380031.3750,  370875.1562],
        [ 282634.6562,  255469.2188,  249751.0312,  220729.5000,  211745.1719,
          198461.9844,  197852.5312,  195452.9531,  195391.4375,  190267.8438],
        [ 424187.3750,  361234.3750,  332965.9375,  319768.6562,  315119.8750,
          308987.2188,  298346.9375,  290753.0938,  289234.8125,  288372.4375],
        [ 528197.7500,  492915.5938,  486628.7500,  463744.7188,  408485.1562,
          385929.8750,  385407.5938,  352440.9375,  339967.2812,  332803.0625],
        [ 430077.0625,  385142.6875,  365736.5312,  316813.0312,  315477.7188,
          284628.4688,  282443.0938,  268624.3125,  225938.2969,  223143.6094],
        [ 759019.3125,  241278.3125,  238977.9844,  198569.1250,  189325.1562,
          159977.4531,  136788.4688,  131486.7188,  128006.8516,  119446.3203],
        [ 416583.9375,  326009.5938,  322056.9688,  295172.7188,  275976.0938,
          260261.2969,  228079.8281,  226328.6406,  212720.7500,  211011.3906],
        [ 362190.5625,  351674.7812,  271082.2812,  237948.4688,  200014.5312,
          196076.6875,  183377.7812,  175385.7812,  162296.7812,  156187.0938],
        [ 666263.8125,  624597.6250,  614648.5625,  608064.4375,  607529.3750,
          605009.0625,  574381.5625,  570798.8750,  557352.0000,  541555.5625],
        [ 307291.5938,  294260.3750,  290841.5938,  253746.9219,  229905.2969,
          216473.6250,  211958.1094,  174404.0312,  154705.6875,  139725.5000],
        [ 399957.9062,  235334.0000,  227125.6406,  224500.7344,  213580.7969,
          206304.8438,  202825.9062,  201955.5938,  190792.5938,  188055.4531],
        [ 328360.0938,  284508.5000,  272526.8750,  231435.5000,  207278.6719,
          194544.5000,  181844.2031,  175103.5000,  157311.2344,  156427.6719],
        [ 309384.6875,  270189.7812,  226186.2344,  217022.2188,  195225.5000,
          192139.7812,  181039.7812,  179261.7812,  171621.2500,  152818.9062],
        [ 208946.8125,  161981.7969,  136536.9219,  135371.8438,  116171.3750,
          115962.7344,   98705.5078,   83187.3672,   71559.3047,   68850.9297],
        [ 147129.4219,  107549.0156,   88846.2656,   78653.0234,   72378.9297,
           70812.0703,   68015.4453,   56267.8008,   52758.6367,   50864.2734],
        [ 407137.1562,  337524.3750,  313584.3438,  303744.0000,  303516.9688,
          299756.9375,  257365.5781,  252926.4688,  252195.2188,  250186.8125],
        [ 109053.1094,   77934.1406,   74824.2656,   69129.7656,   56555.5625,
           55104.9062,   47870.9062,   46302.1094,   38105.0625,   33514.1992],
        [ 164412.6562,  152761.7969,  111916.3906,  111060.6016,  105583.7578,
           99211.6406,   96886.5156,   95539.2891,   83327.6641,   82556.5234],
        [ 362709.4062,  239287.4531,  191175.6406,  169741.3906,  155848.5938,
          152245.7656,  136244.3906,  134684.1875,  124696.8203,  116668.8906],
        [ 301519.9688,  206640.1719,  195077.1562,  192038.1250,  163000.0625,
          140706.5938,  128115.5391,  118064.6328,  110253.9219,  104681.7969],
        [ 340769.0625,  325045.6562,  189455.9375,  130506.1719,  125152.8828,
          120957.1641,  104541.6250,   98656.1094,   96837.9219,   96096.6094],
        [ 576484.0000,  448548.0312,  430954.8750,  367567.0625,  251450.9688,
          225440.4531,  218556.6875,  216072.4531,  176430.2500,  175229.7969],
        [ 472289.6250,  272533.3438,  260742.7812,  226465.5156,  212025.0469,
          206414.6562,  188051.8750,  175615.2344,  173616.1094,  170857.1250],
        [ 573992.2500,  419029.1875,  401283.6562,  396183.6562,  374023.5938,
          366383.4062,  366383.4062,  351171.7188,  349285.2500,  346996.6875],
        [ 411657.9375,  306151.9688,  296451.7812,  256440.7188,  251490.0625,
          248268.4844,  245030.4531,  237130.6719,  236386.2344,  233825.2969],
        [ 456082.9062,  288253.4062,  267630.6562,  253390.9844,  223433.4219,
          221925.0156,  169303.9062,  151546.8281,  151404.3906,  139677.3906],
        [ 693570.2500,  299357.8750,  269669.5000,  242973.8594,  216234.0625,
          199415.1562,  162845.7656,  160451.7188,  158796.2344,  134925.1094],
        [ 450850.1875,  225383.2812,  214511.8281,  187448.1094,  151614.4844,
          149085.2188,  145669.4531,  137789.4062,  129972.2109,  129218.6719],
        [ 390695.8750,  127482.1484,  123346.2500,  105860.2188,   92221.8594,
           91120.1484,   88973.8750,   85516.8047,   79916.3984,   74150.6953],
        [ 306689.0625,  256353.9219,  223577.0781,  215114.3281,  198242.9375,
          189507.2500,  182979.3125,  182054.6875,  181262.9688,  177566.0938],
        [ 206116.8281,   71360.8516,   71115.9297,   67627.8828,   60127.2305,
           36900.6172,   36719.0547,   34131.9531,   33888.7891,   32571.1426],
        [ 315012.6250,  212574.5156,  185774.1562,  129808.0703,  122393.8281,
          117490.5625,  100396.1484,   93816.3906,   91886.2422,   86643.6797],
        [ 164842.2031,  111754.1719,   93640.8281,   75430.9062,   70399.7734,
           69924.7031,   61259.1875,   52524.4922,   49202.7227,   48515.8633]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[297435.8438,      0.0000],
         [188241.7031,      0.0000],
         [175268.0781,      0.0000],
         ...,
         [139373.7500,      0.0000],
         [131004.7109,      0.0000],
         [     0.0000, 128738.0938]],

        [[719680.5625,      0.0000],
         [613809.1250,      0.0000],
         [604855.6250,      0.0000],
         ...,
         [548324.5000,      0.0000],
         [531604.3125,      0.0000],
         [529356.1250,      0.0000]],

        [[424789.3438,      0.0000],
         [387338.0312,      0.0000],
         [301409.8438,      0.0000],
         ...,
         [169358.3281,      0.0000],
         [162318.7500,      0.0000],
         [141317.6719,      0.0000]],

        ...,

        [[     0.0000, 206116.8281],
         [ 71360.8516,      0.0000],
         [ 71115.9297,      0.0000],
         ...,
         [ 34131.9531,      0.0000],
         [ 33888.7891,      0.0000],
         [ 32571.1426,      0.0000]],

        [[315012.6250,      0.0000],
         [     0.0000, 212574.5156],
         [185774.1562,      0.0000],
         ...,
         [ 93816.3906,      0.0000],
         [ 91886.2422,      0.0000],
         [ 86643.6797,      0.0000]],

        [[     0.0000, 164842.2031],
         [     0.0000, 111754.1719],
         [     0.0000,  93640.8281],
         ...,
         [ 52524.4922,      0.0000],
         [     0.0000,  49202.7227],
         [     0.0000,  48515.8633]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1547883.1250,  128738.0938],
        [5838064.5000,       0.0000],
        [2463190.0000,       0.0000],
        [3399595.0000,       0.0000],
        [ 476696.1875,  682439.1250],
        [ 811731.8125,  155864.6562],
        [1696392.0000,  368819.8125],
        [1972669.2500,       0.0000],
        [3360854.7500,       0.0000],
        [3270630.5000,       0.0000],
        [3841508.5000,       0.0000],
        [1652818.8750,       0.0000],
        [3335438.0000,       0.0000],
        [7889158.5000,       0.0000],
        [5495991.0000,       0.0000],
        [4729563.5000,       0.0000],
        [6143673.5000,       0.0000],
        [7169684.0000,       0.0000],
        [3946477.0000,       0.0000],
        [3697627.7500,       0.0000],
        [6205813.0000,       0.0000],
        [4332785.5000,       0.0000],
        [6465990.5000,       0.0000],
        [5765150.0000,       0.0000],
        [3323307.5000,       0.0000],
        [5320671.5000,       0.0000],
        [4533048.0000,       0.0000],
        [6150966.0000,       0.0000],
        [5706756.0000,       0.0000],
        [5354108.0000,       0.0000],
        [6887435.0000,       0.0000],
        [4529442.0000,       0.0000],
        [2197756.2500,       0.0000],
        [2580885.0000,  648085.8125],
        [1897247.7500, 2279273.0000],
        [1102849.0000, 1995175.8750],
        [1171091.2500, 1131784.3750],
        [1460458.8750, 1313742.3750],
        [ 771707.7500, 1524527.0000],
        [5970201.0000,       0.0000],
        [2273312.7500,       0.0000],
        [2290433.2500,       0.0000],
        [2189340.7500,       0.0000],
        [2094890.0000,       0.0000],
        [ 270918.0625,  926356.5000],
        [ 342618.0938,  450656.7500],
        [2570800.7500,  407137.1562],
        [ 388395.0938,  219998.9375],
        [ 485747.6875,  617509.1250],
        [ 479378.3125, 1303924.2500],
        [1409137.5000,  250960.5156],
        [ 772748.5000,  855270.6875],
        [1653876.7500, 1432857.7500],
        [2146586.2500,  212025.0469],
        [3944733.0000,       0.0000],
        [2722833.5000,       0.0000],
        [1068029.0000, 1254619.8750],
        [1779673.7500,  758565.8125],
        [1038148.5625,  883394.2500],
        [ 597457.3125,  661827.0000],
        [1352061.7500,  761285.9375],
        [ 444443.4688,  206116.8281],
        [1003337.3750,  452458.8750],
        [ 122449.1953,  675045.6250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 371/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:09, 60.34s/it]  7%|▋         | 2/30 [01:01<11:48, 25.29s/it] 10%|█         | 3/30 [01:01<06:20, 14.08s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.82s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.91s/it] 20%|██        | 6/30 [01:04<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.39326651096344
Epoch 372/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:09, 58.27s/it]  7%|▋         | 2/30 [00:59<11:24, 24.44s/it] 10%|█         | 3/30 [00:59<06:07, 13.62s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.77s/it] 17%|█▋        | 5/30 [01:01<02:26,  5.88s/it] 20%|██        | 6/30 [01:02<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:04<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.3562819798787435
Epoch 373/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:47, 59.56s/it]  7%|▋         | 2/30 [01:00<11:47, 25.27s/it] 10%|█         | 3/30 [01:01<06:19, 14.07s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.81s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.91s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.377619727452596
Epoch 374/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:18, 60.62s/it]  7%|▋         | 2/30 [01:01<11:51, 25.40s/it] 10%|█         | 3/30 [01:02<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3656189759572346
Epoch 375/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:58, 59.96s/it]  7%|▋         | 2/30 [01:00<11:43, 25.13s/it] 10%|█         | 3/30 [01:01<06:17, 14.00s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.77s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.398244611422221
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0239,  0.0020,  0.0121,  ..., -0.0042,  0.0146,  0.0040],
        [-0.0022,  0.0114,  0.0276,  ...,  0.0164,  0.0017, -0.0158],
        [-0.0296, -0.0382,  0.0247,  ...,  0.0815, -0.0064, -0.0156],
        ...,
        [ 0.0078, -0.0034,  0.0083,  ..., -0.0233, -0.0075, -0.0061],
        [-0.0322,  0.0084, -0.0045,  ...,  0.0045,  0.0162, -0.0115],
        [-0.0343, -0.0132,  0.0182,  ...,  0.0403,  0.0348, -0.0312]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8814, 0.8503, 0.8445, 0.8406, 0.8343, 0.8342, 0.8328, 0.8284, 0.8243,
         0.8228],
        [0.9438, 0.9325, 0.9314, 0.9298, 0.9274, 0.9271, 0.9264, 0.9246, 0.9222,
         0.9220],
        [0.9070, 0.9003, 0.8823, 0.8744, 0.8614, 0.8525, 0.8507, 0.8423, 0.8393,
         0.8300],
        [0.9559, 0.8915, 0.8881, 0.8822, 0.8811, 0.8807, 0.8704, 0.8692, 0.8691,
         0.8653],
        [0.8449, 0.8371, 0.8341, 0.8167, 0.8106, 0.8099, 0.8035, 0.7998, 0.7929,
         0.7814],
        [0.8554, 0.8198, 0.8099, 0.8001, 0.7962, 0.7921, 0.7879, 0.7781, 0.7771,
         0.7685],
        [0.8712, 0.8692, 0.8602, 0.8557, 0.8541, 0.8509, 0.8508, 0.8490, 0.8463,
         0.8457],
        [0.8976, 0.8774, 0.8713, 0.8605, 0.8528, 0.8376, 0.8290, 0.8229, 0.8110,
         0.8099],
        [0.9111, 0.9038, 0.8918, 0.8875, 0.8861, 0.8852, 0.8850, 0.8826, 0.8819,
         0.8817],
        [0.9498, 0.8879, 0.8864, 0.8840, 0.8781, 0.8754, 0.8724, 0.8721, 0.8645,
         0.8629],
        [0.9457, 0.9046, 0.8971, 0.8959, 0.8953, 0.8918, 0.8867, 0.8859, 0.8842,
         0.8825],
        [0.8933, 0.8780, 0.8598, 0.8538, 0.8317, 0.8062, 0.8039, 0.8018, 0.7909,
         0.7844],
        [0.9200, 0.9086, 0.9070, 0.8960, 0.8871, 0.8820, 0.8736, 0.8698, 0.8664,
         0.8564],
        [0.9600, 0.9558, 0.9545, 0.9540, 0.9511, 0.9500, 0.9466, 0.9454, 0.9427,
         0.9382],
        [0.9431, 0.9301, 0.9292, 0.9281, 0.9260, 0.9205, 0.9197, 0.9169, 0.9155,
         0.9148],
        [0.9271, 0.9232, 0.9225, 0.9152, 0.9129, 0.9123, 0.9101, 0.9074, 0.9063,
         0.9038],
        [0.9550, 0.9492, 0.9377, 0.9371, 0.9288, 0.9282, 0.9220, 0.9209, 0.9178,
         0.9169],
        [0.9637, 0.9576, 0.9525, 0.9407, 0.9399, 0.9374, 0.9357, 0.9352, 0.9327,
         0.9322],
        [0.9279, 0.9182, 0.9068, 0.9000, 0.8959, 0.8956, 0.8931, 0.8929, 0.8889,
         0.8845],
        [0.9067, 0.8999, 0.8993, 0.8987, 0.8977, 0.8975, 0.8950, 0.8915, 0.8913,
         0.8912],
        [0.9454, 0.9368, 0.9362, 0.9340, 0.9324, 0.9324, 0.9295, 0.9293, 0.9273,
         0.9263],
        [0.9270, 0.9130, 0.9116, 0.9116, 0.9093, 0.9083, 0.9068, 0.9045, 0.8926,
         0.8902],
        [0.9692, 0.9492, 0.9432, 0.9410, 0.9349, 0.9301, 0.9276, 0.9193, 0.9143,
         0.9142],
        [0.9423, 0.9360, 0.9315, 0.9274, 0.9247, 0.9246, 0.9243, 0.9239, 0.9230,
         0.9227],
        [0.9559, 0.9026, 0.8941, 0.8826, 0.8691, 0.8683, 0.8651, 0.8644, 0.8636,
         0.8597],
        [0.9384, 0.9336, 0.9331, 0.9262, 0.9237, 0.9208, 0.9169, 0.9127, 0.9064,
         0.9054],
        [0.9264, 0.9254, 0.9236, 0.9176, 0.9108, 0.9025, 0.9008, 0.8998, 0.8983,
         0.8976],
        [0.9610, 0.9491, 0.9446, 0.9290, 0.9270, 0.9255, 0.9207, 0.9197, 0.9185,
         0.9181],
        [0.9455, 0.9385, 0.9283, 0.9275, 0.9269, 0.9249, 0.9239, 0.9193, 0.9179,
         0.9159],
        [0.9339, 0.9287, 0.9266, 0.9263, 0.9234, 0.9217, 0.9194, 0.9187, 0.9157,
         0.9143],
        [0.9475, 0.9463, 0.9459, 0.9451, 0.9446, 0.9435, 0.9361, 0.9334, 0.9319,
         0.9298],
        [0.9400, 0.9223, 0.9213, 0.9130, 0.9046, 0.9030, 0.9001, 0.8993, 0.8990,
         0.8975],
        [0.8781, 0.8713, 0.8695, 0.8610, 0.8583, 0.8533, 0.8530, 0.8524, 0.8524,
         0.8509],
        [0.9063, 0.8952, 0.8895, 0.8869, 0.8856, 0.8842, 0.8820, 0.8801, 0.8797,
         0.8797],
        [0.9220, 0.9172, 0.9164, 0.9127, 0.9039, 0.9005, 0.8999, 0.8936, 0.8910,
         0.8897],
        [0.9077, 0.8997, 0.8963, 0.8865, 0.8858, 0.8788, 0.8783, 0.8744, 0.8625,
         0.8615],
        [0.9477, 0.8676, 0.8665, 0.8536, 0.8501, 0.8378, 0.8270, 0.8238, 0.8226,
         0.8177],
        [0.9050, 0.8877, 0.8867, 0.8806, 0.8767, 0.8725, 0.8627, 0.8623, 0.8582,
         0.8576],
        [0.8956, 0.8933, 0.8749, 0.8659, 0.8534, 0.8525, 0.8476, 0.8442, 0.8391,
         0.8366],
        [0.9382, 0.9338, 0.9326, 0.9319, 0.9315, 0.9315, 0.9279, 0.9274, 0.9257,
         0.9239],
        [0.8840, 0.8809, 0.8802, 0.8704, 0.8638, 0.8595, 0.8578, 0.8442, 0.8358,
         0.8287],
        [0.9026, 0.8652, 0.8631, 0.8620, 0.8582, 0.8559, 0.8543, 0.8543, 0.8505,
         0.8495],
        [0.8886, 0.8784, 0.8753, 0.8641, 0.8563, 0.8518, 0.8469, 0.8444, 0.8372,
         0.8365],
        [0.8839, 0.8751, 0.8623, 0.8591, 0.8522, 0.8507, 0.8472, 0.8462, 0.8434,
         0.8354],
        [0.8566, 0.8392, 0.8266, 0.8262, 0.8155, 0.8154, 0.8041, 0.7917, 0.7812,
         0.7785],
        [0.8325, 0.8103, 0.7967, 0.7887, 0.7829, 0.7816, 0.7786, 0.7655, 0.7610,
         0.7585],
        [0.9036, 0.8899, 0.8850, 0.8825, 0.8824, 0.8818, 0.8708, 0.8698, 0.8696,
         0.8688],
        [0.8112, 0.7880, 0.7848, 0.7792, 0.7652, 0.7641, 0.7529, 0.7507, 0.7375,
         0.7282],
        [0.8396, 0.8348, 0.8133, 0.8129, 0.8089, 0.8050, 0.8030, 0.8027, 0.7928,
         0.7916],
        [0.8955, 0.8664, 0.8503, 0.8416, 0.8363, 0.8347, 0.8268, 0.8260, 0.8207,
         0.8163],
        [0.8823, 0.8564, 0.8520, 0.8511, 0.8396, 0.8294, 0.8231, 0.8168, 0.8124,
         0.8085],
        [0.8912, 0.8880, 0.8499, 0.8234, 0.8205, 0.8177, 0.8083, 0.8041, 0.8030,
         0.8021],
        [0.9280, 0.9105, 0.9074, 0.8965, 0.8697, 0.8623, 0.8602, 0.8589, 0.8452,
         0.8442],
        [0.9142, 0.8758, 0.8727, 0.8628, 0.8577, 0.8562, 0.8492, 0.8449, 0.8438,
         0.8427],
        [0.9276, 0.9055, 0.9025, 0.9017, 0.8978, 0.8962, 0.8962, 0.8931, 0.8930,
         0.8922],
        [0.9043, 0.8837, 0.8816, 0.8713, 0.8699, 0.8690, 0.8677, 0.8656, 0.8656,
         0.8645],
        [0.9113, 0.8796, 0.8740, 0.8703, 0.8614, 0.8614, 0.8427, 0.8343, 0.8339,
         0.8286],
        [0.9412, 0.8822, 0.8748, 0.8673, 0.8594, 0.8539, 0.8395, 0.8382, 0.8374,
         0.8260],
        [0.9107, 0.8624, 0.8588, 0.8492, 0.8343, 0.8335, 0.8312, 0.8278, 0.8231,
         0.8230],
        [0.9010, 0.8221, 0.8199, 0.8090, 0.7991, 0.7985, 0.7969, 0.7941, 0.7895,
         0.7839],
        [0.8843, 0.8711, 0.8615, 0.8588, 0.8530, 0.8494, 0.8472, 0.8469, 0.8467,
         0.8458],
        [0.8561, 0.7814, 0.7811, 0.7781, 0.7695, 0.7359, 0.7350, 0.7300, 0.7293,
         0.7271],
        [0.8857, 0.8582, 0.8483, 0.8239, 0.8190, 0.8166, 0.8046, 0.8003, 0.7988,
         0.7945],
        [0.8406, 0.8128, 0.8004, 0.7854, 0.7808, 0.7797, 0.7712, 0.7598, 0.7555,
         0.7539]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 293997.5312,  188444.3125,  173551.3906,  164045.5312,  150106.7031,
          149886.1250,  146866.0000,  137850.9219,  129982.6172,  127236.1875],
        [ 716943.8750,  610266.8125,  600500.1250,  586871.0000,  567135.1875,
          565062.6250,  559325.8750,  544732.8125,  526841.4375,  525325.8125],
        [ 423679.9688,  385010.4688,  298022.7812,  265920.6875,  221075.4062,
          194492.5781,  189692.9531,  168212.4062,  161192.1875,  141040.0469],
        [ 852071.8125,  339757.2500,  323524.7188,  297393.3125,  292670.7188,
          291181.2812,  251408.2812,  246978.3906,  246573.5938,  233764.2031],
        [ 174561.2812,  156175.7656,  149663.8750,  116674.4531,  106886.4297,
          105879.0938,   96655.8828,   91633.6875,   83043.5000,   70481.8672],
        [ 202839.4375,  121946.3125,  105834.8828,   91986.0156,   87022.4453,
           82077.8125,   77275.8281,   67202.8438,   66279.3047,   58631.0781],
        [ 254142.4219,  246923.5156,  217093.0312,  203750.8594,  199085.8438,
          190168.6094,  189865.9688,  185046.0156,  178043.4062,  176591.5000],
        [ 370619.5312,  277798.3750,  254453.8125,  218224.0938,  195344.3125,
          157373.9531,  139124.7500,  127522.1484,  107570.6562,  105880.2031],
        [ 449479.4062,  405061.6875,  341352.2500,  320935.7188,  314533.5000,
          310277.9062,  309764.3438,  298901.4375,  295953.5000,  295107.4062],
        [ 780691.1875,  322599.8125,  315776.5938,  305045.0625,  280534.9688,
          269902.8750,  258475.6719,  257630.5625,  230879.5312,  225786.0156],
        [ 736713.5625,  409371.5625,  368077.7812,  361839.4688,  358549.0312,
          340905.5625,  317287.7500,  313675.2812,  306108.7500,  298473.8750],
        [ 348484.7188,  280156.6250,  215918.3750,  198114.4062,  144510.0781,
          100460.7969,   97225.5547,   94299.0625,   80747.1562,   73532.0625],
        [ 510238.4375,  433894.4062,  423857.4062,  362173.6562,  319081.0938,
          296750.4688,  263042.2812,  249229.4844,  237364.6250,  205639.3281],
        [ 903535.1875,  850857.8125,  835821.5000,  829515.6250,  795305.8750,
          783494.8750,  746723.9375,  733874.2500,  705949.1875,  661861.8125],
        [ 709969.0625,  589451.8125,  581959.1875,  572782.6875,  555809.0000,
          514277.0938,  508412.0938,  488245.5000,  478588.3750,  473913.4375],
        [ 564570.3125,  534167.1875,  529029.1250,  476226.2812,  461235.5312,
          457022.0625,  442992.2188,  426337.2812,  419905.2812,  404758.1562],
        [ 841231.3750,  774432.6875,  657266.8750,  651208.5625,  578923.5625,
          573644.1875,  524805.0000,  517264.2500,  494797.6250,  488518.4688],
        [ 952277.7500,  872860.5625,  812018.8125,  686066.7500,  678614.6875,
          653958.0625,  638973.6875,  634335.5625,  611947.5625,  607890.4375],
        [ 571713.5625,  497195.8125,  422669.0625,  383330.2812,  361632.4688,
          360347.6562,  347330.0938,  346306.0938,  327256.2188,  307386.5625],
        [ 422279.0312,  383022.5938,  379844.4375,  376632.2812,  371269.7500,
          369891.7812,  356836.9375,  339614.4062,  338798.5625,  338084.9375],
        [ 733420.1875,  648539.1875,  643430.2500,  623771.3750,  609379.8750,
          608945.9375,  584838.3750,  582542.8125,  566252.6875,  558317.0625],
        [ 564331.8125,  461926.1875,  452663.5312,  452489.5938,  437983.6250,
          431731.9375,  422865.8125,  408891.6562,  345088.5938,  333641.0938],
        [1030196.9375,  774355.8125,  710778.6250,  688859.6875,  630999.0625,
          589675.0000,  568934.8125,  505412.4375,  470179.1875,  470081.4688],
        [ 701938.6250,  641535.1250,  601809.0000,  567647.6250,  545419.5000,
          545061.8125,  542833.7500,  539658.7500,  532421.1875,  530730.5000],
        [ 851809.3750,  397858.7188,  352559.9375,  298967.5625,  246628.1562,
          243779.2500,  232830.1875,  230659.4531,  227869.3750,  215653.3281],
        [ 663807.4375,  620005.9375,  615057.2500,  557880.0625,  538207.8750,
          515885.3438,  488463.0000,  459949.4062,  420003.7812,  414465.8125],
        [ 558939.3125,  551252.7500,  537022.5000,  493241.0000,  447245.2188,
          397435.1250,  388145.5938,  382305.8750,  374185.5938,  370407.5000],
        [ 916929.1875,  773324.8750,  724868.0000,  580075.2500,  563686.3750,
          551945.0625,  515715.6250,  508024.8750,  499715.2500,  496568.9062],
        [ 734232.0000,  664338.8125,  574981.1250,  567790.5625,  562905.3125,
          547701.0000,  539768.9375,  505112.2500,  495604.2500,  480994.6875],
        [ 622335.2500,  577541.6250,  560603.2500,  558750.1250,  535795.2500,
          522708.6250,  506059.7188,  500907.1250,  479636.5000,  470693.8438],
        [ 756141.0000,  742749.5000,  738998.4375,  730391.5625,  725503.5625,
          713508.8125,  642283.8125,  618410.3750,  604823.9375,  586909.6250],
        [ 678695.0000,  527392.9375,  520069.3438,  461507.9062,  409582.4688,
          400441.8750,  384327.7812,  379432.7812,  378090.8438,  369819.4688],
        [ 280528.0000,  254556.2500,  247992.7969,  219792.4375,  211366.4688,
          196682.0000,  195943.7812,  194290.3125,  194271.5938,  190115.1094],
        [ 419731.5000,  358033.4375,  330075.5938,  317838.6562,  312006.3125,
          305962.2500,  296708.9062,  288480.5312,  286860.5312,  286849.5938],
        [ 524801.0625,  490176.0625,  484441.8125,  459670.5000,  405611.7500,
          386196.8125,  382839.6250,  349907.7188,  337281.0938,  330863.5000],
        [ 428096.9375,  381942.8750,  363571.3750,  316196.0625,  312968.5938,
          283250.1562,  281358.3125,  265992.2188,  224473.1250,  221190.5625],
        [ 758340.6250,  241452.3125,  237817.1094,  197610.0312,  187943.4062,
          157655.9219,  135138.3750,  129059.1875,  126914.6719,  118325.2422],
        [ 411649.6562,  321578.4688,  317337.9688,  290724.8438,  275124.6562,
          258910.3750,  225009.1719,  223666.0156,  211212.3281,  209144.9844],
        [ 360087.9688,  348542.2188,  267840.0312,  235504.4062,  197054.4844,
          194639.1562,  181387.3125,  172893.9062,  160782.5625,  155127.9219],
        [ 662104.8125,  621238.1875,  610629.4375,  604575.3750,  601394.1875,
          601274.9375,  571536.9375,  567529.6250,  553624.1250,  539244.6250],
        [ 305131.1875,  291822.8750,  288929.0625,  251427.0000,  228569.3281,
          214901.9062,  210006.4062,  172940.0781,  153289.4844,  138570.4531],
        [ 398032.9375,  233361.2656,  226544.1719,  222704.3906,  210967.7344,
          204127.2031,  199723.6562,  199572.6719,  189217.9531,  186425.1562],
        [ 325814.7188,  281753.8438,  269640.4375,  229800.0781,  205430.5625,
          192601.0156,  179557.2500,  173275.2031,  156403.2188,  154779.7656],
        [ 304765.6562,  268732.9688,  223960.7969,  213739.1094,  193810.6250,
          189697.1250,  180289.6094,  177901.3594,  170951.1875,  152427.3594],
        [ 206405.0000,  160985.8594,  134340.0156,  133610.2031,  114746.4609,
          114526.2812,   97429.2109,   81587.9375,   70277.1562,   67564.9062],
        [ 146260.5000,  106554.5391,   87725.5547,   78235.9609,   72010.7891,
           70648.2891,   67686.5312,   56118.2852,   52615.2852,   50804.7852],
        [ 403595.6562,  332109.0625,  309533.6875,  298550.1875,  298410.1250,
          295788.4375,  252769.5000,  249073.3750,  248380.2500,  245767.4062],
        [ 107815.0938,   77391.6172,   73937.5859,   68288.4219,   55901.7891,
           55042.0938,   46884.9141,   45415.1211,   37647.6211,   32955.4258],
        [ 161850.2344,  151127.2656,  111133.1797,  110540.7109,  104437.2969,
           98746.1875,   95940.2891,   95526.0703,   82892.2109,   81527.1953],
        [ 359794.7812,  237190.8281,  188585.9844,  166566.1094,  154437.0938,
          150824.0469,  134754.4688,  133236.1094,  123581.3750,  116004.2109],
        [ 297770.7500,  205634.2344,  193160.1875,  190834.4531,  161775.3906,
          139853.3438,  127776.6953,  116769.6250,  109680.3750,  103803.9844],
        [ 338322.0000,  322875.9062,  187609.9531,  128382.1797,  123142.5547,
          118333.3672,  103461.3281,   97429.3906,   95944.0469,   94660.9141],
        [ 572363.3125,  445595.0625,  426275.8750,  364867.6875,  248800.8281,
          223857.6406,  217076.0312,  213242.1406,  175412.8750,  172873.4688],
        [ 469773.1562,  271524.6875,  259478.8906,  225290.6719,  209503.3125,
          205206.1875,  185502.5781,  174648.3750,  171904.7969,  169209.4844],
        [ 569191.5000,  414978.7812,  397557.1562,  392760.6562,  371911.1562,
          363430.3125,  363430.3125,  347538.8438,  346888.8125,  343106.1875],
        [ 408017.5625,  303978.4375,  294839.3125,  254569.5781,  249585.0625,
          246369.0938,  241909.3750,  234728.8281,  234515.1406,  230975.9844],
        [ 450976.1875,  286651.0625,  264400.1562,  251025.9219,  221104.7031,
          220864.0469,  169048.5156,  149961.6250,  149148.3750,  138336.3594],
        [ 691133.1875,  297288.1250,  267571.4375,  240487.6094,  214612.2969,
          198475.7812,  161536.8906,  158513.9062,  156799.1406,  133170.4375],
        [ 446969.7500,  224089.1875,  212829.5000,  185575.8438,  150049.3125,
          148291.7031,  143539.5312,  136749.5938,  127809.1094,  127582.4844],
        [ 389168.2188,  126013.1562,  122050.5547,  104512.6250,   90773.3906,
           89999.3281,   87949.1328,   84430.9219,   79065.1562,   73049.0078],
        [ 306278.9688,  253844.0156,  221286.3438,  213013.8906,  195983.4062,
          186108.6094,  180484.8438,  179669.2812,  179121.3125,  176708.4219],
        [ 204947.4219,   70497.3984,   70171.2031,   67265.0469,   59456.7734,
           36807.9336,   36298.3281,   33827.3125,   33491.1953,   32447.5293],
        [ 312842.3750,  211054.8594,  183246.8438,  129350.8359,  120582.1562,
          116491.7812,   98170.8750,   92266.4688,   90343.1328,   84980.2344],
        [ 164076.5156,  110388.9062,   92441.2031,   74583.7500,   69895.0391,
           68743.6562,   60876.3281,   51750.8750,   48647.7695,   47596.6797]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[293997.5312,      0.0000],
         [188444.3125,      0.0000],
         [173551.3906,      0.0000],
         ...,
         [137850.9219,      0.0000],
         [129982.6172,      0.0000],
         [     0.0000, 127236.1875]],

        [[716943.8750,      0.0000],
         [610266.8125,      0.0000],
         [600500.1250,      0.0000],
         ...,
         [544732.8125,      0.0000],
         [526841.4375,      0.0000],
         [525325.8125,      0.0000]],

        [[423679.9688,      0.0000],
         [385010.4688,      0.0000],
         [298022.7812,      0.0000],
         ...,
         [168212.4062,      0.0000],
         [161192.1875,      0.0000],
         [141040.0469,      0.0000]],

        ...,

        [[     0.0000, 204947.4219],
         [ 70497.3984,      0.0000],
         [ 70171.2031,      0.0000],
         ...,
         [ 33827.3125,      0.0000],
         [ 33491.1953,      0.0000],
         [ 32447.5293,      0.0000]],

        [[312842.3750,      0.0000],
         [     0.0000, 211054.8594],
         [183246.8438,      0.0000],
         ...,
         [ 92266.4688,      0.0000],
         [ 90343.1328,      0.0000],
         [ 84980.2344,      0.0000]],

        [[     0.0000, 164076.5156],
         [     0.0000, 110388.9062],
         [     0.0000,  92441.2031],
         ...,
         [ 51750.8750,      0.0000],
         [     0.0000,  48647.7695],
         [     0.0000,  47596.6797]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1534731.2500,  127236.1875],
        [5803006.0000,       0.0000],
        [2448339.5000,       0.0000],
        [3375323.5000,       0.0000],
        [ 473742.1250,  677913.6875],
        [ 806870.6250,  154225.2812],
        [1679073.6250,  361637.5000],
        [1953911.7500,       0.0000],
        [3341367.0000,       0.0000],
        [3247322.0000,       0.0000],
        [3811002.7500,       0.0000],
        [1633449.0000,       0.0000],
        [3301271.0000,       0.0000],
        [7846940.5000,       0.0000],
        [5473408.0000,       0.0000],
        [4716243.5000,       0.0000],
        [6102093.0000,       0.0000],
        [7148944.0000,       0.0000],
        [3925168.0000,       0.0000],
        [3676274.7500,       0.0000],
        [6159438.0000,       0.0000],
        [4311614.0000,       0.0000],
        [6439473.0000,       0.0000],
        [5749056.0000,       0.0000],
        [3298615.5000,       0.0000],
        [5293725.5000,       0.0000],
        [4500180.5000,       0.0000],
        [6130853.0000,       0.0000],
        [5673429.0000,       0.0000],
        [5335031.0000,       0.0000],
        [6859721.0000,       0.0000],
        [4509360.0000,       0.0000],
        [2185538.7500,       0.0000],
        [2560465.2500,  642081.8750],
        [1885333.7500, 2266456.2500],
        [1093598.7500, 1985441.2500],
        [1161404.7500, 1128852.1250],
        [1445280.7500, 1299077.6250],
        [ 763820.0000, 1510039.8750],
        [5933152.5000,       0.0000],
        [2255587.7500,       0.0000],
        [2270677.0000,       0.0000],
        [2169056.0000,       0.0000],
        [2076275.8750,       0.0000],
        [ 266611.5625,  914861.4375],
        [ 341196.8750,  447463.6562],
        [2530382.0000,  403595.6562],
        [ 383442.8125,  217836.9062],
        [ 482033.5938,  611687.0625],
        [ 475799.0000, 1289176.0000],
        [1397525.2500,  249533.7188],
        [ 761353.7500,  848807.8125],
        [1639928.8750, 1420436.0000],
        [2132538.7500,  209503.3125],
        [3910793.7500,       0.0000],
        [2699488.5000,       0.0000],
        [1057891.7500, 1243625.2500],
        [1767200.6250,  752388.0625],
        [1027935.5000,  875550.6250],
        [ 590282.3125,  656729.1875],
        [1336392.7500,  756106.3750],
        [ 440262.7500,  204947.4219],
        [ 991200.8125,  448128.7812],
        [ 120494.5312,  668506.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 376/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:02, 62.17s/it]  7%|▋         | 2/30 [01:02<12:09, 26.04s/it] 10%|█         | 3/30 [01:03<06:31, 14.49s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.07s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Epoch loss is 2.362511134147644
Epoch 377/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:13, 62.55s/it]  7%|▋         | 2/30 [01:03<12:13, 26.20s/it] 10%|█         | 3/30 [01:04<06:33, 14.58s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.12s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.10s/it] 20%|██        | 6/30 [01:06<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:07<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 2.3939190467198688
Epoch 378/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:12, 60.45s/it]  7%|▋         | 2/30 [01:01<11:53, 25.49s/it] 10%|█         | 3/30 [01:02<06:23, 14.19s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.88s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.95s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.397220770517985
Epoch 379/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:56, 59.89s/it]  7%|▋         | 2/30 [01:00<11:42, 25.10s/it] 10%|█         | 3/30 [01:01<06:17, 13.98s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.76s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:03<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.365359965960185
Epoch 380/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:26, 62.98s/it]  7%|▋         | 2/30 [01:03<12:18, 26.37s/it] 10%|█         | 3/30 [01:04<06:36, 14.68s/it] 13%|█▎        | 4/30 [01:05<03:58,  9.18s/it] 17%|█▋        | 5/30 [01:05<02:33,  6.14s/it] 20%|██        | 6/30 [01:06<01:43,  4.31s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.15s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.38s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  1.33it/s]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Epoch loss is 2.3807427406311037
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0239,  0.0019,  0.0122,  ..., -0.0045,  0.0149,  0.0040],
        [-0.0022,  0.0111,  0.0277,  ...,  0.0164,  0.0020, -0.0157],
        [-0.0296, -0.0382,  0.0248,  ...,  0.0816, -0.0061, -0.0154],
        ...,
        [ 0.0076, -0.0035,  0.0085,  ..., -0.0234, -0.0072, -0.0057],
        [-0.0323,  0.0085, -0.0044,  ...,  0.0046,  0.0163, -0.0115],
        [-0.0341, -0.0133,  0.0183,  ...,  0.0404,  0.0350, -0.0309]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8806, 0.8503, 0.8438, 0.8401, 0.8340, 0.8333, 0.8321, 0.8276, 0.8238,
         0.8221],
        [0.9435, 0.9321, 0.9309, 0.9296, 0.9272, 0.9266, 0.9259, 0.9241, 0.9216,
         0.9216],
        [0.9068, 0.8999, 0.8816, 0.8740, 0.8609, 0.8521, 0.8502, 0.8418, 0.8389,
         0.8299],
        [0.9559, 0.8909, 0.8874, 0.8818, 0.8805, 0.8803, 0.8697, 0.8684, 0.8682,
         0.8647],
        [0.8447, 0.8363, 0.8338, 0.8164, 0.8099, 0.8094, 0.8032, 0.7994, 0.7924,
         0.7814],
        [0.8550, 0.8195, 0.8097, 0.7997, 0.7951, 0.7922, 0.7872, 0.7780, 0.7760,
         0.7682],
        [0.8704, 0.8684, 0.8595, 0.8553, 0.8535, 0.8503, 0.8503, 0.8477, 0.8457,
         0.8443],
        [0.8969, 0.8769, 0.8708, 0.8600, 0.8523, 0.8368, 0.8287, 0.8223, 0.8104,
         0.8089],
        [0.9108, 0.9035, 0.8912, 0.8870, 0.8858, 0.8849, 0.8849, 0.8820, 0.8815,
         0.8814],
        [0.9496, 0.8873, 0.8859, 0.8834, 0.8775, 0.8749, 0.8717, 0.8716, 0.8640,
         0.8625],
        [0.9456, 0.9044, 0.8966, 0.8950, 0.8946, 0.8911, 0.8862, 0.8856, 0.8833,
         0.8821],
        [0.8926, 0.8773, 0.8593, 0.8530, 0.8307, 0.8052, 0.8028, 0.8014, 0.7900,
         0.7836],
        [0.9191, 0.9083, 0.9064, 0.8953, 0.8862, 0.8813, 0.8729, 0.8690, 0.8659,
         0.8558],
        [0.9596, 0.9556, 0.9542, 0.9536, 0.9507, 0.9495, 0.9463, 0.9450, 0.9423,
         0.9378],
        [0.9429, 0.9298, 0.9289, 0.9277, 0.9256, 0.9203, 0.9195, 0.9166, 0.9152,
         0.9143],
        [0.9270, 0.9230, 0.9223, 0.9150, 0.9127, 0.9120, 0.9099, 0.9072, 0.9061,
         0.9036],
        [0.9546, 0.9488, 0.9374, 0.9364, 0.9284, 0.9278, 0.9214, 0.9205, 0.9173,
         0.9164],
        [0.9634, 0.9573, 0.9524, 0.9405, 0.9399, 0.9370, 0.9355, 0.9349, 0.9327,
         0.9320],
        [0.9277, 0.9176, 0.9060, 0.8998, 0.8956, 0.8953, 0.8928, 0.8925, 0.8885,
         0.8845],
        [0.9062, 0.8997, 0.8990, 0.8985, 0.8973, 0.8972, 0.8945, 0.8909, 0.8908,
         0.8908],
        [0.9448, 0.9363, 0.9357, 0.9335, 0.9321, 0.9320, 0.9289, 0.9288, 0.9269,
         0.9259],
        [0.9268, 0.9127, 0.9115, 0.9111, 0.9086, 0.9078, 0.9067, 0.9042, 0.8922,
         0.8896],
        [0.9689, 0.9491, 0.9430, 0.9406, 0.9346, 0.9297, 0.9273, 0.9191, 0.9140,
         0.9139],
        [0.9421, 0.9358, 0.9313, 0.9273, 0.9245, 0.9243, 0.9241, 0.9238, 0.9231,
         0.9224],
        [0.9556, 0.9022, 0.8934, 0.8822, 0.8686, 0.8674, 0.8645, 0.8637, 0.8629,
         0.8593],
        [0.9380, 0.9333, 0.9326, 0.9258, 0.9235, 0.9203, 0.9166, 0.9124, 0.9059,
         0.9052],
        [0.9259, 0.9249, 0.9229, 0.9172, 0.9103, 0.9020, 0.9003, 0.8993, 0.8979,
         0.8971],
        [0.9608, 0.9489, 0.9442, 0.9285, 0.9266, 0.9253, 0.9205, 0.9194, 0.9185,
         0.9178],
        [0.9451, 0.9381, 0.9279, 0.9270, 0.9267, 0.9246, 0.9235, 0.9191, 0.9174,
         0.9153],
        [0.9335, 0.9284, 0.9264, 0.9263, 0.9232, 0.9213, 0.9191, 0.9185, 0.9154,
         0.9142],
        [0.9471, 0.9461, 0.9457, 0.9449, 0.9443, 0.9432, 0.9358, 0.9331, 0.9316,
         0.9295],
        [0.9396, 0.9221, 0.9211, 0.9125, 0.9042, 0.9029, 0.8998, 0.8989, 0.8986,
         0.8972],
        [0.8776, 0.8710, 0.8690, 0.8607, 0.8581, 0.8526, 0.8524, 0.8520, 0.8520,
         0.8508],
        [0.9056, 0.8946, 0.8889, 0.8864, 0.8848, 0.8836, 0.8816, 0.8795, 0.8794,
         0.8791],
        [0.9215, 0.9168, 0.9160, 0.9121, 0.9034, 0.9005, 0.8995, 0.8931, 0.8905,
         0.8893],
        [0.9073, 0.8991, 0.8958, 0.8863, 0.8852, 0.8785, 0.8780, 0.8737, 0.8620,
         0.8609],
        [0.9477, 0.8676, 0.8662, 0.8532, 0.8496, 0.8368, 0.8262, 0.8226, 0.8221,
         0.8174],
        [0.9042, 0.8867, 0.8858, 0.8797, 0.8766, 0.8722, 0.8617, 0.8613, 0.8578,
         0.8569],
        [0.8952, 0.8928, 0.8741, 0.8652, 0.8524, 0.8520, 0.8469, 0.8433, 0.8386,
         0.8363],
        [0.9378, 0.9334, 0.9321, 0.9315, 0.9310, 0.9307, 0.9275, 0.9270, 0.9252,
         0.9238],
        [0.8836, 0.8803, 0.8797, 0.8698, 0.8634, 0.8590, 0.8572, 0.8437, 0.8352,
         0.8282],
        [0.9022, 0.8646, 0.8629, 0.8613, 0.8573, 0.8551, 0.8534, 0.8533, 0.8500,
         0.8489],
        [0.8881, 0.8777, 0.8746, 0.8637, 0.8557, 0.8512, 0.8461, 0.8437, 0.8368,
         0.8359],
        [0.8830, 0.8748, 0.8617, 0.8581, 0.8517, 0.8500, 0.8469, 0.8458, 0.8431,
         0.8352],
        [0.8559, 0.8388, 0.8255, 0.8254, 0.8147, 0.8146, 0.8032, 0.7904, 0.7800,
         0.7778],
        [0.8322, 0.8097, 0.7959, 0.7884, 0.7826, 0.7814, 0.7783, 0.7653, 0.7608,
         0.7584],
        [0.9030, 0.8889, 0.8841, 0.8814, 0.8814, 0.8809, 0.8697, 0.8688, 0.8685,
         0.8677],
        [0.8104, 0.7874, 0.7841, 0.7785, 0.7644, 0.7639, 0.7515, 0.7493, 0.7367,
         0.7271],
        [0.8386, 0.8341, 0.8129, 0.8127, 0.8082, 0.8048, 0.8027, 0.8024, 0.7924,
         0.7908],
        [0.8950, 0.8658, 0.8494, 0.8404, 0.8357, 0.8340, 0.8260, 0.8252, 0.8201,
         0.8158],
        [0.8815, 0.8560, 0.8513, 0.8506, 0.8391, 0.8290, 0.8229, 0.8161, 0.8120,
         0.8080],
        [0.8907, 0.8875, 0.8492, 0.8224, 0.8193, 0.8162, 0.8076, 0.8033, 0.8024,
         0.8010],
        [0.9275, 0.9100, 0.9067, 0.8960, 0.8690, 0.8618, 0.8597, 0.8581, 0.8449,
         0.8433],
        [0.9139, 0.8756, 0.8723, 0.8623, 0.8569, 0.8558, 0.8483, 0.8446, 0.8432,
         0.8421],
        [0.9271, 0.9049, 0.9019, 0.9011, 0.8975, 0.8957, 0.8957, 0.8925, 0.8924,
         0.8914],
        [0.9037, 0.8832, 0.8812, 0.8708, 0.8694, 0.8684, 0.8669, 0.8651, 0.8648,
         0.8636],
        [0.9106, 0.8792, 0.8731, 0.8697, 0.8611, 0.8607, 0.8426, 0.8336, 0.8328,
         0.8280],
        [0.9410, 0.8816, 0.8742, 0.8666, 0.8588, 0.8535, 0.8388, 0.8373, 0.8366,
         0.8251],
        [0.9101, 0.8620, 0.8582, 0.8486, 0.8337, 0.8331, 0.8302, 0.8273, 0.8221,
         0.8219],
        [0.9007, 0.8213, 0.8190, 0.8080, 0.7981, 0.7976, 0.7960, 0.7931, 0.7887,
         0.7828],
        [0.8841, 0.8704, 0.8609, 0.8582, 0.8523, 0.8483, 0.8463, 0.8461, 0.8460,
         0.8455],
        [0.8558, 0.7806, 0.7802, 0.7778, 0.7688, 0.7358, 0.7342, 0.7294, 0.7286,
         0.7269],
        [0.8853, 0.8577, 0.8474, 0.8236, 0.8181, 0.8160, 0.8031, 0.7991, 0.7976,
         0.7932],
        [0.8402, 0.8120, 0.7996, 0.7846, 0.7804, 0.7785, 0.7707, 0.7588, 0.7547,
         0.7527]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 290620.3125,  188646.4375,  171903.7969,  163090.5625,  149410.7500,
          147799.3906,  145475.0938,  136299.1094,  129177.0234,  126076.2734],
        [ 714455.3125,  606870.4375,  596297.1250,  585294.8125,  565915.8125,
          560475.4375,  554939.8125,  541294.2500,  522334.4375,  521830.0312],
        [ 422685.1562,  382724.2812,  294769.6250,  264624.6875,  219348.5000,
          193356.2969,  188371.9062,  166947.6406,  160156.8125,  140793.4531],
        [ 852480.6250,  336597.0000,  320481.2188,  295779.4062,  290280.4375,
          289469.3438,  248920.4375,  244278.2188,  243528.7656,  231571.7188],
        [ 173964.0000,  154423.4062,  148968.5469,  116124.9609,  105853.1484,
          105165.3047,   96258.7656,   91151.8750,   82494.6641,   70441.0078],
        [ 201743.0781,  121520.4844,  105504.3438,   91467.5391,   85736.5547,
           82230.7500,   76563.9062,   67170.2344,   65197.9375,   58332.5273],
        [ 251442.0938,  244095.3906,  214965.2344,  202635.8594,  197244.0000,
          188634.7344,  188503.4531,  181628.2500,  176586.1250,  173176.4219],
        [ 367145.2500,  275539.2500,  252632.8594,  216566.9531,  193974.8281,
          155374.5938,  138544.5625,  126395.6562,  106638.6094,  104359.8359],
        [ 447200.4375,  403327.8750,  338241.6562,  318506.8125,  313057.5625,
          309148.1250,  309026.6875,  296764.6250,  294509.4062,  294042.7188],
        [ 779392.3750,  319974.8438,  313580.4688,  302707.0000,  278047.2500,
          267871.4375,  255886.8906,  255563.0156,  229403.3281,  224366.1250],
        [ 735175.8125,  408646.8438,  365074.7812,  357334.4688,  354831.7188,
          337806.1562,  314757.9688,  312323.9688,  302257.2500,  296973.0000],
        [ 344952.0312,  277478.2500,  214419.3906,  196045.1094,  142599.4688,
           99001.6250,   95707.8984,   93723.7422,   79621.6172,   72750.2734],
        [ 503568.3125,  432035.5000,  420258.2188,  358451.5625,  315032.4688,
          293740.2812,  260263.2812,  246289.0000,  235603.0312,  203864.5781],
        [ 898179.4375,  849122.2500,  831276.0625,  825271.2500,  791702.2500,
          778393.2500,  742696.3125,  729303.6875,  701919.8750,  657805.5625],
        [ 708292.5625,  586964.5000,  579473.6875,  570094.3750,  552706.2500,
          512404.5000,  506655.5938,  485878.9062,  476494.7812,  470467.6250],
        [ 563718.0625,  532878.3750,  527477.9375,  474858.5000,  459939.3125,
          455299.3125,  441599.4062,  424820.1250,  418561.5000,  403496.7500],
        [ 836751.4375,  769916.3125,  653930.0000,  644980.3125,  575377.1875,
          570423.4375,  520777.5938,  514187.3438,  491385.5312,  484766.7188],
        [ 948814.0000,  869736.2500,  810514.0000,  684426.5000,  678298.3125,
          650991.8750,  636940.3750,  631387.3125,  611637.1875,  605735.9375],
        [ 569570.0000,  493143.6562,  417991.0938,  382354.0000,  360052.5938,
          358520.2812,  346017.5625,  344388.9375,  325589.2188,  307206.9062],
        [ 419309.8125,  381824.5312,  378073.9375,  375355.0312,  368912.7812,
          368592.7500,  354398.5000,  336568.1250,  336448.7188,  336194.0625],
        [ 727181.9375,  643758.6250,  638709.2500,  618760.7500,  606871.5625,
          605537.8125,  579138.8750,  578925.7500,  563224.2500,  555342.1875],
        [ 562403.6250,  459849.4062,  451835.0000,  449572.4062,  433542.8125,
          428638.1875,  421866.0625,  406968.6875,  342864.7812,  330362.5000],
        [1026778.7500,  773348.5000,  708864.9375,  685326.5000,  629130.3125,
          586024.8125,  566131.6875,  503732.0938,  468311.7812,  467543.7812],
        [ 699943.2500,  639141.9375,  600045.0625,  566521.6250,  544314.8125,
          542622.5625,  540853.0625,  538570.8750,  533159.9375,  527912.7500],
        [ 848383.2500,  395541.5000,  349152.0312,  297466.5000,  244920.1719,
          240628.2344,  231002.6406,  228189.9219,  225620.2656,  214331.8906],
        [ 660217.6250,  617480.4375,  611046.5625,  554821.8125,  536449.1875,
          512826.4062,  485848.3438,  457657.1250,  417467.1875,  412866.9062],
        [ 555613.9375,  547115.2500,  532275.0000,  490080.2500,  444110.2500,
          394850.3125,  385020.0312,  379649.2188,  371946.6250,  367987.9375],
        [ 914436.8750,  771630.5000,  721490.6250,  576485.5625,  561015.0625,
          550364.0000,  513981.4062,  506142.7188,  499187.0312,  494784.4062],
        [ 730536.5000,  661322.9375,  571210.5625,  564223.1250,  561962.8750,
          544886.1250,  536282.9375,  503737.8438,  491396.2812,  477475.5312],
        [ 619295.0000,  575282.8125,  559163.1875,  558029.0625,  533987.3750,
          519764.4062,  503680.6875,  499547.5312,  477842.2500,  469951.4688],
        [ 751477.5625,  740508.8125,  736665.8125,  728216.6875,  722104.0000,
          710805.7500,  639519.3125,  615711.0000,  602618.2500,  584759.7500],
        [ 674951.8750,  525828.0625,  518239.9375,  458675.6250,  407352.7188,
          399563.3438,  382183.7188,  377577.7500,  376121.4688,  368649.7188],
        [ 278481.6562,  253494.6719,  246219.0000,  218694.9219,  210880.2188,
          194837.3125,  194275.2969,  193233.5312,  193210.5000,  189864.5312],
        [ 415345.4375,  354814.4688,  327391.7188,  315840.7500,  308837.8438,
          303224.7500,  295053.6562,  286239.6875,  285617.1250,  284589.3750],
        [ 521458.9062,  487520.1250,  481976.4062,  455720.2812,  402751.3125,
          386446.9688,  380522.4062,  347484.1562,  334815.3750,  329062.9375],
        [ 425938.5625,  378690.9688,  361299.1250,  315292.4375,  310544.9062,
          282021.8750,  280164.6562,  263505.2812,  222982.1406,  219282.0000],
        [ 757668.3750,  241581.7500,  236661.4375,  196608.4688,  186676.7188,
          155574.7656,  133657.4844,  126884.2969,  125998.1406,  117818.6484],
        [ 407392.3438,  317292.5938,  313174.9062,  286987.2500,  274366.6250,
          257642.1094,  221870.4219,  220763.8125,  209768.0156,  207116.8281],
        [ 358277.9688,  345887.9062,  264793.3125,  233357.7031,  194336.4531,
          193311.3125,  179627.3125,  170647.2344,  159435.9844,  154280.3281],
        [ 657942.3125,  618163.3125,  606774.9375,  601288.6875,  597341.0000,
          595028.0625,  568355.6250,  564228.0000,  550042.3125,  538819.5000],
        [ 303219.2812,  289565.7188,  286884.0938,  249035.6094,  227222.7031,
          213390.6250,  208151.8594,  171566.4219,  151974.3594,  137461.5469],
        [ 395944.9688,  231439.6875,  225830.3906,  220751.6094,  208363.5781,
          202041.9062,  197189.2656,  196785.1875,  187629.6406,  184913.3594],
        [ 323416.7500,  279008.8125,  266746.1875,  228282.4219,  203604.5938,
          190955.3281,  177506.1406,  171478.5781,  155480.4375,  153396.9688],
        [ 300659.6875,  267403.8438,  222033.8281,  210812.2656,  192489.7344,
          187620.8750,  179542.7031,  176708.4219,  170219.4375,  151956.5312],
        [ 204203.9219,  160002.0156,  132351.9062,  132060.0312,  113361.1875,
          113218.2422,   96261.7969,   80123.1250,   69058.7344,   66976.9922],
        [ 145526.2969,  105590.8984,   86681.6953,   77872.8516,   71655.9297,
           70421.2656,   67422.1328,   55975.0898,   52504.8594,   50728.2930],
        [ 400443.0312,  327481.6250,  305570.0312,  294003.7188,  293871.4062,
          291829.8438,  248838.7969,  245463.3594,  244697.4375,  241684.0781],
        [ 106637.8984,   76795.6484,   73195.5938,   67565.3516,   55281.9297,
           54906.4102,   45937.4023,   44558.4219,   37201.5625,   32438.0938],
        [ 159625.5469,  149622.0625,  110518.3594,  110186.3359,  103404.0234,
           98435.2500,   95523.0703,   95086.7031,   82454.7891,   80595.4375],
        [ 356981.5938,  235246.0469,  186169.1250,  163708.7344,  153121.0156,
          149407.3281,  133272.7188,  131794.0469,  122416.3594,  115226.3281],
        [ 294443.1250,  204614.6406,  191142.4531,  189496.0625,  160615.0625,
          139007.9062,  127426.5938,  115626.7969,  109098.0469,  103098.9688],
        [ 335961.7188,  320740.5000,  185703.4844,  126557.6484,  121161.3984,
          115897.8281,  102398.4453,   96345.9297,   95071.5625,   93175.0234],
        [ 568376.7500,  442598.6250,  421846.3438,  362027.2500,  246317.1875,
          222372.8750,  215641.8125,  210670.1719,  174480.7344,  170574.0156],
        [ 467474.6562,  270558.7500,  258193.0781,  223933.2344,  207153.1875,
          203899.5625,  183174.1562,  173814.5781,  170394.5156,  167727.2031],
        [ 564553.0625,  411120.8125,  394030.2500,  389411.4062,  370065.7500,
          360535.7188,  360535.7188,  344574.9062,  344194.5938,  339421.7500],
        [ 404274.8125,  301679.3125,  293217.4688,  252624.6562,  247549.0469,
          244294.0469,  239030.1875,  233003.8906,  231934.8594,  228101.1562],
        [ 445927.9375,  284903.5625,  261223.1406,  248827.8750,  219900.1875,
          218724.1250,  168893.1719,  148546.2031,  146878.7500,  137096.8125],
        [ 688960.8750,  294908.5000,  265438.0312,  238013.8281,  212833.5625,
          197326.4062,  160094.9688,  156696.1406,  154934.5469,  131532.1250],
        [ 443160.4062,  222815.2812,  211224.7969,  183932.8281,  148723.2500,
          147498.3438,  141551.1562,  135775.4844,  126038.8750,  125752.6484],
        [ 387232.0000,  124655.3281,  120656.0078,  103104.6641,   89411.7734,
           88795.4453,   86852.3984,   83309.7031,   78200.3828,   71898.1875],
        [ 305796.8438,  251405.4062,  219344.9531,  211180.0938,  193930.4375,
          183119.6562,  177989.0938,  177626.0312,  177260.8594,  175947.6719],
        [ 204009.6562,   69636.5547,   69217.3672,   66973.9922,   58820.1484,
           36727.3906,   35888.6133,   33501.0664,   33139.9883,   32355.5098],
        [ 310993.9375,  209554.8750,  180812.8750,  128754.9141,  119015.9453,
          115561.6484,   96098.1641,   90688.6797,   88845.9297,   83439.9453],
        [ 163241.7969,  109110.1172,   91382.1797,   73811.2656,   69426.4766,
           67565.4766,   60454.2188,   51012.5352,   48142.0312,   46737.4570]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[290620.3125,      0.0000],
         [188646.4375,      0.0000],
         [171903.7969,      0.0000],
         ...,
         [136299.1094,      0.0000],
         [129177.0234,      0.0000],
         [     0.0000, 126076.2734]],

        [[714455.3125,      0.0000],
         [606870.4375,      0.0000],
         [596297.1250,      0.0000],
         ...,
         [541294.2500,      0.0000],
         [522334.4375,      0.0000],
         [521830.0312,      0.0000]],

        [[422685.1562,      0.0000],
         [382724.2812,      0.0000],
         [294769.6250,      0.0000],
         ...,
         [166947.6406,      0.0000],
         [160156.8125,      0.0000],
         [140793.4531,      0.0000]],

        ...,

        [[     0.0000, 204009.6562],
         [ 69636.5547,      0.0000],
         [ 69217.3672,      0.0000],
         ...,
         [ 33501.0664,      0.0000],
         [ 33139.9883,      0.0000],
         [ 32355.5098,      0.0000]],

        [[310993.9375,      0.0000],
         [     0.0000, 209554.8750],
         [180812.8750,      0.0000],
         ...,
         [ 90688.6797,      0.0000],
         [ 88845.9297,      0.0000],
         [ 83439.9453,      0.0000]],

        [[     0.0000, 163241.7969],
         [     0.0000, 109110.1172],
         [     0.0000,  91382.1797],
         ...,
         [ 51012.5352,      0.0000],
         [     0.0000,  48142.0312],
         [     0.0000,  46737.4570]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1522422.5000,  126076.2734],
        [5769707.5000,       0.0000],
        [2433778.2500,       0.0000],
        [3353387.0000,       0.0000],
        [ 471172.5312,  673673.1250],
        [ 802560.5625,  152906.7812],
        [1664106.8750,  354804.6875],
        [1937172.3750,       0.0000],
        [3323826.0000,       0.0000],
        [3226793.0000,       0.0000],
        [3785182.0000,       0.0000],
        [1616299.3750,       0.0000],
        [3269106.0000,       0.0000],
        [7805670.0000,       0.0000],
        [5449432.0000,       0.0000],
        [4702649.5000,       0.0000],
        [6062495.5000,       0.0000],
        [7128482.0000,       0.0000],
        [3904834.5000,       0.0000],
        [3319229.5000,  336448.7188],
        [6117451.0000,       0.0000],
        [4287903.5000,       0.0000],
        [6415193.0000,       0.0000],
        [5733086.0000,       0.0000],
        [3275236.5000,       0.0000],
        [5266681.5000,       0.0000],
        [4468649.0000,       0.0000],
        [6109518.0000,       0.0000],
        [5643034.5000,       0.0000],
        [5316544.0000,       0.0000],
        [6832387.0000,       0.0000],
        [4489144.5000,       0.0000],
        [2173191.7500,       0.0000],
        [2540725.2500,  636229.5625],
        [1873861.2500, 2253897.7500],
        [1084460.3750, 1975261.5000],
        [1152995.6250, 1126134.3750],
        [1430336.0000, 1286038.8750],
        [ 756752.2500, 1497203.3750],
        [5897984.0000,       0.0000],
        [2238472.2500,       0.0000],
        [2250889.5000,       0.0000],
        [2149876.2500,       0.0000],
        [2059447.2500,       0.0000],
        [ 262543.0625,  905074.8750],
        [ 339877.0312,  444502.2812],
        [2493440.2500,  400443.0312],
        [ 378696.3125,  215822.0312],
        [ 478841.5938,  606609.9375],
        [ 472207.9375, 1275135.3750],
        [1386463.7500,  248105.9531],
        [ 750607.8125,  842405.7500],
        [1626662.7500, 1408243.0000],
        [2119169.7500,  207153.1875],
        [3878443.7500,       0.0000],
        [2675709.5000,       0.0000],
        [1047880.5000, 1233041.2500],
        [1754983.1250,  745755.8750],
        [1018487.6875,  867985.3750],
        [ 583085.6250,  651030.3125],
        [1322468.3750,  751132.6875],
        [ 436260.6250,  204009.6562],
        [ 979634.5000,  444132.4688],
        [ 118578.0156,  662305.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 381/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:56, 61.94s/it]  7%|▋         | 2/30 [01:02<12:06, 25.95s/it] 10%|█         | 3/30 [01:03<06:29, 14.44s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.04s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.05s/it] 20%|██        | 6/30 [01:05<01:41,  4.25s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.365008171399434
Epoch 382/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:53, 61.86s/it]  7%|▋         | 2/30 [01:02<12:05, 25.91s/it] 10%|█         | 3/30 [01:03<06:29, 14.42s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.02s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.04s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.3818513711293536
Epoch 383/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:17, 58.53s/it]  7%|▋         | 2/30 [01:02<12:17, 26.36s/it] 10%|█         | 3/30 [01:03<06:35, 14.66s/it] 13%|█▎        | 4/30 [01:03<03:58,  9.17s/it] 17%|█▋        | 5/30 [01:04<02:33,  6.13s/it] 20%|██        | 6/30 [01:05<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:06<00:52,  2.38s/it] 30%|███       | 9/30 [01:07<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.29s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.29it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.32it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.347097301483154
Epoch 384/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:28, 60.98s/it]  7%|▋         | 2/30 [01:01<11:58, 25.68s/it] 10%|█         | 3/30 [01:02<06:25, 14.29s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.99s/it] 20%|██        | 6/30 [01:04<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.372981333732605
Epoch 385/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:50, 61.75s/it]  7%|▋         | 2/30 [01:02<12:04, 25.87s/it] 10%|█         | 3/30 [01:03<06:28, 14.40s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.03s/it] 20%|██        | 6/30 [01:05<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.387764247258504
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0239,  0.0018,  0.0123,  ..., -0.0047,  0.0153,  0.0040],
        [-0.0022,  0.0109,  0.0278,  ...,  0.0165,  0.0023, -0.0157],
        [-0.0295, -0.0382,  0.0248,  ...,  0.0818, -0.0059, -0.0153],
        ...,
        [ 0.0074, -0.0035,  0.0086,  ..., -0.0236, -0.0070, -0.0053],
        [-0.0324,  0.0086, -0.0042,  ...,  0.0047,  0.0164, -0.0114],
        [-0.0338, -0.0133,  0.0184,  ...,  0.0405,  0.0351, -0.0306]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8798, 0.8504, 0.8432, 0.8398, 0.8336, 0.8323, 0.8315, 0.8268, 0.8233,
         0.8215],
        [0.9433, 0.9317, 0.9304, 0.9294, 0.9271, 0.9260, 0.9253, 0.9237, 0.9211,
         0.9211],
        [0.9066, 0.8994, 0.8807, 0.8737, 0.8603, 0.8516, 0.8497, 0.8412, 0.8384,
         0.8296],
        [0.9559, 0.8902, 0.8868, 0.8814, 0.8802, 0.8796, 0.8690, 0.8677, 0.8673,
         0.8640],
        [0.8444, 0.8356, 0.8334, 0.8160, 0.8092, 0.8091, 0.8029, 0.7990, 0.7920,
         0.7812],
        [0.8547, 0.8193, 0.8095, 0.7992, 0.7941, 0.7923, 0.7866, 0.7780, 0.7749,
         0.7678],
        [0.8697, 0.8676, 0.8588, 0.8549, 0.8528, 0.8499, 0.8497, 0.8464, 0.8451,
         0.8431],
        [0.8963, 0.8762, 0.8703, 0.8595, 0.8518, 0.8359, 0.8284, 0.8217, 0.8098,
         0.8080],
        [0.9104, 0.9033, 0.8906, 0.8865, 0.8855, 0.8848, 0.8846, 0.8816, 0.8812,
         0.8812],
        [0.9495, 0.8868, 0.8854, 0.8829, 0.8770, 0.8744, 0.8710, 0.8710, 0.8636,
         0.8620],
        [0.9454, 0.9043, 0.8960, 0.8942, 0.8939, 0.8905, 0.8856, 0.8853, 0.8825,
         0.8817],
        [0.8919, 0.8767, 0.8588, 0.8523, 0.8299, 0.8042, 0.8018, 0.8009, 0.7890,
         0.7829],
        [0.9182, 0.9080, 0.9058, 0.8946, 0.8853, 0.8807, 0.8721, 0.8682, 0.8654,
         0.8552],
        [0.9592, 0.9555, 0.9538, 0.9533, 0.9504, 0.9491, 0.9459, 0.9446, 0.9419,
         0.9373],
        [0.9428, 0.9295, 0.9287, 0.9275, 0.9253, 0.9200, 0.9193, 0.9163, 0.9150,
         0.9139],
        [0.9269, 0.9229, 0.9221, 0.9148, 0.9126, 0.9118, 0.9097, 0.9069, 0.9059,
         0.9034],
        [0.9543, 0.9484, 0.9370, 0.9358, 0.9281, 0.9274, 0.9209, 0.9202, 0.9169,
         0.9160],
        [0.9632, 0.9571, 0.9522, 0.9404, 0.9399, 0.9367, 0.9353, 0.9346, 0.9326,
         0.9317],
        [0.9274, 0.9170, 0.9053, 0.8996, 0.8953, 0.8949, 0.8925, 0.8920, 0.8882,
         0.8844],
        [0.9058, 0.8995, 0.8987, 0.8983, 0.8969, 0.8969, 0.8941, 0.8906, 0.8905,
         0.8903],
        [0.9442, 0.9358, 0.9352, 0.9329, 0.9319, 0.9316, 0.9284, 0.9282, 0.9266,
         0.9255],
        [0.9266, 0.9124, 0.9114, 0.9107, 0.9079, 0.9073, 0.9065, 0.9039, 0.8917,
         0.8888],
        [0.9687, 0.9490, 0.9428, 0.9403, 0.9345, 0.9293, 0.9270, 0.9189, 0.9138,
         0.9135],
        [0.9419, 0.9355, 0.9311, 0.9272, 0.9244, 0.9240, 0.9238, 0.9236, 0.9231,
         0.9220],
        [0.9553, 0.9018, 0.8928, 0.8818, 0.8682, 0.8665, 0.8640, 0.8629, 0.8622,
         0.8588],
        [0.9377, 0.9331, 0.9322, 0.9255, 0.9233, 0.9199, 0.9162, 0.9120, 0.9056,
         0.9049],
        [0.9256, 0.9244, 0.9224, 0.9168, 0.9098, 0.9017, 0.8998, 0.8989, 0.8975,
         0.8967],
        [0.9607, 0.9488, 0.9439, 0.9281, 0.9263, 0.9251, 0.9203, 0.9192, 0.9184,
         0.9176],
        [0.9448, 0.9378, 0.9275, 0.9267, 0.9266, 0.9243, 0.9230, 0.9189, 0.9168,
         0.9149],
        [0.9332, 0.9281, 0.9262, 0.9261, 0.9229, 0.9208, 0.9188, 0.9183, 0.9151,
         0.9141],
        [0.9467, 0.9458, 0.9455, 0.9447, 0.9440, 0.9429, 0.9355, 0.9329, 0.9314,
         0.9293],
        [0.9393, 0.9219, 0.9209, 0.9121, 0.9039, 0.9027, 0.8994, 0.8986, 0.8983,
         0.8970],
        [0.8771, 0.8707, 0.8685, 0.8603, 0.8579, 0.8519, 0.8518, 0.8516, 0.8516,
         0.8507],
        [0.9049, 0.8940, 0.8884, 0.8860, 0.8842, 0.8830, 0.8812, 0.8791, 0.8790,
         0.8786],
        [0.9211, 0.9164, 0.9156, 0.9115, 0.9030, 0.9005, 0.8991, 0.8926, 0.8900,
         0.8889],
        [0.9070, 0.8985, 0.8954, 0.8861, 0.8847, 0.8781, 0.8777, 0.8731, 0.8616,
         0.8603],
        [0.9476, 0.8677, 0.8659, 0.8529, 0.8492, 0.8360, 0.8255, 0.8216, 0.8215,
         0.8171],
        [0.9035, 0.8859, 0.8849, 0.8789, 0.8764, 0.8718, 0.8608, 0.8605, 0.8573,
         0.8562],
        [0.8949, 0.8922, 0.8733, 0.8646, 0.8516, 0.8514, 0.8462, 0.8424, 0.8380,
         0.8358],
        [0.9374, 0.9331, 0.9317, 0.9311, 0.9305, 0.9300, 0.9271, 0.9266, 0.9248,
         0.9238],
        [0.8831, 0.8798, 0.8792, 0.8692, 0.8630, 0.8585, 0.8566, 0.8432, 0.8346,
         0.8276],
        [0.9019, 0.8640, 0.8627, 0.8608, 0.8564, 0.8544, 0.8526, 0.8523, 0.8493,
         0.8483],
        [0.8876, 0.8771, 0.8739, 0.8632, 0.8551, 0.8506, 0.8453, 0.8430, 0.8364,
         0.8356],
        [0.8820, 0.8744, 0.8611, 0.8572, 0.8512, 0.8492, 0.8465, 0.8452, 0.8428,
         0.8350],
        [0.8552, 0.8385, 0.8247, 0.8246, 0.8140, 0.8139, 0.8024, 0.7892, 0.7789,
         0.7773],
        [0.8318, 0.8091, 0.7951, 0.7881, 0.7822, 0.7811, 0.7780, 0.7651, 0.7606,
         0.7583],
        [0.9025, 0.8880, 0.8833, 0.8804, 0.8803, 0.8800, 0.8687, 0.8678, 0.8675,
         0.8665],
        [0.8096, 0.7869, 0.7834, 0.7777, 0.7638, 0.7637, 0.7501, 0.7480, 0.7359,
         0.7261],
        [0.8377, 0.8334, 0.8125, 0.8124, 0.8076, 0.8045, 0.8026, 0.8017, 0.7920,
         0.7900],
        [0.8944, 0.8653, 0.8485, 0.8392, 0.8351, 0.8333, 0.8253, 0.8245, 0.8194,
         0.8153],
        [0.8807, 0.8556, 0.8506, 0.8501, 0.8386, 0.8286, 0.8226, 0.8153, 0.8117,
         0.8076],
        [0.8903, 0.8871, 0.8485, 0.8214, 0.8182, 0.8148, 0.8068, 0.8025, 0.8017,
         0.7999],
        [0.9271, 0.9096, 0.9060, 0.8955, 0.8683, 0.8614, 0.8592, 0.8572, 0.8445,
         0.8424],
        [0.9135, 0.8754, 0.8720, 0.8620, 0.8562, 0.8554, 0.8474, 0.8443, 0.8426,
         0.8415],
        [0.9265, 0.9043, 0.9013, 0.9005, 0.8972, 0.8952, 0.8952, 0.8921, 0.8918,
         0.8908],
        [0.9030, 0.8827, 0.8808, 0.8703, 0.8688, 0.8679, 0.8661, 0.8645, 0.8641,
         0.8628],
        [0.9098, 0.8788, 0.8724, 0.8691, 0.8608, 0.8600, 0.8425, 0.8330, 0.8318,
         0.8274],
        [0.9408, 0.8811, 0.8737, 0.8659, 0.8583, 0.8531, 0.8383, 0.8366, 0.8357,
         0.8243],
        [0.9096, 0.8616, 0.8578, 0.8480, 0.8331, 0.8328, 0.8294, 0.8269, 0.8213,
         0.8209],
        [0.9003, 0.8207, 0.8183, 0.8072, 0.7971, 0.7967, 0.7952, 0.7923, 0.7880,
         0.7818],
        [0.8840, 0.8698, 0.8603, 0.8577, 0.8516, 0.8472, 0.8454, 0.8453, 0.8453,
         0.8452],
        [0.8556, 0.7798, 0.7792, 0.7777, 0.7681, 0.7357, 0.7335, 0.7288, 0.7280,
         0.7268],
        [0.8849, 0.8573, 0.8465, 0.8233, 0.8172, 0.8155, 0.8018, 0.7981, 0.7966,
         0.7921],
        [0.8399, 0.8113, 0.7989, 0.7839, 0.7800, 0.7774, 0.7703, 0.7579, 0.7540,
         0.7514]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 287384.3750,  188810.5781,  170282.9219,  162184.4531,  148545.7812,
          145811.0781,  144139.6719,  134747.1562,  128280.1094,  124917.7344],
        [ 712192.0000,  603560.8750,  592454.5000,  583649.9375,  564702.7500,
          556005.1250,  550490.5000,  538123.6875,  518384.7500,  518138.6250],
        [ 421534.6875,  380459.6562,  291278.7812,  263233.5312,  217609.3594,
          192011.3906,  187036.3281,  165640.2188,  159016.1250,  140376.4688],
        [ 852421.2500,  333421.6250,  317694.4062,  293849.2812,  289227.6250,
          286478.3438,  246423.8594,  241701.8281,  240368.5781,  229374.2344],
        [ 173394.3906,  152901.1094,  148202.3594,  115595.1562,  104758.5938,
          104626.9062,   95807.8125,   90566.3906,   81930.6250,   70303.9609],
        [ 200855.0156,  121142.6797,  105268.5547,   90848.4766,   84509.7109,
           82392.5312,   75901.1016,   67106.0781,   64220.2695,   58025.7578],
        [ 248923.0469,  241306.1406,  212767.6094,  201437.6094,  195493.0156,
          187448.2969,  186862.8438,  178342.1562,  175141.7500,  170126.6094],
        [ 363951.2500,  272943.2812,  250878.2500,  215074.3281,  192751.4844,
          153544.0625,  137957.0625,  125310.6562,  105703.2422,  103030.1641],
        [ 445038.3125,  401793.3438,  335486.8750,  316242.2188,  311738.9375,
          308644.9688,  307907.0312,  294916.6562,  293225.5625,  293062.0312],
        [ 777821.8750,  317499.3438,  311460.2188,  300493.6875,  276039.7812,
          266059.4375,  253599.1094,  253567.6875,  228040.2500,  222919.2188],
        [ 733889.0000,  407831.9688,  362434.5000,  353101.6875,  351309.7188,
          334870.9062,  312201.0000,  310986.5000,  298626.1875,  295329.0000],
        [ 341444.7188,  274939.7500,  212964.5156,  194024.6094,  140844.8750,
           97583.4766,   94238.4688,   93160.4609,   78516.0234,   71980.0312],
        [ 497615.6250,  430142.2812,  416713.0625,  355075.4375,  311068.6562,
          291017.4688,  257583.6250,  243597.7656,  233954.2188,  202221.7500],
        [ 893273.5000,  847488.0000,  826586.6875,  821090.9375,  787750.5000,
          773420.0625,  739052.0000,  725204.0000,  698088.0000,  653838.3750],
        [ 706750.8125,  584850.6875,  577517.3750,  567879.3750,  550144.6875,
          510608.8750,  505077.0938,  484219.6562,  474962.6562,  467570.5312],
        [ 562975.0625,  531602.8125,  526154.6250,  473625.1562,  458853.2500,
          453779.1250,  440251.7188,  423202.6562,  416995.2812,  402568.1250],
        [ 832695.5625,  765457.6875,  650829.2500,  639352.8125,  572595.9375,
          567225.0000,  516967.8438,  511783.7812,  488439.2812,  482099.1250],
        [ 945782.1875,  866904.1875,  809006.6250,  683234.3750,  677919.3125,
          647987.1875,  635275.7500,  628419.7500,  610994.1250,  603561.4375],
        [ 567187.6875,  489110.5000,  413732.4688,  381309.2500,  358454.6562,
          356670.2188,  344693.8750,  342311.6562,  324197.0938,  306924.5938],
        [ 416599.0000,  380623.6875,  376294.4062,  374052.8750,  367117.2188,
          366679.5000,  352271.5625,  335517.9375,  334919.7812,  333653.1875],
        [ 721140.5625,  639310.1250,  634503.8125,  614035.6875,  604563.2500,
          602666.5000,  575705.4375,  573935.8750,  560439.1875,  552415.3750],
        [ 561030.6250,  457955.2812,  451201.5938,  447050.3750,  429550.7812,
          425888.2188,  420802.8750,  405247.5312,  340818.7812,  326982.6250],
        [1023899.0000,  772408.0000,  707245.0625,  682165.3125,  627569.8750,
          582753.3750,  563715.9375,  502301.5938,  466994.7812,  465310.7812],
        [ 698126.6250,  636761.8750,  598290.8125,  565251.2500,  543593.6875,
          540018.6250,  538939.7500,  537438.5000,  533346.6250,  525145.4375],
        [ 845063.6250,  393383.6562,  346034.0625,  295897.6250,  243443.5469,
          237731.8438,  229203.0156,  225732.6250,  223388.4688,  212924.5312],
        [ 657033.1250,  615225.6250,  607578.1250,  552129.8750,  534885.4375,
          509705.9062,  483109.3750,  455405.3125,  415390.9688,  411517.4062],
        [ 552928.1875,  543463.1250,  528183.6875,  487568.5312,  441412.8750,
          392861.0625,  382597.2812,  377516.5625,  370293.7812,  366057.2188],
        [ 912198.4375,  770085.2500,  718538.0625,  573315.5000,  558635.0000,
          548902.1250,  512500.2812,  504297.8438,  498616.5312,  493202.9062],
        [ 727190.3125,  658389.8750,  567697.4375,  561263.9375,  561116.7500,
          542528.3750,  533008.5000,  502148.3438,  487709.4375,  474608.1250],
        [ 616249.1250,  572995.2500,  557629.0000,  557111.8125,  532096.3125,
          516520.8750,  501437.2188,  498066.1875,  475891.2812,  469073.4375],
        [ 747214.7500,  738054.6250,  734467.3125,  726241.5000,  718594.1875,
          708071.7500,  636932.5000,  613277.8125,  600430.2500,  582761.1875],
        [ 671896.8125,  524588.8750,  516757.8438,  456173.8125,  405240.5938,
          398649.4688,  380297.5000,  375847.1875,  374515.0938,  367411.4375],
        [ 276528.0312,  252198.1094,  244433.4062,  217358.7969,  210219.8125,
          192959.1406,  192572.3438,  192091.9688,  192019.4375,  189532.2031],
        [ 411160.0312,  351765.3438,  324806.1562,  313896.7500,  305959.0312,
          300618.0938,  293389.4688,  284434.7188,  284212.0938,  282387.3438],
        [ 518265.6250,  484763.9375,  479471.8750,  452099.6875,  400160.8750,
          386472.7812,  378425.2500,  345148.1562,  332509.9688,  327142.0312],
        [ 423868.7188,  375427.6875,  359112.2812,  314317.9062,  308367.8125,
          280531.4688,  278846.2812,  261085.8906,  221519.6562,  217459.3594],
        [ 757292.6875,  241631.5312,  235592.2500,  195691.8750,  185588.9375,
          153614.6562,  132290.7031,  125105.6328,  124949.7891,  117377.8984],
        [ 403203.6250,  313478.8125,  309293.2188,  283517.7188,  273795.8125,
          256247.3438,  218935.3281,  218152.3125,  208422.4062,  205148.0625],
        [ 356487.2500,  343227.9375,  261918.1250,  231341.9219,  191988.6875,
          191547.1875,  177924.4375,  168418.0312,  158102.9375,  153274.2812],
        [ 653940.0000,  615093.6250,  603172.5000,  598104.8125,  593363.6875,
          588954.5000,  565229.6875,  561096.9375,  546694.3750,  538713.1875],
        [ 301385.4062,  287527.7188,  285097.0625,  246951.2969,  225963.0781,
          211920.7188,  206406.5938,  170283.2500,  150721.2344,  136375.4375],
        [ 393976.8750,  229387.1406,  225015.8281,  219056.4531,  205805.5156,
          200039.5156,  194839.1719,  194044.5781,  185991.6719,  183334.5781],
        [ 321320.0625,  276615.3438,  264104.5625,  226755.7812,  201825.6406,
          189424.3281,  175515.9531,  169815.3750,  154696.5469,  152913.0781],
        [ 296678.3125,  265869.4688,  220156.8281,  208017.5156,  191066.0781,
          185530.8906,  178582.3125,  175369.0625,  169436.6875,  151554.3438],
        [ 202134.2031,  159215.2031,  130771.2891,  130629.5703,  112205.1562,
          112070.4062,   95183.1484,   78771.1016,   67964.7344,   66479.2266],
        [ 144843.1406,  104657.0469,   85655.3203,   77549.5078,   71277.1953,
           70187.0625,   67111.3281,   55830.1289,   52322.8594,   50667.7070],
        [ 397272.5625,  323194.4375,  301921.3438,  289702.4375,  289500.0000,
          288249.0000,  245101.4844,  242206.9375,  241182.5938,  237775.8281],
        [ 105480.0938,   76208.3438,   72519.9531,   66829.7344,   54785.2734,
           54733.2070,   45080.2227,   43750.7109,   36766.2227,   31986.5762],
        [ 157486.4062,  148055.8594,  109940.8203,  109777.9062,  102454.1250,
           98016.0547,   95384.5156,   94188.3359,   81941.2578,   79697.9609],
        [ 354069.1250,  233519.1094,  183887.7500,  160959.6094,  151756.0938,
          147982.3281,  131896.7812,  130371.4453,  121343.5234,  114376.7422],
        [ 291293.2188,  203484.2344,  189488.2812,  188077.8750,  159449.5156,
          138232.4375,  126937.9141,  114370.7500,  108620.8125,  102424.0312],
        [ 333731.7812,  318797.0000,  183891.6094,  124721.0938,  119283.0859,
          113596.4531,  101335.2734,   95221.1953,   94153.8516,   91807.8438],
        [ 564595.0625,  439892.0312,  417591.4375,  359385.6875,  244002.0781,
          221003.9531,  214097.3594,  208176.2812,  173589.7969,  168495.7969],
        [ 465382.6875,  269775.4688,  257007.2500,  222855.0156,  205056.5156,
          202814.2969,  180899.9844,  173142.3906,  168880.6094,  166269.0156],
        [ 560377.6875,  407729.7188,  390909.4375,  386252.7812,  368543.5312,
          357980.8125,  357980.8125,  342586.6562,  341230.5000,  336417.9062],
        [ 400556.8125,  299419.5312,  291699.9062,  250902.9062,  245600.5625,
          242283.6406,  236373.4062,  231092.7500,  229568.7656,  225484.5312],
        [ 441047.2188,  283323.9062,  258455.7031,  246756.3594,  219055.8438,
          216640.2812,  168740.8750,  147206.1875,  144744.0000,  136018.1094],
        [ 686934.2500,  292849.6875,  263490.9688,  235754.5156,  211308.8281,
          196369.0000,  158821.3750,  154935.7188,  153134.0312,  129997.1250],
        [ 439762.0000,  221722.9844,  209901.0781,  182457.5938,  147527.0469,
          146888.5625,  139952.5938,  134888.7031,  124641.0625,  123957.4453],
        [ 385334.4688,  123542.7344,  119441.7656,  101804.2891,   88154.8672,
           87693.0156,   85847.0000,   82289.5078,   77419.6016,   70831.7891],
        [ 305304.4062,  249218.5469,  217523.4531,  209482.5312,  192062.6562,
          180378.5156,  175750.4531,  175631.6562,  175478.2969,  175193.5312],
        [ 203280.0000,   68830.3125,   68326.0078,   66814.5000,   58281.4219,
           36690.7383,   35549.9492,   33222.8008,   32862.3711,   32308.2109],
        [ 309310.9062,  208269.2031,  178701.2188,  128275.4609,  117556.1250,
          114663.8750,   94262.1094,   89411.1797,   87523.0703,   82114.5391],
        [ 162530.1875,  108037.0000,   90435.6250,   73026.9297,   69030.8125,
           66570.7734,   60112.7852,   50373.4258,   47613.7031,   45933.8984]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[287384.3750,      0.0000],
         [188810.5781,      0.0000],
         [170282.9219,      0.0000],
         ...,
         [134747.1562,      0.0000],
         [128280.1094,      0.0000],
         [     0.0000, 124917.7344]],

        [[712192.0000,      0.0000],
         [603560.8750,      0.0000],
         [592454.5000,      0.0000],
         ...,
         [538123.6875,      0.0000],
         [518384.7500,      0.0000],
         [518138.6250,      0.0000]],

        [[421534.6875,      0.0000],
         [380459.6562,      0.0000],
         [291278.7812,      0.0000],
         ...,
         [165640.2188,      0.0000],
         [159016.1250,      0.0000],
         [140376.4688,      0.0000]],

        ...,

        [[     0.0000, 203280.0000],
         [ 68830.3125,      0.0000],
         [ 68326.0078,      0.0000],
         ...,
         [ 33222.8008,      0.0000],
         [ 32862.3711,      0.0000],
         [ 32308.2109,      0.0000]],

        [[309310.9062,      0.0000],
         [     0.0000, 208269.2031],
         [178701.2188,      0.0000],
         ...,
         [ 89411.1797,      0.0000],
         [ 87523.0703,      0.0000],
         [ 82114.5391,      0.0000]],

        [[     0.0000, 162530.1875],
         [     0.0000, 108037.0000],
         [     0.0000,  90435.6250],
         ...,
         [ 50373.4258,      0.0000],
         [     0.0000,  47613.7031],
         [     0.0000,  45933.8984]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1510186.1250,  124917.7344],
        [5737702.5000,       0.0000],
        [2418196.5000,       0.0000],
        [3330961.0000,       0.0000],
        [ 468396.1562,  669691.1250],
        [ 798654.3750,  151615.7812],
        [1649380.3750,  348468.7500],
        [1921143.7500,       0.0000],
        [3308056.0000,       0.0000],
        [3207500.5000,       0.0000],
        [3760580.7500,       0.0000],
        [1599697.0000,       0.0000],
        [3238990.0000,       0.0000],
        [7765792.0000,       0.0000],
        [5429581.5000,       0.0000],
        [4690008.0000,       0.0000],
        [6027446.0000,       0.0000],
        [7109084.5000,       0.0000],
        [3884592.5000,       0.0000],
        [3302211.5000,  335517.9375],
        [6078715.5000,       0.0000],
        [4266528.5000,       0.0000],
        [6394364.0000,       0.0000],
        [5716913.0000,       0.0000],
        [3252802.7500,       0.0000],
        [5241981.0000,       0.0000],
        [4442882.5000,       0.0000],
        [6090292.0000,       0.0000],
        [5615661.0000,       0.0000],
        [5297070.5000,       0.0000],
        [6806046.0000,       0.0000],
        [4471378.5000,       0.0000],
        [2159913.2500,       0.0000],
        [2521864.0000,  630765.1875],
        [1862697.2500, 2241763.0000],
        [1075492.6250, 1965044.3750],
        [1145262.0000, 1123874.0000],
        [1416198.7500, 1273996.0000],
        [ 749851.5625, 1484379.2500],
        [5864364.0000,       0.0000],
        [2222631.7500,       0.0000],
        [2231491.2500,       0.0000],
        [2132986.7500,       0.0000],
        [2042261.5000,       0.0000],
        [ 258941.0000,  896483.0625],
        [ 338447.9688,  441653.3125],
        [2458834.0000,  397272.5625],
        [ 374115.3125,  214025.0156],
        [ 475546.2812,  601396.9375],
        [ 468445.8750, 1261716.7500],
        [1375525.8750,  246853.2500],
        [ 740118.8125,  836420.3750],
        [1613875.7500, 1396953.7500],
        [2107026.7500,  205056.5156],
        [3850009.7500,       0.0000],
        [2652983.0000,       0.0000],
        [1038670.7500, 1223317.7500],
        [1743682.5000,  739913.0000],
        [1010207.2500,  861491.8125],
        [ 576695.4375,  645663.5625],
        [1309438.5000,  746585.6250],
        [ 432886.3125,  203280.0000],
        [ 969598.4375,  440489.1875],
        [ 116944.2031,  656720.9375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 386/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:21, 60.73s/it]  7%|▋         | 2/30 [01:01<11:52, 25.45s/it] 10%|█         | 3/30 [01:02<06:22, 14.17s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.3827411969502768
Epoch 387/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:24, 62.90s/it]  7%|▋         | 2/30 [01:03<12:17, 26.34s/it] 10%|█         | 3/30 [01:04<06:35, 14.66s/it] 13%|█▎        | 4/30 [01:05<03:58,  9.17s/it] 17%|█▋        | 5/30 [01:05<02:33,  6.13s/it] 20%|██        | 6/30 [01:06<01:43,  4.30s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.14s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.38s/it] 30%|███       | 9/30 [01:08<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.33it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Epoch loss is 2.4061979134877522
Epoch 388/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:04, 58.09s/it]  7%|▋         | 2/30 [00:58<11:22, 24.36s/it] 10%|█         | 3/30 [00:59<06:06, 13.58s/it] 13%|█▎        | 4/30 [01:01<03:48,  8.79s/it] 17%|█▋        | 5/30 [01:01<02:27,  5.89s/it] 20%|██        | 6/30 [01:02<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.31s/it] 30%|███       | 9/30 [01:04<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.365309460957845
Epoch 389/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:14, 60.51s/it]  7%|▋         | 2/30 [01:01<11:49, 25.36s/it] 10%|█         | 3/30 [01:02<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3719748417536417
Epoch 390/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:17, 60.61s/it]  7%|▋         | 2/30 [01:01<11:51, 25.40s/it] 10%|█         | 3/30 [01:02<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.327414886156718
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0238,  0.0017,  0.0125,  ..., -0.0049,  0.0156,  0.0039],
        [-0.0022,  0.0107,  0.0280,  ...,  0.0166,  0.0025, -0.0156],
        [-0.0295, -0.0382,  0.0248,  ...,  0.0820, -0.0057, -0.0152],
        ...,
        [ 0.0072, -0.0035,  0.0088,  ..., -0.0237, -0.0068, -0.0050],
        [-0.0326,  0.0088, -0.0041,  ...,  0.0047,  0.0164, -0.0113],
        [-0.0337, -0.0134,  0.0185,  ...,  0.0406,  0.0352, -0.0304]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8792, 0.8505, 0.8425, 0.8394, 0.8333, 0.8315, 0.8310, 0.8261, 0.8229,
         0.8210],
        [0.9431, 0.9314, 0.9300, 0.9292, 0.9270, 0.9255, 0.9248, 0.9233, 0.9207,
         0.9205],
        [0.9064, 0.8991, 0.8800, 0.8733, 0.8597, 0.8511, 0.8493, 0.8407, 0.8379,
         0.8295],
        [0.9559, 0.8896, 0.8862, 0.8811, 0.8801, 0.8789, 0.8685, 0.8670, 0.8665,
         0.8634],
        [0.8442, 0.8350, 0.8332, 0.8158, 0.8087, 0.8087, 0.8027, 0.7986, 0.7915,
         0.7812],
        [0.8544, 0.8191, 0.8094, 0.7988, 0.7932, 0.7925, 0.7861, 0.7779, 0.7740,
         0.7675],
        [0.8691, 0.8668, 0.8581, 0.8545, 0.8522, 0.8495, 0.8491, 0.8453, 0.8446,
         0.8420],
        [0.8957, 0.8756, 0.8698, 0.8590, 0.8514, 0.8351, 0.8281, 0.8211, 0.8092,
         0.8071],
        [0.9101, 0.9031, 0.8901, 0.8860, 0.8853, 0.8847, 0.8844, 0.8812, 0.8810,
         0.8809],
        [0.9494, 0.8863, 0.8850, 0.8825, 0.8764, 0.8740, 0.8705, 0.8705, 0.8632,
         0.8616],
        [0.9453, 0.9042, 0.8956, 0.8935, 0.8932, 0.8900, 0.8851, 0.8851, 0.8817,
         0.8814],
        [0.8912, 0.8761, 0.8583, 0.8516, 0.8291, 0.8032, 0.8007, 0.8005, 0.7881,
         0.7821],
        [0.9175, 0.9077, 0.9053, 0.8940, 0.8845, 0.8801, 0.8715, 0.8675, 0.8649,
         0.8546],
        [0.9588, 0.9554, 0.9534, 0.9530, 0.9501, 0.9487, 0.9456, 0.9442, 0.9416,
         0.9370],
        [0.9427, 0.9293, 0.9284, 0.9272, 0.9249, 0.9198, 0.9191, 0.9161, 0.9148,
         0.9135],
        [0.9268, 0.9227, 0.9219, 0.9146, 0.9124, 0.9115, 0.9094, 0.9066, 0.9056,
         0.9032],
        [0.9539, 0.9480, 0.9367, 0.9352, 0.9277, 0.9270, 0.9204, 0.9198, 0.9165,
         0.9157],
        [0.9630, 0.9569, 0.9521, 0.9403, 0.9399, 0.9364, 0.9351, 0.9343, 0.9325,
         0.9315],
        [0.9271, 0.9165, 0.9046, 0.8994, 0.8950, 0.8946, 0.8922, 0.8917, 0.8879,
         0.8844],
        [0.9054, 0.8993, 0.8984, 0.8981, 0.8967, 0.8965, 0.8937, 0.8905, 0.8902,
         0.8898],
        [0.9436, 0.9353, 0.9348, 0.9324, 0.9316, 0.9313, 0.9280, 0.9276, 0.9262,
         0.9252],
        [0.9265, 0.9121, 0.9113, 0.9103, 0.9074, 0.9069, 0.9063, 0.9036, 0.8913,
         0.8883],
        [0.9686, 0.9489, 0.9427, 0.9400, 0.9343, 0.9290, 0.9267, 0.9187, 0.9136,
         0.9133],
        [0.9418, 0.9352, 0.9309, 0.9270, 0.9244, 0.9237, 0.9235, 0.9234, 0.9231,
         0.9216],
        [0.9550, 0.9014, 0.8922, 0.8815, 0.8678, 0.8657, 0.8635, 0.8623, 0.8616,
         0.8584],
        [0.9374, 0.9328, 0.9318, 0.9252, 0.9231, 0.9195, 0.9158, 0.9117, 0.9052,
         0.9047],
        [0.9253, 0.9239, 0.9219, 0.9165, 0.9095, 0.9013, 0.8994, 0.8985, 0.8972,
         0.8964],
        [0.9605, 0.9487, 0.9437, 0.9278, 0.9260, 0.9249, 0.9201, 0.9189, 0.9183,
         0.9174],
        [0.9445, 0.9375, 0.9271, 0.9266, 0.9263, 0.9239, 0.9226, 0.9186, 0.9163,
         0.9145],
        [0.9329, 0.9278, 0.9260, 0.9260, 0.9227, 0.9204, 0.9185, 0.9181, 0.9149,
         0.9140],
        [0.9463, 0.9456, 0.9453, 0.9445, 0.9436, 0.9427, 0.9352, 0.9326, 0.9311,
         0.9291],
        [0.9389, 0.9217, 0.9207, 0.9117, 0.9035, 0.9025, 0.8990, 0.8982, 0.8980,
         0.8967],
        [0.8767, 0.8704, 0.8680, 0.8599, 0.8578, 0.8513, 0.8512, 0.8512, 0.8512,
         0.8506],
        [0.9042, 0.8934, 0.8879, 0.8856, 0.8836, 0.8824, 0.8809, 0.8789, 0.8786,
         0.8781],
        [0.9207, 0.9161, 0.9153, 0.9110, 0.9025, 0.9005, 0.8987, 0.8922, 0.8895,
         0.8885],
        [0.9067, 0.8980, 0.8950, 0.8859, 0.8842, 0.8777, 0.8774, 0.8725, 0.8612,
         0.8597],
        [0.9476, 0.8676, 0.8656, 0.8526, 0.8488, 0.8352, 0.8249, 0.8211, 0.8205,
         0.8169],
        [0.9029, 0.8851, 0.8841, 0.8781, 0.8763, 0.8714, 0.8599, 0.8597, 0.8569,
         0.8557],
        [0.8946, 0.8918, 0.8726, 0.8641, 0.8511, 0.8506, 0.8457, 0.8416, 0.8375,
         0.8354],
        [0.9370, 0.9328, 0.9313, 0.9308, 0.9301, 0.9294, 0.9268, 0.9263, 0.9244,
         0.9238],
        [0.8827, 0.8794, 0.8789, 0.8686, 0.8626, 0.8580, 0.8561, 0.8427, 0.8341,
         0.8271],
        [0.9016, 0.8635, 0.8624, 0.8603, 0.8557, 0.8538, 0.8519, 0.8514, 0.8488,
         0.8478],
        [0.8872, 0.8765, 0.8732, 0.8628, 0.8545, 0.8501, 0.8445, 0.8423, 0.8361,
         0.8354],
        [0.8811, 0.8739, 0.8605, 0.8563, 0.8508, 0.8483, 0.8461, 0.8447, 0.8425,
         0.8348],
        [0.8545, 0.8382, 0.8240, 0.8237, 0.8133, 0.8132, 0.8017, 0.7880, 0.7778,
         0.7769],
        [0.8315, 0.8086, 0.7943, 0.7878, 0.7818, 0.7809, 0.7776, 0.7649, 0.7603,
         0.7582],
        [0.9020, 0.8871, 0.8825, 0.8794, 0.8793, 0.8793, 0.8677, 0.8670, 0.8666,
         0.8655],
        [0.8090, 0.7864, 0.7828, 0.7770, 0.7636, 0.7632, 0.7490, 0.7470, 0.7351,
         0.7252],
        [0.8368, 0.8327, 0.8122, 0.8121, 0.8070, 0.8042, 0.8025, 0.8010, 0.7916,
         0.7893],
        [0.8938, 0.8648, 0.8478, 0.8382, 0.8346, 0.8327, 0.8246, 0.8237, 0.8188,
         0.8149],
        [0.8800, 0.8552, 0.8501, 0.8496, 0.8381, 0.8282, 0.8224, 0.8146, 0.8113,
         0.8071],
        [0.8898, 0.8867, 0.8479, 0.8205, 0.8172, 0.8136, 0.8062, 0.8018, 0.8011,
         0.7989],
        [0.9266, 0.9092, 0.9053, 0.8950, 0.8677, 0.8610, 0.8588, 0.8565, 0.8442,
         0.8416],
        [0.9132, 0.8752, 0.8717, 0.8617, 0.8555, 0.8550, 0.8466, 0.8441, 0.8420,
         0.8409],
        [0.9261, 0.9037, 0.9008, 0.9000, 0.8969, 0.8947, 0.8947, 0.8917, 0.8912,
         0.8905],
        [0.9025, 0.8821, 0.8805, 0.8698, 0.8682, 0.8673, 0.8654, 0.8640, 0.8634,
         0.8625],
        [0.9091, 0.8785, 0.8717, 0.8686, 0.8605, 0.8593, 0.8425, 0.8324, 0.8308,
         0.8269],
        [0.9406, 0.8807, 0.8732, 0.8653, 0.8578, 0.8528, 0.8378, 0.8358, 0.8349,
         0.8235],
        [0.9091, 0.8613, 0.8573, 0.8475, 0.8326, 0.8326, 0.8287, 0.8264, 0.8206,
         0.8200],
        [0.9000, 0.8202, 0.8177, 0.8063, 0.7962, 0.7959, 0.7945, 0.7915, 0.7874,
         0.7808],
        [0.8839, 0.8692, 0.8597, 0.8571, 0.8509, 0.8462, 0.8449, 0.8446, 0.8446,
         0.8445],
        [0.8553, 0.7790, 0.7785, 0.7775, 0.7675, 0.7357, 0.7329, 0.7283, 0.7274,
         0.7268],
        [0.8846, 0.8569, 0.8458, 0.8231, 0.8164, 0.8150, 0.8005, 0.7972, 0.7956,
         0.7910],
        [0.8396, 0.8107, 0.7982, 0.7832, 0.7796, 0.7765, 0.7699, 0.7571, 0.7533,
         0.7504]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 284737.0625,  189013.6094,  168738.4531,  161421.8594,  147923.4844,
          144221.6250,  143124.1250,  133482.0781,  127473.2656,  124003.4375],
        [ 709840.4375,  600509.8750,  588571.0000,  582071.8750,  563765.9375,
          552074.0625,  546730.3125,  534955.3125,  515784.4688,  514162.7812],
        [ 420386.0938,  378531.7500,  288184.9375,  261798.4844,  215801.4531,
          190770.2188,  185755.5469,  164391.4844,  158024.4062,  140010.1406],
        [ 852560.2500,  330649.3438,  315115.0625,  292563.8125,  288603.2500,
          283844.5312,  244397.0625,  239518.9688,  237570.2500,  227425.1875],
        [ 172728.6094,  151523.8438,  147746.8281,  115246.2188,  104086.7031,
          104004.2500,   95533.1875,   90076.6016,   81436.3594,   70246.9375],
        [ 200001.9375,  120670.9688,  105144.7500,   90316.7656,   83444.4844,
           82544.6328,   75402.2031,   67070.9531,   63434.0742,   57787.1328],
        [ 246626.9844,  238804.5938,  210808.0469,  200278.6875,  193775.5156,
          186306.4375,  185358.2656,  175605.5312,  173760.7188,  167501.9844],
        [ 360844.2500,  270562.0938,  249189.7812,  213626.6250,  191577.5156,
          151722.6719,  137381.3438,  124278.5703,  104842.4531,  101701.2344],
        [ 442944.0312,  400673.7500,  333161.2812,  314113.5625,  310652.4375,
          308104.4375,  306728.5625,  292987.9688,  292282.1562,  291850.7188],
        [ 776826.3125,  315212.4688,  309406.1875,  298756.9375,  273949.3750,
          264370.6562,  251659.6875,  251613.3594,  226666.2656,  221608.4219],
        [ 732927.2500,  407436.2500,  359964.6875,  349671.8750,  348122.6562,
          332293.7812,  310080.5938,  310031.5000,  295439.6875,  293989.4375],
        [ 338210.0625,  272453.0625,  211470.9219,  192094.1719,  139247.0156,
           96221.7812,   92859.7578,   92610.8828,   77514.3828,   71198.3203],
        [ 492138.7188,  428204.3125,  413488.7188,  352017.6875,  307509.4062,
          288541.3438,  255339.8594,  241228.1562,  232359.2656,  200553.1719],
        [ 888753.8125,  845782.8125,  822298.4375,  817348.6250,  784292.5625,
          769039.3750,  735292.2500,  721203.7500,  694756.6250,  650307.5000],
        [ 705438.3750,  582977.9375,  575693.9375,  565834.3125,  547596.5625,
          508874.3438,  503637.0000,  482532.8750,  473611.6250,  464884.5000],
        [ 562101.1250,  530403.6250,  524624.8750,  472301.7812,  457774.0938,
          452236.8125,  438949.5625,  421498.0938,  415482.1250,  401617.9062],
        [ 828594.5000,  761607.9375,  647735.6875,  634125.6875,  569940.5625,
          564336.6875,  513457.6875,  509249.6562,  485604.2188,  479945.3750],
        [ 942897.5625,  864121.6875,  807510.4375,  681773.1250,  677772.0000,
          645177.1875,  633505.5625,  625680.8750,  610192.8750,  601584.6250],
        [ 564849.8125,  485434.2500,  409593.0000,  380304.7188,  356925.4062,
          354830.6875,  343129.4375,  340519.5625,  322674.2812,  306818.0938],
        [ 414277.3438,  379638.7188,  374792.7188,  373000.2188,  365623.5312,
          364754.2500,  350328.4062,  334721.1875,  333287.1250,  331453.1250],
        [ 715433.0000,  635175.8125,  630439.0625,  609662.3125,  602650.3750,
          600013.5625,  572363.8750,  568926.1250,  557941.7500,  549780.1250],
        [ 559733.0000,  456028.0938,  450657.1875,  444586.9688,  426006.0000,
          423545.8438,  419824.4062,  403750.4375,  338897.4375,  324290.8125],
        [1021386.7500,  771517.8750,  706012.5000,  679542.6875,  626068.8750,
          580105.6875,  561591.5625,  501027.5625,  465888.9375,  463525.8750],
        [ 696470.7500,  634426.9375,  596376.1875,  564074.1250,  543089.5000,
          537752.2500,  536892.3750,  536062.5625,  533512.5000,  522496.3438],
        [ 841921.5625,  391480.9688,  343297.6562,  294362.8438,  242012.2812,
          234986.6094,  227559.4844,  223687.7812,  221438.7656,  211639.5781],
        [ 653920.6875,  613177.2500,  603972.6250,  549464.5625,  533155.9375,
          506917.5625,  480412.4375,  453275.6875,  413279.7812,  410152.3438],
        [ 550479.0000,  539966.6250,  524584.8750,  485276.9062,  438976.7812,
          390891.5625,  380349.7500,  375586.3438,  368638.4688,  364316.9062],
        [ 910130.3125,  768699.1875,  715876.0000,  570429.3750,  556335.0625,
          547284.8750,  511123.8438,  502504.7188,  497966.0000,  491780.2812],
        [ 724165.3750,  655505.3750,  564520.2500,  560448.2500,  558176.0000,
          539957.3750,  529878.8750,  500584.3125,  484051.1250,  471727.8438],
        [ 613393.0625,  570770.6250,  556081.5000,  556038.0000,  530434.0000,
          513438.0938,  499369.8750,  496733.7500,  474195.9688,  468143.4375],
        [ 743282.3125,  735590.3125,  732494.7500,  724311.0625,  715428.3125,
          705600.5625,  634554.0625,  610655.6875,  598329.5625,  581048.0625],
        [ 668716.5625,  523038.2500,  515366.5312,  453571.0312,  402978.3750,
          397668.2812,  378224.2812,  373924.4688,  372842.3125,  365976.2500],
        [ 274822.5625,  251110.2031,  242952.3125,  216306.8750,  209741.0000,
          191318.2500,  191122.5781,  191051.3281,  190999.4062,  189336.7188],
        [ 407387.6562,  348960.9375,  322692.0938,  312117.9375,  303220.7188,
          298130.7812,  292043.3750,  283658.8750,  282388.4062,  280453.0938],
        [ 515420.5938,  482384.7500,  477156.9062,  448576.2812,  397637.9375,
          386500.7812,  376311.6562,  342968.7812,  330290.0312,  325250.0312],
        [ 422279.4375,  372551.5625,  357193.4062,  313537.4062,  306158.4062,
          278969.4375,  277639.4375,  259055.0938,  220230.7344,  215679.8594],
        [ 756841.5000,  241579.9219,  234668.1562,  194825.6094,  184586.8750,
          151913.6406,  131095.9375,  124315.4453,  123158.8828,  117089.6641],
        [ 399852.2812,  310141.2188,  305666.5312,  280304.6875,  273246.1875,
          255032.9844,  216383.6250,  215719.1406,  207223.9219,  203604.2031],
        [ 354958.9688,  341156.9688,  259452.6719,  229581.4844,  190847.9062,
          189263.2500,  176496.5469,  166567.3906,  157128.3125,  152442.7656],
        [ 650291.9375,  612466.0000,  599946.0000,  595416.3125,  589608.1250,
          583592.6250,  562293.1250,  558062.0625,  543802.6875,  538723.9375],
        [ 299725.2188,  285765.0625,  283568.0000,  245009.8750,  224860.2969,
          210622.1719,  204921.2500,  169091.4062,  149584.8125,  135356.2188],
        [ 392177.5312,  227625.6719,  224177.4688,  217595.4531,  203641.7031,
          198259.9375,  192862.0156,  191647.8750,  184656.9531,  181977.4531],
        [ 319229.9375,  274323.7500,  261572.8906,  225311.9219,  200184.1719,
          188006.8594,  173596.9062,  168261.0312,  153979.7344,  152320.4062],
        [ 292938.5000,  264131.2812,  218287.5781,  205422.3438,  189779.4375,
          183279.0000,  177542.7188,  174061.9062,  168661.5469,  151156.0938],
        [ 200285.2031,  158526.1406,  129537.2500,  129035.4297,  111099.9062,
          111032.2188,   94171.1797,   77470.6328,   66917.4219,   66030.2969],
        [ 144084.1562,  103855.4766,   84681.3125,   77195.4688,   70896.0625,
           69976.2656,   66764.5625,   55699.7852,   52143.9297,   50599.7148],
        [ 394444.9375,  319181.2188,  298866.3750,  285831.3125,  285397.9375,
          285302.1562,  241610.7969,  239293.6094,  238019.9688,  234162.4844],
        [ 104484.5234,   75728.7344,   71891.2578,   66175.6016,   54674.0977,
           54312.1367,   44346.7539,   43080.3047,   36352.2344,   31582.4629],
        [ 155562.0000,  146725.5938,  109398.5234,  109267.4609,  101583.6562,
           97658.3203,   95303.3203,   93219.3828,   81544.5391,   78875.5938],
        [ 351243.7188,  231821.4062,  181960.2812,  158603.5625,  150644.2188,
          146649.7656,  130674.2969,  129019.7969,  120284.2266,  113663.9609],
        [ 288210.7812,  202319.5625,  187976.7344,  186809.4062,  158296.0625,
          137498.1250,  126495.6172,  113191.4688,  108066.7812,  101757.2109],
        [ 331733.9375,  317038.5000,  182215.7031,  123108.2656,  117584.8359,
          111591.6562,  100484.4688,   94279.2812,   93364.4922,   90505.1719],
        [ 560940.7500,  437176.9688,  413547.0625,  357117.0938,  241751.6094,
          219691.0000,  212859.3438,  206048.4375,  172759.8906,  166547.3594],
        [ 463220.9688,  269156.1562,  255817.3594,  221813.5156,  203223.5938,
          201777.9062,  178816.6406,  172592.2656,  167469.4062,  164937.3438],
        [ 556427.3750,  404407.8125,  388077.1250,  383548.9688,  366962.1562,
          355531.5000,  355531.5000,  340648.5000,  338352.9688,  334953.3438],
        [ 397200.9375,  297187.1875,  290363.5000,  249260.1250,  243656.2969,
          240499.0781,  234004.4375,  229326.7656,  227269.9531,  224325.2500],
        [ 436485.0000,  281896.8125,  255910.5625,  244776.3281,  218206.8281,
          214576.0781,  168569.8906,  145997.6719,  142807.8125,  134898.2188],
        [ 685195.8125,  291096.3125,  261604.0625,  233634.0469,  209942.9375,
          195456.8594,  157626.1406,  153321.5000,  151417.3906,  128546.7109],
        [ 436510.4062,  220732.0156,  208523.5938,  181110.5781,  146409.5469,
          146351.7500,  138422.7969,  133993.0156,  123376.0078,  122320.7734],
        [ 383347.0938,  122598.7266,  118349.6172,  100627.3516,   87035.4766,
           86711.7891,   84961.2734,   81356.2500,   76756.4766,   69827.4766],
        [ 304829.5938,  247103.5000,  215730.2500,  207810.3125,  190169.1406,
          177740.5938,  174504.3594,  173800.1719,  173760.0625,  173604.7031],
        [ 202440.1875,   68114.0391,   67561.2266,   66617.3906,   57743.1172,
           36654.7500,   35227.4766,   33012.6758,   32567.1973,   32278.5215],
        [ 307856.8125,  207214.4375,  176732.5312,  127872.5156,  116132.8281,
          113899.6562,   92519.7812,   88247.5625,   86290.4922,   80850.7188],
        [ 161895.7812,  107087.7344,   89622.4688,   72288.7031,   68685.5938,
           65669.8906,   59807.6016,   49766.5195,   47173.6172,   45232.8438]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[284737.0625,      0.0000],
         [189013.6094,      0.0000],
         [168738.4531,      0.0000],
         ...,
         [133482.0781,      0.0000],
         [127473.2656,      0.0000],
         [     0.0000, 124003.4375]],

        [[709840.4375,      0.0000],
         [600509.8750,      0.0000],
         [588571.0000,      0.0000],
         ...,
         [534955.3125,      0.0000],
         [515784.4688,      0.0000],
         [514162.7812,      0.0000]],

        [[420386.0938,      0.0000],
         [378531.7500,      0.0000],
         [288184.9375,      0.0000],
         ...,
         [164391.4844,      0.0000],
         [158024.4062,      0.0000],
         [140010.1406,      0.0000]],

        ...,

        [[     0.0000, 202440.1875],
         [ 68114.0391,      0.0000],
         [ 67561.2266,      0.0000],
         ...,
         [ 33012.6758,      0.0000],
         [ 32567.1973,      0.0000],
         [ 32278.5215,      0.0000]],

        [[307856.8125,      0.0000],
         [     0.0000, 207214.4375],
         [176732.5312,      0.0000],
         ...,
         [ 88247.5625,      0.0000],
         [ 86290.4922,      0.0000],
         [ 80850.7188,      0.0000]],

        [[     0.0000, 161895.7812],
         [     0.0000, 107087.7344],
         [     0.0000,  89622.4688],
         ...,
         [ 49766.5195,      0.0000],
         [     0.0000,  47173.6172],
         [     0.0000,  45232.8438]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1500135.5000,  124003.4375],
        [5708466.0000,       0.0000],
        [2403654.5000,       0.0000],
        [3312247.7500,       0.0000],
        [ 466549.4062,  666080.1250],
        [ 795302.4375,  150515.4375],
        [1635719.2500,  343107.5000],
        [1905726.5000,       0.0000],
        [3293499.0000,       0.0000],
        [3190069.7500,       0.0000],
        [3739957.5000,       0.0000],
        [1583880.2500,       0.0000],
        [3211380.5000,       0.0000],
        [7729076.5000,       0.0000],
        [5411081.5000,       0.0000],
        [4676990.0000,       0.0000],
        [5994598.0000,       0.0000],
        [7090216.0000,       0.0000],
        [3865079.2500,       0.0000],
        [3287155.2500,  334721.1875],
        [6042386.0000,       0.0000],
        [4247320.5000,       0.0000],
        [6376668.0000,       0.0000],
        [5701154.0000,       0.0000],
        [3232387.5000,       0.0000],
        [5217729.0000,       0.0000],
        [4419067.0000,       0.0000],
        [6072129.5000,       0.0000],
        [5589014.5000,       0.0000],
        [5278598.0000,       0.0000],
        [6781295.0000,       0.0000],
        [4452306.0000,       0.0000],
        [2148761.2500,       0.0000],
        [2505141.2500,  625912.8125],
        [1851977.3750, 2230520.2500],
        [1067517.2500, 1955777.6250],
        [1138495.2500, 1121580.2500],
        [1403630.3750, 1263544.5000],
        [ 744125.1250, 1473771.2500],
        [5834203.0000,       0.0000],
        [2208504.2500,       0.0000],
        [2214622.0000,       0.0000],
        [2116787.7500,       0.0000],
        [2025260.5000,       0.0000],
        [ 255487.9688,  888617.7500],
        [ 336976.2812,  438920.4375],
        [2427666.0000,  394444.9375],
        [ 370168.1250,  212459.9688],
        [ 472305.5000,  596832.8750],
        [ 464907.6875, 1249657.5000],
        [1365056.8750,  245564.9062],
        [ 730918.1875,  830988.1250],
        [1601858.6250, 1386580.8750],
        [2095601.5000,  203223.5938],
        [3824441.0000,       0.0000],
        [2633093.5000,       0.0000],
        [1030054.6875, 1214070.5000],
        [1733168.5000,  734673.3125],
        [1002472.5625,  855277.9375],
        [ 571057.8125,  640513.7500],
        [1296950.5000,  742102.2500],
        [ 429776.3750,  202440.1875],
        [ 960370.4375,  437246.9375],
        [ 115436.4062,  651794.3125]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 391/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:04, 58.09s/it]  7%|▋         | 2/30 [01:00<11:41, 25.04s/it] 10%|█         | 3/30 [01:01<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:03<02:29,  6.00s/it] 20%|██        | 6/30 [01:03<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.3527445077896116
Epoch 392/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:43, 61.52s/it]  7%|▋         | 2/30 [01:04<12:30, 26.79s/it] 10%|█         | 3/30 [01:04<06:42, 14.90s/it] 13%|█▎        | 4/30 [01:05<04:02,  9.31s/it] 17%|█▋        | 5/30 [01:06<02:35,  6.22s/it] 20%|██        | 6/30 [01:06<01:44,  4.36s/it] 23%|██▎       | 7/30 [01:07<01:13,  3.18s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.41s/it] 30%|███       | 9/30 [01:09<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.84s/it]
Epoch loss is 2.3815102497736613
Epoch 393/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:28, 60.99s/it]  7%|▋         | 2/30 [01:01<11:55, 25.55s/it] 10%|█         | 3/30 [01:02<06:24, 14.23s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.91s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.96s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.407437268892924
Epoch 394/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:16, 62.63s/it]  7%|▋         | 2/30 [01:03<12:14, 26.23s/it] 10%|█         | 3/30 [01:04<06:34, 14.59s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.13s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.11s/it] 20%|██        | 6/30 [01:06<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:07<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 2.3478982925415037
Epoch 395/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:21, 60.72s/it]  7%|▋         | 2/30 [01:01<11:52, 25.44s/it] 10%|█         | 3/30 [01:02<06:22, 14.17s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3667461554209392
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0237,  0.0016,  0.0126,  ..., -0.0050,  0.0158,  0.0039],
        [-0.0022,  0.0105,  0.0280,  ...,  0.0166,  0.0027, -0.0155],
        [-0.0295, -0.0381,  0.0247,  ...,  0.0822, -0.0055, -0.0151],
        ...,
        [ 0.0070, -0.0035,  0.0089,  ..., -0.0238, -0.0065, -0.0047],
        [-0.0327,  0.0089, -0.0040,  ...,  0.0047,  0.0164, -0.0112],
        [-0.0336, -0.0135,  0.0186,  ...,  0.0407,  0.0352, -0.0301]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8785, 0.8505, 0.8419, 0.8391, 0.8330, 0.8308, 0.8306, 0.8255, 0.8225,
         0.8205],
        [0.9429, 0.9310, 0.9296, 0.9290, 0.9268, 0.9250, 0.9244, 0.9229, 0.9204,
         0.9201],
        [0.9062, 0.8988, 0.8793, 0.8729, 0.8592, 0.8507, 0.8488, 0.8402, 0.8375,
         0.8292],
        [0.9559, 0.8891, 0.8857, 0.8807, 0.8799, 0.8783, 0.8679, 0.8664, 0.8657,
         0.8629],
        [0.8439, 0.8344, 0.8330, 0.8156, 0.8082, 0.8082, 0.8025, 0.7983, 0.7911,
         0.7812],
        [0.8542, 0.8189, 0.8093, 0.7984, 0.7926, 0.7924, 0.7857, 0.7779, 0.7733,
         0.7672],
        [0.8685, 0.8661, 0.8574, 0.8541, 0.8516, 0.8490, 0.8485, 0.8444, 0.8440,
         0.8410],
        [0.8952, 0.8751, 0.8694, 0.8586, 0.8511, 0.8344, 0.8278, 0.8206, 0.8087,
         0.8062],
        [0.9098, 0.9029, 0.8897, 0.8856, 0.8850, 0.8845, 0.8841, 0.8808, 0.8807,
         0.8806],
        [0.9493, 0.8858, 0.8845, 0.8822, 0.8759, 0.8735, 0.8700, 0.8700, 0.8628,
         0.8612],
        [0.9452, 0.9041, 0.8951, 0.8929, 0.8926, 0.8895, 0.8849, 0.8846, 0.8811,
         0.8811],
        [0.8906, 0.8755, 0.8579, 0.8510, 0.8284, 0.8024, 0.8001, 0.7998, 0.7873,
         0.7815],
        [0.9167, 0.9074, 0.9048, 0.8934, 0.8838, 0.8795, 0.8709, 0.8670, 0.8644,
         0.8541],
        [0.9585, 0.9552, 0.9530, 0.9527, 0.9498, 0.9483, 0.9453, 0.9438, 0.9413,
         0.9366],
        [0.9425, 0.9291, 0.9282, 0.9270, 0.9246, 0.9196, 0.9189, 0.9158, 0.9146,
         0.9131],
        [0.9266, 0.9225, 0.9217, 0.9144, 0.9122, 0.9113, 0.9093, 0.9063, 0.9054,
         0.9031],
        [0.9536, 0.9477, 0.9363, 0.9347, 0.9274, 0.9267, 0.9200, 0.9195, 0.9161,
         0.9153],
        [0.9627, 0.9567, 0.9520, 0.9401, 0.9398, 0.9362, 0.9349, 0.9340, 0.9324,
         0.9313],
        [0.9268, 0.9160, 0.9040, 0.8992, 0.8947, 0.8942, 0.8919, 0.8913, 0.8876,
         0.8843],
        [0.9050, 0.8991, 0.8981, 0.8978, 0.8963, 0.8961, 0.8933, 0.8903, 0.8898,
         0.8894],
        [0.9431, 0.9349, 0.9344, 0.9320, 0.9314, 0.9310, 0.9277, 0.9271, 0.9259,
         0.9249],
        [0.9263, 0.9118, 0.9112, 0.9100, 0.9068, 0.9065, 0.9062, 0.9033, 0.8909,
         0.8877],
        [0.9684, 0.9488, 0.9426, 0.9398, 0.9341, 0.9286, 0.9264, 0.9185, 0.9134,
         0.9130],
        [0.9416, 0.9350, 0.9307, 0.9269, 0.9243, 0.9234, 0.9233, 0.9233, 0.9231,
         0.9213],
        [0.9548, 0.9011, 0.8917, 0.8811, 0.8673, 0.8649, 0.8630, 0.8616, 0.8610,
         0.8579],
        [0.9370, 0.9326, 0.9314, 0.9248, 0.9228, 0.9192, 0.9154, 0.9114, 0.9049,
         0.9045],
        [0.9250, 0.9235, 0.9214, 0.9162, 0.9091, 0.9010, 0.8990, 0.8982, 0.8969,
         0.8960],
        [0.9604, 0.9486, 0.9434, 0.9274, 0.9258, 0.9247, 0.9199, 0.9187, 0.9181,
         0.9172],
        [0.9442, 0.9372, 0.9267, 0.9265, 0.9259, 0.9236, 0.9222, 0.9184, 0.9158,
         0.9141],
        [0.9326, 0.9276, 0.9259, 0.9258, 0.9225, 0.9200, 0.9182, 0.9179, 0.9146,
         0.9139],
        [0.9460, 0.9454, 0.9451, 0.9443, 0.9434, 0.9424, 0.9350, 0.9323, 0.9309,
         0.9289],
        [0.9386, 0.9215, 0.9205, 0.9114, 0.9031, 0.9024, 0.8987, 0.8979, 0.8977,
         0.8965],
        [0.8763, 0.8701, 0.8677, 0.8596, 0.8576, 0.8509, 0.8509, 0.8508, 0.8507,
         0.8505],
        [0.9036, 0.8928, 0.8874, 0.8852, 0.8830, 0.8818, 0.8806, 0.8787, 0.8781,
         0.8776],
        [0.9203, 0.9157, 0.9150, 0.9105, 0.9021, 0.9006, 0.8983, 0.8918, 0.8891,
         0.8881],
        [0.9065, 0.8975, 0.8947, 0.8857, 0.8838, 0.8774, 0.8771, 0.8720, 0.8608,
         0.8593],
        [0.9475, 0.8676, 0.8654, 0.8523, 0.8484, 0.8345, 0.8243, 0.8208, 0.8196,
         0.8167],
        [0.9024, 0.8844, 0.8833, 0.8773, 0.8761, 0.8712, 0.8592, 0.8590, 0.8565,
         0.8552],
        [0.8943, 0.8914, 0.8720, 0.8636, 0.8508, 0.8498, 0.8451, 0.8408, 0.8371,
         0.8351],
        [0.9366, 0.9325, 0.9310, 0.9305, 0.9297, 0.9288, 0.9264, 0.9259, 0.9241,
         0.9238],
        [0.8824, 0.8790, 0.8785, 0.8681, 0.8623, 0.8577, 0.8556, 0.8422, 0.8336,
         0.8266],
        [0.9012, 0.8629, 0.8621, 0.8599, 0.8550, 0.8532, 0.8512, 0.8506, 0.8483,
         0.8473],
        [0.8867, 0.8760, 0.8726, 0.8624, 0.8540, 0.8496, 0.8439, 0.8417, 0.8358,
         0.8351],
        [0.8803, 0.8735, 0.8599, 0.8555, 0.8503, 0.8475, 0.8457, 0.8442, 0.8422,
         0.8347],
        [0.8539, 0.8378, 0.8234, 0.8229, 0.8126, 0.8126, 0.8010, 0.7869, 0.7767,
         0.7764],
        [0.8311, 0.8080, 0.7935, 0.7875, 0.7814, 0.7807, 0.7773, 0.7647, 0.7601,
         0.7581],
        [0.9015, 0.8863, 0.8819, 0.8787, 0.8786, 0.8784, 0.8667, 0.8662, 0.8658,
         0.8645],
        [0.8084, 0.7860, 0.7822, 0.7763, 0.7635, 0.7626, 0.7479, 0.7459, 0.7343,
         0.7244],
        [0.8360, 0.8321, 0.8118, 0.8118, 0.8064, 0.8040, 0.8025, 0.8003, 0.7913,
         0.7886],
        [0.8934, 0.8643, 0.8472, 0.8373, 0.8341, 0.8321, 0.8241, 0.8231, 0.8183,
         0.8145],
        [0.8793, 0.8549, 0.8496, 0.8492, 0.8376, 0.8279, 0.8222, 0.8139, 0.8110,
         0.8067],
        [0.8895, 0.8863, 0.8473, 0.8195, 0.8163, 0.8124, 0.8057, 0.8011, 0.8005,
         0.7980],
        [0.9262, 0.9088, 0.9046, 0.8946, 0.8671, 0.8606, 0.8584, 0.8558, 0.8438,
         0.8408],
        [0.9129, 0.8750, 0.8713, 0.8614, 0.8549, 0.8547, 0.8459, 0.8439, 0.8414,
         0.8404],
        [0.9256, 0.9032, 0.9003, 0.8996, 0.8966, 0.8943, 0.8943, 0.8913, 0.8907,
         0.8903],
        [0.9019, 0.8817, 0.8802, 0.8694, 0.8677, 0.8669, 0.8647, 0.8635, 0.8627,
         0.8622],
        [0.9084, 0.8781, 0.8710, 0.8680, 0.8603, 0.8587, 0.8424, 0.8318, 0.8299,
         0.8263],
        [0.9405, 0.8803, 0.8727, 0.8647, 0.8574, 0.8525, 0.8373, 0.8351, 0.8342,
         0.8228],
        [0.9085, 0.8610, 0.8569, 0.8469, 0.8322, 0.8320, 0.8278, 0.8259, 0.8199,
         0.8191],
        [0.8996, 0.8197, 0.8171, 0.8056, 0.7953, 0.7952, 0.7938, 0.7907, 0.7868,
         0.7798],
        [0.8838, 0.8687, 0.8592, 0.8566, 0.8503, 0.8452, 0.8446, 0.8440, 0.8439,
         0.8438],
        [0.8550, 0.7784, 0.7778, 0.7772, 0.7668, 0.7355, 0.7322, 0.7278, 0.7267,
         0.7266],
        [0.8843, 0.8566, 0.8451, 0.8229, 0.8156, 0.8146, 0.7993, 0.7962, 0.7947,
         0.7901],
        [0.8394, 0.8101, 0.7976, 0.7826, 0.7792, 0.7756, 0.7695, 0.7563, 0.7527,
         0.7494]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 0, 1],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 282203.1875,  189173.9219,  167259.8125,  160699.6406,  147216.5781,
          142706.9219,  142220.5469,  132327.7969,  126816.1953,  123140.6797],
        [ 707607.3750,  597400.1875,  585055.4375,  580514.6250,  562731.9375,
          548276.3750,  543120.0625,  532000.9375,  513241.3125,  511098.5000],
        [ 419334.6250,  376957.4688,  285430.5938,  260395.6094,  214072.6406,
          189664.7344,  184574.2031,  163260.3281,  157105.0781,  139532.6875],
        [ 852848.1875,  328018.3125,  312746.6250,  291305.1562,  287738.6875,
          281289.0938,  242623.5312,  237441.3594,  235118.4375,  225665.8906],
        [ 172168.6250,  150250.9375,  147268.4062,  114925.6328,  103379.8672,
          103345.8516,   95254.9844,   89663.7656,   80954.7188,   70216.1172],
        [ 199298.7812,  120336.6641,  104996.9453,   89878.8125,   82670.6094,
           82506.4688,   74884.7266,   67048.4375,   62759.1758,   57553.9961],
        [ 244384.4844,  236328.3125,  208809.9688,  199078.4375,  192147.4844,
          185148.7500,  183829.0156,  173223.4844,  172408.3438,  165041.6562],
        [ 357929.9688,  268595.8750,  247720.0312,  212319.6406,  190591.2812,
          150176.8750,  136796.4219,  123325.1875,  104042.0469,  100490.4062],
        [ 440900.8750,  399545.8125,  330968.9062,  312255.7812,  309640.2812,
          307487.1250,  305440.6875,  291376.0000,  291229.3125,  290482.3438],
        [ 775762.4375,  313046.5000,  307412.0625,  297207.9062,  271872.6875,
          262743.6875,  249907.0781,  249710.0469,  225423.2656,  220283.4531],
        [ 731719.7500,  406744.4062,  357534.2188,  346400.8750,  345087.2812,
          329908.1875,  308973.3438,  307774.9062,  292645.8750,  292567.4375],
        [ 335499.0312,  270377.9375,  210204.3750,  190425.0469,  137864.3281,
           95092.4219,   92088.5312,   91698.4688,   76617.5234,   70570.1094],
        [ 487163.6875,  426430.7812,  410478.3125,  349097.0938,  304191.0000,
          286040.7500,  253221.3906,  239196.6406,  230783.3438,  199012.5625],
        [ 884452.6250,  844205.7500,  818117.5625,  813877.1875,  780894.5000,
          764881.1875,  732063.8750,  717376.1250,  691631.0000,  646790.6250],
        [ 703977.3125,  581213.1875,  573854.3125,  563682.6250,  544931.3125,
          507429.2500,  502106.1875,  480829.0938,  472315.7500,  462764.4062],
        [ 560990.9375,  528934.7500,  522938.0000,  470780.0312,  456587.3125,
          450710.0312,  437751.8750,  419826.0000,  414124.4375,  400857.2188],
        [ 824688.3125,  757808.5625,  644485.9375,  629201.7500,  567192.0000,
          561596.4375,  510112.4062,  506689.8750,  482645.1875,  477508.7500],
        [ 939873.8125,  861514.1250,  805906.4375,  679884.9375,  677162.0000,
          642863.5625,  631499.8750,  623065.1250,  609432.7500,  599546.2500],
        [ 562614.9375,  482169.4688,  405783.1562,  379140.1562,  355408.4688,
          353047.1250,  341753.8750,  338818.5938,  321477.3125,  306534.6562],
        [ 411939.5000,  378479.3750,  373101.9688,  371870.0312,  363914.4688,
          362805.2500,  348472.7500,  333935.2188,  331661.5000,  329393.8438],
        [ 710328.6875,  631199.5000,  626911.3125,  605583.4375,  600732.1250,
          597566.0625,  569423.3750,  564458.8750,  555573.1875,  547141.3125],
        [ 558260.6250,  454097.7500,  450208.6875,  442261.0938,  422771.0625,
          421134.8750,  418942.0938,  402064.0000,  336992.3750,  321730.9375],
        [1018814.5000,  770630.3750,  704704.1250,  677121.3750,  624260.6250,
          577211.2500,  559189.8750,  499724.7812,  464603.0625,  461642.5625],
        [ 694896.3750,  632219.4375,  594659.8750,  562998.1875,  542556.8125,
          535662.9375,  535085.9375,  534768.1875,  533394.4375,  519872.4688],
        [ 838545.6875,  389405.5000,  340471.8125,  292740.5000,  240500.4375,
          232365.6875,  225984.6250,  221596.7969,  219590.0469,  210223.2188],
        [ 651011.1250,  611131.0625,  600564.3125,  546814.8125,  531539.4375,
          504311.2812,  477966.1875,  451077.6875,  411179.2188,  408772.3438],
        [ 547891.1875,  536664.6250,  521020.9688,  483049.0312,  436656.9688,
          389012.7812,  378177.7812,  373664.9375,  367011.8438,  362377.5000],
        [ 908331.0625,  767508.0625,  713435.3125,  567453.8750,  554232.6875,
          545542.8125,  509592.1562,  500727.5938,  497036.0938,  490439.8125],
        [ 721366.1250,  652718.8125,  561487.1250,  559665.2500,  555387.7500,
          537527.1875,  526971.6250,  499019.4688,  480607.1875,  469076.5625],
        [ 610586.9375,  568752.5000,  554909.1250,  554383.3750,  528704.7500,
          510454.5312,  497593.3438,  495485.1562,  472628.4688,  467484.0312],
        [ 739764.8750,  733381.6875,  730692.5625,  722411.1875,  712540.5000,
          703226.4375,  632286.3125,  608313.2500,  596376.1875,  579363.1875],
        [ 665725.2500,  521461.4062,  513767.7500,  451134.4688,  400767.0000,
          396674.7812,  376296.2188,  372403.0625,  371036.1250,  364763.6562],
        [ 273423.9688,  250185.1406,  241674.1719,  215375.2344,  209269.8750,
          190207.4219,  190080.4844,  190000.0156,  189705.6094,  189058.3125],
        [ 403973.4062,  346090.1875,  320520.6250,  310433.2812,  300866.1875,
          295754.8750,  290758.6562,  282944.2812,  280572.1562,  278437.5625],
        [ 512670.8750,  480195.8125,  475086.8125,  445411.9688,  395320.5312,
          386644.5625,  374308.0000,  340880.8750,  328219.8438,  323518.2188],
        [ 420794.8438,  370102.0938,  355447.4375,  312518.8438,  304070.3438,
          277684.2188,  276681.8125,  257247.5625,  218978.9688,  214543.3438],
        [ 756135.2500,  241473.2656,  233885.2812,  194101.7812,  183542.9375,
          150399.0312,  130018.2031,  123706.4922,  121577.9688,  116709.1719],
        [ 396906.3438,  307092.6562,  302179.1562,  277316.0625,  272547.3750,
          253999.9375,  213994.6875,  213431.3438,  206143.3750,  202149.0625],
        [ 353668.9062,  339195.8750,  257099.6562,  227990.9062,  189884.0781,
          187106.0781,  175038.2188,  164692.9219,  156224.1875,  151698.0781],
        [ 646481.0000,  610052.0000,  596996.4375,  592918.0000,  586389.8750,
          578791.0625,  559518.9375,  555240.0000,  540926.3125,  538491.2500],
        [ 298335.2812,  284208.0312,  282132.6875,  243099.2500,  223922.5625,
          209483.1406,  203459.4062,  167899.7344,  148455.2812,  134416.7812],
        [ 390284.7500,  225794.4062,  223259.8281,  216226.8438,  201577.7031,
          196589.7188,  191083.0469,  189456.2969,  183312.3906,  180743.0469],
        [ 317073.2812,  272243.9688,  259423.9688,  224137.0781,  198734.5156,
          186616.0312,  171959.3906,  166865.9844,  153276.3281,  151788.6719],
        [ 289482.8750,  262667.5312,  216402.2031,  203035.2969,  188635.9844,
          181069.4688,  176647.6094,  172889.9531,  167979.8125,  150856.2656],
        [ 198619.1250,  157805.5781,  128344.2266,  127412.3750,  110022.9453,
          109981.6172,   93273.0000,   76208.1172,   65903.3438,   65582.7656],
        [ 143393.9688,  103094.7422,   83814.6328,   76839.8906,   70496.9922,
           69767.1016,   66475.4844,   55532.9219,   51982.8086,   50521.3633],
        [ 392042.5312,  315435.5938,  296147.7500,  282718.5000,  282356.6250,
          281662.7500,  238470.2812,  236628.7188,  235161.0156,  230939.4375],
        [ 103582.7734,   75260.1719,   71310.1641,   65478.0898,   54596.6719,
           53880.7422,   43656.9727,   42451.5352,   35978.2969,   31214.3965],
        [ 153698.0312,  145416.8438,  108800.0625,  108784.2969,  100737.9609,
           97317.2109,   95216.4688,   92364.4453,   81170.1016,   78052.8516],
        [ 348782.6250,  230234.6406,  180368.3750,  156607.0938,  149587.1094,
          145401.4531,  129649.2266,  127890.0781,  119387.1016,  113092.8516],
        [ 285384.0625,  201258.4531,  186596.4375,  185726.3281,  157286.0312,
          136843.2656,  126127.7344,  112134.5547,  107501.4297,  101171.1172],
        [ 329907.2500,  315502.3750,  180647.9219,  121488.4922,  116020.8047,
          109741.0547,   99663.9844,   93409.9141,   92620.3359,   89283.5312],
        [ 557347.2500,  434672.1875,  409633.2188,  354950.8438,  239581.7969,
          218479.3750,  211542.1250,  203888.6875,  171857.4219,  164728.7344],
        [ 461280.4062,  268364.1562,  254593.8750,  220797.7031,  201463.3438,
          200714.0938,  176944.3438,  172056.8438,  166125.5625,  163693.2812],
        [ 552912.3125,  401411.1250,  385398.0312,  381083.1562,  365572.6250,
          353292.3438,  353292.3438,  338893.5625,  335656.1875,  333665.2812],
        [ 394155.3750,  295149.0625,  289168.3125,  247838.1875,  241878.4531,
          238934.2344,  231603.0781,  227611.1406,  225096.3125,  223604.7969],
        [ 432128.2188,  280475.5625,  253359.0938,  242965.7500,  217373.3125,
          212675.9062,  168395.3906,  144794.3906,  140981.2812,  133781.8281],
        [ 683779.9375,  289380.7500,  259705.6562,  231622.2969,  208616.2969,
          194585.7031,  156543.3438,  151850.2031,  149913.2969,  127285.7031],
        [ 433229.5625,  219555.9219,  207093.1250,  179624.5625,  145605.4219,
          145152.7500,  136794.0781,  132991.1094,  122117.3828,  120707.3359],
        [ 381544.2500,  121710.1094,  117309.3047,   99548.8516,   85957.6797,
           85792.3359,   84113.1484,   80482.3750,   76124.4453,   68900.9141],
        [ 304416.5000,  245088.8594,  214195.1719,  206375.6875,  188500.5938,
          175414.7188,  173853.7188,  172318.5938,  172110.6562,  171733.2344],
        [ 201651.9219,   67542.8047,   66932.8672,   66346.0859,   57219.6641,
           36591.3242,   34882.3359,   32786.5195,   32237.1426,   32202.2988],
        [ 306456.0625,  206308.7812,  174930.5781,  127389.0469,  114834.5859,
          113147.1094,   90983.6484,   87078.8984,   85150.8359,   79739.8516],
        [ 161265.9844,  106155.9297,   88830.0000,   71645.2031,   68313.6953,
           64840.2852,   59485.9258,   49220.9336,   46753.0625,   44580.3555]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[282203.1875,      0.0000],
         [189173.9219,      0.0000],
         [167259.8125,      0.0000],
         ...,
         [132327.7969,      0.0000],
         [126816.1953,      0.0000],
         [     0.0000, 123140.6797]],

        [[707607.3750,      0.0000],
         [597400.1875,      0.0000],
         [585055.4375,      0.0000],
         ...,
         [532000.9375,      0.0000],
         [513241.3125,      0.0000],
         [511098.5000,      0.0000]],

        [[419334.6250,      0.0000],
         [376957.4688,      0.0000],
         [285430.5938,      0.0000],
         ...,
         [163260.3281,      0.0000],
         [157105.0781,      0.0000],
         [139532.6875,      0.0000]],

        ...,

        [[     0.0000, 201651.9219],
         [ 67542.8047,      0.0000],
         [ 66932.8672,      0.0000],
         ...,
         [ 32786.5195,      0.0000],
         [ 32237.1426,      0.0000],
         [ 32202.2988,      0.0000]],

        [[306456.0625,      0.0000],
         [     0.0000, 206308.7812],
         [174930.5781,      0.0000],
         ...,
         [ 87078.8984,      0.0000],
         [ 85150.8359,      0.0000],
         [ 79739.8516,      0.0000]],

        [[     0.0000, 161265.9844],
         [     0.0000, 106155.9297],
         [     0.0000,  88830.0000],
         ...,
         [ 49220.9336,      0.0000],
         [     0.0000,  46753.0625],
         [     0.0000,  44580.3555]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1490624.5000,  123140.6797],
        [5681046.5000,       0.0000],
        [2390328.0000,       0.0000],
        [3294795.0000,       0.0000],
        [ 464697.3125,  662731.6250],
        [ 792379.6875,  149554.9062],
        [1622134.7500,  338265.1250],
        [1891987.7500,       0.0000],
        [3279327.2500,       0.0000],
        [3173369.0000,       0.0000],
        [3719356.2500,       0.0000],
        [1570437.7500,       0.0000],
        [3185615.5000,       0.0000],
        [7694290.0000,       0.0000],
        [5393104.0000,       0.0000],
        [4663500.5000,       0.0000],
        [5961929.0000,       0.0000],
        [7070749.0000,       0.0000],
        [3846747.7500,       0.0000],
        [3271638.7500,  333935.2188],
        [6008918.0000,       0.0000],
        [4228463.5000,       0.0000],
        [6357902.5000,       0.0000],
        [5686114.5000,       0.0000],
        [3211424.2500,       0.0000],
        [5194367.5000,       0.0000],
        [4395528.0000,       0.0000],
        [6054299.5000,       0.0000],
        [5563827.0000,       0.0000],
        [5260982.5000,       0.0000],
        [6758356.0000,       0.0000],
        [4434030.0000,       0.0000],
        [2138980.0000,       0.0000],
        [2488964.5000,  621386.8125],
        [1842013.7500, 2220243.7500],
        [1060872.0000, 1947197.5000],
        [1132362.8750, 1119186.5000],
        [1391897.6250, 1253862.5000],
        [ 738792.0625, 1463806.7500],
        [5805805.0000,       0.0000],
        [2195412.2500,       0.0000],
        [2198328.0000,       0.0000],
        [2102119.2500,       0.0000],
        [2009667.0000,       0.0000],
        [ 252134.4062,  881018.6250],
        [ 335562.2812,  436357.6250],
        [2399520.7500,  392042.5312],
        [ 366407.8125,  211001.9688],
        [ 469171.7500,  592386.5000],
        [ 461875.4688, 1239125.0000],
        [1355684.6250,  244344.6875],
        [ 722228.1250,  826057.5625],
        [1589961.8750, 1376719.7500],
        [2084570.2500,  201463.3438],
        [3801177.2500,       0.0000],
        [2615038.7500,       0.0000],
        [1021892.8750, 1205037.7500],
        [1723663.8750,  729619.3750],
        [ 994251.1250,  848620.1875],
        [ 565697.0625,  635786.3750],
        [1286001.7500,  738005.9375],
        [ 426741.0625,  201651.9219],
        [ 951728.9375,  434290.4688],
        [ 114061.2188,  647030.1250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 396/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:12, 60.44s/it]  7%|▋         | 2/30 [01:01<11:49, 25.33s/it] 10%|█         | 3/30 [01:01<06:20, 14.10s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.83s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.3573964913686116
Epoch 397/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:37, 61.29s/it]  7%|▋         | 2/30 [01:02<11:58, 25.68s/it] 10%|█         | 3/30 [01:02<06:25, 14.29s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.99s/it] 20%|██        | 6/30 [01:05<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.3357327222824096
Epoch 398/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:27, 56.82s/it]  7%|▋         | 2/30 [00:57<11:07, 23.84s/it] 10%|█         | 3/30 [00:59<06:12, 13.80s/it] 13%|█▎        | 4/30 [01:00<03:44,  8.65s/it] 17%|█▋        | 5/30 [01:00<02:25,  5.80s/it] 20%|██        | 6/30 [01:01<01:38,  4.08s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.28s/it] 30%|███       | 9/30 [01:03<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.66s/it]
Epoch loss is 2.3528438250223798
Epoch 399/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:41, 59.37s/it]  7%|▋         | 2/30 [01:00<11:36, 24.89s/it] 10%|█         | 3/30 [01:00<06:14, 13.87s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.69s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.83s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.370639705657959
Epoch 400/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:55<26:50, 55.54s/it]  7%|▋         | 2/30 [01:00<11:55, 25.57s/it] 10%|█         | 3/30 [01:01<06:33, 14.56s/it] 13%|█▎        | 4/30 [01:02<03:56,  9.11s/it] 17%|█▋        | 5/30 [01:03<02:32,  6.09s/it] 20%|██        | 6/30 [01:03<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:05<00:52,  2.37s/it] 30%|███       | 9/30 [01:06<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.3459951639175416
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0237,  0.0015,  0.0127,  ..., -0.0051,  0.0161,  0.0039],
        [-0.0022,  0.0104,  0.0281,  ...,  0.0167,  0.0029, -0.0154],
        [-0.0295, -0.0381,  0.0248,  ...,  0.0823, -0.0053, -0.0150],
        ...,
        [ 0.0068, -0.0034,  0.0090,  ..., -0.0238, -0.0064, -0.0044],
        [-0.0328,  0.0091, -0.0039,  ...,  0.0048,  0.0165, -0.0111],
        [-0.0335, -0.0135,  0.0187,  ...,  0.0408,  0.0353, -0.0298]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8780, 0.8506, 0.8413, 0.8388, 0.8327, 0.8302, 0.8301, 0.8249, 0.8222,
         0.8200],
        [0.9427, 0.9307, 0.9292, 0.9288, 0.9267, 0.9245, 0.9239, 0.9225, 0.9201,
         0.9197],
        [0.9061, 0.8985, 0.8787, 0.8726, 0.8588, 0.8504, 0.8484, 0.8398, 0.8372,
         0.8291],
        [0.9559, 0.8886, 0.8852, 0.8805, 0.8797, 0.8777, 0.8675, 0.8659, 0.8651,
         0.8624],
        [0.8437, 0.8338, 0.8328, 0.8154, 0.8079, 0.8077, 0.8023, 0.7980, 0.7907,
         0.7811],
        [0.8539, 0.8187, 0.8092, 0.7981, 0.7927, 0.7917, 0.7852, 0.7779, 0.7726,
         0.7670],
        [0.8678, 0.8654, 0.8568, 0.8537, 0.8511, 0.8486, 0.8480, 0.8435, 0.8435,
         0.8402],
        [0.8946, 0.8746, 0.8690, 0.8582, 0.8507, 0.8337, 0.8275, 0.8201, 0.8082,
         0.8054],
        [0.9094, 0.9027, 0.8893, 0.8852, 0.8848, 0.8844, 0.8838, 0.8806, 0.8803,
         0.8802],
        [0.9492, 0.8854, 0.8841, 0.8818, 0.8754, 0.8731, 0.8696, 0.8695, 0.8625,
         0.8608],
        [0.9451, 0.9040, 0.8947, 0.8922, 0.8920, 0.8890, 0.8847, 0.8841, 0.8808,
         0.8804],
        [0.8900, 0.8749, 0.8575, 0.8503, 0.8277, 0.8016, 0.7997, 0.7989, 0.7864,
         0.7808],
        [0.9161, 0.9071, 0.9043, 0.8928, 0.8831, 0.8789, 0.8704, 0.8664, 0.8640,
         0.8535],
        [0.9582, 0.9551, 0.9527, 0.9524, 0.9495, 0.9480, 0.9450, 0.9435, 0.9410,
         0.9362],
        [0.9424, 0.9289, 0.9280, 0.9267, 0.9243, 0.9194, 0.9187, 0.9156, 0.9145,
         0.9130],
        [0.9265, 0.9223, 0.9215, 0.9142, 0.9121, 0.9111, 0.9091, 0.9061, 0.9052,
         0.9030],
        [0.9533, 0.9473, 0.9360, 0.9342, 0.9271, 0.9264, 0.9196, 0.9192, 0.9157,
         0.9150],
        [0.9626, 0.9564, 0.9519, 0.9399, 0.9397, 0.9359, 0.9347, 0.9337, 0.9323,
         0.9311],
        [0.9266, 0.9156, 0.9033, 0.8990, 0.8944, 0.8939, 0.8917, 0.8910, 0.8873,
         0.8842],
        [0.9046, 0.8989, 0.8978, 0.8976, 0.8960, 0.8958, 0.8930, 0.8902, 0.8896,
         0.8890],
        [0.9427, 0.9345, 0.9340, 0.9315, 0.9312, 0.9308, 0.9273, 0.9266, 0.9257,
         0.9245],
        [0.9261, 0.9116, 0.9111, 0.9096, 0.9063, 0.9062, 0.9060, 0.9031, 0.8906,
         0.8872],
        [0.9682, 0.9488, 0.9425, 0.9396, 0.9339, 0.9283, 0.9261, 0.9183, 0.9132,
         0.9127],
        [0.9415, 0.9348, 0.9306, 0.9268, 0.9242, 0.9231, 0.9231, 0.9231, 0.9231,
         0.9210],
        [0.9545, 0.9007, 0.8911, 0.8807, 0.8669, 0.8641, 0.8625, 0.8610, 0.8604,
         0.8576],
        [0.9368, 0.9324, 0.9311, 0.9245, 0.9226, 0.9189, 0.9151, 0.9111, 0.9045,
         0.9043],
        [0.9247, 0.9231, 0.9210, 0.9159, 0.9087, 0.9007, 0.8986, 0.8979, 0.8966,
         0.8957],
        [0.9602, 0.9485, 0.9432, 0.9271, 0.9255, 0.9245, 0.9197, 0.9185, 0.9181,
         0.9170],
        [0.9439, 0.9369, 0.9264, 0.9264, 0.9256, 0.9234, 0.9219, 0.9182, 0.9154,
         0.9137],
        [0.9323, 0.9274, 0.9257, 0.9256, 0.9223, 0.9197, 0.9180, 0.9178, 0.9144,
         0.9138],
        [0.9457, 0.9452, 0.9450, 0.9442, 0.9431, 0.9422, 0.9348, 0.9321, 0.9307,
         0.9287],
        [0.9383, 0.9213, 0.9203, 0.9110, 0.9028, 0.9022, 0.8984, 0.8977, 0.8974,
         0.8963],
        [0.8760, 0.8699, 0.8673, 0.8593, 0.8574, 0.8506, 0.8505, 0.8504, 0.8504,
         0.8503],
        [0.9031, 0.8923, 0.8870, 0.8848, 0.8825, 0.8813, 0.8803, 0.8785, 0.8777,
         0.8771],
        [0.9200, 0.9154, 0.9147, 0.9100, 0.9018, 0.9005, 0.8980, 0.8913, 0.8887,
         0.8878],
        [0.9062, 0.8971, 0.8943, 0.8855, 0.8833, 0.8771, 0.8769, 0.8716, 0.8604,
         0.8591],
        [0.9475, 0.8676, 0.8651, 0.8521, 0.8481, 0.8338, 0.8237, 0.8204, 0.8187,
         0.8166],
        [0.9020, 0.8838, 0.8826, 0.8766, 0.8760, 0.8709, 0.8585, 0.8583, 0.8562,
         0.8547],
        [0.8941, 0.8910, 0.8714, 0.8631, 0.8504, 0.8490, 0.8446, 0.8402, 0.8367,
         0.8348],
        [0.9362, 0.9322, 0.9307, 0.9302, 0.9294, 0.9283, 0.9262, 0.9256, 0.9238,
         0.9237],
        [0.8821, 0.8786, 0.8782, 0.8676, 0.8620, 0.8573, 0.8552, 0.8417, 0.8331,
         0.8261],
        [0.9010, 0.8625, 0.8619, 0.8595, 0.8544, 0.8527, 0.8506, 0.8499, 0.8479,
         0.8469],
        [0.8863, 0.8755, 0.8721, 0.8620, 0.8535, 0.8491, 0.8432, 0.8412, 0.8355,
         0.8348],
        [0.8795, 0.8732, 0.8594, 0.8547, 0.8499, 0.8467, 0.8454, 0.8438, 0.8419,
         0.8346],
        [0.8534, 0.8376, 0.8228, 0.8221, 0.8120, 0.8120, 0.8004, 0.7859, 0.7760,
         0.7758],
        [0.8308, 0.8076, 0.7929, 0.7872, 0.7811, 0.7805, 0.7771, 0.7646, 0.7600,
         0.7581],
        [0.9011, 0.8856, 0.8813, 0.8781, 0.8778, 0.8775, 0.8659, 0.8655, 0.8650,
         0.8636],
        [0.8078, 0.7857, 0.7817, 0.7756, 0.7635, 0.7621, 0.7468, 0.7449, 0.7337,
         0.7237],
        [0.8352, 0.8315, 0.8115, 0.8114, 0.8059, 0.8037, 0.8024, 0.7998, 0.7910,
         0.7879],
        [0.8929, 0.8638, 0.8466, 0.8365, 0.8336, 0.8316, 0.8235, 0.8226, 0.8179,
         0.8142],
        [0.8787, 0.8545, 0.8491, 0.8489, 0.8372, 0.8276, 0.8219, 0.8134, 0.8107,
         0.8064],
        [0.8891, 0.8860, 0.8468, 0.8187, 0.8155, 0.8113, 0.8051, 0.8005, 0.8000,
         0.7971],
        [0.9258, 0.9084, 0.9040, 0.8942, 0.8665, 0.8603, 0.8579, 0.8551, 0.8435,
         0.8402],
        [0.9127, 0.8748, 0.8710, 0.8610, 0.8544, 0.8544, 0.8452, 0.8437, 0.8409,
         0.8399],
        [0.9252, 0.9027, 0.8999, 0.8992, 0.8964, 0.8939, 0.8939, 0.8910, 0.8902,
         0.8900],
        [0.9014, 0.8812, 0.8799, 0.8691, 0.8673, 0.8665, 0.8641, 0.8630, 0.8621,
         0.8620],
        [0.9077, 0.8778, 0.8704, 0.8676, 0.8600, 0.8582, 0.8423, 0.8312, 0.8291,
         0.8258],
        [0.9404, 0.8800, 0.8723, 0.8642, 0.8570, 0.8522, 0.8369, 0.8345, 0.8336,
         0.8221],
        [0.9081, 0.8606, 0.8565, 0.8464, 0.8319, 0.8315, 0.8271, 0.8254, 0.8192,
         0.8183],
        [0.8993, 0.8192, 0.8165, 0.8049, 0.7945, 0.7945, 0.7932, 0.7901, 0.7863,
         0.7791],
        [0.8838, 0.8682, 0.8587, 0.8561, 0.8497, 0.8444, 0.8444, 0.8434, 0.8433,
         0.8431],
        [0.8548, 0.7778, 0.7772, 0.7769, 0.7662, 0.7355, 0.7316, 0.7275, 0.7265,
         0.7261],
        [0.8840, 0.8563, 0.8444, 0.8227, 0.8149, 0.8142, 0.7983, 0.7954, 0.7938,
         0.7892],
        [0.8391, 0.8095, 0.7970, 0.7820, 0.7789, 0.7748, 0.7692, 0.7555, 0.7521,
         0.7484]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 0, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 279899.7188,  189321.0156,  165854.5625,  160001.2500,  146547.2812,
          141432.4062,  141342.7344,  131213.0156,  126152.2812,  122311.7891],
        [ 705634.8125,  594644.0000,  582093.5625,  579058.2500,  561981.1250,
          544489.7500,  539974.3125,  529250.1250,  510853.8750,  508276.8438],
        [ 418506.8125,  375388.3125,  283073.2812,  259271.8594,  212761.3125,
          188777.9844,  183553.2656,  162288.8906,  156300.0469,  139239.5781],
        [ 852898.5625,  325768.7500,  310443.6562,  290286.2500,  286995.4375,
          279014.6562,  241100.5000,  235695.3750,  232846.1875,  224132.3750],
        [ 171681.6562,  149036.8906,  146776.2500,  114580.3516,  102851.4844,
          102651.4844,   95014.7266,   89264.6328,   80509.6250,   70192.2891],
        [ 198568.3750,  120053.5312,  104894.1641,   89484.2891,   82801.7422,
           81670.9297,   74394.2891,   67049.3359,   62120.7734,   57342.5703],
        [ 242247.1250,  234048.1719,  207027.1875,  197911.9844,  190637.0938,
          184003.7031,  182410.7969,  171148.8906,  171036.4688,  163306.5781],
        [ 355166.8750,  266705.2500,  246168.5312,  210974.7812,  189615.5469,
          148676.4531,  136213.0781,  122454.7734,  103299.4453,   99352.1484],
        [ 438898.9375,  398582.9375,  328992.3438,  310295.6562,  308792.5000,
          306928.7188,  304348.5625,  290530.5312,  289644.1562,  289178.8125],
        [ 774583.3125,  311174.3125,  305740.2812,  295793.2188,  270035.7188,
          261274.2188,  248245.9688,  247997.5312,  224431.1719,  219107.0156],
        [ 730708.6250,  406244.3125,  355444.3750,  343279.9688,  342074.7188,
          327660.6250,  308017.4688,  305761.5625,  291459.0938,  290033.3125],
        [ 332689.7812,  268026.5312,  208850.5938,  188635.4531,  136535.6250,
           93972.1953,   91513.7812,   90535.7266,   75713.7109,   69877.1719],
        [ 482506.2188,  424750.0625,  407723.5000,  346176.3438,  301198.6562,
          283749.2500,  251337.3281,  237266.4062,  229311.0156,  197499.6094],
        [ 880536.5625,  842822.1250,  814335.3125,  810768.3750,  778054.8125,
          761228.1250,  729087.3750,  713930.8750,  688709.9375,  643602.1250],
        [ 702924.0625,  579871.6875,  572340.9375,  561833.7500,  542649.9375,
          506166.8438,  500673.5938,  479477.8125,  471472.8438,  461704.2188],
        [ 560030.3750,  527636.9375,  521543.9688,  469440.3750,  455571.6875,
          449539.4375,  436569.5312,  418280.5938,  412958.6562,  400209.3438],
        [ 821107.3125,  754310.8750,  641544.3125,  624921.1250,  564827.7500,
          558960.0625,  507392.0000,  504563.3750,  480032.7812,  475409.4688],
        [ 937416.6250,  859014.3125,  804675.2500,  678415.4375,  676650.1250,
          640668.1875,  629595.5000,  620530.6250,  608817.0000,  597792.3125],
        [ 560509.6875,  479241.9375,  402034.8125,  378061.6562,  353957.0312,
          351398.1875,  340506.2500,  337181.0938,  320047.5000,  306213.8750],
        [ 409680.1250,  377615.9062,  371597.7500,  370753.1562,  362472.5312,
          361098.3125,  346916.9375,  333303.3438,  330406.3125,  327566.5938],
        [ 705622.0625,  627474.1250,  623697.6250,  601726.3750,  598826.2500,
          595361.8125,  566704.8750,  560423.0625,  553376.5625,  544577.0000],
        [ 557060.8125,  452338.6250,  449703.1875,  439980.5625,  419739.9375,
          418914.1250,  418131.8125,  400650.4375,  335168.6875,  319461.7188],
        [1016171.3125,  769787.1250,  703337.1250,  674804.5000,  622625.0000,
          574558.0000,  556881.8125,  498387.3750,  463355.2500,  459936.2188],
        [ 693523.9375,  630163.1875,  593393.1875,  562225.0000,  542036.0625,
          533769.0000,  533676.3125,  533517.5625,  533417.3125,  517761.6875],
        [ 835354.5625,  387456.2500,  337815.8125,  291294.5938,  239014.4375,
          229797.0000,  224426.2500,  219605.3281,  217859.1562,  209352.1094],
        [ 648517.0000,  609353.1250,  597645.2500,  544437.8125,  530023.8750,
          502062.1250,  475660.7500,  449338.4062,  409259.1562,  407672.1875],
        [ 545555.3125,  533728.2500,  517975.0625,  481024.5000,  434458.3750,
          387414.1250,  376112.8750,  372034.2500,  365549.6250,  360738.9688],
        [ 906888.1250,  766688.0000,  711199.0625,  564874.0625,  552260.4375,
          544301.3125,  508246.8125,  499348.9062,  496353.4688,  489279.3750],
        [ 718549.6875,  650165.5000,  558933.9375,  558848.1250,  552934.0000,
          535516.8125,  524385.7500,  497443.8750,  477830.8438,  466680.0000],
        [ 608145.0000,  566959.4375,  553772.0000,  552829.5625,  527211.9375,
          507942.0000,  496196.8125,  494409.4688,  471304.6875,  466911.9375],
        [ 736544.9375,  731510.3750,  729193.7500,  720842.7500,  709776.1250,
          700897.7500,  630171.0000,  606322.0000,  594667.8125,  577960.9375],
        [ 662975.5000,  520221.0938,  512408.4375,  448917.7812,  398891.7188,
          395885.2812,  374612.9688,  371106.9062,  369414.0938,  363647.6875],
        [ 272045.4375,  249375.9219,  240375.2344,  214562.3750,  208826.0938,
          189373.1875,  189186.1875,  188831.0938,  188738.3906,  188490.7031],
        [ 400755.5312,  343615.6875,  318582.1250,  308712.9688,  298687.7188,
          293622.0938,  289568.4688,  282192.1562,  279027.1562,  276564.9375],
        [ 510405.3438,  478232.9375,  473046.4688,  442613.0000,  393289.1562,
          386503.0000,  372475.5312,  338876.1250,  326362.9688,  321969.4062],
        [ 419333.0312,  367800.5625,  353637.1875,  311670.2812,  302165.6250,
          276519.3125,  275690.4062,  255550.0938,  217863.3125,  213722.0000],
        [ 755767.5000,  241417.5469,  233102.5781,  193371.2344,  182701.3594,
          149000.3750,  129024.2344,  123053.9297,  120063.8438,  116516.5547],
        [ 394437.0312,  304410.9688,  299063.6875,  274500.6562,  272102.0000,
          252972.5469,  211856.4688,  211375.9531,  205049.4844,  200844.2969],
        [ 352445.2812,  337466.1250,  254931.8281,  226514.5625,  188896.1250,
          185068.2500,  173708.8594,  163111.2500,  155363.6250,  151067.6094],
        [ 643302.0625,  607834.2500,  594392.2500,  590615.4375,  583387.2500,
          574480.7500,  557454.0625,  552744.6875,  538470.1875,  538229.9375],
        [ 296901.9375,  282610.6562,  280770.7500,  241445.1719,  222932.6094,
          208364.9688,  202109.3438,  166789.2969,  147568.0000,  133527.6562],
        [ 388833.2500,  224309.6406,  222508.4375,  214960.5312,  199902.2188,
          195204.2656,  189499.4844,  187443.0938,  182120.6875,  179734.9219],
        [ 315187.8125,  270176.8750,  257390.8750,  223000.0000,  197423.3594,
          185221.6875,  170485.2188,  165512.1562,  152544.5938,  151208.2812],
        [ 286220.5625,  261384.6250,  214792.9062,  200809.0469,  187413.2344,
          179082.7031,  175814.0000,  171843.1562,  167259.9844,  150598.5312],
        [ 197092.2656,  157209.0938,  127252.6953,  126009.1875,  109115.2188,
          109085.7656,   92475.8594,   75104.2969,   65208.3867,   65027.1758],
        [ 142760.2969,  102437.0234,   83015.5469,   76497.7891,   70167.5938,
           69589.4844,   66260.3516,   55447.1914,   51876.7812,   50492.4102],
        [ 389621.6875,  312012.0000,  293737.1875,  280445.8750,  279217.7500,
          278245.4062,  235488.2344,  234265.6719,  232549.2344,  228059.6094],
        [ 102722.2969,   74902.8750,   70775.8125,   64858.8359,   54553.4219,
           53502.0391,   43000.2227,   41854.7734,   35626.4453,   30890.9258],
        [ 152064.3906,  144218.0625,  108322.4688,  108182.2656,   99982.8984,
           96947.4219,   95165.1797,   91637.2734,   80782.0391,   77305.7578],
        [ 346630.9062,  228729.6094,  178959.7656,  154740.5156,  148621.8750,
          144385.3906,  128650.3438,  126920.0000,  118621.0391,  112656.6719],
        [ 282812.8750,  200350.9062,  185359.0000,  184737.2812,  156465.2812,
          136301.1719,  125725.8984,  111237.2969,  107026.1719,  100689.8438],
        [ 328223.5938,  314074.3125,  179245.3594,  120088.0000,  114708.8281,
          108087.6016,   98906.6875,   92635.5312,   91951.8125,   88241.0781],
        [ 554258.5625,  432517.8438,  406216.0625,  353132.0000,  237706.2344,
          217427.8438,  210306.0469,  201944.8125,  171073.3281,  163201.6562],
        [ 459807.2812,  267725.5938,  253488.8750,  219848.8281,  199852.8438,
          199844.6406,  175253.6875,  171659.5469,  164906.9844,  162588.3438],
        [ 549661.1250,  398838.0938,  383005.7812,  378910.2500,  364298.5000,
          351262.4688,  351262.4688,  337233.1875,  333286.5000,  332436.7188],
        [ 391345.8438,  293327.0938,  287853.1562,  246471.5625,  240299.5938,
          237573.6562,  229507.0312,  226104.9375,  223079.3438,  222906.2500],
        [ 428262.2812,  279109.9375,  251158.5781,  241296.2500,  216537.8281,
          211066.3281,  168211.1250,  143602.7969,  139231.6094,  132876.7500],
        [ 682557.0625,  288105.2500,  258042.1719,  229846.5469,  207407.5938,
          193826.9062,  155676.5781,  150517.8281,  148518.4375,  126092.3750],
        [ 430502.6250,  218560.6719,  206001.0781,  178294.2188,  145028.7656,
          144126.3438,  135374.4219,  132156.6562,  120984.7422,  119346.8047],
        [ 379955.2812,  120913.6875,  116423.5938,   98564.9766,   85011.7578,
           84978.6094,   83356.9141,   79758.7891,   75569.7969,   68146.1406],
        [ 304303.5625,  243457.0000,  212667.3906,  204938.0469,  186986.9219,
          173326.2812,  173277.1875,  170935.5312,  170629.1719,  170092.3750],
        [ 200952.7188,   66948.0000,   66324.4531,   66090.8359,   56713.5469,
           36577.2617,   34610.7812,   32603.0254,   32163.9648,   31980.0195],
        [ 305157.9688,  205473.6719,  173393.2344,  127026.4375,  113709.7188,
          112560.7656,   89643.8359,   86029.1172,   84115.3906,   78802.9688],
        [ 160653.2031,  105329.5156,   88065.2969,   71094.6406,   67969.8594,
           64122.5898,   59177.0977,   48699.7109,   46340.1016,   43983.0156]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[279899.7188,      0.0000],
         [189321.0156,      0.0000],
         [165854.5625,      0.0000],
         ...,
         [131213.0156,      0.0000],
         [126152.2812,      0.0000],
         [     0.0000, 122311.7891]],

        [[705634.8125,      0.0000],
         [594644.0000,      0.0000],
         [582093.5625,      0.0000],
         ...,
         [529250.1250,      0.0000],
         [510853.8750,      0.0000],
         [508276.8438,      0.0000]],

        [[418506.8125,      0.0000],
         [375388.3125,      0.0000],
         [283073.2812,      0.0000],
         ...,
         [162288.8906,      0.0000],
         [156300.0469,      0.0000],
         [139239.5781,      0.0000]],

        ...,

        [[     0.0000, 200952.7188],
         [ 66948.0000,      0.0000],
         [ 66324.4531,      0.0000],
         ...,
         [ 32603.0254,      0.0000],
         [ 32163.9648,      0.0000],
         [ 31980.0195,      0.0000]],

        [[305157.9688,      0.0000],
         [     0.0000, 205473.6719],
         [173393.2344,      0.0000],
         ...,
         [ 86029.1172,      0.0000],
         [ 84115.3906,      0.0000],
         [ 78802.9688,      0.0000]],

        [[     0.0000, 160653.2031],
         [     0.0000, 105329.5156],
         [     0.0000,  88065.2969],
         ...,
         [ 48699.7109,      0.0000],
         [     0.0000,  46340.1016],
         [     0.0000,  43983.0156]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1481764.2500,  122311.7891],
        [5656257.0000,       0.0000],
        [2379161.2500,       0.0000],
        [3279181.5000,       0.0000],
        [ 462948.4688,  659610.8750],
        [ 789659.7500,  148720.2656],
        [1772741.5000,  171036.4688],
        [1878627.0000,       0.0000],
        [3266193.2500,       0.0000],
        [3158382.7500,       0.0000],
        [3700684.0000,       0.0000],
        [1556350.5000,       0.0000],
        [3161518.2500,       0.0000],
        [7663076.0000,       0.0000],
        [5379115.5000,       0.0000],
        [4651781.0000,       0.0000],
        [5933069.0000,       0.0000],
        [7053575.0000,       0.0000],
        [3829152.0000,       0.0000],
        [3258108.0000,  333303.3438],
        [5977789.5000,       0.0000],
        [4211150.0000,       0.0000],
        [6339844.0000,       0.0000],
        [5673483.5000,       0.0000],
        [3191975.5000,       0.0000],
        [5173970.0000,       0.0000],
        [4374591.0000,       0.0000],
        [6039439.5000,       0.0000],
        [5541289.0000,       0.0000],
        [5245683.0000,       0.0000],
        [6737887.0000,       0.0000],
        [4418081.5000,       0.0000],
        [2129804.7500,       0.0000],
        [2474059.0000,  617269.8750],
        [1832730.5000, 2211043.5000],
        [1054936.0000, 1939015.7500],
        [1126770.2500, 1117248.8750],
        [1381162.5000, 1245450.6250],
        [ 733885.5625, 1454688.0000],
        [5780911.0000,       0.0000],
        [2183020.2500,       0.0000],
        [2184516.5000,       0.0000],
        [2088150.8750,       0.0000],
        [1995218.7500,       0.0000],
        [ 249217.2344,  874362.7500],
        [ 334392.0000,  434152.5000],
        [2374021.0000,  389621.6875],
        [ 362965.4375,  209722.2031],
        [ 466229.7812,  588377.9375],
        [ 459287.5625, 1229628.5000],
        [1347378.2500,  243327.3438],
        [ 714619.5000,  821543.2500],
        [1579560.2500, 1368224.2500],
        [2075131.8750,  199844.6406],
        [3780195.2500,       0.0000],
        [2598468.5000,       0.0000],
        [1014306.9375, 1197046.5000],
        [1715231.3750,  725359.3750],
        [ 987313.8750,  843062.5000],
        [ 561034.5625,  631645.0000],
        [1275866.0000,  734747.5000],
        [ 424011.8750,  200952.7188],
        [ 944168.9375,  431744.1562],
        [ 112822.2969,  642612.6875]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 401/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:54, 63.95s/it]  7%|▋         | 2/30 [01:04<12:29, 26.77s/it] 10%|█         | 3/30 [01:05<06:41, 14.89s/it] 13%|█▎        | 4/30 [01:06<04:01,  9.31s/it] 17%|█▋        | 5/30 [01:06<02:35,  6.22s/it] 20%|██        | 6/30 [01:07<01:44,  4.36s/it] 23%|██▎       | 7/30 [01:08<01:13,  3.18s/it] 27%|██▋       | 8/30 [01:09<00:52,  2.40s/it] 30%|███       | 9/30 [01:09<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.33it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.86s/it]
Epoch loss is 2.379229108492533
Epoch 402/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:01, 60.07s/it]  7%|▋         | 2/30 [01:00<11:44, 25.17s/it] 10%|█         | 3/30 [01:01<06:18, 14.02s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.3794531345367433
Epoch 403/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:44, 59.48s/it]  7%|▋         | 2/30 [01:00<11:38, 24.93s/it] 10%|█         | 3/30 [01:00<06:15, 13.89s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.70s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.83s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.3698054949442544
Epoch 404/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:37, 61.30s/it]  7%|▋         | 2/30 [01:02<11:59, 25.68s/it] 10%|█         | 3/30 [01:02<06:25, 14.30s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.99s/it] 20%|██        | 6/30 [01:05<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.3696559111277264
Epoch 405/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:25, 58.83s/it]  7%|▋         | 2/30 [01:00<11:37, 24.92s/it] 10%|█         | 3/30 [01:00<06:14, 13.88s/it] 13%|█▎        | 4/30 [01:01<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.93s/it] 20%|██        | 6/30 [01:03<01:39,  4.17s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.342967414855957
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0237,  0.0015,  0.0128,  ..., -0.0052,  0.0163,  0.0039],
        [-0.0022,  0.0103,  0.0282,  ...,  0.0168,  0.0031, -0.0153],
        [-0.0295, -0.0381,  0.0248,  ...,  0.0825, -0.0052, -0.0149],
        ...,
        [ 0.0067, -0.0034,  0.0091,  ..., -0.0239, -0.0062, -0.0042],
        [-0.0329,  0.0092, -0.0039,  ...,  0.0048,  0.0166, -0.0110],
        [-0.0334, -0.0136,  0.0188,  ...,  0.0408,  0.0354, -0.0296]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8774, 0.8506, 0.8408, 0.8385, 0.8323, 0.8298, 0.8295, 0.8244, 0.8218,
         0.8196],
        [0.9425, 0.9304, 0.9289, 0.9287, 0.9266, 0.9241, 0.9236, 0.9222, 0.9198,
         0.9193],
        [0.9059, 0.8982, 0.8781, 0.8723, 0.8583, 0.8500, 0.8480, 0.8394, 0.8368,
         0.8289],
        [0.9560, 0.8881, 0.8847, 0.8803, 0.8795, 0.8772, 0.8671, 0.8655, 0.8644,
         0.8620],
        [0.8436, 0.8333, 0.8325, 0.8152, 0.8075, 0.8073, 0.8022, 0.7976, 0.7904,
         0.7811],
        [0.8537, 0.8185, 0.8092, 0.7979, 0.7928, 0.7911, 0.7847, 0.7779, 0.7719,
         0.7668],
        [0.8673, 0.8649, 0.8563, 0.8533, 0.8506, 0.8482, 0.8475, 0.8431, 0.8427,
         0.8396],
        [0.8941, 0.8741, 0.8686, 0.8578, 0.8504, 0.8330, 0.8272, 0.8196, 0.8077,
         0.8047],
        [0.9091, 0.9025, 0.8889, 0.8848, 0.8846, 0.8843, 0.8836, 0.8803, 0.8800,
         0.8799],
        [0.9491, 0.8850, 0.8838, 0.8815, 0.8750, 0.8728, 0.8692, 0.8690, 0.8622,
         0.8605],
        [0.9450, 0.9040, 0.8943, 0.8917, 0.8915, 0.8885, 0.8844, 0.8837, 0.8805,
         0.8799],
        [0.8895, 0.8744, 0.8570, 0.8497, 0.8271, 0.8008, 0.7993, 0.7981, 0.7857,
         0.7802],
        [0.9154, 0.9069, 0.9038, 0.8923, 0.8824, 0.8784, 0.8700, 0.8658, 0.8636,
         0.8530],
        [0.9579, 0.9550, 0.9524, 0.9521, 0.9493, 0.9477, 0.9447, 0.9432, 0.9407,
         0.9359],
        [0.9423, 0.9288, 0.9278, 0.9265, 0.9240, 0.9192, 0.9185, 0.9154, 0.9143,
         0.9128],
        [0.9264, 0.9222, 0.9213, 0.9140, 0.9119, 0.9109, 0.9089, 0.9059, 0.9050,
         0.9029],
        [0.9530, 0.9471, 0.9357, 0.9337, 0.9268, 0.9261, 0.9192, 0.9189, 0.9154,
         0.9148],
        [0.9624, 0.9563, 0.9518, 0.9398, 0.9397, 0.9357, 0.9345, 0.9334, 0.9323,
         0.9309],
        [0.9263, 0.9152, 0.9027, 0.8988, 0.8941, 0.8935, 0.8915, 0.8907, 0.8871,
         0.8842],
        [0.9043, 0.8987, 0.8975, 0.8974, 0.8958, 0.8954, 0.8927, 0.8900, 0.8893,
         0.8886],
        [0.9422, 0.9341, 0.9337, 0.9311, 0.9310, 0.9306, 0.9270, 0.9261, 0.9254,
         0.9242],
        [0.9260, 0.9113, 0.9111, 0.9093, 0.9059, 0.9059, 0.9059, 0.9028, 0.8902,
         0.8867],
        [0.9681, 0.9487, 0.9423, 0.9393, 0.9338, 0.9280, 0.9259, 0.9182, 0.9131,
         0.9125],
        [0.9413, 0.9345, 0.9304, 0.9267, 0.9242, 0.9231, 0.9230, 0.9229, 0.9229,
         0.9208],
        [0.9543, 0.9004, 0.8906, 0.8804, 0.8665, 0.8635, 0.8620, 0.8604, 0.8599,
         0.8575],
        [0.9365, 0.9322, 0.9307, 0.9242, 0.9224, 0.9186, 0.9148, 0.9109, 0.9042,
         0.9041],
        [0.9244, 0.9228, 0.9207, 0.9156, 0.9084, 0.9005, 0.8983, 0.8976, 0.8964,
         0.8955],
        [0.9601, 0.9484, 0.9430, 0.9268, 0.9253, 0.9243, 0.9195, 0.9183, 0.9180,
         0.9169],
        [0.9437, 0.9367, 0.9263, 0.9261, 0.9253, 0.9231, 0.9216, 0.9180, 0.9150,
         0.9134],
        [0.9320, 0.9272, 0.9256, 0.9254, 0.9221, 0.9194, 0.9179, 0.9176, 0.9142,
         0.9137],
        [0.9454, 0.9451, 0.9449, 0.9440, 0.9429, 0.9420, 0.9345, 0.9319, 0.9305,
         0.9285],
        [0.9381, 0.9212, 0.9201, 0.9107, 0.9025, 0.9021, 0.8981, 0.8975, 0.8971,
         0.8961],
        [0.8756, 0.8696, 0.8669, 0.8591, 0.8573, 0.8503, 0.8503, 0.8502, 0.8499,
         0.8498],
        [0.9026, 0.8919, 0.8866, 0.8845, 0.8820, 0.8809, 0.8800, 0.8783, 0.8774,
         0.8767],
        [0.9197, 0.9152, 0.9144, 0.9096, 0.9014, 0.9005, 0.8976, 0.8910, 0.8883,
         0.8875],
        [0.9060, 0.8967, 0.8940, 0.8853, 0.8829, 0.8768, 0.8767, 0.8711, 0.8601,
         0.8588],
        [0.9475, 0.8676, 0.8649, 0.8518, 0.8478, 0.8332, 0.8232, 0.8200, 0.8179,
         0.8164],
        [0.9016, 0.8833, 0.8819, 0.8759, 0.8759, 0.8706, 0.8578, 0.8576, 0.8558,
         0.8542],
        [0.8939, 0.8907, 0.8709, 0.8628, 0.8501, 0.8483, 0.8441, 0.8395, 0.8364,
         0.8345],
        [0.9359, 0.9320, 0.9304, 0.9300, 0.9291, 0.9278, 0.9259, 0.9253, 0.9237,
         0.9234],
        [0.8817, 0.8783, 0.8779, 0.8672, 0.8617, 0.8569, 0.8548, 0.8413, 0.8327,
         0.8257],
        [0.9007, 0.8621, 0.8617, 0.8591, 0.8539, 0.8523, 0.8502, 0.8492, 0.8474,
         0.8466],
        [0.8859, 0.8750, 0.8716, 0.8617, 0.8531, 0.8486, 0.8427, 0.8406, 0.8351,
         0.8346],
        [0.8788, 0.8728, 0.8589, 0.8540, 0.8495, 0.8460, 0.8451, 0.8434, 0.8416,
         0.8345],
        [0.8529, 0.8374, 0.8223, 0.8214, 0.8115, 0.8115, 0.7999, 0.7850, 0.7756,
         0.7749],
        [0.8305, 0.8072, 0.7923, 0.7869, 0.7808, 0.7804, 0.7769, 0.7645, 0.7598,
         0.7580],
        [0.9007, 0.8849, 0.8808, 0.8775, 0.8771, 0.8768, 0.8651, 0.8648, 0.8642,
         0.8628],
        [0.8072, 0.7853, 0.7812, 0.7749, 0.7634, 0.7616, 0.7458, 0.7440, 0.7331,
         0.7229],
        [0.8346, 0.8310, 0.8113, 0.8111, 0.8054, 0.8035, 0.8024, 0.7993, 0.7907,
         0.7873],
        [0.8925, 0.8634, 0.8461, 0.8357, 0.8332, 0.8311, 0.8230, 0.8221, 0.8174,
         0.8140],
        [0.8781, 0.8543, 0.8487, 0.8486, 0.8369, 0.8273, 0.8217, 0.8128, 0.8104,
         0.8060],
        [0.8888, 0.8857, 0.8462, 0.8180, 0.8147, 0.8103, 0.8046, 0.8000, 0.7996,
         0.7964],
        [0.9254, 0.9081, 0.9035, 0.8939, 0.8660, 0.8600, 0.8576, 0.8545, 0.8432,
         0.8396],
        [0.9125, 0.8747, 0.8707, 0.8608, 0.8541, 0.8539, 0.8446, 0.8436, 0.8405,
         0.8395],
        [0.9248, 0.9023, 0.8994, 0.8988, 0.8962, 0.8935, 0.8935, 0.8907, 0.8898,
         0.8897],
        [0.9010, 0.8808, 0.8796, 0.8687, 0.8669, 0.8661, 0.8635, 0.8626, 0.8618,
         0.8615],
        [0.9071, 0.8774, 0.8698, 0.8671, 0.8598, 0.8577, 0.8422, 0.8307, 0.8283,
         0.8254],
        [0.9402, 0.8797, 0.8718, 0.8637, 0.8566, 0.8520, 0.8365, 0.8340, 0.8330,
         0.8215],
        [0.9076, 0.8603, 0.8561, 0.8459, 0.8317, 0.8310, 0.8264, 0.8250, 0.8187,
         0.8175],
        [0.8991, 0.8187, 0.8161, 0.8043, 0.7939, 0.7938, 0.7926, 0.7895, 0.7858,
         0.7783],
        [0.8837, 0.8677, 0.8583, 0.8557, 0.8492, 0.8441, 0.8436, 0.8429, 0.8428,
         0.8424],
        [0.8545, 0.7773, 0.7767, 0.7767, 0.7657, 0.7355, 0.7312, 0.7271, 0.7265,
         0.7256],
        [0.8837, 0.8561, 0.8439, 0.8225, 0.8143, 0.8138, 0.7973, 0.7946, 0.7931,
         0.7885],
        [0.8388, 0.8090, 0.7965, 0.7815, 0.7785, 0.7741, 0.7688, 0.7549, 0.7515,
         0.7475]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 277783.2500,  189416.3750,  164622.7344,  159281.8750,  145889.6719,
          140737.0625,  140075.8438,  130198.2500,  125504.2812,  121566.5000],
        [ 703922.9375,  592238.1250,  579267.5625,  577667.7500,  561092.6250,
          541124.9375,  537089.0625,  526630.0000,  508766.1875,  505569.5625],
        [ 417440.9375,  374026.8125,  280652.6875,  258189.8750,  211461.6406,
          187815.6406,  182484.9219,  161283.6719,  155502.6719,  138795.5625],
        [ 853084.0625,  323657.4375,  308264.9062,  289305.4062,  286341.2188,
          277038.5312,  239668.8438,  234146.1719,  230771.0000,  222950.6875],
        [ 171227.4219,  147906.8438,  146300.1250,  114257.9141,  102342.0156,
          102005.1719,   94817.7656,   88847.6250,   80122.4375,   70184.7188],
        [ 197876.1250,  119766.6172,  104772.6875,   89167.1250,   82922.6562,
           80967.1484,   73891.1328,   67003.3125,   61530.6914,   57191.7266],
        [ 240438.2969,  232118.2969,  205528.3438,  196927.4844,  189357.3125,
          183083.1562,  181220.2812,  170130.3438,  169115.4375,  161857.0312],
        [ 352739.5312,  265024.9688,  244852.4375,  209793.4219,  188693.2188,
          147319.6719,  135649.8125,  121597.8047,  102593.5469,   98351.0781],
        [ 437014.4062,  397491.5938,  327138.8750,  308459.3125,  307825.4062,
          306353.1875,  303423.7812,  289624.2812,  288172.5625,  287989.0625],
        [ 773509.2500,  309391.1562,  304267.5938,  294399.3125,  268435.0625,
          260098.7656,  246827.2031,  246470.8594,  223511.8438,  218037.5000],
        [ 729887.4375,  405794.3750,  353557.9375,  340541.6562,  339493.5938,
          325575.2500,  307107.0312,  304038.7500,  290377.9062,  287729.0938],
        [ 330191.1562,  265898.3750,  207621.9375,  187065.7656,  135319.3125,
           92965.8203,   91043.3594,   89498.9609,   74904.4453,   69261.0156],
        [ 478159.0625,  423117.9062,  405081.0000,  343693.3750,  298447.1250,
          281639.6562,  249757.7031,  235360.7031,  227936.9688,  196090.5312],
        [ 876870.3750,  841462.5000,  811100.8750,  807755.3750,  775183.3750,
          757752.2500,  726438.2500,  710764.4375,  686065.4375,  640646.1875],
        [ 701853.0000,  578630.4375,  570777.6875,  560208.8125,  540528.7500,
          504694.7812,  499482.2812,  478199.6250,  470711.7500,  460788.4375],
        [ 559259.1875,  526529.0625,  520265.7500,  468357.3438,  454723.5312,
          448438.5312,  435488.7812,  416979.4062,  411791.8125,  399570.9688],
        [ 817684.6875,  751308.4375,  639012.6875,  620781.6250,  562437.8750,
          556740.0000,  504566.2500,  502377.2500,  477762.9688,  473567.3750],
        [ 935041.6250,  856856.7500,  803357.2500,  677014.8125,  676377.8125,
          638749.5000,  627943.5000,  618159.7500,  608092.2500,  596369.3750],
        [ 558577.5000,  476551.1250,  398677.2188,  376980.1250,  352501.0938,
          349706.5625,  339466.7500,  335572.3125,  318767.5312,  305820.4688],
        [ 407652.7500,  376712.7188,  370237.2812,  369717.2188,  361023.5625,
          359366.1250,  345458.0312,  332519.4688,  329212.0312,  325918.5000],
        [ 701280.2500,  623879.0625,  620717.6875,  598068.3125,  597091.5000,
          593465.6250,  564203.7500,  556715.6250,  551332.7500,  542130.1250],
        [ 556075.6875,  450762.9062,  449174.7500,  437999.5000,  417275.7500,
          417081.6250,  416966.6562,  399144.7812,  333632.5000,  317380.6562],
        [1013885.8125,  769021.0000,  702083.9375,  672836.1875,  621430.2500,
          572380.8125,  555088.0000,  497205.8125,  462430.4375,  458362.5000],
        [ 692182.6875,  628192.0625,  592112.7500,  561340.4375,  541527.6875,
          533163.0625,  532653.8125,  532179.5625,  532160.7500,  516087.0938],
        [ 832566.9375,  385695.8750,  335407.8750,  290044.9375,  237678.1094,
          227528.6719,  222988.3281,  217740.5625,  216337.4219,  208889.6250],
        [ 646255.4375,  607802.3750,  595024.0625,  542177.1875,  528406.3750,
          499976.4688,  473576.8438,  447825.2500,  407466.5312,  406466.0000],
        [ 543448.5625,  531231.3125,  515325.7188,  479190.7500,  432436.1875,
          386094.8125,  374566.1562,  370718.5000,  364239.7812,  359484.4062],
        [ 905287.8125,  765529.1875,  709186.8125,  562386.9375,  550502.5625,
          543012.3125,  506857.1250,  497961.2500,  495723.8438,  488110.0312],
        [ 715976.3125,  648013.7500,  558344.1875,  556504.8750,  550693.6875,
          533630.5000,  522052.5312,  496115.4375,  475211.8438,  464577.8438],
        [ 605867.6875,  565246.4375,  552734.1250,  551330.5625,  525720.2500,
          505754.7500,  494965.1875,  493289.9375,  469963.1250,  466431.7188],
        [ 733577.6250,  730060.8125,  728159.7500,  719320.3125,  707435.2500,
          698943.3750,  628160.9375,  604582.8750,  593279.4375,  576606.5625],
        [ 660485.9375,  519273.9062,  511003.4375,  446884.9688,  397306.2812,
          395205.5312,  373053.5938,  369953.8750,  368003.0312,  362642.6562],
        [ 270684.6875,  248492.3125,  239156.2656,  213681.8438,  208277.3438,
          188496.2656,  188478.6562,  188316.0469,  187540.7344,  187166.2344],
        [ 397910.3438,  341430.6875,  316798.8438,  307178.7812,  296681.7188,
          291744.1250,  288400.5000,  281439.3750,  277552.8750,  274792.9375],
        [ 508292.8438,  476503.8750,  471071.9375,  439950.7812,  391364.9062,
          386341.5938,  370720.6250,  337148.5938,  324576.0625,  320621.5312],
        [ 418037.3125,  365631.9062,  351902.2188,  310840.0312,  300318.3750,
          275455.1875,  274765.1875,  253928.7344,  216947.9375,  212958.6406],
        [ 755417.3125,  241289.3594,  232294.3281,  192642.1562,  181858.0938,
          147619.6406,  128006.7344,  122346.4453,  118671.9531,  116169.8281],
        [ 392162.1562,  302028.1875,  296270.6250,  271947.3750,  271623.0938,
          251936.0781,  209929.1094,  209397.2500,  203972.3125,  199461.3594],
        [ 351287.5938,  335820.0938,  252977.1250,  225270.6719,  188051.8750,
          183290.3594,  172551.1250,  161685.4688,  154559.5312,  150510.3750],
        [ 640045.2500,  605468.0000,  591863.8125,  588366.1250,  580806.5000,
          570424.0000,  555257.5000,  550471.0625,  538033.8750,  535979.7500],
        [ 295482.2500,  281096.8125,  279501.5000,  239967.3125,  222012.2188,
          207311.6719,  200951.7812,  165855.3594,  146646.8281,  132680.7344],
        [ 387462.5312,  223059.3594,  221950.2031,  213829.6250,  198424.1250,
          194069.1875,  188155.9062,  185584.8594,  181012.6719,  178955.1719],
        [ 313509.6250,  268400.5000,  255589.5781,  222042.5156,  196195.2969,
          183977.0312,  169248.3750,  164220.2188,  151849.6250,  150654.8438],
        [ 283212.8750,  260192.2969,  213206.9531,  198878.0469,  186317.4375,
          177263.5625,  175040.2188,  170812.0000,  166575.3281,  150365.0312],
        [ 195732.3750,  156712.7344,  126349.6172,  124788.4297,  108297.4766,
          108258.9531,   91736.4297,   74171.3516,   64859.0859,   64253.7773],
        [ 142159.1094,  101858.4766,   82386.8750,   76206.5234,   69880.7031,
           69470.7266,   66078.1094,   55327.2344,   51754.3281,   50441.8789],
        [ 387325.0938,  308912.0625,  291312.0938,  278072.1875,  276427.5625,
          275130.9688,  232816.6562,  232051.4375,  230100.2812,  225297.7500],
        [ 101896.5625,   74544.9297,   70260.9375,   64224.4922,   54488.5312,
           53125.5938,   42376.0195,   41296.6719,   35326.2188,   30570.4238],
        [ 150751.1406,  143155.3906,  107935.6641,  107656.4609,   99296.9297,
           96647.2188,   95051.6172,   91006.6406,   80424.2188,   76647.3438],
        [ 344522.9688,  227467.2656,  177677.0312,  153026.7188,  147746.9688,
          143407.2188,  127713.3516,  126021.2031,  117853.3672,  112183.5391],
        [ 280426.0625,  199527.3750,  184227.2344,  183927.2188,  155669.8906,
          135845.5625,  125340.2969,  110359.7500,  106652.4453,  100205.5078],
        [ 326680.5938,  312754.6875,  177948.6875,  118829.6016,  113442.2969,
          106553.6250,   98138.4922,   91941.8984,   91351.3281,   87276.0234],
        [ 551361.6250,  430583.5312,  403202.0938,  351611.7188,  236005.7969,
          216497.9844,  209318.3906,  200250.4375,  170389.4844,  161822.9219],
        [ 458375.6250,  267118.8750,  252406.4844,  218952.0469,  198957.3281,
          198391.7656,  173716.3125,  171244.5781,  163836.6562,  161573.7188],
        [ 546796.0625,  396362.0625,  380476.6875,  376839.5938,  363164.9062,
          349399.8438,  349399.8438,  335598.5625,  331389.9375,  330923.1562],
        [ 388794.3438,  291722.1562,  286690.7188,  245247.3906,  238853.1250,
          236334.4062,  227622.8594,  224647.6719,  222355.5000,  221130.2344],
        [ 424676.3438,  277882.6250,  249161.9844,  239789.7812,  215834.3906,
          209569.2500,  168029.6406,  142508.5000,  137670.9375,  132050.7031],
        [ 681434.4375,  286890.3750,  256529.2656,  228352.3281,  206226.3594,
          193086.5312,  154855.9531,  149316.4375,  147232.2969,  125010.1016],
        [ 427805.0938,  217641.7344,  204887.2344,  177029.4219,  144512.8281,
          143170.6875,  134011.2969,  131361.8906,  119998.2422,  118067.1094],
        [ 378570.3750,  120128.7812,  115615.7734,   97681.6094,   84258.2188,
           84129.3516,   82699.2344,   79085.4375,   74995.0078,   67424.6406],
        [ 303960.4375,  241887.9219,  211319.1094,  203673.1562,  185691.9688,
          172604.9375,  171404.8438,  169697.8438,  169286.3125,  168534.2031],
        [ 200297.9844,   66457.6094,   65853.3359,   65849.0000,   56274.4531,
           36583.3359,   34378.7617,   32432.5273,   32149.4277,   31746.9805],
        [ 303988.0000,  204866.5312,  172094.7500,  126759.6016,  112667.4141,
          111973.8281,   88443.4531,   85123.4766,   83251.8828,   77953.8438],
        [ 160088.3906,  104564.2656,   87388.1250,   70555.5703,   67641.4922,
           63509.0742,   58891.6016,   48229.2070,   45970.6211,   43433.8438]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[277783.2500,      0.0000],
         [189416.3750,      0.0000],
         [164622.7344,      0.0000],
         ...,
         [130198.2500,      0.0000],
         [125504.2812,      0.0000],
         [     0.0000, 121566.5000]],

        [[703922.9375,      0.0000],
         [592238.1250,      0.0000],
         [579267.5625,      0.0000],
         ...,
         [526630.0000,      0.0000],
         [508766.1875,      0.0000],
         [505569.5625,      0.0000]],

        [[417440.9375,      0.0000],
         [374026.8125,      0.0000],
         [280652.6875,      0.0000],
         ...,
         [161283.6719,      0.0000],
         [155502.6719,      0.0000],
         [138795.5625,      0.0000]],

        ...,

        [[     0.0000, 200297.9844],
         [ 66457.6094,      0.0000],
         [ 65853.3359,      0.0000],
         ...,
         [ 32432.5273,      0.0000],
         [ 32149.4277,      0.0000],
         [ 31746.9805,      0.0000]],

        [[303988.0000,      0.0000],
         [     0.0000, 204866.5312],
         [172094.7500,      0.0000],
         ...,
         [ 85123.4766,      0.0000],
         [ 83251.8828,      0.0000],
         [ 77953.8438,      0.0000]],

        [[     0.0000, 160088.3906],
         [     0.0000, 104564.2656],
         [     0.0000,  87388.1250],
         ...,
         [ 48229.2070,      0.0000],
         [     0.0000,  45970.6211],
         [     0.0000,  43433.8438]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1473509.3750,  121566.5000],
        [5633369.0000,       0.0000],
        [2367654.5000,       0.0000],
        [3265228.2500,       0.0000],
        [ 461388.0000,  656624.0000],
        [ 787118.7500,  147970.4688],
        [1760660.5000,  169115.4375],
        [1866615.5000,       0.0000],
        [3253492.5000,       0.0000],
        [3144948.5000,       0.0000],
        [3684103.0000,       0.0000],
        [1543770.2500,       0.0000],
        [3139284.0000,       0.0000],
        [7634039.0000,       0.0000],
        [5365875.5000,       0.0000],
        [4641404.0000,       0.0000],
        [5906239.5000,       0.0000],
        [7037962.5000,       0.0000],
        [3812620.7500,       0.0000],
        [3245298.2500,  332519.4688],
        [5948885.0000,       0.0000],
        [4195495.0000,       0.0000],
        [6324725.0000,       0.0000],
        [5661600.0000,       0.0000],
        [3174878.2500,       0.0000],
        [5154976.5000,       0.0000],
        [4356736.0000,       0.0000],
        [6024558.0000,       0.0000],
        [5521121.0000,       0.0000],
        [5231304.0000,       0.0000],
        [6720127.0000,       0.0000],
        [4403813.5000,       0.0000],
        [2120290.5000,       0.0000],
        [2460449.5000,  613480.5625],
        [1824138.8750, 2202454.0000],
        [1049467.2500, 1931318.2500],
        [1120937.2500, 1115378.6250],
        [1370966.7500, 1237760.7500],
        [ 729567.5000, 1446436.6250],
        [5756716.0000,       0.0000],
        [2171506.5000,       0.0000],
        [2172503.5000,       0.0000],
        [2075687.7500,       0.0000],
        [1981863.7500,       0.0000],
        [ 246722.5938,  868437.6250],
        [ 333390.3750,  432173.5625],
        [2350121.0000,  387325.0938],
        [ 359664.9062,  208445.4844],
        [ 463670.3125,  584902.3125],
        [ 456706.5000, 1220913.2500],
        [1339683.3750,  242498.0000],
        [ 707533.2500,  817383.9375],
        [1570277.5000, 1360766.5000],
        [2066181.6250,  198391.7656],
        [3760350.5000,       0.0000],
        [2583398.2500,       0.0000],
        [1007421.1250, 1189753.0000],
        [1707465.0000,  721469.0625],
        [ 980794.5000,  837691.1250],
        [ 556653.5625,  627934.8750],
        [1266520.3750,  731540.3125],
        [ 421725.4375,  200297.9844],
        [ 937615.0000,  429507.7812],
        [ 111738.2812,  638533.9375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 406/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:55, 59.83s/it]  7%|▋         | 2/30 [01:00<11:42, 25.08s/it] 10%|█         | 3/30 [01:01<06:17, 13.97s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.75s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.341749731699626
Epoch 407/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:56, 57.81s/it]  7%|▋         | 2/30 [01:00<11:59, 25.68s/it] 10%|█         | 3/30 [01:01<06:25, 14.30s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.99s/it] 20%|██        | 6/30 [01:03<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.3458175738652547
Epoch 408/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:10, 60.37s/it]  7%|▋         | 2/30 [01:02<12:08, 26.03s/it] 10%|█         | 3/30 [01:03<06:31, 14.49s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 2.3367640574773154
Epoch 409/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:50, 61.73s/it]  7%|▋         | 2/30 [01:02<12:03, 25.86s/it] 10%|█         | 3/30 [01:03<06:28, 14.39s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.01s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.03s/it] 20%|██        | 6/30 [01:05<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.3837656736373902
Epoch 410/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:10, 58.28s/it]  7%|▋         | 2/30 [01:01<12:11, 26.12s/it] 10%|█         | 3/30 [01:02<06:32, 14.53s/it] 13%|█▎        | 4/30 [01:03<03:56,  9.09s/it] 17%|█▋        | 5/30 [01:04<02:32,  6.08s/it] 20%|██        | 6/30 [01:04<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:07<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.77s/it]
Epoch loss is 2.315570036570231
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0237,  0.0014,  0.0129,  ..., -0.0053,  0.0165,  0.0039],
        [-0.0022,  0.0101,  0.0283,  ...,  0.0168,  0.0033, -0.0153],
        [-0.0294, -0.0380,  0.0248,  ...,  0.0826, -0.0050, -0.0148],
        ...,
        [ 0.0066, -0.0034,  0.0092,  ..., -0.0240, -0.0060, -0.0039],
        [-0.0329,  0.0093, -0.0038,  ...,  0.0048,  0.0166, -0.0110],
        [-0.0333, -0.0136,  0.0189,  ...,  0.0409,  0.0355, -0.0294]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8769, 0.8507, 0.8403, 0.8382, 0.8320, 0.8295, 0.8289, 0.8239, 0.8215,
         0.8191],
        [0.9424, 0.9302, 0.9286, 0.9285, 0.9265, 0.9237, 0.9232, 0.9219, 0.9195,
         0.9190],
        [0.9058, 0.8980, 0.8776, 0.8720, 0.8579, 0.8497, 0.8476, 0.8390, 0.8365,
         0.8287],
        [0.9560, 0.8877, 0.8843, 0.8800, 0.8794, 0.8768, 0.8667, 0.8650, 0.8639,
         0.8618],
        [0.8434, 0.8328, 0.8323, 0.8151, 0.8072, 0.8069, 0.8021, 0.7974, 0.7901,
         0.7812],
        [0.8535, 0.8184, 0.8091, 0.7976, 0.7929, 0.7906, 0.7844, 0.7778, 0.7713,
         0.7666],
        [0.8668, 0.8643, 0.8558, 0.8530, 0.8502, 0.8479, 0.8471, 0.8427, 0.8420,
         0.8390],
        [0.8937, 0.8737, 0.8682, 0.8574, 0.8500, 0.8325, 0.8270, 0.8191, 0.8073,
         0.8042],
        [0.9088, 0.9023, 0.8885, 0.8844, 0.8844, 0.8842, 0.8834, 0.8802, 0.8797,
         0.8797],
        [0.9490, 0.8846, 0.8835, 0.8812, 0.8747, 0.8725, 0.8688, 0.8687, 0.8619,
         0.8602],
        [0.9450, 0.9039, 0.8940, 0.8912, 0.8910, 0.8881, 0.8842, 0.8834, 0.8803,
         0.8794],
        [0.8890, 0.8738, 0.8567, 0.8492, 0.8265, 0.8001, 0.7990, 0.7974, 0.7850,
         0.7796],
        [0.9149, 0.9067, 0.9035, 0.8919, 0.8819, 0.8779, 0.8696, 0.8653, 0.8632,
         0.8526],
        [0.9576, 0.9549, 0.9522, 0.9519, 0.9490, 0.9474, 0.9445, 0.9429, 0.9405,
         0.9356],
        [0.9422, 0.9287, 0.9277, 0.9263, 0.9238, 0.9190, 0.9183, 0.9153, 0.9142,
         0.9127],
        [0.9263, 0.9220, 0.9212, 0.9139, 0.9118, 0.9108, 0.9087, 0.9056, 0.9048,
         0.9027],
        [0.9527, 0.9468, 0.9355, 0.9333, 0.9266, 0.9258, 0.9188, 0.9186, 0.9151,
         0.9145],
        [0.9622, 0.9561, 0.9517, 0.9397, 0.9397, 0.9355, 0.9344, 0.9332, 0.9322,
         0.9308],
        [0.9261, 0.9149, 0.9022, 0.8986, 0.8939, 0.8933, 0.8912, 0.8904, 0.8868,
         0.8841],
        [0.9040, 0.8986, 0.8973, 0.8973, 0.8955, 0.8951, 0.8924, 0.8898, 0.8891,
         0.8883],
        [0.9419, 0.9337, 0.9334, 0.9308, 0.9307, 0.9304, 0.9268, 0.9257, 0.9252,
         0.9240],
        [0.9259, 0.9111, 0.9110, 0.9090, 0.9058, 0.9056, 0.9055, 0.9026, 0.8899,
         0.8863],
        [0.9679, 0.9486, 0.9422, 0.9392, 0.9337, 0.9278, 0.9257, 0.9180, 0.9130,
         0.9123],
        [0.9412, 0.9344, 0.9303, 0.9266, 0.9241, 0.9230, 0.9229, 0.9228, 0.9227,
         0.9207],
        [0.9541, 0.9001, 0.8902, 0.8802, 0.8662, 0.8628, 0.8616, 0.8599, 0.8595,
         0.8574],
        [0.9363, 0.9321, 0.9305, 0.9240, 0.9223, 0.9183, 0.9145, 0.9106, 0.9040,
         0.9039],
        [0.9242, 0.9225, 0.9203, 0.9154, 0.9081, 0.9002, 0.8981, 0.8974, 0.8962,
         0.8952],
        [0.9600, 0.9483, 0.9428, 0.9265, 0.9251, 0.9242, 0.9194, 0.9181, 0.9179,
         0.9167],
        [0.9435, 0.9365, 0.9262, 0.9258, 0.9251, 0.9229, 0.9213, 0.9179, 0.9147,
         0.9131],
        [0.9318, 0.9269, 0.9255, 0.9252, 0.9219, 0.9191, 0.9177, 0.9175, 0.9140,
         0.9136],
        [0.9451, 0.9449, 0.9448, 0.9439, 0.9427, 0.9418, 0.9343, 0.9317, 0.9304,
         0.9284],
        [0.9378, 0.9211, 0.9199, 0.9104, 0.9022, 0.9020, 0.8978, 0.8973, 0.8969,
         0.8959],
        [0.8753, 0.8694, 0.8667, 0.8588, 0.8571, 0.8502, 0.8500, 0.8499, 0.8495,
         0.8494],
        [0.9021, 0.8915, 0.8863, 0.8842, 0.8817, 0.8804, 0.8798, 0.8782, 0.8770,
         0.8763],
        [0.9194, 0.9150, 0.9141, 0.9092, 0.9011, 0.9005, 0.8973, 0.8907, 0.8880,
         0.8872],
        [0.9059, 0.8963, 0.8937, 0.8851, 0.8825, 0.8766, 0.8764, 0.8708, 0.8599,
         0.8586],
        [0.9474, 0.8675, 0.8647, 0.8516, 0.8475, 0.8326, 0.8227, 0.8197, 0.8172,
         0.8162],
        [0.9011, 0.8828, 0.8813, 0.8757, 0.8754, 0.8703, 0.8572, 0.8570, 0.8555,
         0.8538],
        [0.8937, 0.8904, 0.8704, 0.8624, 0.8498, 0.8477, 0.8437, 0.8390, 0.8361,
         0.8343],
        [0.9355, 0.9317, 0.9301, 0.9297, 0.9288, 0.9273, 0.9257, 0.9250, 0.9237,
         0.9231],
        [0.8814, 0.8779, 0.8776, 0.8668, 0.8615, 0.8566, 0.8544, 0.8410, 0.8323,
         0.8253],
        [0.9005, 0.8617, 0.8615, 0.8588, 0.8534, 0.8519, 0.8497, 0.8486, 0.8470,
         0.8463],
        [0.8856, 0.8746, 0.8712, 0.8615, 0.8527, 0.8482, 0.8423, 0.8402, 0.8349,
         0.8344],
        [0.8781, 0.8726, 0.8584, 0.8534, 0.8491, 0.8453, 0.8448, 0.8430, 0.8414,
         0.8343],
        [0.8524, 0.8372, 0.8218, 0.8208, 0.8110, 0.8110, 0.7993, 0.7842, 0.7753,
         0.7742],
        [0.8303, 0.8068, 0.7918, 0.7866, 0.7806, 0.7803, 0.7767, 0.7643, 0.7597,
         0.7580],
        [0.9003, 0.8842, 0.8802, 0.8770, 0.8764, 0.8760, 0.8643, 0.8642, 0.8635,
         0.8620],
        [0.8067, 0.7851, 0.7808, 0.7744, 0.7633, 0.7612, 0.7449, 0.7432, 0.7326,
         0.7223],
        [0.8340, 0.8305, 0.8110, 0.8107, 0.8050, 0.8034, 0.8023, 0.7989, 0.7903,
         0.7868],
        [0.8921, 0.8631, 0.8457, 0.8350, 0.8328, 0.8307, 0.8226, 0.8217, 0.8170,
         0.8137],
        [0.8775, 0.8540, 0.8483, 0.8483, 0.8366, 0.8272, 0.8215, 0.8123, 0.8102,
         0.8057],
        [0.8885, 0.8854, 0.8458, 0.8173, 0.8140, 0.8094, 0.8041, 0.7995, 0.7991,
         0.7957],
        [0.9251, 0.9078, 0.9030, 0.8936, 0.8655, 0.8597, 0.8573, 0.8540, 0.8429,
         0.8390],
        [0.9123, 0.8745, 0.8705, 0.8605, 0.8538, 0.8534, 0.8440, 0.8434, 0.8401,
         0.8391],
        [0.9245, 0.9019, 0.8990, 0.8984, 0.8960, 0.8932, 0.8932, 0.8903, 0.8895,
         0.8892],
        [0.9006, 0.8805, 0.8794, 0.8684, 0.8665, 0.8658, 0.8630, 0.8622, 0.8617,
         0.8609],
        [0.9066, 0.8772, 0.8693, 0.8667, 0.8595, 0.8572, 0.8421, 0.8302, 0.8275,
         0.8249],
        [0.9401, 0.8794, 0.8715, 0.8633, 0.8562, 0.8517, 0.8362, 0.8334, 0.8324,
         0.8210],
        [0.9072, 0.8601, 0.8558, 0.8454, 0.8315, 0.8306, 0.8258, 0.8246, 0.8181,
         0.8168],
        [0.8989, 0.8183, 0.8156, 0.8037, 0.7934, 0.7932, 0.7921, 0.7889, 0.7853,
         0.7776],
        [0.8837, 0.8673, 0.8579, 0.8553, 0.8487, 0.8439, 0.8429, 0.8424, 0.8422,
         0.8419],
        [0.8543, 0.7768, 0.7765, 0.7762, 0.7652, 0.7355, 0.7307, 0.7268, 0.7264,
         0.7252],
        [0.8835, 0.8560, 0.8434, 0.8223, 0.8137, 0.8135, 0.7965, 0.7940, 0.7924,
         0.7878],
        [0.8386, 0.8085, 0.7960, 0.7810, 0.7782, 0.7735, 0.7685, 0.7542, 0.7510,
         0.7467]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 275749.8438,  189530.9375,  163432.3125,  158574.5312,  145214.9219,
          140129.5625,  138841.5156,  129247.5156,  124917.9688,  120816.7500],
        [ 702320.3125,  590020.4375,  576761.6250,  576493.3125,  560311.3750,
          538149.8125,  534517.2500,  524374.3125,  506965.4062,  503390.1562],
        [ 416430.5938,  372769.7812,  278448.7500,  257213.9531,  210241.2656,
          187020.4531,  181543.2500,  160391.2812,  154776.5156,  138420.1562],
        [ 853142.6250,  321819.0000,  306429.1562,  288405.1875,  285760.7188,
          275160.5938,  238326.8125,  232701.4375,  228965.9688,  222083.3750],
        [ 170889.8906,  146886.7344,  145886.0469,  113965.9297,  101847.2109,
          101404.2969,   94661.8125,   88545.2266,   79765.4844,   70222.2188],
        [ 197316.2500,  119529.1641,  104599.2734,   88854.5703,   83033.6797,
           80324.2656,   73525.9609,   66961.0938,   60976.0312,   57048.8945],
        [ 238746.3125,  230325.3281,  204108.7031,  196029.3906,  188216.5781,
          182189.6406,  180090.4375,  169198.3438,  167443.8438,  160538.6562],
        [ 350462.0938,  263507.5625,  243643.0469,  208721.5625,  187740.2500,
          146126.6562,  135155.0000,  120766.6328,  101992.4219,   97528.7734],
        [ 435115.1562,  396518.1562,  325643.5625,  307088.8750,  306980.8125,
          305901.2812,  302467.7812,  288903.1562,  286948.6250,  286873.6875],
        [ 772601.6875,  307821.0000,  302995.2500,  293219.4062,  267015.7188,
          259031.6406,  245537.3438,  245105.4531,  222678.7031,  217132.9844],
        [ 728941.3750,  405367.0000,  351954.5625,  338066.2500,  337269.8438,
          323783.6875,  306191.9688,  302481.3125,  289466.8750,  285593.4375],
        [ 327946.6875,  263896.5938,  206469.7812,  185591.7656,  134190.4688,
           92067.1094,   90638.7031,   88587.5391,   74189.0391,   68692.8672],
        [ 474435.7188,  421909.1250,  402896.1562,  341487.0312,  296087.0312,
          279846.6250,  248358.7031,  233768.8906,  226828.6562,  194898.8281],
        [ 873658.3750,  840391.8750,  808131.3750,  804988.4375,  772816.1875,
          754781.4375,  724058.3125,  707785.4375,  683896.0625,  638177.7500],
        [ 700927.1875,  577483.8125,  569306.5625,  558720.8125,  538557.0000,
          503353.2188,  498346.0312,  477054.5312,  470046.5000,  460006.4062],
        [ 558601.4375,  525479.1250,  519121.4062,  467506.7500,  453985.1562,
          447414.5938,  434525.0625,  415756.0000,  410789.2500,  398862.0312],
        [ 814792.0000,  748789.1875,  636932.5000,  617230.7500,  560427.8750,
          554672.1250,  501998.4375,  500481.7188,  475835.9062,  471872.7188],
        [ 932918.1875,  855020.0625,  802252.4375,  676205.0000,  675813.0625,
          637176.7500,  626473.8125,  615967.6875,  607273.9375,  595070.6250],
        [ 557077.3125,  474210.4375,  395650.9062,  375909.9375,  351370.6875,
          348293.6875,  338435.2500,  334200.9375,  317595.3438,  305703.8438],
        [ 405872.5938,  375759.0312,  369093.6562,  368808.6562,  359710.7500,
          357709.1875,  343933.7188,  331749.1250,  328055.2500,  324408.9688],
        [ 697407.2500,  620766.8125,  617996.4375,  595700.3125,  594803.3750,
          591831.6250,  562156.3750,  553386.0625,  549623.9375,  540120.1250],
        [ 555338.5000,  449524.4375,  448882.2500,  436252.8125,  416682.8750,
          415446.8750,  414837.1562,  398073.1562,  332204.7500,  315477.4062],
        [1012144.0000,  768384.6875,  700891.7500,  671005.4375,  620378.0000,
          570433.7500,  553534.3750,  496095.1250,  461698.0938,  456992.8750],
        [ 690993.5000,  626529.9375,  590967.6250,  560650.2500,  541025.3750,
          532952.6250,  531709.2500,  530941.6250,  530643.5000,  515470.7188],
        [ 830182.0000,  384079.6875,  333247.4062,  289049.7812,  236513.1875,
          225558.7344,  221721.7188,  216140.0469,  215032.6875,  208574.1250],
        [ 644301.0000,  606301.1875,  592620.6250,  540077.3750,  527088.2500,
          498173.5625,  471642.8125,  446300.1875,  405905.5000,  405369.6562],
        [ 541587.0625,  528991.7500,  512785.3125,  477611.7188,  430657.8750,
          384871.3438,  373188.4375,  369544.8438,  363177.3750,  358287.5312],
        [ 903811.8125,  764487.4375,  707128.3750,  560228.5625,  548812.5625,
          541797.2500,  505731.1562,  496799.1250,  495230.5312,  487005.7500],
        [ 713683.0625,  646142.0000,  557760.8750,  554456.8125,  548651.9375,
          531969.5000,  520128.3438,  494979.8125,  472926.4688,  462726.9062],
        [ 603870.6250,  563615.4375,  551826.1250,  549857.6875,  524412.8125,
          503757.0625,  493755.9062,  492342.4688,  468578.9375,  465926.2188],
        [ 730957.4375,  728715.4375,  727095.2500,  717878.4375,  705353.6250,
          697181.8750,  626285.6250,  602944.1875,  592131.9375,  575275.1875],
        [ 658244.1875,  518477.2188,  509789.9688,  444885.1250,  395966.4688,
          394571.3438,  371626.8125,  368931.7812,  366651.1562,  361720.7812],
        [ 269595.4375,  247807.2188,  238186.6250,  212979.7656,  207924.8906,
          188293.9688,  187786.6406,  187593.6719,  186504.4688,  186082.1562],
        [ 395400.8125,  339607.2500,  315221.4688,  305913.2188,  295123.1875,
          290036.0938,  287397.5000,  280745.0625,  276254.6875,  273284.2188],
        [ 506192.4375,  475017.0312,  469399.2188,  437611.1875,  389756.9688,
          386160.0000,  369041.5625,  335588.3125,  323054.2188,  319458.9688],
        [ 417080.8125,  363835.3438,  350349.5000,  310135.5938,  298685.7188,
          274460.5938,  273892.1562,  252535.0625,  216154.2812,  212261.3438],
        [ 755135.0000,  241155.4531,  231632.9062,  192014.6875,  181085.3594,
          146359.8438,  127063.8828,  121770.3594,  117442.0547,  115903.7969],
        [ 389838.3438,  299859.0312,  293664.3750,  271198.8750,  269695.7500,
          250977.5781,  208199.3125,  207630.4375,  202954.1875,  198241.6094],
        [ 350304.0312,  334288.9062,  251359.1250,  224131.7188,  187275.6719,
          181653.9062,  171552.5156,  160360.2344,  153907.3438,  150005.2500],
        [ 637187.6875,  603337.0625,  589642.3750,  586445.8125,  578624.9375,
          566757.8125,  553297.4375,  548365.2500,  537856.8750,  533614.7500],
        [ 294132.4375,  279679.8750,  278307.2188,  238643.6562,  221203.0000,
          206409.3281,  199905.2656,  165005.7812,  145848.0781,  131942.7031],
        [ 386269.7500,  221926.2969,  221378.7969,  212860.7656,  197040.2031,
          192921.2344,  186889.7656,  183939.3281,  179982.4219,  178177.6094],
        [ 312050.9688,  266842.3750,  254024.4219,  221114.2188,  195074.3750,
          182905.5156,  168117.1562,  163111.7188,  151244.4844,  150192.3438],
        [ 280401.5000,  259116.8594,  211794.4531,  197061.0625,  185363.5938,
          175560.3125,  174289.6406,  169890.5469,  166029.9062,  150085.2344],
        [ 194390.5781,  156288.1094,  125525.7031,  123647.7500,  107529.6328,
          107485.4453,   91046.9219,   73353.7344,   64548.9453,   63579.0078],
        [ 141616.7812,  101308.7891,   81799.7812,   75950.3359,   69645.9844,
           69319.6953,   65922.5156,   55210.0078,   51646.3984,   50409.4688],
        [ 385259.8750,  306084.8125,  289142.6875,  275973.9688,  273851.6875,
          272314.8750,  230366.8594,  230009.9062,  227840.6875,  222783.6250],
        [ 101210.6797,   74248.4219,   69814.5547,   63713.7539,   54435.7656,
           52806.4062,   41851.3789,   40820.1836,   35077.0234,   30293.1230],
        [ 149431.7031,  142134.0312,  107614.4688,  107116.5312,   98675.8672,
           96416.3359,   94924.1641,   90450.2031,   80046.4531,   76086.9922],
        [ 342733.3750,  226325.6250,  176492.8438,  151516.0469,  146918.9688,
          142565.6094,  126952.3281,  125283.0469,  117188.8672,  111775.5938],
        [ 278242.2188,  198867.2344,  183251.9062,  183210.6719,  154981.9688,
          135476.8438,  125006.0469,  109541.6641,  106343.6875,   99773.0625],
        [ 325205.3438,  311538.0312,  176761.6875,  117757.5391,  112301.2812,
          105109.8516,   97421.3984,   91256.0781,   90768.6328,   86403.4688],
        [ 548689.6250,  428921.5938,  400424.6875,  350141.3750,  234423.9062,
          215643.4531,  208351.4531,  198687.5312,  169649.9531,  160559.1719],
        [ 457067.3750,  266585.4688,  251576.6719,  218161.8750,  198208.7188,
          197047.3594,  172320.8906,  170805.6562,  162878.8438,  160667.4688],
        [ 544170.5000,  394261.0312,  378174.5312,  374917.8438,  361988.2188,
          347773.2188,  347773.2188,  334003.6875,  330313.3438,  328723.2500],
        [ 386616.5312,  290357.1250,  285599.9688,  244179.2188,  237555.0625,
          235194.8906,  225932.4844,  223358.4219,  221800.3906,  219458.7656],
        [ 421376.3125,  276743.5625,  247284.7656,  238434.5781,  215178.3594,
          208181.2344,  167821.4531,  141527.6562,  136220.3594,  131191.2500],
        [ 680292.9375,  285778.9688,  255085.7812,  226902.6562,  205223.5938,
          192380.9062,  154087.1094,  148183.2656,  146040.2812,  123981.3203],
        [ 425308.2188,  216822.5938,  203948.5781,  175924.1719,  144052.0156,
          142353.9375,  132818.6094,  130627.5703,  119101.8984,  116912.2500],
        [ 377442.0312,  119416.5938,  114877.9609,   96895.8438,   83591.6719,
           83356.9141,   82100.1250,   78486.3047,   74500.0078,   66787.8750],
        [ 303688.6562,  240511.0000,  210078.7188,  202523.7969,  184379.7969,
          172080.7969,  169598.1875,  168530.5000,  168027.0625,  167165.2500],
        [ 199675.4688,   66028.0312,   65673.0781,   65390.8516,   55899.4961,
           36588.2852,   34174.8828,   32295.6094,   32130.6387,   31561.3262],
        [ 302979.3438,  204447.0938,  170947.4219,  126462.2109,  111727.7422,
          111469.0156,   87409.2109,   84300.5781,   82463.2812,   77225.3672],
        [ 159515.5312,  103822.1016,   86797.1719,   70057.9375,   67359.7344,
           62908.7422,   58634.2656,   47772.8984,   45618.8359,   42940.2266]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[275749.8438,      0.0000],
         [189530.9375,      0.0000],
         [163432.3125,      0.0000],
         ...,
         [129247.5156,      0.0000],
         [124917.9688,      0.0000],
         [     0.0000, 120816.7500]],

        [[702320.3125,      0.0000],
         [590020.4375,      0.0000],
         [576761.6250,      0.0000],
         ...,
         [524374.3125,      0.0000],
         [506965.4062,      0.0000],
         [503390.1562,      0.0000]],

        [[416430.5938,      0.0000],
         [372769.7812,      0.0000],
         [278448.7500,      0.0000],
         ...,
         [160391.2812,      0.0000],
         [154776.5156,      0.0000],
         [138420.1562,      0.0000]],

        ...,

        [[     0.0000, 199675.4688],
         [ 66028.0312,      0.0000],
         [ 65673.0781,      0.0000],
         ...,
         [ 32295.6094,      0.0000],
         [ 32130.6387,      0.0000],
         [ 31561.3262,      0.0000]],

        [[302979.3438,      0.0000],
         [     0.0000, 204447.0938],
         [170947.4219,      0.0000],
         ...,
         [ 84300.5781,      0.0000],
         [ 82463.2812,      0.0000],
         [ 77225.3672,      0.0000]],

        [[     0.0000, 159515.5312],
         [     0.0000, 103822.1016],
         [     0.0000,  86797.1719],
         ...,
         [ 47772.8984,      0.0000],
         [     0.0000,  45618.8359],
         [     0.0000,  42940.2266]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1465639.0000,  120816.7500],
        [5613304.0000,       0.0000],
        [2357256.0000,       0.0000],
        [3252795.0000,       0.0000],
        [ 460019.7500,  654055.1250],
        [ 784883.8125,  147285.3594],
        [1749443.3750,  167443.8438],
        [1855644.0000,       0.0000],
        [3242441.0000,       0.0000],
        [3133139.5000,       0.0000],
        [3669116.2500,       0.0000],
        [1532270.5000,       0.0000],
        [3120517.0000,       0.0000],
        [7608685.0000,       0.0000],
        [5353802.0000,       0.0000],
        [4632041.0000,       0.0000],
        [5883033.5000,       0.0000],
        [7024172.0000,       0.0000],
        [3798448.5000,       0.0000],
        [3233352.2500,  331749.1250],
        [5923792.5000,       0.0000],
        [4182720.5000,       0.0000],
        [6311558.0000,       0.0000],
        [5651884.5000,       0.0000],
        [3160099.2500,       0.0000],
        [5137780.0000,       0.0000],
        [4340703.5000,       0.0000],
        [6011033.0000,       0.0000],
        [5503425.5000,       0.0000],
        [5217943.5000,       0.0000],
        [6703819.0000,       0.0000],
        [4390864.5000,       0.0000],
        [2112754.7500,       0.0000],
        [2448639.0000,  610344.6250],
        [1816542.2500, 2194737.5000],
        [1044786.0625, 1924604.3750],
        [1115830.7500, 1113732.5000],
        [1361667.5000, 1230592.0000],
        [ 725674.9375, 1439163.7500],
        [5735130.0000,       0.0000],
        [2161077.2500,       0.0000],
        [2161386.0000,       0.0000],
        [2064677.5000,       0.0000],
        [1969593.2500,       0.0000],
        [ 244462.3750,  862933.5000],
        [ 332484.9375,  430344.8125],
        [2328369.0000,  385259.8750],
        [ 356921.4375,  207349.8438],
        [ 461314.6250,  581582.1250],
        [ 454508.9688, 1213243.3750],
        [1332874.7500,  241820.5312],
        [ 701018.2500,  813505.0625],
        [1722098.7500, 1193394.0000],
        [2058273.0000,  197047.3594],
        [3742098.7500,       0.0000],
        [2570053.0000,       0.0000],
        [1001107.4375, 1182852.1250],
        [1700051.6250,  717905.2500],
        [ 975063.4375,  832806.4375],
        [ 552737.8750,  624717.3750],
        [1258004.2500,  728579.4375],
        [ 419742.2188,  199675.4688],
        [ 931787.3750,  427643.8750],
        [ 110681.6406,  634745.8125]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 65.625
Top1 accuracy for validation set is 65.625 size is torch.Size([64, 1])
Epoch 411/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:36, 61.24s/it]  7%|▋         | 2/30 [01:02<12:06, 25.94s/it] 10%|█         | 3/30 [01:03<06:29, 14.43s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.03s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.04s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.357818619410197
Epoch 412/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:03<30:42, 63.54s/it]  7%|▋         | 2/30 [01:04<12:24, 26.60s/it] 10%|█         | 3/30 [01:05<06:39, 14.80s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.25s/it] 17%|█▋        | 5/30 [01:06<02:34,  6.18s/it] 20%|██        | 6/30 [01:07<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:08<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:09<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.33it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.85s/it]
Epoch loss is 2.331855034828186
Epoch 413/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:39, 59.28s/it]  7%|▋         | 2/30 [01:01<11:59, 25.71s/it] 10%|█         | 3/30 [01:02<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:03<02:29,  6.00s/it] 20%|██        | 6/30 [01:04<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.367038933436076
Epoch 414/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:15, 60.55s/it]  7%|▋         | 2/30 [01:01<11:50, 25.37s/it] 10%|█         | 3/30 [01:02<06:21, 14.13s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:39,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3271378676096597
Epoch 415/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:33, 61.15s/it]  7%|▋         | 2/30 [01:02<12:12, 26.15s/it] 10%|█         | 3/30 [01:03<06:32, 14.55s/it] 13%|█▎        | 4/30 [01:04<03:56,  9.10s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.09s/it] 20%|██        | 6/30 [01:05<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.36s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.3757595856984457
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0237,  0.0013,  0.0130,  ..., -0.0054,  0.0166,  0.0039],
        [-0.0022,  0.0100,  0.0284,  ...,  0.0169,  0.0034, -0.0152],
        [-0.0294, -0.0380,  0.0248,  ...,  0.0827, -0.0049, -0.0147],
        ...,
        [ 0.0065, -0.0034,  0.0093,  ..., -0.0240, -0.0059, -0.0037],
        [-0.0330,  0.0093, -0.0037,  ...,  0.0048,  0.0167, -0.0109],
        [-0.0332, -0.0137,  0.0189,  ...,  0.0410,  0.0355, -0.0292]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8764, 0.8507, 0.8398, 0.8379, 0.8317, 0.8293, 0.8283, 0.8234, 0.8212,
         0.8188],
        [0.9422, 0.9299, 0.9284, 0.9283, 0.9264, 0.9233, 0.9229, 0.9216, 0.9193,
         0.9188],
        [0.9056, 0.8978, 0.8771, 0.8718, 0.8575, 0.8494, 0.8473, 0.8386, 0.8362,
         0.8284],
        [0.9560, 0.8874, 0.8839, 0.8798, 0.8793, 0.8764, 0.8663, 0.8646, 0.8634,
         0.8615],
        [0.8433, 0.8324, 0.8322, 0.8149, 0.8069, 0.8066, 0.8019, 0.7971, 0.7898,
         0.7812],
        [0.8533, 0.8183, 0.8089, 0.7974, 0.7930, 0.7901, 0.7840, 0.7778, 0.7706,
         0.7664],
        [0.8663, 0.8638, 0.8554, 0.8527, 0.8498, 0.8476, 0.8467, 0.8423, 0.8414,
         0.8385],
        [0.8933, 0.8734, 0.8679, 0.8571, 0.8497, 0.8319, 0.8267, 0.8187, 0.8069,
         0.8036],
        [0.9086, 0.9021, 0.8882, 0.8842, 0.8841, 0.8841, 0.8832, 0.8800, 0.8794,
         0.8794],
        [0.9490, 0.8843, 0.8832, 0.8809, 0.8743, 0.8723, 0.8685, 0.8683, 0.8617,
         0.8599],
        [0.9449, 0.9038, 0.8937, 0.8907, 0.8906, 0.8878, 0.8841, 0.8831, 0.8801,
         0.8789],
        [0.8886, 0.8734, 0.8563, 0.8487, 0.8259, 0.7995, 0.7987, 0.7967, 0.7844,
         0.7791],
        [0.9144, 0.9065, 0.9031, 0.8914, 0.8814, 0.8775, 0.8692, 0.8649, 0.8629,
         0.8522],
        [0.9574, 0.9548, 0.9519, 0.9517, 0.9488, 0.9471, 0.9443, 0.9426, 0.9403,
         0.9354],
        [0.9421, 0.9285, 0.9275, 0.9262, 0.9235, 0.9188, 0.9182, 0.9151, 0.9141,
         0.9126],
        [0.9262, 0.9219, 0.9210, 0.9137, 0.9117, 0.9106, 0.9086, 0.9054, 0.9046,
         0.9026],
        [0.9525, 0.9466, 0.9353, 0.9329, 0.9263, 0.9256, 0.9185, 0.9184, 0.9148,
         0.9142],
        [0.9621, 0.9560, 0.9516, 0.9397, 0.9395, 0.9354, 0.9342, 0.9329, 0.9321,
         0.9306],
        [0.9260, 0.9145, 0.9017, 0.8984, 0.8937, 0.8930, 0.8910, 0.8901, 0.8866,
         0.8841],
        [0.9037, 0.8984, 0.8971, 0.8971, 0.8953, 0.8948, 0.8921, 0.8897, 0.8888,
         0.8880],
        [0.9415, 0.9334, 0.9331, 0.9307, 0.9304, 0.9302, 0.9265, 0.9253, 0.9250,
         0.9237],
        [0.9258, 0.9110, 0.9109, 0.9088, 0.9057, 0.9053, 0.9052, 0.9024, 0.8896,
         0.8859],
        [0.9678, 0.9486, 0.9421, 0.9390, 0.9336, 0.9276, 0.9255, 0.9179, 0.9129,
         0.9121],
        [0.9411, 0.9342, 0.9301, 0.9265, 0.9240, 0.9230, 0.9227, 0.9226, 0.9225,
         0.9206],
        [0.9539, 0.8998, 0.8897, 0.8800, 0.8658, 0.8623, 0.8613, 0.8594, 0.8591,
         0.8573],
        [0.9361, 0.9319, 0.9302, 0.9237, 0.9221, 0.9181, 0.9142, 0.9104, 0.9037,
         0.9037],
        [0.9239, 0.9222, 0.9200, 0.9151, 0.9078, 0.9000, 0.8978, 0.8972, 0.8960,
         0.8950],
        [0.9599, 0.9482, 0.9427, 0.9263, 0.9249, 0.9241, 0.9192, 0.9180, 0.9178,
         0.9166],
        [0.9432, 0.9363, 0.9261, 0.9256, 0.9248, 0.9227, 0.9211, 0.9177, 0.9144,
         0.9129],
        [0.9316, 0.9268, 0.9253, 0.9251, 0.9217, 0.9188, 0.9175, 0.9173, 0.9138,
         0.9135],
        [0.9449, 0.9448, 0.9447, 0.9438, 0.9425, 0.9417, 0.9341, 0.9315, 0.9303,
         0.9282],
        [0.9376, 0.9210, 0.9198, 0.9101, 0.9020, 0.9019, 0.8976, 0.8971, 0.8966,
         0.8957],
        [0.8750, 0.8692, 0.8664, 0.8586, 0.8570, 0.8501, 0.8497, 0.8497, 0.8492,
         0.8490],
        [0.9017, 0.8911, 0.8860, 0.8839, 0.8813, 0.8801, 0.8796, 0.8780, 0.8767,
         0.8759],
        [0.9192, 0.9148, 0.9139, 0.9089, 0.9009, 0.9005, 0.8970, 0.8904, 0.8877,
         0.8870],
        [0.9057, 0.8960, 0.8934, 0.8850, 0.8821, 0.8764, 0.8762, 0.8704, 0.8596,
         0.8584],
        [0.9474, 0.8675, 0.8645, 0.8514, 0.8472, 0.8320, 0.8222, 0.8194, 0.8165,
         0.8161],
        [0.9008, 0.8823, 0.8807, 0.8756, 0.8748, 0.8701, 0.8567, 0.8565, 0.8551,
         0.8534],
        [0.8935, 0.8901, 0.8700, 0.8620, 0.8495, 0.8471, 0.8433, 0.8384, 0.8358,
         0.8340],
        [0.9352, 0.9315, 0.9299, 0.9295, 0.9285, 0.9269, 0.9254, 0.9248, 0.9236,
         0.9228],
        [0.8811, 0.8776, 0.8773, 0.8664, 0.8612, 0.8563, 0.8540, 0.8406, 0.8320,
         0.8249],
        [0.9003, 0.8614, 0.8613, 0.8584, 0.8529, 0.8515, 0.8493, 0.8480, 0.8467,
         0.8461],
        [0.8853, 0.8742, 0.8708, 0.8612, 0.8523, 0.8478, 0.8418, 0.8397, 0.8346,
         0.8342],
        [0.8774, 0.8723, 0.8580, 0.8528, 0.8488, 0.8447, 0.8445, 0.8426, 0.8412,
         0.8342],
        [0.8520, 0.8370, 0.8214, 0.8202, 0.8106, 0.8105, 0.7989, 0.7835, 0.7749,
         0.7735],
        [0.8300, 0.8065, 0.7914, 0.7864, 0.7804, 0.7801, 0.7766, 0.7642, 0.7595,
         0.7579],
        [0.9000, 0.8836, 0.8798, 0.8765, 0.8758, 0.8754, 0.8637, 0.8637, 0.8629,
         0.8613],
        [0.8063, 0.7848, 0.7804, 0.7739, 0.7633, 0.7608, 0.7441, 0.7424, 0.7321,
         0.7218],
        [0.8334, 0.8301, 0.8108, 0.8104, 0.8045, 0.8032, 0.8022, 0.7985, 0.7900,
         0.7863],
        [0.8918, 0.8627, 0.8453, 0.8344, 0.8325, 0.8304, 0.8222, 0.8213, 0.8166,
         0.8134],
        [0.8770, 0.8538, 0.8481, 0.8479, 0.8363, 0.8270, 0.8214, 0.8118, 0.8100,
         0.8054],
        [0.8882, 0.8852, 0.8454, 0.8168, 0.8134, 0.8085, 0.8036, 0.7990, 0.7987,
         0.7950],
        [0.9248, 0.9076, 0.9026, 0.8933, 0.8651, 0.8595, 0.8570, 0.8535, 0.8427,
         0.8387],
        [0.9121, 0.8744, 0.8703, 0.8603, 0.8535, 0.8529, 0.8435, 0.8432, 0.8397,
         0.8387],
        [0.9242, 0.9016, 0.8986, 0.8981, 0.8957, 0.8929, 0.8929, 0.8900, 0.8893,
         0.8888],
        [0.9002, 0.8802, 0.8791, 0.8681, 0.8661, 0.8654, 0.8625, 0.8618, 0.8615,
         0.8604],
        [0.9061, 0.8769, 0.8688, 0.8664, 0.8594, 0.8568, 0.8421, 0.8298, 0.8269,
         0.8245],
        [0.9400, 0.8792, 0.8711, 0.8629, 0.8559, 0.8515, 0.8359, 0.8330, 0.8319,
         0.8204],
        [0.9069, 0.8598, 0.8555, 0.8451, 0.8312, 0.8303, 0.8252, 0.8242, 0.8177,
         0.8163],
        [0.8987, 0.8179, 0.8152, 0.8032, 0.7929, 0.7926, 0.7916, 0.7885, 0.7849,
         0.7771],
        [0.8836, 0.8670, 0.8575, 0.8550, 0.8483, 0.8437, 0.8422, 0.8420, 0.8418,
         0.8413],
        [0.8541, 0.7764, 0.7763, 0.7757, 0.7648, 0.7355, 0.7304, 0.7265, 0.7264,
         0.7248],
        [0.8833, 0.8558, 0.8430, 0.8222, 0.8132, 0.8132, 0.7957, 0.7934, 0.7918,
         0.7873],
        [0.8384, 0.8081, 0.7955, 0.7805, 0.7779, 0.7729, 0.7683, 0.7536, 0.7505,
         0.7460]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 273891.3750,  189627.6562,  162361.9531,  157951.9375,  144647.5469,
          139594.0312,  137753.6719,  128359.5312,  124375.6797,  120154.4375],
        [ 700808.8750,  587760.4375,  575297.6250,  574394.7500,  559565.9375,
          535291.1250,  532050.1250,  522380.7500,  505243.2812,  501361.1875],
        [ 415444.4688,  371575.7812,  276374.5938,  256237.8125,  209084.3438,
          186219.0312,  180699.1094,  159581.8750,  154054.0469,  137986.5312],
        [ 853156.4375,  320246.5625,  304713.3125,  287560.9062,  285171.8438,
          273564.2812,  237119.3594,  231390.4844,  227384.8438,  221320.9688],
        [ 170547.3438,  145967.1875,  145516.7188,  113716.9844,  101380.1250,
          100930.1953,   94501.0000,   88230.3125,   79415.7188,   70250.6875],
        [ 196804.3281,  119334.3984,  104439.0938,   88539.7422,   83136.8438,
           79757.0391,   73144.4453,   66899.5547,   60417.1602,   56893.4531],
        [ 237072.3281,  228564.7500,  202751.2500,  195240.5781,  187106.7969,
          181393.0312,  178995.4531,  168297.4688,  165982.4219,  159298.2969],
        [ 348412.5938,  262206.5000,  242537.9219,  207725.9062,  186905.9844,
          145053.9531,  134641.4219,  120010.4922,  101403.1406,   96787.0469],
        [ 433344.4062,  395458.5312,  324205.7500,  306109.3438,  305507.0938,
          305449.4062,  301627.2500,  288057.1875,  285761.8125,  285705.9375],
        [ 771929.3125,  306391.4688,  301745.4688,  292115.5312,  265707.5000,
          258020.0469,  244377.7188,  243800.6406,  221951.0469,  216302.9531],
        [ 727918.1250,  404925.3750,  350458.0938,  335839.6562,  335240.5938,
          322150.6562,  305415.3438,  301083.7500,  288544.0938,  283621.5625],
        [ 325871.5625,  262126.2656,  205392.1719,  184261.3281,  133106.3125,
           91229.0078,   90281.9766,   87736.3438,   73520.2031,   68162.0703],
        [ 471136.6562,  420643.1562,  400795.6250,  339379.0312,  293917.6562,
          278103.7188,  247144.2500,  232323.1406,  225754.5781,  193745.0312],
        [ 870747.0625,  839387.3750,  805323.3125,  802431.5000,  770643.5625,
          752035.3125,  721662.7500,  705072.5000,  681886.2500,  635869.2500],
        [ 699937.2500,  576170.0625,  567943.3125,  557271.2500,  536679.9375,
          502034.3750,  497257.5000,  475801.3750,  469366.5312,  459287.0938],
        [ 557904.5625,  524610.8750,  518065.0000,  466671.5625,  453275.2188,
          446407.0312,  433648.6875,  414555.5625,  409704.3125,  398229.1875],
        [ 811997.9375,  746451.2500,  634968.0625,  613990.0000,  558484.2500,
          552990.3750,  499602.3125,  498756.8438,  473929.2812,  470076.0938],
        [ 930741.8125,  853315.9375,  801179.7500,  675971.5625,  674607.5625,
          635678.8125,  625095.8125,  614025.7500,  606383.8750,  593857.9375],
        [ 555721.5000,  472029.3750,  392920.6250,  374966.8125,  350278.3125,
          346996.3438,  337344.8125,  332881.4688,  316584.7188,  305651.3750],
        [ 404320.6875,  374890.3125,  367979.1562,  367977.4062,  358545.5938,
          356239.5000,  342474.5938,  331069.6250,  326914.0312,  323122.3125],
        [ 694032.1250,  617858.0000,  615549.0000,  594313.5000,  592012.8125,
          590237.0625,  560267.6250,  550305.1875,  547967.4375,  538376.1875],
        [ 554512.3750,  448534.3750,  448225.1875,  434612.9375,  415947.5625,
          413886.7812,  412873.5938,  396961.6250,  330800.7188,  313639.6875],
        [1010469.7500,  767761.3125,  699880.5625,  669505.2500,  619482.3125,
          568731.9375,  552165.1250,  495162.0938,  461010.3750,  455676.3750],
        [ 689807.6875,  624989.1250,  589744.1875,  559993.0000,  540532.8125,
          532712.1875,  530800.3750,  529777.8125,  529137.5625,  514758.9062],
        [ 827998.9375,  382524.3125,  331264.4688,  288156.0938,  235449.8281,
          223810.0312,  220597.3438,  214803.5469,  213792.1250,  208254.9062],
        [ 642656.9375,  604881.0000,  590511.8125,  538285.8750,  525783.8750,
          496481.7812,  469843.9375,  444990.3750,  404597.2188,  404492.3125],
        [ 539740.1250,  526822.3750,  510458.4062,  476124.1250,  428956.7812,
          383524.0625,  371903.0000,  368392.7812,  362194.7500,  357085.7812],
        [ 902544.7500,  763603.5625,  705362.3750,  558353.8125,  547314.0625,
          540757.1250,  504782.4062,  495810.3750,  494809.9375,  486042.9688],
        [ 711400.5000,  644351.3750,  557113.9375,  552525.4375,  546724.0625,
          530342.4375,  518274.5312,  493979.6250,  470789.8750,  460940.4688],
        [ 601943.3125,  562179.4375,  550748.8750,  548704.2500,  523229.8438,
          501834.7500,  492686.2812,  491312.4062,  467364.5938,  465301.0312],
        [ 728647.3750,  727462.1875,  726073.2500,  716598.6875,  703367.9375,
          695526.3125,  624563.1250,  601526.1250,  590949.5625,  573989.5000],
        [ 656205.9375,  517599.8125,  508612.8750,  443032.7812,  394671.4375,
          393909.2500,  370412.4688,  367931.7812,  365349.9375,  360858.6875],
        [ 268472.4375,  247143.5625,  237257.7969,  212344.1562,  207568.0625,
          188108.7344,  187040.2500,  186874.2656,  185510.9062,  185136.5781],
        [ 393019.5625,  337966.3125,  313788.0938,  304621.8125,  293698.5312,
          288478.9062,  286403.7812,  279945.3750,  275011.5938,  271873.1875],
        [ 504255.5312,  473655.8750,  467721.2812,  435422.7500,  388347.0312,
          386017.4688,  367602.0938,  334173.1875,  321599.9375,  318434.2188],
        [ 416159.8125,  362207.1562,  348939.9688,  309499.7500,  297176.1562,
          273574.9688,  273131.8125,  251312.6406,  215404.6094,  211588.5156],
        [ 754837.6250,  241031.2969,  230981.0625,  191410.5938,  180428.4062,
          145188.4688,  126199.8125,  121211.5547,  116340.3438,  115648.9609],
        [ 387742.3438,  297902.2812,  291277.9375,  270835.2188,  267599.0000,
          250102.3594,  206649.2344,  205962.3750,  201972.9375,  197132.1250],
        [ 349338.5625,  332973.5625,  249821.0625,  223002.7656,  186526.1719,
          180145.0469,  170575.1562,  159136.2812,  153308.9375,  149486.2812],
        [ 634494.1250,  601383.8750,  587570.5000,  584676.1250,  576477.3750,
          563463.8750,  551452.1250,  546306.5625,  537658.9375,  531441.6250],
        [ 292973.6875,  278445.8125,  277196.5625,  237427.3281,  220444.8594,
          205552.2812,  198907.6406,  164128.4688,  145079.1250,  131265.5781],
        [ 385119.1875,  220808.2500,  220786.1406,  211801.3125,  195746.1875,
          191867.1406,  185784.2656,  182408.1875,  179014.4062,  177510.3906],
        [ 310699.8438,  265466.1250,  252642.7344,  220296.2812,  194079.0156,
          181982.1406,  167109.3125,  162160.9375,  150715.0625,  149818.3906],
        [ 277861.9688,  258104.7031,  210603.0781,  195383.8125,  184475.6562,
          174041.8281,  173589.7969,  169022.3906,  165455.0312,  149769.2500],
        [ 193241.0781,  155911.0312,  124777.3594,  122616.9766,  106894.0781,
          106839.0391,   90456.1562,   72650.2344,   64255.7969,   62969.8477],
        [ 141117.1406,  100812.1641,   81238.4062,   75690.6094,   69457.4062,
           69172.3672,   65788.5547,   55133.7148,   51570.0156,   50404.4648],
        [ 383455.6875,  303588.1875,  287181.9062,  274069.3125,  271521.5938,
          269762.8750,  228297.0312,  228274.1719,  225900.1719,  220546.0156],
        [ 100620.5391,   74011.5234,   69415.9531,   63320.5664,   54379.1016,
           52511.1172,   41366.2031,   40380.9883,   34844.0039,   30056.3184],
        [ 148198.8281,  141224.3125,  107286.5547,  106652.1328,   98061.8672,
           96168.9375,   94789.1953,   89899.1328,   79690.9688,   75599.7109],
        [ 341120.5312,  225230.0781,  175430.7812,  150170.7188,  146155.2344,
          141862.6562,  126229.5312,  124601.8359,  116559.3516,  111297.1484],
        [ 276244.6562,  198247.2656,  182621.9219,  182205.7969,  154290.0312,
          135117.3594,  124693.2578,  108817.8125,  106024.0938,   99341.8203],
        [ 323947.6875,  310411.3750,  175758.8281,  116809.6172,  111274.0156,
          103816.3594,   96793.6953,   90654.3516,   90262.2656,   85619.2188],
        [ 546254.5000,  427423.8125,  397876.9375,  348754.0312,  233031.9062,
          214940.6406,  207485.3438,  197277.6875,  169038.5156,  159690.8750],
        [ 455835.9062,  266006.6875,  250847.1406,  217356.1094,  197513.1719,
          195810.4219,  171078.7188,  170401.5000,  162007.6094,  159847.0625],
        [ 541763.6875,  392210.0625,  376146.9688,  373084.8750,  360864.2188,
          346282.9688,  346282.9688,  332640.6250,  329287.4062,  326774.0938],
        [ 384588.0938,  289059.4062,  284566.0312,  243158.8281,  236382.6406,
          234075.4062,  224335.9375,  222139.7344,  221276.6406,  217952.8750],
        [ 418595.0625,  275779.5625,  245710.4531,  237265.9531,  214627.0312,
          207022.2500,  167678.9062,  140678.1406,  134990.6250,  130470.5781],
        [ 679144.2500,  284892.4062,  253897.2656,  225685.9062,  204404.3906,
          191760.6719,  153425.6406,  147199.3125,  144983.1406,  123084.3203],
        [ 423016.6250,  216093.8906,  203087.4062,  174977.7969,  143615.9375,
          141645.9531,  131738.2656,  129942.3359,  118302.6797,  115927.5625],
        [ 376528.1250,  118752.7891,  114206.6016,   96186.9141,   83026.7109,
           82641.4453,   81570.9844,   77943.2891,   74056.2812,   66235.1406],
        [ 303507.7188,  239260.2969,  208957.5781,  201514.8438,  183244.3906,
          171576.5625,  167968.9219,  167469.7188,  166886.9844,  165886.5156],
        [ 199082.7969,   65620.3047,   65542.6250,   64941.0312,   55561.5273,
           36601.7930,   33999.2422,   32182.0371,   32126.4102,   31391.5430],
        [ 301991.3438,  203981.8281,  169925.0469,  126224.7266,  111014.4297,
          110915.1641,   86487.9766,   83595.9766,   81748.0781,   76608.9688],
        [ 158965.6406,  103222.2422,   86245.7344,   69587.1641,   67073.4453,
           62378.0625,   58404.3359,   47379.8867,   45284.5508,   42510.1172]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[273891.3750,      0.0000],
         [189627.6562,      0.0000],
         [162361.9531,      0.0000],
         ...,
         [128359.5312,      0.0000],
         [124375.6797,      0.0000],
         [     0.0000, 120154.4375]],

        [[700808.8750,      0.0000],
         [587760.4375,      0.0000],
         [575297.6250,      0.0000],
         ...,
         [522380.7500,      0.0000],
         [505243.2812,      0.0000],
         [501361.1875,      0.0000]],

        [[415444.4688,      0.0000],
         [371575.7812,      0.0000],
         [276374.5938,      0.0000],
         ...,
         [159581.8750,      0.0000],
         [154054.0469,      0.0000],
         [137986.5312,      0.0000]],

        ...,

        [[     0.0000, 199082.7969],
         [ 65620.3047,      0.0000],
         [ 65542.6250,      0.0000],
         ...,
         [ 32182.0371,      0.0000],
         [ 32126.4102,      0.0000],
         [ 31391.5430,      0.0000]],

        [[301991.3438,      0.0000],
         [     0.0000, 203981.8281],
         [169925.0469,      0.0000],
         ...,
         [ 83595.9766,      0.0000],
         [ 81748.0781,      0.0000],
         [ 76608.9688,      0.0000]],

        [[     0.0000, 158965.6406],
         [     0.0000, 103222.2422],
         [     0.0000,  86245.7344],
         ...,
         [ 47379.8867,      0.0000],
         [     0.0000,  45284.5508],
         [     0.0000,  42510.1172]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1458563.5000,  120154.4375],
        [5594154.0000,       0.0000],
        [2347257.7500,       0.0000],
        [3241629.0000,       0.0000],
        [ 458814.5625,  651641.6875],
        [ 782709.5000,  146656.5938],
        [1738720.0000,  165982.4219],
        [1845685.0000,       0.0000],
        [3231226.7500,       0.0000],
        [3122341.7500,       0.0000],
        [3655197.5000,       0.0000],
        [1521687.2500,       0.0000],
        [3102942.7500,       0.0000],
        [7585059.0000,       0.0000],
        [5341748.5000,       0.0000],
        [4623072.0000,       0.0000],
        [5861246.5000,       0.0000],
        [7010859.0000,       0.0000],
        [3785375.2500,       0.0000],
        [3222463.7500,  331069.6250],
        [5900919.0000,       0.0000],
        [4169994.7500,       0.0000],
        [6299845.5000,       0.0000],
        [5642253.0000,       0.0000],
        [3146651.5000,       0.0000],
        [5122525.0000,       0.0000],
        [4325202.5000,       0.0000],
        [5999381.0000,       0.0000],
        [5486442.0000,       0.0000],
        [5205304.5000,       0.0000],
        [6688704.0000,       0.0000],
        [4378585.0000,       0.0000],
        [2105456.7500,       0.0000],
        [2437320.5000,  607486.6250],
        [1809530.7500, 2187698.7500],
        [1040512.9375, 1918482.5000],
        [1111068.8750, 1112209.2500],
        [1353097.0000, 1224078.8750],
        [ 721974.1875, 1432339.6250],
        [5714925.0000,       0.0000],
        [2151421.2500,       0.0000],
        [2150845.5000,       0.0000],
        [2054969.7500,       0.0000],
        [1958307.5000,       0.0000],
        [ 242514.1562,  858097.4375],
        [ 331678.9375,  428705.8750],
        [2309141.2500,  383455.6875],
        [ 354543.8125,  206362.4844],
        [ 459128.5000,  578443.1250],
        [ 452417.6875, 1206240.2500],
        [1326462.5000,  241141.4531],
        [ 695229.5000,  810117.8750],
        [1713378.0000, 1188396.2500],
        [2050894.0000,  195810.4219],
        [3725338.0000,       0.0000],
        [2557535.5000,       0.0000],
        [ 995736.5000, 1177082.0000],
        [1693494.5000,  714982.7500],
        [ 970035.3125,  828313.1250],
        [ 549171.3750,  621976.9375],
        [1250261.1250,  726012.4375],
        [ 417966.5000,  199082.7969],
        [ 926582.1250,  425911.4375],
        [ 109757.9531,  631293.2500]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 416/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:35, 59.15s/it]  7%|▋         | 2/30 [00:59<11:34, 24.80s/it] 10%|█         | 3/30 [01:00<06:13, 13.82s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.66s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.80s/it] 20%|██        | 6/30 [01:02<01:38,  4.08s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.70s/it]
Epoch loss is 2.361019190152486
Epoch 417/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:47, 61.63s/it]  7%|▋         | 2/30 [01:02<12:02, 25.82s/it] 10%|█         | 3/30 [01:03<06:27, 14.37s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.99s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.02s/it] 20%|██        | 6/30 [01:05<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 2.3490394910176593
Epoch 418/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:21, 60.73s/it]  7%|▋         | 2/30 [01:01<11:52, 25.45s/it] 10%|█         | 3/30 [01:02<06:22, 14.17s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:04<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.31871919631958
Epoch 419/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:47, 61.62s/it]  7%|▋         | 2/30 [01:02<12:02, 25.82s/it] 10%|█         | 3/30 [01:03<06:32, 14.55s/it] 13%|█▎        | 4/30 [01:04<03:56,  9.10s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.09s/it] 20%|██        | 6/30 [01:05<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.3581326405207315
Epoch 420/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:22, 58.72s/it]  7%|▋         | 2/30 [01:00<11:53, 25.50s/it] 10%|█         | 3/30 [01:02<06:31, 14.49s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:03<02:31,  6.07s/it] 20%|██        | 6/30 [01:04<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:06<00:38,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.3209391196568805
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0237,  0.0012,  0.0130,  ..., -0.0055,  0.0168,  0.0039],
        [-0.0022,  0.0100,  0.0284,  ...,  0.0170,  0.0036, -0.0151],
        [-0.0294, -0.0380,  0.0248,  ...,  0.0828, -0.0048, -0.0147],
        ...,
        [ 0.0064, -0.0034,  0.0094,  ..., -0.0240, -0.0057, -0.0035],
        [-0.0330,  0.0094, -0.0037,  ...,  0.0049,  0.0167, -0.0108],
        [-0.0331, -0.0137,  0.0190,  ...,  0.0411,  0.0356, -0.0291]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8760, 0.8507, 0.8394, 0.8377, 0.8315, 0.8290, 0.8278, 0.8230, 0.8209,
         0.8184],
        [0.9421, 0.9297, 0.9283, 0.9280, 0.9264, 0.9230, 0.9226, 0.9214, 0.9191,
         0.9185],
        [0.9055, 0.8976, 0.8766, 0.8715, 0.8572, 0.8492, 0.8470, 0.8383, 0.8359,
         0.8283],
        [0.9560, 0.8871, 0.8836, 0.8797, 0.8791, 0.8760, 0.8660, 0.8643, 0.8630,
         0.8613],
        [0.8431, 0.8320, 0.8320, 0.8147, 0.8066, 0.8062, 0.8018, 0.7969, 0.7895,
         0.7812],
        [0.8531, 0.8181, 0.8088, 0.7972, 0.7930, 0.7896, 0.7837, 0.7777, 0.7701,
         0.7663],
        [0.8659, 0.8633, 0.8550, 0.8525, 0.8494, 0.8473, 0.8463, 0.8420, 0.8408,
         0.8380],
        [0.8929, 0.8731, 0.8677, 0.8568, 0.8494, 0.8315, 0.8265, 0.8183, 0.8065,
         0.8032],
        [0.9083, 0.9020, 0.8880, 0.8841, 0.8840, 0.8838, 0.8830, 0.8798, 0.8792,
         0.8792],
        [0.9489, 0.8840, 0.8829, 0.8807, 0.8740, 0.8720, 0.8681, 0.8680, 0.8615,
         0.8597],
        [0.9448, 0.9037, 0.8934, 0.8903, 0.8902, 0.8875, 0.8839, 0.8828, 0.8799,
         0.8785],
        [0.8882, 0.8729, 0.8560, 0.8483, 0.8254, 0.7989, 0.7985, 0.7962, 0.7838,
         0.7786],
        [0.9140, 0.9063, 0.9028, 0.8911, 0.8809, 0.8771, 0.8689, 0.8645, 0.8626,
         0.8518],
        [0.9572, 0.9547, 0.9517, 0.9515, 0.9487, 0.9469, 0.9440, 0.9424, 0.9401,
         0.9352],
        [0.9420, 0.9284, 0.9273, 0.9260, 0.9233, 0.9187, 0.9180, 0.9149, 0.9140,
         0.9125],
        [0.9262, 0.9218, 0.9209, 0.9136, 0.9116, 0.9105, 0.9085, 0.9053, 0.9045,
         0.9025],
        [0.9523, 0.9464, 0.9351, 0.9326, 0.9261, 0.9254, 0.9182, 0.9182, 0.9146,
         0.9140],
        [0.9619, 0.9559, 0.9515, 0.9397, 0.9394, 0.9352, 0.9341, 0.9327, 0.9320,
         0.9305],
        [0.9258, 0.9142, 0.9013, 0.8982, 0.8935, 0.8927, 0.8908, 0.8898, 0.8864,
         0.8841],
        [0.9034, 0.8982, 0.8969, 0.8969, 0.8951, 0.8946, 0.8918, 0.8896, 0.8886,
         0.8877],
        [0.9412, 0.9331, 0.9329, 0.9305, 0.9301, 0.9300, 0.9263, 0.9249, 0.9248,
         0.9235],
        [0.9257, 0.9109, 0.9107, 0.9085, 0.9056, 0.9051, 0.9048, 0.9022, 0.8894,
         0.8856],
        [0.9677, 0.9485, 0.9420, 0.9388, 0.9335, 0.9274, 0.9253, 0.9178, 0.9128,
         0.9119],
        [0.9410, 0.9340, 0.9300, 0.9264, 0.9240, 0.9230, 0.9227, 0.9225, 0.9224,
         0.9205],
        [0.9537, 0.8996, 0.8894, 0.8798, 0.8656, 0.8618, 0.8610, 0.8591, 0.8587,
         0.8571],
        [0.9360, 0.9317, 0.9300, 0.9235, 0.9219, 0.9179, 0.9140, 0.9102, 0.9036,
         0.9035],
        [0.9237, 0.9220, 0.9197, 0.9149, 0.9076, 0.8998, 0.8976, 0.8970, 0.8958,
         0.8948],
        [0.9598, 0.9481, 0.9425, 0.9261, 0.9247, 0.9239, 0.9191, 0.9178, 0.9178,
         0.9164],
        [0.9431, 0.9361, 0.9261, 0.9253, 0.9246, 0.9225, 0.9209, 0.9176, 0.9141,
         0.9126],
        [0.9313, 0.9266, 0.9252, 0.9249, 0.9216, 0.9186, 0.9174, 0.9172, 0.9137,
         0.9135],
        [0.9447, 0.9447, 0.9446, 0.9437, 0.9423, 0.9415, 0.9340, 0.9314, 0.9301,
         0.9281],
        [0.9374, 0.9209, 0.9196, 0.9098, 0.9018, 0.9018, 0.8974, 0.8969, 0.8964,
         0.8956],
        [0.8748, 0.8691, 0.8661, 0.8584, 0.8569, 0.8500, 0.8495, 0.8494, 0.8488,
         0.8487],
        [0.9013, 0.8908, 0.8857, 0.8836, 0.8810, 0.8797, 0.8793, 0.8778, 0.8764,
         0.8756],
        [0.9189, 0.9146, 0.9137, 0.9086, 0.9006, 0.9004, 0.8968, 0.8901, 0.8874,
         0.8868],
        [0.9056, 0.8957, 0.8931, 0.8849, 0.8818, 0.8761, 0.8761, 0.8701, 0.8594,
         0.8581],
        [0.9474, 0.8675, 0.8643, 0.8511, 0.8470, 0.8315, 0.8218, 0.8191, 0.8159,
         0.8159],
        [0.9004, 0.8819, 0.8802, 0.8756, 0.8743, 0.8699, 0.8563, 0.8560, 0.8548,
         0.8531],
        [0.8933, 0.8899, 0.8696, 0.8617, 0.8493, 0.8466, 0.8430, 0.8380, 0.8356,
         0.8338],
        [0.9350, 0.9313, 0.9297, 0.9293, 0.9283, 0.9266, 0.9252, 0.9245, 0.9236,
         0.9226],
        [0.8809, 0.8773, 0.8770, 0.8661, 0.8610, 0.8561, 0.8537, 0.8403, 0.8316,
         0.8246],
        [0.9001, 0.8612, 0.8610, 0.8581, 0.8525, 0.8512, 0.8489, 0.8475, 0.8463,
         0.8458],
        [0.8850, 0.8739, 0.8704, 0.8609, 0.8520, 0.8475, 0.8415, 0.8394, 0.8344,
         0.8340],
        [0.8769, 0.8721, 0.8577, 0.8522, 0.8485, 0.8443, 0.8442, 0.8423, 0.8409,
         0.8341],
        [0.8516, 0.8368, 0.8210, 0.8196, 0.8102, 0.8102, 0.7985, 0.7830, 0.7747,
         0.7729],
        [0.8298, 0.8061, 0.7909, 0.7862, 0.7802, 0.7800, 0.7764, 0.7641, 0.7595,
         0.7579],
        [0.8997, 0.8832, 0.8793, 0.8760, 0.8753, 0.8748, 0.8632, 0.8631, 0.8624,
         0.8607],
        [0.8060, 0.7846, 0.7800, 0.7735, 0.7632, 0.7605, 0.7434, 0.7417, 0.7317,
         0.7213],
        [0.8329, 0.8297, 0.8107, 0.8102, 0.8042, 0.8030, 0.8021, 0.7981, 0.7897,
         0.7859],
        [0.8915, 0.8624, 0.8449, 0.8338, 0.8321, 0.8300, 0.8218, 0.8209, 0.8163,
         0.8131],
        [0.8766, 0.8536, 0.8478, 0.8476, 0.8360, 0.8268, 0.8212, 0.8114, 0.8098,
         0.8052],
        [0.8879, 0.8850, 0.8450, 0.8162, 0.8128, 0.8078, 0.8032, 0.7986, 0.7984,
         0.7945],
        [0.9245, 0.9074, 0.9022, 0.8931, 0.8648, 0.8592, 0.8567, 0.8530, 0.8424,
         0.8383],
        [0.9119, 0.8742, 0.8701, 0.8600, 0.8533, 0.8525, 0.8431, 0.8430, 0.8393,
         0.8384],
        [0.9239, 0.9012, 0.8983, 0.8978, 0.8955, 0.8926, 0.8926, 0.8898, 0.8891,
         0.8884],
        [0.8999, 0.8799, 0.8789, 0.8678, 0.8658, 0.8651, 0.8620, 0.8614, 0.8614,
         0.8600],
        [0.9057, 0.8767, 0.8684, 0.8660, 0.8592, 0.8565, 0.8420, 0.8294, 0.8263,
         0.8242],
        [0.9399, 0.8790, 0.8708, 0.8625, 0.8557, 0.8513, 0.8356, 0.8325, 0.8314,
         0.8200],
        [0.9065, 0.8597, 0.8552, 0.8447, 0.8310, 0.8299, 0.8247, 0.8239, 0.8172,
         0.8157],
        [0.8985, 0.8176, 0.8148, 0.8027, 0.7925, 0.7920, 0.7912, 0.7880, 0.7845,
         0.7765],
        [0.8836, 0.8666, 0.8572, 0.8546, 0.8479, 0.8435, 0.8416, 0.8416, 0.8413,
         0.8409],
        [0.8539, 0.7762, 0.7760, 0.7753, 0.7644, 0.7356, 0.7301, 0.7264, 0.7263,
         0.7245],
        [0.8831, 0.8557, 0.8426, 0.8220, 0.8129, 0.8127, 0.7951, 0.7928, 0.7912,
         0.7867],
        [0.8381, 0.8077, 0.7952, 0.7801, 0.7777, 0.7724, 0.7680, 0.7531, 0.7500,
         0.7454]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 272297.4375,  189708.7031,  161445.8750,  157413.2812,  144128.0000,
          139103.7969,  136763.0312,  127592.5859,  123902.1250,  119544.8984],
        [ 699447.4375,  585869.4375,  574400.7500,  572385.6875,  558843.3125,
          532764.5625,  529956.6875,  520625.6250,  503648.0312,  499513.6875],
        [ 414598.6562,  370529.0625,  274678.1875,  255424.6406,  208118.9062,
          185543.9844,  179953.5938,  158936.0781,  153502.3438,  137675.2656],
        [ 853272.0000,  318847.1875,  303202.7812,  286818.6875,  284646.0938,
          272111.3125,  236128.7031,  230354.7656,  225937.8750,  220663.1875],
        [ 170224.2969,  145215.8750,  145154.5469,  113427.2578,  100956.6641,
          100463.6719,   94343.4141,   87955.1719,   79061.4609,   70247.9375],
        [ 196312.0781,  119107.5703,  104275.4766,   88278.4531,   83192.8359,
           79242.4688,   72795.7344,   66840.8203,   59982.2734,   56770.5820],
        [ 235697.8594,  227083.4219,  201623.2656,  194515.0000,  186099.0156,
          180702.8906,  178104.2031,  167454.2188,  164613.9531,  158249.7031],
        [ 346731.0625,  261018.6719,  241636.3750,  206814.6406,  186187.0625,
          144096.2500,  134213.6250,  119329.7344,  100885.5391,   96148.4844],
        [ 431924.2500,  394641.7500,  322915.9062,  305395.8438,  305152.1562,
          304330.8438,  300971.8125,  287383.2812,  284844.0625,  284759.8750],
        [ 771364.9375,  305185.9062,  300596.0312,  291218.2188,  264600.4375,
          257134.2500,  243265.7656,  242648.2969,  221287.3906,  215535.9219],
        [ 727019.6875,  404590.6875,  349169.3750,  333814.2188,  333597.1875,
          320641.4062,  304723.2188,  299823.0312,  287750.7500,  281949.5000],
        [ 323971.1875,  260572.7344,  204544.7969,  183179.9219,  132192.3438,
           90507.8438,   89940.6328,   87013.2344,   72955.8594,   67725.9844],
        [ 468193.8750,  419490.5938,  398951.8438,  337714.3438,  292084.0312,
          276619.2812,  246094.5781,  231072.2656,  224801.9844,  192728.7031],
        [ 868073.9375,  838409.7500,  802836.3750,  800131.3750,  768684.5000,
          749455.6875,  719564.5625,  702603.6875,  680075.0000,  633682.5625],
        [ 699112.0625,  575079.3125,  566599.4375,  556003.5625,  534997.1875,
          500739.5000,  496156.6250,  474677.8750,  468633.0312,  458611.7500],
        [ 557302.5625,  523784.5312,  517124.6250,  465877.8125,  452635.0625,
          445507.1250,  432866.9375,  413436.6562,  408782.8750,  397635.2812],
        [ 809549.9375,  744423.0625,  633193.8750,  611237.7500,  556654.0000,
          551315.8750,  497322.4688,  497289.7188,  472291.4375,  468443.5312],
        [ 928926.6250,  851803.6875,  800324.4375,  675762.7500,  673490.3750,
          634214.0000,  623854.1250,  612313.5625,  605678.1875,  592829.2500],
        [ 554402.9375,  470016.0312,  390453.0312,  373998.2812,  349271.2500,
          345713.4375,  336373.6562,  331603.3438,  315735.9375,  305450.5938],
        [ 402862.3125,  374007.1875,  367139.6250,  366954.8125,  357453.0625,
          354867.5938,  341201.8750,  330497.6875,  325979.4375,  321826.6562],
        [ 690755.0000,  615193.3125,  613308.8750,  592866.5000,  589316.8750,
          588657.4375,  558649.9375,  547537.5625,  546297.7500,  536488.5625],
        [ 553481.6250,  448048.2500,  446838.9375,  432923.0938,  415266.5938,
          412406.0938,  411013.0000,  395843.7812,  329512.3125,  312065.5625],
        [1009011.8125,  767161.1875,  698954.6875,  667969.5000,  618552.5000,
          567161.1875,  550881.1875,  494289.2500,  460260.5000,  454506.7500],
        [ 688721.7500,  623488.3750,  588622.0625,  559366.9375,  540083.0625,
          532419.6250,  530064.8750,  528622.6250,  527817.6250,  514044.6562],
        [ 826023.3125,  381189.6250,  329530.5312,  287315.5625,  234560.0781,
          222264.9688,  219645.5312,  213700.7969,  212683.6250,  207813.4688],
        [ 641063.0000,  603463.6250,  588479.5625,  536573.5000,  524517.3125,
          494984.0625,  468220.2188,  443711.8438,  403676.8750,  403230.9375],
        [ 537978.5000,  524832.5625,  508393.1875,  474793.3125,  427394.0625,
          382280.6875,  370634.7188,  367273.4062,  361272.2500,  356022.8438],
        [ 901331.1250,  762827.6875,  703839.6875,  556640.1875,  545939.9375,
          539776.6250,  503976.6562,  494868.4375,  494471.2188,  485102.4688],
        [ 709448.5625,  642682.1250,  556516.5625,  550809.2500,  544974.9375,
          528886.3125,  516609.0625,  492949.4375,  468857.4375,  459345.7812],
        [ 600171.5625,  560839.0625,  549888.6250,  547628.9375,  522152.1250,
          500146.2500,  491696.3125,  490378.5625,  466379.2500,  464769.7188],
        [ 726491.6250,  726364.8125,  725169.4375,  715574.2500,  701606.6250,
          694003.6875,  623006.2500,  600258.5000,  589889.2500,  572818.7500],
        [ 654333.0625,  516755.8750,  507573.5000,  441335.4375,  393497.7500,
          393259.5000,  369279.9062,  366891.8438,  364226.5938,  360021.0000],
        [ 267511.4688,  246545.3906,  236459.9531,  211741.5312,  207178.2812,
          187876.5625,  186354.5938,  186216.9062,  184583.7031,  184240.5938],
        [ 390801.7188,  336320.4062,  312486.3438,  303335.5312,  292414.8750,
          286944.8125,  285416.4375,  279101.9688,  273873.3438,  270466.1250],
        [ 502573.2812,  472301.3438,  466329.8750,  433485.3750,  387033.7500,
          385887.2188,  366329.9688,  332945.0000,  320272.2188,  317433.3438],
        [ 415213.1562,  360714.1875,  347574.2812,  308915.0000,  295815.5000,
          272773.3438,  272432.7812,  250189.4375,  214630.9219,  210928.3125],
        [ 754489.2500,  240940.9688,  230347.0781,  190834.8125,  179842.7656,
          144127.1719,  125428.6562,  120693.6406,  115428.8125,  115318.2344],
        [ 385877.2500,  296172.8750,  289150.1250,  270525.4688,  265824.8438,
          249392.1094,  205283.0938,  204477.3281,  201127.4062,  196113.1719],
        [ 348501.0312,  331783.0000,  248455.1094,  222002.7031,  185846.8125,
          178751.5156,  169764.3750,  158066.9062,  152776.9375,  149036.7500],
        [ 632000.5625,  599498.8125,  585799.6250,  583137.5000,  574704.8750,
          560502.7500,  549786.4375,  544543.2500,  537474.8750,  529495.9375],
        [ 291895.5312,  277309.4688,  276200.1562,  236348.8281,  219742.1250,
          204759.2969,  197976.7344,  163345.2031,  144424.3750,  130612.3750],
        [ 384163.2500,  220231.3750,  219838.7656,  210872.3906,  194581.7969,
          190871.0312,  184754.7031,  181049.7812,  178111.8594,  176858.9844],
        [ 309521.5938,  264119.4375,  251315.2656,  219517.3906,  193154.6719,
          181147.1875,  166173.5938,  161263.2188,  150198.2188,  149472.5938],
        [ 275665.1875,  257304.5000,  209529.4844,  193870.7031,  183662.1875,
          173048.4688,  172772.2656,  168216.5938,  164929.4688,  149506.9531],
        [ 192207.0469,  155582.7656,  124122.9375,  121692.2344,  106331.7188,
          106267.5547,   89944.8438,   72050.6328,   64016.3359,   62431.0859],
        [ 140695.9844,  100311.8281,   80758.9375,   75460.1172,   69288.3047,
           69078.2969,   65650.5391,   55046.3984,   51514.2734,   50386.4922],
        [ 381903.5625,  301478.5625,  285446.6562,  272331.7500,  269555.5938,
          267514.5312,  226704.9688,  226457.7500,  224214.8906,  218627.1562],
        [ 100110.1875,   73805.5000,   69024.2266,   62977.4727,   54324.7266,
           52260.1250,   40928.3516,   39979.4883,   34652.7578,   29851.0723],
        [ 147113.1562,  140497.8125,  107014.0234,  106274.2344,   97525.4219,
           95964.4453,   94670.1250,   89414.8438,   79363.2500,   75194.5312],
        [ 339524.3750,  224249.9688,  174466.9219,  148925.2188,  145447.0781,
          141131.1406,  125561.8672,  123946.3281,  116003.8750,  110859.4297],
        [ 274421.3438,  197716.1562,  182022.9219,  181359.9844,  153743.4844,
          134773.3594,  124397.1562,  108229.2266,  105756.7891,   98984.3516],
        [ 322816.7812,  309400.5938,  174839.3594,  115919.9375,  110357.8516,
          102674.1016,   96238.8516,   90078.4141,   89816.0078,   84930.4062],
        [ 544075.0000,  425995.8750,  395523.7500,  347483.4688,  231790.4531,
          214266.6875,  206617.9062,  195948.8438,  168500.7812,  158932.1250],
        [ 454543.1250,  265445.0938,  250190.8594,  216681.0000,  196919.4219,
          194705.7969,  170026.6875,  169916.4688,  161183.4219,  159086.6562],
        [ 539614.5000,  390379.3125,  374328.3438,  371456.0312,  359863.0938,
          344881.3125,  344881.3125,  331390.5625,  328293.7188,  325097.1250],
        [ 382867.0312,  287983.5625,  283640.7500,  242271.8438,  235355.5469,
          233057.4531,  222967.9062,  221073.9219,  220848.8750,  216621.0781],
        [ 416066.1562,  274858.2188,  244188.7812,  236122.8438,  214083.2812,
          206022.3125,  167481.8594,  139874.6719,  133834.4062,  129858.1016],
        [ 677996.9375,  284004.8125,  252774.7969,  224545.4844,  203597.2344,
          191181.6562,  152787.5625,  146290.9219,  143984.5781,  122235.6484],
        [ 420901.5938,  215514.9688,  202293.3125,  174128.9844,  143219.1562,
          140978.3281,  130788.5000,  129344.6719,  117564.1953,  115035.2891],
        [ 375500.3750,  118146.1797,  113569.1641,   95531.6250,   82513.7891,
           81983.8594,   81076.6406,   77443.9609,   73664.0156,   65726.2812],
        [ 303322.2500,  238125.7656,  207966.7344,  200610.7500,  182181.6562,
          171101.5625,  166524.8281,  166524.0156,  165866.4219,  164770.6875],
        [ 198600.9375,   65408.1289,   65249.1328,   64550.5508,   55275.4414,
           36619.1836,   33843.7344,   32121.7246,   32077.2754,   31239.8887],
        [ 301109.9062,  203570.6094,  168972.2812,  125893.8750,  110568.2344,
          110154.0781,   85656.5391,   82923.3672,   81070.4531,   76026.3594],
        [ 158502.5625,  102650.4141,   85777.2812,   69176.0625,   66844.3281,
           61926.9922,   58207.9922,   47049.3008,   44993.7617,   42159.7969]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[272297.4375,      0.0000],
         [189708.7031,      0.0000],
         [161445.8750,      0.0000],
         ...,
         [127592.5859,      0.0000],
         [123902.1250,      0.0000],
         [     0.0000, 119544.8984]],

        [[699447.4375,      0.0000],
         [585869.4375,      0.0000],
         [574400.7500,      0.0000],
         ...,
         [520625.6250,      0.0000],
         [503648.0312,      0.0000],
         [499513.6875,      0.0000]],

        [[414598.6562,      0.0000],
         [370529.0625,      0.0000],
         [274678.1875,      0.0000],
         ...,
         [158936.0781,      0.0000],
         [153502.3438,      0.0000],
         [137675.2656,      0.0000]],

        ...,

        [[     0.0000, 198600.9375],
         [ 65408.1289,      0.0000],
         [ 65249.1328,      0.0000],
         ...,
         [ 32121.7246,      0.0000],
         [ 32077.2754,      0.0000],
         [ 31239.8887,      0.0000]],

        [[301109.9062,      0.0000],
         [     0.0000, 203570.6094],
         [168972.2812,      0.0000],
         ...,
         [ 82923.3672,      0.0000],
         [ 81070.4531,      0.0000],
         [ 76026.3594,      0.0000]],

        [[     0.0000, 158502.5625],
         [     0.0000, 102650.4141],
         [     0.0000,  85777.2812],
         ...,
         [ 47049.3008,      0.0000],
         [     0.0000,  44993.7617],
         [     0.0000,  42159.7969]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1452354.8750,  119544.8984],
        [5577455.0000,       0.0000],
        [2338960.7500,       0.0000],
        [3231982.7500,       0.0000],
        [ 457543.7188,  649506.5625],
        [ 780715.0000,  146083.2812],
        [1729529.6250,  164613.9531],
        [1837061.5000,       0.0000],
        [3222320.0000,       0.0000],
        [3112837.0000,       0.0000],
        [3643079.2500,       0.0000],
        [1512604.5000,       0.0000],
        [3087751.5000,       0.0000],
        [7563517.5000,       0.0000],
        [5330610.5000,       0.0000],
        [4614953.5000,       0.0000],
        [5841722.0000,       0.0000],
        [6999197.0000,       0.0000],
        [3773018.5000,       0.0000],
        [3212292.2500,  330497.6875],
        [5879072.0000,       0.0000],
        [4157399.5000,       0.0000],
        [6288749.0000,       0.0000],
        [5633251.5000,       0.0000],
        [3134727.5000,       0.0000],
        [5107921.0000,       0.0000],
        [4310876.0000,       0.0000],
        [5988774.0000,       0.0000],
        [5471079.5000,       0.0000],
        [5194050.5000,       0.0000],
        [6675183.0000,       0.0000],
        [4367174.0000,       0.0000],
        [2098709.0000,       0.0000],
        [2426260.5000,  604901.2500],
        [1803310.5000, 2181281.0000],
        [1036462.8750, 1912724.0000],
        [1106703.0000, 1110748.5000],
        [1345543.2500, 1218400.5000],
        [ 718693.3750, 1426291.7500],
        [5696945.0000,       0.0000],
        [2142614.0000,       0.0000],
        [2141334.0000,       0.0000],
        [2045883.2500,       0.0000],
        [1948505.7500,       0.0000],
        [ 240813.4375,  853833.7500],
        [ 330991.5312,  427199.6562],
        [2292332.0000,  381903.5625],
        [ 352453.7500,  205460.1562],
        [ 457260.8750,  575770.9375],
        [ 450383.8125, 1199732.5000],
        [1320874.6250,  240530.1562],
        [ 690015.5625,  807056.7500],
        [1705440.0000, 1183694.8750],
        [2043992.7500,  194705.7969],
        [3710185.0000,       0.0000],
        [2546688.0000,       0.0000],
        [ 990712.4375, 1171678.2500],
        [1687252.0000,  712147.5625],
        [ 965515.1250,  824253.9375],
        [ 545883.8125,  619272.0625],
        [1243365.0000,  723629.6875],
        [ 416385.0625,  198600.9375],
        [ 921652.8125,  424292.9375],
        [ 108976.2969,  628312.1875]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 421/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:19, 58.60s/it]  7%|▋         | 2/30 [00:59<11:27, 24.57s/it] 10%|█         | 3/30 [01:00<06:09, 13.69s/it] 13%|█▎        | 4/30 [01:00<03:43,  8.58s/it] 17%|█▋        | 5/30 [01:01<02:23,  5.76s/it] 20%|██        | 6/30 [01:02<01:37,  4.05s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.97s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.26s/it] 30%|███       | 9/30 [01:04<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 2.3564589103062947
Epoch 422/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:09, 62.41s/it]  7%|▋         | 2/30 [01:03<12:11, 26.14s/it] 10%|█         | 3/30 [01:03<06:32, 14.55s/it] 13%|█▎        | 4/30 [01:05<04:04,  9.38s/it] 17%|█▋        | 5/30 [01:06<02:36,  6.27s/it] 20%|██        | 6/30 [01:06<01:45,  4.39s/it] 23%|██▎       | 7/30 [01:07<01:13,  3.20s/it] 27%|██▋       | 8/30 [01:08<00:53,  2.42s/it] 30%|███       | 9/30 [01:09<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.30s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:14<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:17<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:20<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:23<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.83s/it]
Epoch loss is 2.3283644755681356
Epoch 423/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:41, 59.37s/it]  7%|▋         | 2/30 [01:00<11:36, 24.89s/it] 10%|█         | 3/30 [01:00<06:14, 13.87s/it] 13%|█▎        | 4/30 [01:02<03:57,  9.13s/it] 17%|█▋        | 5/30 [01:03<02:32,  6.11s/it] 20%|██        | 6/30 [01:04<01:42,  4.29s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:05<00:52,  2.37s/it] 30%|███       | 9/30 [01:06<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.327095135052999
Epoch 424/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:39, 61.35s/it]  7%|▋         | 2/30 [01:02<11:59, 25.70s/it] 10%|█         | 3/30 [01:02<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.99s/it] 20%|██        | 6/30 [01:05<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.3346302270889283
Epoch 425/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:53, 61.86s/it]  7%|▋         | 2/30 [01:02<12:05, 25.91s/it] 10%|█         | 3/30 [01:03<06:29, 14.42s/it] 13%|█▎        | 4/30 [01:04<03:54,  9.02s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.04s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.3247231324513753
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0237,  0.0012,  0.0131,  ..., -0.0055,  0.0170,  0.0039],
        [-0.0022,  0.0099,  0.0284,  ...,  0.0170,  0.0037, -0.0150],
        [-0.0294, -0.0380,  0.0248,  ...,  0.0829, -0.0047, -0.0146],
        ...,
        [ 0.0063, -0.0034,  0.0095,  ..., -0.0241, -0.0056, -0.0034],
        [-0.0331,  0.0095, -0.0036,  ...,  0.0049,  0.0168, -0.0107],
        [-0.0330, -0.0137,  0.0191,  ...,  0.0411,  0.0357, -0.0290]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8757, 0.8507, 0.8390, 0.8374, 0.8313, 0.8288, 0.8273, 0.8226, 0.8207,
         0.8181],
        [0.9419, 0.9295, 0.9282, 0.9278, 0.9263, 0.9227, 0.9224, 0.9212, 0.9189,
         0.9183],
        [0.9053, 0.8974, 0.8762, 0.8713, 0.8569, 0.8489, 0.8467, 0.8380, 0.8356,
         0.8281],
        [0.9560, 0.8868, 0.8832, 0.8795, 0.8790, 0.8757, 0.8658, 0.8641, 0.8625,
         0.8611],
        [0.8430, 0.8319, 0.8316, 0.8145, 0.8063, 0.8059, 0.8017, 0.7967, 0.7892,
         0.7812],
        [0.8530, 0.8180, 0.8088, 0.7970, 0.7930, 0.7892, 0.7834, 0.7777, 0.7697,
         0.7661],
        [0.8655, 0.8629, 0.8546, 0.8522, 0.8490, 0.8471, 0.8460, 0.8417, 0.8403,
         0.8376],
        [0.8926, 0.8728, 0.8674, 0.8565, 0.8492, 0.8311, 0.8263, 0.8179, 0.8062,
         0.8027],
        [0.9081, 0.9019, 0.8877, 0.8839, 0.8839, 0.8836, 0.8829, 0.8796, 0.8790,
         0.8789],
        [0.9489, 0.8838, 0.8827, 0.8805, 0.8737, 0.8718, 0.8679, 0.8677, 0.8613,
         0.8594],
        [0.9447, 0.9037, 0.8932, 0.8899, 0.8899, 0.8871, 0.8838, 0.8825, 0.8797,
         0.8781],
        [0.8878, 0.8726, 0.8557, 0.8479, 0.8250, 0.7985, 0.7983, 0.7957, 0.7833,
         0.7782],
        [0.9136, 0.9061, 0.9025, 0.8908, 0.8805, 0.8768, 0.8686, 0.8642, 0.8623,
         0.8515],
        [0.9570, 0.9547, 0.9515, 0.9513, 0.9485, 0.9467, 0.9439, 0.9422, 0.9399,
         0.9349],
        [0.9420, 0.9282, 0.9272, 0.9259, 0.9231, 0.9185, 0.9179, 0.9148, 0.9139,
         0.9124],
        [0.9261, 0.9217, 0.9208, 0.9135, 0.9115, 0.9104, 0.9083, 0.9051, 0.9043,
         0.9024],
        [0.9521, 0.9462, 0.9349, 0.9323, 0.9259, 0.9252, 0.9180, 0.9179, 0.9144,
         0.9138],
        [0.9618, 0.9557, 0.9514, 0.9396, 0.9393, 0.9351, 0.9339, 0.9326, 0.9319,
         0.9304],
        [0.9256, 0.9139, 0.9009, 0.8981, 0.8933, 0.8925, 0.8906, 0.8896, 0.8862,
         0.8840],
        [0.9032, 0.8981, 0.8968, 0.8967, 0.8949, 0.8943, 0.8916, 0.8895, 0.8884,
         0.8875],
        [0.9409, 0.9328, 0.9326, 0.9303, 0.9298, 0.9298, 0.9262, 0.9246, 0.9246,
         0.9233],
        [0.9256, 0.9108, 0.9105, 0.9082, 0.9055, 0.9048, 0.9046, 0.9020, 0.8891,
         0.8852],
        [0.9676, 0.9485, 0.9419, 0.9387, 0.9334, 0.9272, 0.9252, 0.9177, 0.9127,
         0.9117],
        [0.9409, 0.9339, 0.9299, 0.9263, 0.9239, 0.9229, 0.9225, 0.9223, 0.9222,
         0.9204],
        [0.9536, 0.8993, 0.8890, 0.8796, 0.8653, 0.8614, 0.8607, 0.8587, 0.8584,
         0.8570],
        [0.9358, 0.9316, 0.9297, 0.9233, 0.9218, 0.9177, 0.9137, 0.9100, 0.9035,
         0.9033],
        [0.9235, 0.9217, 0.9195, 0.9148, 0.9074, 0.8996, 0.8974, 0.8968, 0.8956,
         0.8946],
        [0.9597, 0.9481, 0.9424, 0.9259, 0.9246, 0.9238, 0.9190, 0.9177, 0.9177,
         0.9163],
        [0.9429, 0.9360, 0.9260, 0.9251, 0.9244, 0.9223, 0.9207, 0.9174, 0.9138,
         0.9124],
        [0.9312, 0.9265, 0.9251, 0.9248, 0.9215, 0.9184, 0.9173, 0.9171, 0.9136,
         0.9134],
        [0.9446, 0.9445, 0.9445, 0.9436, 0.9421, 0.9414, 0.9338, 0.9312, 0.9300,
         0.9280],
        [0.9372, 0.9208, 0.9195, 0.9096, 0.9017, 0.9016, 0.8972, 0.8967, 0.8962,
         0.8954],
        [0.8745, 0.8689, 0.8659, 0.8582, 0.8568, 0.8500, 0.8492, 0.8492, 0.8485,
         0.8484],
        [0.9009, 0.8905, 0.8854, 0.8833, 0.8807, 0.8794, 0.8791, 0.8776, 0.8762,
         0.8752],
        [0.9187, 0.9144, 0.9135, 0.9083, 0.9004, 0.9004, 0.8966, 0.8899, 0.8871,
         0.8865],
        [0.9054, 0.8954, 0.8929, 0.8847, 0.8815, 0.8760, 0.8759, 0.8698, 0.8591,
         0.8579],
        [0.9473, 0.8674, 0.8641, 0.8509, 0.8468, 0.8310, 0.8214, 0.8188, 0.8158,
         0.8153],
        [0.9001, 0.8815, 0.8798, 0.8755, 0.8739, 0.8697, 0.8558, 0.8555, 0.8546,
         0.8527],
        [0.8932, 0.8896, 0.8692, 0.8615, 0.8491, 0.8461, 0.8426, 0.8375, 0.8354,
         0.8336],
        [0.9347, 0.9311, 0.9294, 0.9292, 0.9281, 0.9262, 0.9250, 0.9243, 0.9236,
         0.9224],
        [0.8806, 0.8770, 0.8768, 0.8658, 0.8608, 0.8558, 0.8534, 0.8400, 0.8313,
         0.8243],
        [0.9000, 0.8610, 0.8608, 0.8579, 0.8521, 0.8508, 0.8486, 0.8470, 0.8460,
         0.8456],
        [0.8847, 0.8736, 0.8701, 0.8607, 0.8517, 0.8472, 0.8411, 0.8390, 0.8342,
         0.8339],
        [0.8764, 0.8718, 0.8573, 0.8518, 0.8482, 0.8441, 0.8437, 0.8420, 0.8407,
         0.8340],
        [0.8513, 0.8367, 0.8207, 0.8192, 0.8098, 0.8098, 0.7981, 0.7824, 0.7744,
         0.7724],
        [0.8296, 0.8058, 0.7906, 0.7860, 0.7801, 0.7799, 0.7763, 0.7640, 0.7594,
         0.7579],
        [0.8994, 0.8827, 0.8789, 0.8756, 0.8748, 0.8743, 0.8628, 0.8626, 0.8620,
         0.8601],
        [0.8056, 0.7844, 0.7796, 0.7732, 0.7631, 0.7601, 0.7427, 0.7411, 0.7314,
         0.7208],
        [0.8325, 0.8294, 0.8105, 0.8099, 0.8038, 0.8029, 0.8020, 0.7977, 0.7895,
         0.7856],
        [0.8912, 0.8621, 0.8445, 0.8333, 0.8318, 0.8297, 0.8215, 0.8206, 0.8160,
         0.8129],
        [0.8761, 0.8534, 0.8476, 0.8473, 0.8358, 0.8266, 0.8210, 0.8111, 0.8097,
         0.8049],
        [0.8877, 0.8848, 0.8447, 0.8158, 0.8123, 0.8071, 0.8029, 0.7982, 0.7981,
         0.7940],
        [0.9242, 0.9072, 0.9018, 0.8929, 0.8644, 0.8591, 0.8564, 0.8525, 0.8422,
         0.8380],
        [0.9117, 0.8741, 0.8699, 0.8598, 0.8531, 0.8522, 0.8429, 0.8426, 0.8390,
         0.8381],
        [0.9236, 0.9010, 0.8980, 0.8975, 0.8954, 0.8923, 0.8923, 0.8895, 0.8889,
         0.8881],
        [0.8996, 0.8797, 0.8786, 0.8676, 0.8655, 0.8649, 0.8616, 0.8612, 0.8611,
         0.8596],
        [0.9053, 0.8765, 0.8680, 0.8657, 0.8590, 0.8562, 0.8419, 0.8291, 0.8258,
         0.8239],
        [0.9398, 0.8788, 0.8705, 0.8622, 0.8554, 0.8511, 0.8353, 0.8321, 0.8310,
         0.8195],
        [0.9062, 0.8595, 0.8550, 0.8444, 0.8309, 0.8296, 0.8242, 0.8236, 0.8168,
         0.8152],
        [0.8984, 0.8172, 0.8144, 0.8023, 0.7921, 0.7915, 0.7908, 0.7876, 0.7841,
         0.7761],
        [0.8835, 0.8663, 0.8569, 0.8544, 0.8475, 0.8433, 0.8413, 0.8411, 0.8410,
         0.8404],
        [0.8538, 0.7760, 0.7757, 0.7749, 0.7641, 0.7356, 0.7297, 0.7264, 0.7261,
         0.7241],
        [0.8829, 0.8555, 0.8423, 0.8219, 0.8127, 0.8122, 0.7944, 0.7923, 0.7907,
         0.7862],
        [0.8380, 0.8074, 0.7948, 0.7797, 0.7775, 0.7719, 0.7678, 0.7526, 0.7496,
         0.7450]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 270876.0312,  189719.0000,  160544.6250,  156855.9688,  143661.4219,
          138670.5312,  135844.9062,  126924.4766,  123466.6406,  118989.2656],
        [ 698183.2500,  584181.2500,  573477.9375,  570610.0625,  558227.5625,
          530508.3750,  528135.8750,  519071.3750,  502183.7812,  497894.7812],
        [ 413782.5625,  369496.9062,  273117.7500,  254598.7188,  207232.6250,
          184889.1875,  179221.2656,  158266.1562,  152947.2031,  137372.8281],
        [ 853465.6875,  317589.2812,  301754.4062,  286185.3438,  284077.4375,
          270897.9688,  235240.8750,  229502.6719,  224582.5469,  219939.2031],
        [ 169908.6875,  144914.2969,  144425.6094,  113132.1094,  100557.3203,
          100003.6875,   94165.3438,   87705.9766,   78727.9219,   70228.6484],
        [ 195878.5781,  118845.4688,  104153.1328,   88022.1328,   83216.4844,
           78761.1953,   72491.6016,   66806.9219,   59580.2852,   56629.1836],
        [ 234419.2031,  225751.9844,  200564.4531,  193857.7656,  185184.4062,
          180103.3125,  177253.5938,  166650.6406,  163457.2500,  157251.9844],
        [ 345255.8125,  259934.3594,  240815.7812,  206051.9688,  185591.2344,
          143299.4844,  133798.2812,  118720.8594,  100396.3359,   95582.3047],
        [ 430534.6562,  393903.2500,  321754.5625,  304769.6875,  304747.0312,
          303199.5938,  300284.5625,  286735.2812,  283938.1875,  283819.9062],
        [ 770823.6250,  304089.4688,  299488.3438,  290343.5625,  263568.6250,
          256335.8281,  242294.2656,  241665.1719,  220627.6406,  214829.3594],
        [ 726248.4375,  404301.4062,  348024.7188,  332094.5000,  332005.5312,
          319187.6250,  304162.8438,  298687.4375,  286992.7188,  280546.1875],
        [ 322259.1250,  259233.5312,  203799.2656,  182223.3594,  131433.1875,
           89905.8203,   89682.7500,   86408.5000,   72435.4141,   67341.2969],
        [ 465609.5312,  418373.1250,  397229.3750,  336047.5938,  290261.9062,
          275251.6875,  245058.2500,  229948.5000,  223934.0938,  191820.1250],
        [ 865745.8125,  837582.5625,  800507.6875,  798011.3125,  766847.3750,
          747111.5000,  717765.5000,  700411.3125,  678389.5000,  631791.4375],
        [ 698333.0625,  574158.1250,  565520.8125,  554872.6250,  533449.8750,
          499621.8438,  495159.7188,  473766.5938,  468051.9062,  458103.8438],
        [ 556801.6250,  523012.3125,  516261.8438,  465256.6562,  452104.4375,
          444728.5938,  432085.3750,  412422.6250,  407972.8125,  397155.1250],
        [ 807350.2500,  742537.6875,  631627.0000,  608817.5625,  555019.1875,
          549700.4375,  495975.4375,  495385.5000,  470918.7500,  467086.0625],
        [ 927147.6250,  850374.3125,  799458.6250,  675512.1250,  672501.3125,
          632788.8125,  622779.3125,  610737.8125,  605025.8125,  591899.3750],
        [ 553148.6250,  468049.2500,  388309.6250,  373139.0000,  348368.0938,
          344539.0938,  335500.6562,  330446.6250,  315000.5938,  305231.0312],
        [ 401566.5938,  373276.7188,  366397.0312,  366025.4375,  356510.0312,
          353663.8125,  340005.8750,  330014.2500,  325088.7500,  320685.4375],
        [ 687872.3750,  612806.0625,  611226.0625,  591598.0000,  587267.4375,
          586928.6250,  557260.0625,  545158.4375,  544865.3125,  534824.2500],
        [ 552721.5000,  447554.1250,  445705.5938,  431440.1250,  414673.3750,
          411002.8125,  409313.0312,  394877.7812,  328318.7812,  310576.9062],
        [1007783.6875,  766478.8750,  698108.0000,  666559.9375,  617782.0000,
          565833.2500,  549829.9375,  493580.3125,  459666.1250,  453475.8750],
        [ 687670.3125,  622035.6250,  587539.6875,  558766.6250,  539683.4375,
          532134.3750,  529264.7500,  527539.3125,  526482.8750,  513391.1250],
        [ 824203.1875,  379963.2500,  327925.4375,  286412.7812,  233737.2344,
          220808.6719,  218713.2812,  212571.0781,  211631.5156,  207409.5781],
        [ 639624.8750,  602295.3125,  586611.9375,  535069.6250,  523401.0312,
          493635.3750,  466753.0000,  442516.3438,  402960.6875,  402036.3750],
        [ 536477.3125,  523136.0312,  506603.9062,  473606.6562,  426107.1875,
          381240.1875,  369583.9375,  366255.5625,  360393.7188,  355124.9062],
        [ 900194.5625,  762115.7500,  702544.0625,  555089.1250,  544742.6875,
          538948.0000,  503189.5312,  494149.2500,  494023.9062,  484308.7500],
        [ 707630.2500,  641154.1250,  555979.1250,  549232.5000,  543371.3750,
          527536.3125,  515189.6250,  491960.4062,  467122.6250,  457921.2188],
        [ 598626.3750,  559697.2500,  549196.3750,  546639.6250,  521195.9375,
          498670.3125,  490866.5625,  489597.7188,  465454.1562,  464340.4062],
        [ 725377.0000,  724646.8125,  724397.3750,  714518.6875,  700137.5625,
          692696.3750,  621641.2500,  599165.5625,  588978.6250,  571841.2500],
        [ 652676.4375,  515920.2500,  506670.5938,  439913.8438,  392733.6875,
          392455.5000,  368317.9688,  366054.7812,  363260.1875,  359301.7500],
        [ 266580.3750,  245954.7500,  235686.4062,  211149.6875,  206794.9219,
          187628.7500,  185727.0312,  185610.5312,  183755.0469,  183404.7188],
        [ 388738.6875,  334816.9375,  311384.4688,  302162.1562,  291204.8750,
          285584.7188,  284509.3125,  278399.0938,  272823.5625,  269237.2812],
        [ 500986.4375,  471017.0938,  465117.7812,  431766.5312,  385767.9688,
          385736.7188,  365319.9688,  331914.3438,  319148.9688,  316471.5312],
        [ 414393.8750,  359376.4375,  346362.5625,  308276.9688,  294573.1562,
          272035.5625,  271762.0000,  249171.4844,  213917.7500,  210315.2656],
        [ 754200.0625,  240770.5312,  229775.3281,  190312.2969,  179328.7969,
          143211.7969,  124757.0156,  120228.8281,  115227.4297,  114433.0469],
        [ 384095.4688,  294503.5000,  287274.4688,  270132.8125,  264151.4062,
          248704.5000,  203977.7500,  203113.9375,  200359.7031,  195222.5156],
        [ 347803.0938,  330734.5000,  247156.7500,  221215.0312,  185231.9375,
          177507.8281,  168951.8125,  157089.2031,  152347.7188,  148564.0625],
        [ 629806.8750,  597864.7500,  584058.1250,  581775.5000,  572964.6250,
          557936.5000,  548278.4375,  542973.5625,  537157.6875,  527815.5625],
        [ 290852.9375,  276224.3750,  275300.2500,  235320.7656,  219036.8438,
          203969.5781,  197143.7656,  162666.8281,  143795.1875,  129987.6953],
        [ 383306.1562,  219777.3438,  218998.4062,  210057.2969,  193591.9219,
          190029.9219,  183934.7500,  179882.2188,  177384.8125,  176299.0469],
        [ 308427.5312,  262927.6875,  250134.7969,  218773.1719,  192344.7656,
          180373.6875,  165332.0000,  160463.5000,  149748.3906,  149192.7500],
        [ 273684.8125,  256525.8438,  208488.4062,  192552.5156,  182948.4219,
          172531.0469,  171596.3750,  167495.9062,  164432.2500,  149290.6719],
        [ 191277.5625,  155295.0469,  123512.2266,  120832.8750,  105791.6875,
          105701.1250,   89500.9219,   71497.3672,   63784.4609,   61972.5469],
        [ 140311.5625,   99838.9297,   80314.9219,   75229.1016,   69122.7109,
           68969.4141,   65518.6914,   54955.1836,   51454.9102,   50353.1094],
        [ 380471.6250,  299579.2188,  283854.8125,  270794.6875,  267741.6875,
          265535.2500,  225337.0781,  224817.4219,  222743.9062,  216975.2344],
        [  99613.9062,   73576.1797,   68692.1484,   62661.5117,   54259.4922,
           52014.1992,   40544.0234,   39629.2266,   34489.0000,   29657.1523],
        [ 146165.6875,  139824.4062,  106737.8047,  105871.4219,   97017.8984,
           95790.3516,   94572.7578,   88977.9453,   79088.0781,   74804.0703],
        [ 338141.3750,  223314.3438,  173583.1719,  147864.2500,  144765.5312,
          140517.2500,  124956.2188,  123394.8438,  115522.4141,  110500.8750],
        [ 272754.6250,  197225.2031,  181429.0000,  180540.6406,  153222.5469,
          134446.6562,  124103.2891,  107646.1875,  105531.6094,   98634.1875],
        [ 321861.6562,  308503.4375,  174002.3281,  115177.4453,  109593.3828,
          101698.1250,   95806.1641,   89609.7344,   89420.9844,   84327.6016],
        [ 542076.8750,  424778.0000,  393308.6562,  346279.6562,  230670.8906,
          213675.3281,  205844.9688,  194626.1719,  167978.2031,  158233.8594],
        [ 453368.6250,  264974.4375,  249547.4688,  216060.7188,  196315.0625,
          193691.8281,  169682.7969,  168812.0156,  160387.0000,  158348.5938],
        [ 537626.0625,  388822.8438,  372727.0938,  369975.7500,  358939.0312,
          343653.0625,  343653.0625,  330260.1250,  327360.5000,  323603.4062],
        [ 381286.0000,  286934.6875,  282678.6250,  241354.6875,  234409.8281,
          232169.6562,  221739.9062,  220446.5625,  220079.5625,  215375.6406],
        [ 413645.2812,  274112.1562,  242851.7812,  235032.5625,  213630.9062,
          205088.7969,  167329.5469,  139204.5312,  132813.2812,  129259.4688],
        [ 677031.6250,  283272.0312,  251784.5312,  223606.7188,  202885.2812,
          190735.6562,  152286.2812,  145488.9688,  143107.7500,  121482.9375],
        [ 419068.3438,  214932.0469,  201580.3750,  173352.7344,  142871.5625,
          140324.9375,  129943.6953,  128803.3125,  116872.0156,  114245.3828],
        [ 374614.3750,  117583.3750,  112971.7969,   94953.4062,   82063.1719,
           81411.3516,   80636.5703,   76988.4297,   73285.0000,   65307.0898],
        [ 303121.5312,  237125.0156,  207079.5000,  199804.4375,  181229.0938,
          170704.8594,  165706.5781,  165237.0938,  164988.7812,  163749.9531],
        [ 198133.3125,   65266.0586,   64922.2109,   64213.8359,   55011.2891,
           36624.2461,   33689.0039,   32111.5234,   31958.4648,   31092.8516],
        [ 300319.2500,  203207.5156,  168083.4844,  125633.8516,  110172.1484,
          109484.8438,   84891.0469,   82350.6641,   80501.7188,   75475.2969],
        [ 158090.2812,  102111.6562,   85352.4688,   68803.7969,   66611.7344,
           61502.8203,   58032.6172,   46726.9844,   44740.8398,   41901.6602]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[270876.0312,      0.0000],
         [189719.0000,      0.0000],
         [160544.6250,      0.0000],
         ...,
         [126924.4766,      0.0000],
         [123466.6406,      0.0000],
         [     0.0000, 118989.2656]],

        [[698183.2500,      0.0000],
         [584181.2500,      0.0000],
         [573477.9375,      0.0000],
         ...,
         [519071.3750,      0.0000],
         [502183.7812,      0.0000],
         [497894.7812,      0.0000]],

        [[413782.5625,      0.0000],
         [369496.9062,      0.0000],
         [273117.7500,      0.0000],
         ...,
         [158266.1562,      0.0000],
         [152947.2031,      0.0000],
         [137372.8281,      0.0000]],

        ...,

        [[     0.0000, 198133.3125],
         [ 65266.0586,      0.0000],
         [ 64922.2109,      0.0000],
         ...,
         [ 32111.5234,      0.0000],
         [ 31958.4648,      0.0000],
         [ 31092.8516,      0.0000]],

        [[300319.2500,      0.0000],
         [     0.0000, 203207.5156],
         [168083.4844,      0.0000],
         ...,
         [ 82350.6641,      0.0000],
         [ 80501.7188,      0.0000],
         [ 75475.2969,      0.0000]],

        [[     0.0000, 158090.2812],
         [     0.0000, 102111.6562],
         [     0.0000,  85352.4688],
         ...,
         [ 46726.9844,      0.0000],
         [     0.0000,  44740.8398],
         [     0.0000,  41901.6602]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1446563.5000,  118989.2656],
        [5562474.5000,       0.0000],
        [2330925.0000,       0.0000],
        [3223235.2500,       0.0000],
        [ 456257.7500,  647511.9375],
        [ 778816.8750,  145568.1250],
        [1721037.3750,  163457.2500],
        [1829446.2500,       0.0000],
        [3213686.7500,       0.0000],
        [3104065.7500,       0.0000],
        [3632251.5000,       0.0000],
        [1504722.2500,       0.0000],
        [3073534.0000,       0.0000],
        [7544163.5000,       0.0000],
        [5321038.0000,       0.0000],
        [4607801.5000,       0.0000],
        [5824418.0000,       0.0000],
        [6988225.0000,       0.0000],
        [3761732.5000,       0.0000],
        [3203219.5000,  330014.2500],
        [5859806.5000,       0.0000],
        [4146184.0000,       0.0000],
        [6279098.0000,       0.0000],
        [5624508.0000,       0.0000],
        [3123376.0000,       0.0000],
        [5094904.5000,       0.0000],
        [4298530.0000,       0.0000],
        [5979305.5000,       0.0000],
        [5457098.0000,       0.0000],
        [5184284.5000,       0.0000],
        [6663400.0000,       0.0000],
        [4357305.0000,       0.0000],
        [2092292.2500,       0.0000],
        [2416271.7500,  602589.3750],
        [1797972.6250, 2175274.7500],
        [1032780.9375, 1907404.1250],
        [1102841.5000, 1109403.6250],
        [1338652.7500, 1212883.2500],
        [ 715883.8750, 1420718.0000],
        [5680631.5000,       0.0000],
        [2134298.2500,       0.0000],
        [2133262.0000,       0.0000],
        [2037718.2500,       0.0000],
        [1939546.2500,       0.0000],
        [ 239261.5938,  849904.2500],
        [ 330294.8125,  425773.6875],
        [2277379.5000,  380471.6250],
        [ 350513.8750,  204623.0000],
        [ 455479.3438,  573371.1250],
        [ 448642.2500, 1193918.0000],
        [1315555.7500,  239978.2656],
        [ 685633.4375,  804367.4375],
        [1698113.3750, 1179359.1250],
        [2037496.7500,  193691.8281],
        [3696621.0000,       0.0000],
        [2536475.0000,       0.0000],
        [ 986251.3750, 1166717.0000],
        [1681917.7500,  709764.0000],
        [ 961307.5000,  820686.8750],
        [ 542876.5000,  616938.0625],
        [1237271.1250,  721475.6250],
        [ 414889.5000,  198133.3125],
        [ 917255.2500,  422864.5312],
        [ 108229.8047,  625645.0625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 426/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:56, 59.87s/it]  7%|▋         | 2/30 [01:01<11:50, 25.36s/it] 10%|█         | 3/30 [01:01<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.92s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.3363518397013348
Epoch 427/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:27, 60.94s/it]  7%|▋         | 2/30 [01:01<11:54, 25.53s/it] 10%|█         | 3/30 [01:02<06:23, 14.21s/it] 13%|█▎        | 4/30 [01:03<03:51,  8.90s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.96s/it] 20%|██        | 6/30 [01:04<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.337981184323629
Epoch 428/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:07, 58.21s/it]  7%|▋         | 2/30 [00:58<11:23, 24.41s/it] 10%|█         | 3/30 [00:59<06:07, 13.61s/it] 13%|█▎        | 4/30 [01:00<03:41,  8.53s/it] 17%|█▋        | 5/30 [01:01<02:23,  5.72s/it] 20%|██        | 6/30 [01:01<01:36,  4.03s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.96s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.25s/it] 30%|███       | 9/30 [01:04<00:37,  1.78s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.46s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.24s/it] 40%|████      | 12/30 [01:06<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 2.2996108373006185
Epoch 429/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:02, 58.02s/it]  7%|▋         | 2/30 [00:59<11:30, 24.66s/it] 10%|█         | 3/30 [01:00<06:16, 13.95s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:02<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 2.350230034192403
Epoch 430/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:17, 56.47s/it]  7%|▋         | 2/30 [01:00<12:03, 25.83s/it] 10%|█         | 3/30 [01:01<06:28, 14.38s/it] 13%|█▎        | 4/30 [01:02<03:53,  9.00s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.02s/it] 20%|██        | 6/30 [01:03<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.35s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.362019975980123
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0236,  0.0011,  0.0131,  ..., -0.0056,  0.0171,  0.0039],
        [-0.0022,  0.0098,  0.0285,  ...,  0.0171,  0.0038, -0.0150],
        [-0.0293, -0.0380,  0.0248,  ...,  0.0830, -0.0046, -0.0146],
        ...,
        [ 0.0062, -0.0034,  0.0096,  ..., -0.0241, -0.0055, -0.0032],
        [-0.0331,  0.0096, -0.0036,  ...,  0.0049,  0.0168, -0.0107],
        [-0.0329, -0.0138,  0.0191,  ...,  0.0412,  0.0358, -0.0288]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8754, 0.8508, 0.8387, 0.8372, 0.8311, 0.8286, 0.8269, 0.8223, 0.8204,
         0.8178],
        [0.9418, 0.9293, 0.9281, 0.9276, 0.9262, 0.9225, 0.9222, 0.9210, 0.9187,
         0.9181],
        [0.9052, 0.8972, 0.8759, 0.8711, 0.8566, 0.8487, 0.8465, 0.8378, 0.8354,
         0.8280],
        [0.9560, 0.8866, 0.8829, 0.8794, 0.8789, 0.8754, 0.8656, 0.8638, 0.8622,
         0.8609],
        [0.8429, 0.8317, 0.8313, 0.8144, 0.8060, 0.8057, 0.8016, 0.7966, 0.7889,
         0.7812],
        [0.8528, 0.8179, 0.8087, 0.7968, 0.7931, 0.7888, 0.7831, 0.7776, 0.7693,
         0.7659],
        [0.8652, 0.8625, 0.8543, 0.8520, 0.8488, 0.8469, 0.8457, 0.8414, 0.8398,
         0.8372],
        [0.8924, 0.8725, 0.8672, 0.8563, 0.8490, 0.8308, 0.8261, 0.8176, 0.8059,
         0.8024],
        [0.9079, 0.9018, 0.8875, 0.8839, 0.8838, 0.8833, 0.8828, 0.8795, 0.8788,
         0.8787],
        [0.9488, 0.8835, 0.8825, 0.8803, 0.8735, 0.8716, 0.8676, 0.8674, 0.8611,
         0.8592],
        [0.9446, 0.9037, 0.8930, 0.8897, 0.8896, 0.8869, 0.8837, 0.8823, 0.8796,
         0.8778],
        [0.8875, 0.8723, 0.8555, 0.8476, 0.8247, 0.7981, 0.7980, 0.7952, 0.7829,
         0.7778],
        [0.9132, 0.9059, 0.9022, 0.8905, 0.8802, 0.8765, 0.8684, 0.8639, 0.8621,
         0.8512],
        [0.9568, 0.9546, 0.9513, 0.9511, 0.9484, 0.9465, 0.9437, 0.9420, 0.9398,
         0.9348],
        [0.9419, 0.9282, 0.9271, 0.9257, 0.9229, 0.9184, 0.9178, 0.9147, 0.9139,
         0.9124],
        [0.9261, 0.9216, 0.9207, 0.9134, 0.9115, 0.9103, 0.9083, 0.9050, 0.9042,
         0.9024],
        [0.9520, 0.9461, 0.9348, 0.9321, 0.9257, 0.9250, 0.9179, 0.9177, 0.9142,
         0.9136],
        [0.9617, 0.9556, 0.9514, 0.9396, 0.9392, 0.9349, 0.9338, 0.9324, 0.9319,
         0.9303],
        [0.9255, 0.9137, 0.9005, 0.8979, 0.8931, 0.8923, 0.8905, 0.8894, 0.8861,
         0.8840],
        [0.9030, 0.8980, 0.8967, 0.8966, 0.8947, 0.8941, 0.8914, 0.8894, 0.8883,
         0.8873],
        [0.9406, 0.9326, 0.9324, 0.9302, 0.9297, 0.9295, 0.9260, 0.9244, 0.9244,
         0.9231],
        [0.9255, 0.9108, 0.9104, 0.9080, 0.9054, 0.9046, 0.9043, 0.9019, 0.8889,
         0.8850],
        [0.9676, 0.9484, 0.9419, 0.9386, 0.9333, 0.9271, 0.9251, 0.9176, 0.9126,
         0.9116],
        [0.9408, 0.9337, 0.9298, 0.9263, 0.9239, 0.9229, 0.9225, 0.9222, 0.9220,
         0.9203],
        [0.9534, 0.8992, 0.8888, 0.8794, 0.8651, 0.8610, 0.8605, 0.8584, 0.8581,
         0.8569],
        [0.9357, 0.9315, 0.9296, 0.9231, 0.9216, 0.9175, 0.9136, 0.9099, 0.9033,
         0.9031],
        [0.9233, 0.9215, 0.9193, 0.9146, 0.9072, 0.8994, 0.8972, 0.8966, 0.8955,
         0.8945],
        [0.9597, 0.9480, 0.9423, 0.9257, 0.9244, 0.9237, 0.9189, 0.9177, 0.9176,
         0.9162],
        [0.9427, 0.9358, 0.9259, 0.9250, 0.9242, 0.9222, 0.9205, 0.9173, 0.9136,
         0.9122],
        [0.9310, 0.9263, 0.9251, 0.9247, 0.9214, 0.9182, 0.9172, 0.9170, 0.9134,
         0.9133],
        [0.9445, 0.9445, 0.9444, 0.9435, 0.9420, 0.9413, 0.9337, 0.9311, 0.9299,
         0.9279],
        [0.9371, 0.9207, 0.9194, 0.9094, 0.9016, 0.9014, 0.8970, 0.8966, 0.8960,
         0.8953],
        [0.8743, 0.8688, 0.8657, 0.8581, 0.8567, 0.8499, 0.8491, 0.8490, 0.8482,
         0.8481],
        [0.9006, 0.8902, 0.8852, 0.8831, 0.8805, 0.8791, 0.8789, 0.8774, 0.8760,
         0.8750],
        [0.9185, 0.9142, 0.9133, 0.9081, 0.9004, 0.9002, 0.8964, 0.8897, 0.8869,
         0.8864],
        [0.9053, 0.8952, 0.8927, 0.8846, 0.8813, 0.8758, 0.8758, 0.8696, 0.8589,
         0.8578],
        [0.9473, 0.8674, 0.8640, 0.8508, 0.8466, 0.8307, 0.8211, 0.8186, 0.8157,
         0.8149],
        [0.8998, 0.8812, 0.8794, 0.8754, 0.8735, 0.8695, 0.8554, 0.8551, 0.8543,
         0.8525],
        [0.8930, 0.8894, 0.8689, 0.8613, 0.8489, 0.8456, 0.8423, 0.8371, 0.8352,
         0.8334],
        [0.9345, 0.9309, 0.9293, 0.9290, 0.9279, 0.9260, 0.9248, 0.9242, 0.9236,
         0.9222],
        [0.8804, 0.8768, 0.8766, 0.8656, 0.8606, 0.8556, 0.8532, 0.8397, 0.8311,
         0.8240],
        [0.8998, 0.8609, 0.8606, 0.8576, 0.8519, 0.8506, 0.8483, 0.8466, 0.8458,
         0.8454],
        [0.8845, 0.8733, 0.8698, 0.8605, 0.8515, 0.8469, 0.8408, 0.8387, 0.8340,
         0.8338],
        [0.8759, 0.8717, 0.8570, 0.8514, 0.8480, 0.8439, 0.8433, 0.8418, 0.8405,
         0.8339],
        [0.8510, 0.8366, 0.8204, 0.8187, 0.8096, 0.8095, 0.7978, 0.7820, 0.7742,
         0.7720],
        [0.8294, 0.8055, 0.7902, 0.7858, 0.7799, 0.7798, 0.7762, 0.7639, 0.7593,
         0.7578],
        [0.8992, 0.8823, 0.8786, 0.8753, 0.8744, 0.8738, 0.8624, 0.8622, 0.8616,
         0.8597],
        [0.8053, 0.7842, 0.7793, 0.7729, 0.7630, 0.7599, 0.7421, 0.7406, 0.7311,
         0.7205],
        [0.8321, 0.8291, 0.8103, 0.8097, 0.8035, 0.8028, 0.8019, 0.7974, 0.7893,
         0.7853],
        [0.8909, 0.8619, 0.8442, 0.8329, 0.8315, 0.8294, 0.8212, 0.8203, 0.8157,
         0.8127],
        [0.8758, 0.8533, 0.8474, 0.8470, 0.8356, 0.8265, 0.8209, 0.8107, 0.8095,
         0.8047],
        [0.8875, 0.8846, 0.8444, 0.8154, 0.8119, 0.8065, 0.8026, 0.7979, 0.7978,
         0.7935],
        [0.9240, 0.9070, 0.9014, 0.8926, 0.8641, 0.8589, 0.8562, 0.8521, 0.8420,
         0.8378],
        [0.9116, 0.8740, 0.8698, 0.8597, 0.8529, 0.8519, 0.8428, 0.8422, 0.8387,
         0.8378],
        [0.9234, 0.9007, 0.8977, 0.8972, 0.8952, 0.8921, 0.8921, 0.8893, 0.8888,
         0.8878],
        [0.8993, 0.8795, 0.8784, 0.8674, 0.8653, 0.8646, 0.8613, 0.8611, 0.8608,
         0.8593],
        [0.9049, 0.8763, 0.8677, 0.8654, 0.8589, 0.8559, 0.8419, 0.8288, 0.8253,
         0.8236],
        [0.9397, 0.8786, 0.8703, 0.8620, 0.8552, 0.8510, 0.8351, 0.8318, 0.8306,
         0.8191],
        [0.9059, 0.8593, 0.8548, 0.8441, 0.8307, 0.8293, 0.8239, 0.8234, 0.8165,
         0.8148],
        [0.8982, 0.8169, 0.8141, 0.8019, 0.7917, 0.7911, 0.7905, 0.7872, 0.7838,
         0.7757],
        [0.8835, 0.8661, 0.8566, 0.8541, 0.8472, 0.8432, 0.8410, 0.8406, 0.8406,
         0.8400],
        [0.8536, 0.7759, 0.7754, 0.7746, 0.7638, 0.7356, 0.7295, 0.7264, 0.7259,
         0.7238],
        [0.8827, 0.8554, 0.8419, 0.8218, 0.8125, 0.8119, 0.7939, 0.7919, 0.7903,
         0.7858],
        [0.8378, 0.8070, 0.7945, 0.7794, 0.7773, 0.7715, 0.7676, 0.7522, 0.7493,
         0.7446]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 269685.9688,  189789.9531,  159777.0938,  156372.0469,  143248.5312,
          138285.0469,  135041.2344,  126394.2109,  123080.5703,  118536.2266],
        [ 697100.0625,  582848.4375,  572731.9375,  569109.5625,  557753.5000,
          528552.5625,  526611.3750,  517757.3125,  500932.9375,  496447.6875],
        [ 413113.8750,  368651.8125,  271746.7188,  253892.1719,  206437.1094,
          184315.4531,  178591.1719,  157698.7656,  152497.1562,  137085.9531],
        [ 853718.8125,  316521.3125,  300586.2812,  285710.2812,  283677.2812,
          269786.5625,  234559.8750,  228787.4219,  223469.2188,  219351.2344],
        [ 169678.2812,  144642.0156,  143799.5938,  112884.4531,  100191.1719,
           99639.9453,   94018.2734,   87492.5312,   78448.8984,   70231.9922],
        [ 195510.7500,  118647.6328,  104031.4297,   87799.3672,   83236.7969,
           78365.7422,   72237.5703,   66780.9297,   59265.2031,   56502.7852],
        [ 233337.4531,  224611.8906,  199692.2344,  193317.5781,  184437.3125,
          179630.0469,  176562.8750,  165997.4531,  162388.9062,  156403.8281],
        [ 343996.0312,  259017.5625,  240165.7969,  205383.1562,  185061.3750,
          142613.4688,  133466.5469,  118229.5859,   99976.2266,   95095.9609],
        [ 429408.6562,  393388.9375,  320724.5938,  304534.3750,  304247.5625,
          302241.1250,  299772.1250,  286250.5938,  283318.5312,  283099.4688],
        [ 770406.1875,  303182.2500,  298543.9062,  289585.3438,  262686.3125,
          255637.8438,  241469.3594,  240894.7969,  220129.5312,  214255.2500],
        [ 725666.1875,  404124.8438,  347140.6875,  330855.9375,  330511.2188,
          318144.0312,  303666.9688,  297748.9062,  286362.2812,  279382.0938],
        [ 320762.5312,  258089.6875,  203072.6875,  181312.9375,  130739.7344,
           89470.0312,   89334.1250,   85833.0859,   71956.6250,   66977.3125],
        [ 463437.0000,  417489.5000,  395740.3438,  334710.9688,  288824.9062,
          274184.0625,  244277.2656,  229053.1094,  223215.9688,  191111.8281],
        [ 863864.5625,  836911.1250,  798588.4375,  796224.8750,  765323.3750,
          745183.0625,  716133.4375,  698544.8750,  677014.1875,  630217.8125],
        [ 697630.7500,  573398.1250,  564642.5000,  553954.7500,  532130.8125,
          498779.6562,  494352.3750,  472958.9688,  467525.0625,  457734.3438],
        [ 556450.7500,  522367.2812,  515504.6875,  464720.0625,  451684.2500,
          444028.4688,  431502.2500,  411647.3125,  407309.9688,  396723.9375],
        [ 805600.5625,  741018.1875,  630336.8750,  606895.3125,  553728.6875,
          548314.0000,  494985.9688,  493809.5625,  469742.6875,  465873.7812],
        [ 925505.4375,  849099.5625,  798834.4375,  675353.6875,  671726.4375,
          631669.7500,  621947.8125,  609517.0000,  604492.9375,  591164.8750],
        [ 552133.5625,  466377.9375,  386466.8750,  372444.9688,  347605.4375,
          343559.6562,  334752.7812,  329460.4688,  314472.3125,  305096.5938],
        [ 400426.2188,  372774.7500,  365775.2500,  365355.5000,  355728.5625,
          352713.2812,  339033.5312,  329633.3125,  324419.1875,  319727.4688],
        [ 685437.6250,  610832.6875,  609525.1250,  590646.4375,  586170.6875,
          584905.9375,  556058.6875,  543784.5000,  543177.0625,  533522.6875],
        [ 552040.8750,  447212.8125,  444695.9375,  430124.2812,  414192.0000,
          409856.3438,  407867.0000,  394057.2812,  327402.3125,  309366.9688],
        [1006711.6875,  765922.8125,  697344.1250,  665335.5000,  617122.5000,
          564639.1875,  548874.3750,  492968.7500,  459212.2188,  452581.9688],
        [ 686776.3750,  620839.0000,  586631.5625,  558292.0625,  539384.0000,
          531953.2500,  528572.1875,  526662.1250,  525353.3750,  512860.6250],
        [ 822720.4375,  379019.0312,  326654.4375,  285832.0938,  233077.2344,
          219618.3125,  217997.7656,  211636.5469,  210872.3906,  207132.6406],
        [ 638279.9375,  601305.8750,  585064.3750,  533789.8750,  522501.3125,
          492525.5938,  465535.8125,  441562.3438,  402261.8750,  401029.2500],
        [ 535304.4375,  521776.7812,  505013.5312,  472505.4062,  424947.3750,
          380337.7500,  368715.1250,  365330.0625,  359642.8125,  354333.6250],
        [ 899291.9375,  761535.3125,  701407.3125,  553793.6250,  543724.3750,
          538110.3750,  502464.0000,  493818.0625,  493345.0000,  483703.1875],
        [ 706073.0625,  639854.8750,  555518.0625,  547828.5000,  542021.0625,
          526328.2500,  513947.0938,  491070.7188,  465684.1250,  456684.3750],
        [ 597318.7500,  558692.5625,  548643.5625,  545804.0625,  520360.5625,
          497384.0938,  490128.4062,  488983.6250,  464625.6875,  463952.6562],
        [ 724536.2500,  723759.3125,  723059.8125,  713614.9375,  698874.6875,
          691547.9375,  620461.4375,  598265.1250,  588178.2500,  571010.6250],
        [ 651304.8125,  515160.1562,  505918.3125,  438629.0312,  392288.6250,
          391491.0625,  367522.9062,  365397.6562,  362383.0312,  358722.7812],
        [ 265787.3438,  245450.0156,  235030.9844,  210667.7656,  206506.0156,
          187409.6719,  185239.5156,  185133.9219,  183081.4219,  182716.0000],
        [ 387077.6875,  333646.1875,  310435.6562,  301275.9375,  290297.8750,
          284432.2812,  283822.5938,  277882.6250,  272039.9688,  268270.2500],
        [ 499609.9688,  469931.3125,  464051.3438,  430274.8125,  385561.2812,
          384795.7500,  364484.0938,  331030.1562,  318165.8750,  315746.4688],
        [ 413711.1562,  358197.3438,  345373.7188,  307853.3125,  293582.5938,
          271489.2188,  271245.1875,  248343.0625,  213306.0000,  209839.0469],
        [ 753998.7500,  240642.9062,  229286.5156,  189864.5312,  178924.1094,
          142447.0938,  124179.1797,  119834.3672,  115042.8516,  113681.5312],
        [ 382610.7812,  293089.1250,  285716.2812,  269750.8125,  262716.6250,
          248146.7969,  202921.6719,  202015.8906,  199727.2812,  194512.5938],
        [ 347233.3750,  329775.4375,  246053.5312,  220532.1406,  184708.2031,
          176413.5938,  168263.2656,  156251.9062,  151983.3438,  148177.2031],
        [ 627973.4375,  596460.3125,  582630.5625,  580539.5625,  571463.3750,
          555832.8125,  546945.1875,  541715.6875,  537047.0625,  526481.8750],
        [ 289884.2812,  275199.9688,  274517.6562,  234469.9531,  218387.9375,
          203275.7344,  196467.1562,  162084.7188,  143283.0938,  129444.3828],
        [ 382634.1250,  219435.5469,  218317.7500,  209404.0312,  192787.3438,
          189311.4375,  183279.6875,  178943.7500,  176769.6094,  175843.0000],
        [ 307530.2500,  261868.4062,  249144.6250,  218146.2656,  191679.3125,
          179731.1562,  164629.4844,  159798.4375,  149349.7656,  148919.9688],
        [ 271966.2812,  255850.5469,  207571.8438,  191431.9688,  182332.3594,
          172060.4531,  170609.9688,  166897.9688,  164035.9844,  149107.6875],
        [ 190511.5156,  155055.2969,  123008.3906,  120077.0078,  105364.0703,
          105234.3281,   89113.6562,   71052.1484,   63581.3711,   61578.0039],
        [ 139973.5625,   99457.3750,   79929.5078,   75062.4844,   68983.2969,
           68872.0703,   65403.7031,   54884.4258,   51394.0508,   50329.7266],
        [ 379238.8750,  297972.4688,  282457.6250,  269502.3750,  266178.2188,
          263727.7812,  224155.4531,  223352.6875,  221479.3281,  215570.2500],
        [  99182.7812,   73383.6094,   68424.0781,   62367.7070,   54212.3672,
           51809.4375,   40207.4492,   39324.5820,   34334.4297,   29531.9805],
        [ 145340.0312,  139206.7812,  106528.6250,  105557.7812,   96626.9453,
           95653.6094,   94467.2109,   88606.3828,   78845.5859,   74477.0625],
        [ 336907.8750,  222564.4688,  172801.9219,  146970.5312,  144197.7031,
          139982.3750,  124436.8984,  122904.8516,  115085.6484,  110207.9766],
        [ 271329.0000,  196826.4844,  180958.1250,  179845.8594,  152799.0781,
          134192.1250,  123884.8750,  107153.4219,  105323.7891,   98347.3281],
        [ 320986.5312,  307741.1875,  173214.4062,  114512.9531,  108933.5781,
          100855.3359,   95454.0391,   89201.6562,   89097.2500,   83779.3047],
        [ 540341.1250,  423668.2812,  391364.5000,  345274.5625,  229711.5625,
          213140.4688,  205125.1719,  193532.6562,  167512.5312,  157610.8125],
        [ 452349.3750,  264566.6562,  248999.9688,  215552.1719,  195812.0938,
          192823.5781,  169386.2812,  167853.7812,  159706.8594,  157703.5781],
        [ 535905.6250,  387482.8438,  371355.7812,  368705.9688,  358173.4375,
          342563.4375,  342563.4375,  329325.3750,  326589.3438,  322356.5625],
        [ 379897.6562,  286001.1875,  281879.3438,  240616.0625,  233616.0000,
          231385.4062,  220694.3594,  220099.0938,  219218.8438,  214317.1719],
        [ 411556.2812,  273425.0000,  241689.3750,  234113.1250,  213250.8750,
          204314.5625,  167193.7969,  138613.1562,  131925.9688,  128749.6406],
        [ 676088.9375,  282619.8125,  250913.4219,  222801.4844,  202230.0469,
          190336.4375,  151854.1094,  144746.2031,  142319.3281,  120819.8516],
        [ 417522.5625,  214441.8750,  200977.4531,  172665.5156,  142583.8125,
          139770.4062,  129233.5781,  128364.3047,  116265.5938,  113534.9375],
        [ 373998.6250,  117059.1953,  112430.3047,   94462.2500,   81651.8516,
           80898.3047,   80215.5625,   76576.9766,   72949.6016,   64942.4570],
        [ 302959.7188,  236253.7188,  206301.8906,  199098.0000,  180408.2656,
          170331.9688,  164986.9062,  164204.0938,  164108.1250,  162851.6719],
        [ 197784.7969,   65160.3945,   64635.2500,   63915.3750,   54808.0586,
           36639.5117,   33552.3828,   32109.0430,   31872.3262,   30964.7500],
        [ 299629.5000,  202939.6719,  167322.6875,  125415.0234,  109850.0625,
          108915.8125,   84225.8438,   81845.6641,   80015.4609,   75029.9922],
        [ 157714.7031,  101631.1328,   85024.4922,   68478.1328,   66431.6875,
           61131.6133,   57893.2070,   46443.4531,   44541.0430,   41681.1016]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[269685.9688,      0.0000],
         [189789.9531,      0.0000],
         [159777.0938,      0.0000],
         ...,
         [126394.2109,      0.0000],
         [123080.5703,      0.0000],
         [     0.0000, 118536.2266]],

        [[697100.0625,      0.0000],
         [582848.4375,      0.0000],
         [572731.9375,      0.0000],
         ...,
         [517757.3125,      0.0000],
         [500932.9375,      0.0000],
         [496447.6875,      0.0000]],

        [[413113.8750,      0.0000],
         [368651.8125,      0.0000],
         [271746.7188,      0.0000],
         ...,
         [157698.7656,      0.0000],
         [152497.1562,      0.0000],
         [137085.9531,      0.0000]],

        ...,

        [[     0.0000, 197784.7969],
         [ 65160.3945,      0.0000],
         [ 64635.2500,      0.0000],
         ...,
         [ 32109.0430,      0.0000],
         [ 31872.3262,      0.0000],
         [ 30964.7500,      0.0000]],

        [[299629.5000,      0.0000],
         [     0.0000, 202939.6719],
         [167322.6875,      0.0000],
         ...,
         [ 81845.6641,      0.0000],
         [ 80015.4609,      0.0000],
         [ 75029.9922,      0.0000]],

        [[     0.0000, 157714.7031],
         [     0.0000, 101631.1328],
         [     0.0000,  85024.4922],
         ...,
         [ 46443.4531,      0.0000],
         [     0.0000,  44541.0430],
         [     0.0000,  41681.1016]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1441674.6250,  118536.2266],
        [5549845.5000,       0.0000],
        [2324030.0000,       0.0000],
        [3216168.2500,       0.0000],
        [ 455223.5625,  645803.5000],
        [ 777231.5625,  145146.6719],
        [1713990.6250,  162388.9062],
        [1823005.7500,       0.0000],
        [3206986.0000,       0.0000],
        [3096790.5000,       0.0000],
        [3623603.0000,       0.0000],
        [1497548.7500,       0.0000],
        [3062045.0000,       0.0000],
        [7528006.0000,       0.0000],
        [5313107.0000,       0.0000],
        [4601939.0000,       0.0000],
        [5810306.0000,       0.0000],
        [6979312.0000,       0.0000],
        [3752370.5000,       0.0000],
        [3195953.7500,  329633.3125],
        [5844061.0000,       0.0000],
        [4136816.0000,       0.0000],
        [6270713.0000,       0.0000],
        [5617324.0000,       0.0000],
        [3114561.0000,       0.0000],
        [5083856.5000,       0.0000],
        [4287907.0000,       0.0000],
        [5971193.0000,       0.0000],
        [5445010.0000,       0.0000],
        [5175894.0000,       0.0000],
        [6653308.0000,       0.0000],
        [4348818.5000,       0.0000],
        [2087022.7500,       0.0000],
        [2408447.5000,  600733.5000],
        [1793477.8750, 2170173.0000],
        [1029685.4375, 1903255.2500],
        [1099578.6250, 1108323.2500],
        [1333040.3750, 1208167.2500],
        [ 713475.6250, 1415916.5000],
        [5667090.0000,       0.0000],
        [2127015.0000,       0.0000],
        [2126726.2500,       0.0000],
        [2030797.5000,       0.0000],
        [1931865.0000,       0.0000],
        [ 237994.2188,  846581.5625],
        [ 329715.5938,  424574.5938],
        [2264396.0000,  379238.8750],
        [ 348800.5625,  203977.8594],
        [ 454015.4375,  571294.5625],
        [ 447115.8438, 1188944.3750],
        [1311144.1250,  239515.9062],
        [ 681834.1250,  801942.1250],
        [1691665.7500, 1175616.0000],
        [2031930.7500,  192823.5781],
        [3685021.5000,       0.0000],
        [2527725.2500,       0.0000],
        [ 982391.7500, 1162440.0000],
        [1677078.2500,  707651.3750],
        [ 957655.7500,  817704.2500],
        [ 540129.9375,  615055.1875],
        [1231882.6250,  719621.7500],
        [ 413657.0625,  197784.7969],
        [ 913484.1875,  421705.5625],
        [ 107575.0625,  623395.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 431/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:55, 59.84s/it]  7%|▋         | 2/30 [01:00<11:42, 25.08s/it] 10%|█         | 3/30 [01:01<06:17, 13.97s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.75s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.3237482706705728
Epoch 432/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:28, 58.90s/it]  7%|▋         | 2/30 [00:59<11:31, 24.69s/it] 10%|█         | 3/30 [01:00<06:11, 13.76s/it] 13%|█▎        | 4/30 [01:01<03:44,  8.62s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.78s/it] 20%|██        | 6/30 [01:02<01:37,  4.07s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:04<00:49,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.2992369572321576
Epoch 433/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:26, 60.91s/it]  7%|▋         | 2/30 [01:03<12:25, 26.62s/it] 10%|█         | 3/30 [01:04<06:39, 14.80s/it] 13%|█▎        | 4/30 [01:05<04:00,  9.26s/it] 17%|█▋        | 5/30 [01:05<02:34,  6.19s/it] 20%|██        | 6/30 [01:06<01:44,  4.34s/it] 23%|██▎       | 7/30 [01:07<01:12,  3.16s/it] 27%|██▋       | 8/30 [01:08<00:52,  2.39s/it] 30%|███       | 9/30 [01:08<00:39,  1.88s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.53s/it] 37%|███▋      | 11/30 [01:10<00:24,  1.29s/it] 40%|████      | 12/30 [01:11<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.07it/s] 50%|█████     | 15/30 [01:13<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.23it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:16<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:19<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:22<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.82s/it]
Epoch loss is 2.3205333153406777
Epoch 434/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<30:56, 64.00s/it]  7%|▋         | 2/30 [01:04<12:30, 26.79s/it] 10%|█         | 3/30 [01:05<06:42, 14.90s/it] 13%|█▎        | 4/30 [01:06<04:02,  9.31s/it] 17%|█▋        | 5/30 [01:06<02:35,  6.22s/it] 20%|██        | 6/30 [01:07<01:44,  4.36s/it] 23%|██▎       | 7/30 [01:08<01:13,  3.18s/it] 27%|██▋       | 8/30 [01:09<00:52,  2.41s/it] 30%|███       | 9/30 [01:09<00:39,  1.89s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.29s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.33it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.86s/it]
Epoch loss is 2.339375885327657
Epoch 435/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:49, 61.70s/it]  7%|▋         | 2/30 [01:02<12:03, 25.85s/it] 10%|█         | 3/30 [01:03<06:28, 14.39s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.00s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.03s/it] 20%|██        | 6/30 [01:05<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.361727563540141
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0236,  0.0011,  0.0131,  ..., -0.0057,  0.0172,  0.0040],
        [-0.0022,  0.0097,  0.0285,  ...,  0.0171,  0.0039, -0.0149],
        [-0.0293, -0.0380,  0.0248,  ...,  0.0831, -0.0045, -0.0145],
        ...,
        [ 0.0062, -0.0034,  0.0097,  ..., -0.0241, -0.0054, -0.0031],
        [-0.0331,  0.0096, -0.0036,  ...,  0.0050,  0.0168, -0.0106],
        [-0.0329, -0.0138,  0.0191,  ...,  0.0412,  0.0358, -0.0287]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8751, 0.8508, 0.8384, 0.8370, 0.8309, 0.8284, 0.8266, 0.8221, 0.8202,
         0.8176],
        [0.9417, 0.9292, 0.9280, 0.9275, 0.9262, 0.9222, 0.9220, 0.9209, 0.9186,
         0.9179],
        [0.9051, 0.8971, 0.8756, 0.8710, 0.8564, 0.8485, 0.8463, 0.8376, 0.8353,
         0.8279],
        [0.9560, 0.8864, 0.8827, 0.8793, 0.8788, 0.8751, 0.8654, 0.8636, 0.8619,
         0.8607],
        [0.8428, 0.8316, 0.8311, 0.8143, 0.8058, 0.8054, 0.8015, 0.7964, 0.7887,
         0.7812],
        [0.8527, 0.8178, 0.8086, 0.7966, 0.7931, 0.7885, 0.7829, 0.7776, 0.7689,
         0.7658],
        [0.8649, 0.8623, 0.8540, 0.8519, 0.8485, 0.8468, 0.8455, 0.8412, 0.8394,
         0.8369],
        [0.8921, 0.8723, 0.8671, 0.8561, 0.8488, 0.8305, 0.8259, 0.8174, 0.8056,
         0.8021],
        [0.9078, 0.9017, 0.8873, 0.8838, 0.8837, 0.8831, 0.8826, 0.8794, 0.8787,
         0.8786],
        [0.9488, 0.8834, 0.8823, 0.8802, 0.8733, 0.8714, 0.8674, 0.8672, 0.8610,
         0.8591],
        [0.9446, 0.9036, 0.8929, 0.8894, 0.8893, 0.8867, 0.8836, 0.8821, 0.8794,
         0.8776],
        [0.8872, 0.8720, 0.8553, 0.8472, 0.8243, 0.7980, 0.7976, 0.7948, 0.7825,
         0.7775],
        [0.9130, 0.9058, 0.9020, 0.8902, 0.8798, 0.8763, 0.8682, 0.8637, 0.8619,
         0.8510],
        [0.9567, 0.9546, 0.9512, 0.9510, 0.9482, 0.9463, 0.9436, 0.9418, 0.9397,
         0.9346],
        [0.9418, 0.9281, 0.9270, 0.9256, 0.9228, 0.9183, 0.9177, 0.9146, 0.9138,
         0.9123],
        [0.9260, 0.9216, 0.9206, 0.9134, 0.9114, 0.9102, 0.9082, 0.9048, 0.9041,
         0.9023],
        [0.9518, 0.9460, 0.9347, 0.9319, 0.9256, 0.9249, 0.9177, 0.9175, 0.9140,
         0.9135],
        [0.9616, 0.9555, 0.9513, 0.9396, 0.9392, 0.9348, 0.9338, 0.9323, 0.9318,
         0.9302],
        [0.9254, 0.9135, 0.9002, 0.8978, 0.8930, 0.8921, 0.8904, 0.8892, 0.8860,
         0.8840],
        [0.9028, 0.8979, 0.8966, 0.8965, 0.8946, 0.8940, 0.8912, 0.8893, 0.8882,
         0.8871],
        [0.9404, 0.9324, 0.9323, 0.9301, 0.9296, 0.9293, 0.9259, 0.9243, 0.9241,
         0.9230],
        [0.9254, 0.9107, 0.9102, 0.9079, 0.9053, 0.9045, 0.9041, 0.9018, 0.8887,
         0.8847],
        [0.9675, 0.9484, 0.9418, 0.9385, 0.9332, 0.9269, 0.9250, 0.9175, 0.9125,
         0.9115],
        [0.9407, 0.9336, 0.9297, 0.9262, 0.9238, 0.9229, 0.9224, 0.9221, 0.9219,
         0.9203],
        [0.9533, 0.8990, 0.8885, 0.8793, 0.8650, 0.8607, 0.8603, 0.8581, 0.8579,
         0.8568],
        [0.9355, 0.9314, 0.9294, 0.9230, 0.9215, 0.9174, 0.9134, 0.9097, 0.9032,
         0.9030],
        [0.9232, 0.9214, 0.9191, 0.9145, 0.9070, 0.8993, 0.8971, 0.8964, 0.8954,
         0.8943],
        [0.9596, 0.9480, 0.9422, 0.9256, 0.9243, 0.9236, 0.9188, 0.9177, 0.9175,
         0.9162],
        [0.9426, 0.9357, 0.9259, 0.9248, 0.9241, 0.9220, 0.9204, 0.9172, 0.9134,
         0.9121],
        [0.9309, 0.9262, 0.9250, 0.9246, 0.9213, 0.9180, 0.9171, 0.9169, 0.9133,
         0.9133],
        [0.9445, 0.9444, 0.9442, 0.9434, 0.9419, 0.9412, 0.9336, 0.9310, 0.9299,
         0.9278],
        [0.9369, 0.9206, 0.9193, 0.9092, 0.9015, 0.9013, 0.8969, 0.8965, 0.8959,
         0.8952],
        [0.8741, 0.8686, 0.8655, 0.8579, 0.8566, 0.8498, 0.8489, 0.8489, 0.8480,
         0.8479],
        [0.9004, 0.8900, 0.8850, 0.8829, 0.8803, 0.8788, 0.8788, 0.8773, 0.8758,
         0.8748],
        [0.9183, 0.9141, 0.9132, 0.9078, 0.9003, 0.9001, 0.8963, 0.8895, 0.8867,
         0.8862],
        [0.9052, 0.8950, 0.8925, 0.8845, 0.8811, 0.8757, 0.8756, 0.8694, 0.8588,
         0.8576],
        [0.9473, 0.8673, 0.8639, 0.8506, 0.8465, 0.8303, 0.8208, 0.8184, 0.8156,
         0.8145],
        [0.8996, 0.8809, 0.8791, 0.8753, 0.8732, 0.8694, 0.8551, 0.8548, 0.8541,
         0.8522],
        [0.8929, 0.8892, 0.8687, 0.8611, 0.8487, 0.8453, 0.8421, 0.8368, 0.8350,
         0.8333],
        [0.9343, 0.9308, 0.9291, 0.9289, 0.9277, 0.9258, 0.9247, 0.9240, 0.9236,
         0.9220],
        [0.8802, 0.8765, 0.8764, 0.8653, 0.8604, 0.8554, 0.8530, 0.8395, 0.8308,
         0.8237],
        [0.8997, 0.8608, 0.8604, 0.8575, 0.8516, 0.8504, 0.8481, 0.8463, 0.8456,
         0.8452],
        [0.8844, 0.8730, 0.8696, 0.8603, 0.8512, 0.8467, 0.8405, 0.8385, 0.8338,
         0.8337],
        [0.8755, 0.8715, 0.8568, 0.8510, 0.8477, 0.8437, 0.8429, 0.8416, 0.8404,
         0.8338],
        [0.8508, 0.8365, 0.8201, 0.8183, 0.8093, 0.8092, 0.7976, 0.7816, 0.7740,
         0.7716],
        [0.8293, 0.8053, 0.7899, 0.7857, 0.7798, 0.7797, 0.7761, 0.7638, 0.7592,
         0.7578],
        [0.8990, 0.8820, 0.8783, 0.8750, 0.8741, 0.8734, 0.8621, 0.8618, 0.8612,
         0.8593],
        [0.8051, 0.7841, 0.7791, 0.7726, 0.7630, 0.7596, 0.7416, 0.7401, 0.7308,
         0.7205],
        [0.8317, 0.8288, 0.8102, 0.8095, 0.8033, 0.8027, 0.8019, 0.7971, 0.7891,
         0.7850],
        [0.8907, 0.8617, 0.8439, 0.8325, 0.8313, 0.8292, 0.8210, 0.8201, 0.8155,
         0.8125],
        [0.8755, 0.8532, 0.8473, 0.8467, 0.8354, 0.8264, 0.8208, 0.8105, 0.8094,
         0.8046],
        [0.8874, 0.8844, 0.8441, 0.8150, 0.8115, 0.8060, 0.8024, 0.7976, 0.7976,
         0.7931],
        [0.9238, 0.9068, 0.9011, 0.8925, 0.8639, 0.8587, 0.8560, 0.8518, 0.8418,
         0.8375],
        [0.9114, 0.8739, 0.8696, 0.8595, 0.8528, 0.8516, 0.8427, 0.8418, 0.8384,
         0.8376],
        [0.9232, 0.9005, 0.8975, 0.8970, 0.8951, 0.8919, 0.8919, 0.8892, 0.8886,
         0.8876],
        [0.8991, 0.8793, 0.8783, 0.8672, 0.8651, 0.8644, 0.8610, 0.8610, 0.8606,
         0.8590],
        [0.9046, 0.8762, 0.8674, 0.8652, 0.8588, 0.8557, 0.8418, 0.8285, 0.8249,
         0.8233],
        [0.9396, 0.8785, 0.8701, 0.8617, 0.8550, 0.8508, 0.8350, 0.8315, 0.8303,
         0.8188],
        [0.9057, 0.8592, 0.8546, 0.8439, 0.8306, 0.8291, 0.8235, 0.8232, 0.8161,
         0.8144],
        [0.8981, 0.8167, 0.8138, 0.8016, 0.7914, 0.7907, 0.7902, 0.7869, 0.7836,
         0.7753],
        [0.8834, 0.8659, 0.8564, 0.8539, 0.8469, 0.8431, 0.8407, 0.8403, 0.8402,
         0.8397],
        [0.8536, 0.7758, 0.7751, 0.7743, 0.7636, 0.7357, 0.7292, 0.7264, 0.7257,
         0.7236],
        [0.8826, 0.8553, 0.8417, 0.8216, 0.8123, 0.8116, 0.7934, 0.7915, 0.7899,
         0.7855],
        [0.8377, 0.8068, 0.7943, 0.7791, 0.7771, 0.7711, 0.7675, 0.7518, 0.7490,
         0.7443]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 268686.5625,  189861.6250,  159070.8750,  155936.4531,  142922.9375,
          137952.7188,  134388.0625,  125973.8672,  122738.2891,  118133.4531],
        [ 696281.5000,  581770.5000,  572118.3125,  567809.0000,  557408.3750,
          526849.5000,  525256.6875,  516609.0625,  499924.5000,  495373.2188],
        [ 412560.2812,  367917.4062,  270524.9375,  253263.4219,  205736.0312,
          183839.8750,  178071.7812,  157211.3438,  152093.6875,  136838.1719],
        [ 853882.5000,  315572.5000,  299583.7812,  285298.8750,  283370.6562,
          268778.3125,  233968.9375,  228134.2188,  222524.3594,  218833.8750],
        [ 169419.2344,  144405.2188,  143241.9688,  112680.8359,   99888.9297,
           99314.1562,   93874.1094,   87289.9219,   78207.3125,   70219.3359],
        [ 195165.3594,  118474.5156,  103914.0234,   87600.8984,   83250.5391,
           77998.6094,   72030.5078,   66761.1250,   58969.2148,   56398.4570],
        [ 232439.9375,  223663.0312,  198927.5469,  192917.3750,  183849.8750,
          179274.9531,  175937.6094,  165490.8594,  161471.2812,  155707.0156],
        [ 342774.5625,  258158.3594,  239553.4375,  204769.8438,  184606.7656,
          142018.8750,  133133.4844,  117794.0391,   99585.2188,   94677.1641],
        [ 428546.2188,  392989.2188,  319817.1250,  304341.8750,  303836.6875,
          301334.5312,  299318.4688,  285855.0000,  282805.6250,  282506.6562],
        [ 769988.2500,  302364.8125,  297726.1875,  288959.9375,  261914.8750,
          254991.4062,  240782.9531,  240176.7969,  219701.8906,  213689.1719],
        [ 725357.6250,  404007.6875,  346432.9375,  329774.8125,  329251.9062,
          317233.5938,  303285.7812,  297022.0312,  285856.6562,  278432.5312],
        [ 319468.4062,  257063.8750,  202376.8750,  180457.6719,  130133.4453,
           89293.5000,   88823.9844,   85322.6797,   71532.5547,   66622.6016],
        [ 461544.8438,  416728.1562,  394465.6250,  333578.4062,  287565.5938,
          273228.4688,  243563.6094,  228263.2812,  222536.0312,  190439.0156],
        [ 862169.1250,  836322.2500,  796909.3125,  794619.0000,  764015.8125,
          743545.3750,  714779.0000,  696844.8125,  675848.5000,  628947.3750],
        [ 697059.5000,  572787.0625,  563920.8125,  553162.8750,  531036.3125,
          498060.9688,  493704.5938,  472321.1250,  467117.7188,  457445.8750],
        [ 556065.6250,  521821.0625,  514796.2500,  464243.0000,  451325.9688,
          443384.0000,  430940.5000,  410969.9062,  406726.5625,  396307.2188],
        [ 804011.0000,  739661.8750,  629165.1250,  605121.6250,  552557.6250,
          547057.8750,  493988.1250,  492389.4062,  468645.0625,  464820.6875],
        [ 924145.4375,  847996.5625,  798216.1250,  675181.0625,  671075.8750,
          630647.6875,  621146.4375,  608411.8750,  604116.5625,  590518.5625],
        [ 551206.0000,  464912.9062,  384825.4688,  371820.7500,  346905.7188,
          342722.9062,  334166.5000,  328637.3750,  313993.7500,  304962.4375],
        [ 399438.3750,  372333.8438,  365183.0938,  364721.9062,  355000.2500,
          351911.2812,  338167.1562,  329310.0000,  323806.2188,  318873.9375],
        [ 683281.2500,  609059.1250,  608040.6250,  589792.5625,  585187.1250,
          583138.6250,  555031.9375,  542835.2500,  541444.0000,  532394.2500],
        [ 551413.6875,  446961.2188,  443712.6875,  429046.7812,  413797.9688,
          408799.2812,  406576.0938,  393296.2500,  326532.6562,  308273.7188],
        [1005762.5625,  765389.0625,  696740.5000,  664282.4375,  616508.3125,
          563607.3750,  548063.5625,  492444.3438,  458776.6562,  451796.6875],
        [ 686026.2500,  619861.6875,  585928.1250,  557902.9375,  539143.3125,
          531859.4375,  527998.3750,  525974.5000,  524499.3125,  512470.9688],
        [ 821466.8125,  378175.9688,  325585.8125,  285242.2812,  232463.2031,
          218601.1094,  217364.8125,  210787.5469,  210165.8906,  206933.0156],
        [ 637091.1250,  600499.5625,  583728.4375,  532647.6875,  521714.5938,
          491651.3125,  464449.3750,  440709.1562,  401671.9375,  400147.1562],
        [ 534303.7500,  520597.3125,  503606.7188,  471541.5938,  423866.3125,
          379525.4375,  367873.5312,  364547.0000,  358969.1562,  353654.7188],
        [ 898412.4375,  761022.6875,  700404.0000,  552624.5625,  542821.3125,
          537318.5625,  501846.6875,  493521.4688,  492675.9375,  483160.5312],
        [ 704900.3750,  638734.2500,  555170.0625,  546644.8125,  540965.0000,
          525316.8125,  512927.6562,  490374.3438,  464473.7188,  455642.0625],
        [ 596214.6250,  557859.3125,  548118.0000,  545008.2500,  519658.3438,
          496312.7812,  489408.1875,  488402.4688,  463930.0938,  463645.2500],
        [ 723801.4375,  723237.0000,  721583.5000,  712780.4375,  697691.3750,
          690548.1875,  619475.2500,  597376.3125,  587476.8750,  570191.6875],
        [ 650107.1875,  514559.6562,  505219.2188,  437567.3750,  391887.3750,
          390725.7188,  366795.9688,  364817.2188,  361646.2812,  358253.7188],
        [ 265054.5312,  244997.7344,  234447.1562,  210228.4219,  206237.7500,
          187217.8125,  184804.2188,  184716.6562,  182457.4219,  182121.0312],
        [ 385632.2500,  332572.1250,  309513.9375,  300471.6562,  289374.9688,
          283363.9062,  283171.2812,  277408.6562,  271325.1250,  267412.0000],
        [ 498365.0625,  469026.0312,  463157.3750,  428961.2812,  385377.4688,
          383884.1562,  363763.8438,  330257.5938,  317252.0625,  315028.5625],
        [ 413098.0938,  357261.5312,  344511.1562,  307421.4375,  292705.6250,
          270968.5312,  270744.0625,  247621.2812,  212747.9219,  209396.8594],
        [ 753820.3750,  240512.3594,  228854.6406,  189482.5000,  178541.9531,
          141789.0781,  123687.6094,  119484.0391,  114889.6797,  113036.5625],
        [ 381354.7188,  291830.4062,  284370.4375,  269460.0000,  261473.8750,
          247626.4844,  201988.1562,  201048.9531,  199168.2500,  193821.5312],
        [ 346657.6875,  328920.1875,  245097.5156,  219916.1406,  184258.1562,
          175478.9688,  167668.6719,  155528.4688,  151622.1406,  147809.2500],
        [ 626341.8125,  595176.1875,  581361.7500,  579569.3125,  570007.9375,
          554067.2500,  545783.7500,  540700.9375,  537004.5625,  525291.2500],
        [ 289067.4062,  274338.6562,  273862.9062,  233765.5469,  217875.3594,
          202722.6406,  195869.8125,  161585.2656,  142798.6875,  128981.2969],
        [ 382033.5938,  219109.1250,  217717.9219,  208851.1875,  192067.4375,
          188692.6719,  182686.9062,  178091.4844,  176241.3906,  175404.0156],
        [ 306741.1562,  260960.6875,  248307.5312,  217608.3281,  191106.1719,
          179144.3750,  164005.0156,  159256.3594,  149009.7500,  148668.5156],
        [ 270428.2188,  255244.1875,  206793.3594,  190484.0781,  181780.5938,
          171675.2656,  169738.9688,  166399.7188,  163670.1875,  148989.8594],
        [ 189847.1562,  154809.0000,  122555.7109,  119437.8906,  104983.7344,
          104828.3516,   88766.9922,   70654.0156,   63397.0039,   61242.3086],
        [ 139644.5000,   99124.5391,   79588.8203,   74906.2266,   68857.8906,
           68780.3125,   65298.9961,   54801.8906,   51332.6250,   50287.0742],
        [ 378159.0000,  296488.8125,  281207.2812,  268307.3438,  264773.8438,
          262160.7500,  223058.9219,  222074.7031,  220349.2344,  214341.8906],
        [  98810.3359,   73202.6406,   68195.9453,   62096.4258,   54152.6875,
           51631.1836,   39913.7695,   39057.2383,   34189.1914,   29514.6660],
        [ 144612.2344,  138662.0625,  106317.0156,  105272.2734,   96301.8359,
           95521.0703,   94389.6641,   88240.7422,   78645.1484,   74174.6016],
        [ 335805.0625,  221942.5781,  172088.0156,  146150.3594,  143694.3125,
          139492.3750,  124012.3047,  122493.8984,  114712.3281,  109940.1953],
        [ 270118.1250,  196495.2500,  180537.7188,  179198.3594,  152385.0625,
          133948.0469,  123676.6484,  106742.4922,  105162.1953,   98105.3594],
        [ 320229.1875,  307085.0625,  172549.3125,  113952.5625,  108335.8984,
          100141.2188,   95130.4297,   88858.3906,   88849.8281,   83324.2500],
        [ 538768.1250,  422715.4062,  389710.1250,  344410.6250,  228887.1562,
          212671.2500,  204496.4375,  192578.6094,  167104.3594,  157089.3438],
        [ 451453.3750,  264213.9062,  248521.0000,  215121.1094,  195379.5312,
          192097.2812,  169127.6875,  167037.2812,  159136.7188,  157181.0781],
        [ 534405.6875,  386388.7500,  370232.0000,  367600.0000,  357530.1250,
          341603.3125,  341603.3125,  328465.9688,  325906.0625,  321344.8750],
        [ 378664.9688,  285176.4688,  281178.5938,  239950.6094,  232909.7031,
          230695.3125,  219818.4219,  219798.2969,  218478.5469,  213379.4531],
        [ 409664.4688,  272812.8750,  240665.3906,  233309.6406,  212872.3281,
          203586.9375,  167056.4062,  138089.6094,  131153.0938,  128263.7188],
        [ 675271.8750,  282070.8125,  250169.3750,  222051.1875,  201682.6719,
          189932.8125,  151475.1562,  144116.3125,  141627.1719,  120244.7734],
        [ 416131.6562,  214035.0938,  200464.0625,  172071.7656,  142367.6562,
          139269.8438,  128602.9922,  127980.1250,  115750.1484,  112912.8750],
        [ 373428.0312,  116623.3984,  111977.7734,   94049.2031,   81292.7344,
           80449.9219,   79862.2266,   76217.5703,   72668.6641,   64616.6406],
        [ 302750.8750,  235493.3906,  205638.3438,  198497.7500,  179686.9375,
          170025.2344,  164349.9375,  163525.5469,  163130.5312,  162106.0469],
        [ 197543.7031,   65076.5547,   64396.8320,   63671.2344,   54629.6914,
           36666.7070,   33445.2031,   32108.0352,   31804.4668,   30853.0332],
        [ 299063.3750,  202656.3281,  166653.8125,  125222.1250,  109534.7656,
          108403.1797,   83664.8125,   81411.8984,   79585.1719,   74664.7422],
        [ 157383.2656,  101229.1172,   84741.1719,   68203.1641,   66276.4688,
           60799.5078,   57759.9727,   46197.4453,   44373.3594,   41490.9727]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[268686.5625,      0.0000],
         [189861.6250,      0.0000],
         [159070.8750,      0.0000],
         ...,
         [125973.8672,      0.0000],
         [122738.2891,      0.0000],
         [     0.0000, 118133.4531]],

        [[696281.5000,      0.0000],
         [581770.5000,      0.0000],
         [572118.3125,      0.0000],
         ...,
         [516609.0625,      0.0000],
         [499924.5000,      0.0000],
         [495373.2188,      0.0000]],

        [[412560.2812,      0.0000],
         [367917.4062,      0.0000],
         [270524.9375,      0.0000],
         ...,
         [157211.3438,      0.0000],
         [152093.6875,      0.0000],
         [136838.1719,      0.0000]],

        ...,

        [[     0.0000, 197543.7031],
         [ 65076.5547,      0.0000],
         [ 64396.8320,      0.0000],
         ...,
         [ 32108.0352,      0.0000],
         [ 31804.4668,      0.0000],
         [ 30853.0332,      0.0000]],

        [[299063.3750,      0.0000],
         [     0.0000, 202656.3281],
         [166653.8125,      0.0000],
         ...,
         [ 81411.8984,      0.0000],
         [ 79585.1719,      0.0000],
         [ 74664.7422,      0.0000]],

        [[     0.0000, 157383.2656],
         [     0.0000, 101229.1172],
         [     0.0000,  84741.1719],
         ...,
         [ 46197.4453,      0.0000],
         [     0.0000,  44373.3594],
         [     0.0000,  41490.9727]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1437531.2500,  118133.4531],
        [5539400.5000,       0.0000],
        [2318057.0000,       0.0000],
        [3209948.0000,       0.0000],
        [ 454295.7812,  644245.3125],
        [ 775803.5000,  144759.7344],
        [1708208.2500,  161471.2812],
        [1817071.6250,       0.0000],
        [3201351.5000,       0.0000],
        [3090296.2500,       0.0000],
        [3616655.5000,       0.0000],
        [1491095.6250,       0.0000],
        [3051913.2500,       0.0000],
        [7514001.0000,       0.0000],
        [5306617.0000,       0.0000],
        [4596580.0000,       0.0000],
        [5797418.5000,       0.0000],
        [6971456.0000,       0.0000],
        [3744154.0000,       0.0000],
        [3189436.2500,  329310.0000],
        [5830205.0000,       0.0000],
        [4128410.2500,       0.0000],
        [6263372.0000,       0.0000],
        [5611665.0000,       0.0000],
        [3106786.2500,       0.0000],
        [5074310.5000,       0.0000],
        [4278485.5000,       0.0000],
        [5963808.0000,       0.0000],
        [5435149.0000,       0.0000],
        [5168557.5000,       0.0000],
        [6644162.0000,       0.0000],
        [4341580.0000,       0.0000],
        [2082282.7500,       0.0000],
        [2401357.0000,  598888.8750],
        [1789459.5000, 2165614.0000],
        [1027027.6250, 1899449.0000],
        [1096729.5000, 1107369.2500],
        [1328023.8750, 1204119.0000],
        [ 711324.9375, 1411632.2500],
        [5655305.0000,       0.0000],
        [2120867.5000,       0.0000],
        [2120895.7500,       0.0000],
        [2024808.0000,       0.0000],
        [1925204.2500,       0.0000],
        [ 236880.0625,  843642.0625],
        [ 329176.0625,  423446.8125],
        [2252762.7500,  378159.0000],
        [ 347269.5938,  203494.4688],
        [ 452649.7812,  569486.8750],
        [ 445745.2500, 1184586.1250],
        [1307259.0000,  239110.2500],
        [ 678592.5625,  799863.5625],
        [1686055.5000, 1172375.8750],
        [2027171.6250,  192097.2812],
        [3675080.2500,       0.0000],
        [2520050.5000,       0.0000],
        [ 978952.1250, 1158522.2500],
        [1672837.5000,  705804.6875],
        [ 954503.7500,  815082.4375],
        [ 537799.5625,  613386.5625],
        [1227273.5000,  717931.1875],
        [ 412651.7500,  197543.7031],
        [ 910266.0000,  420594.2500],
        [ 106996.9531,  621457.5000]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 436/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:30, 56.90s/it]  7%|▋         | 2/30 [01:00<11:55, 25.55s/it] 10%|█         | 3/30 [01:01<06:24, 14.23s/it] 13%|█▎        | 4/30 [01:02<03:51,  8.90s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.96s/it] 20%|██        | 6/30 [01:03<01:40,  4.19s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.33s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.336216425895691
Epoch 437/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:04, 60.14s/it]  7%|▋         | 2/30 [01:00<11:45, 25.21s/it] 10%|█         | 3/30 [01:01<06:19, 14.04s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.79s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.73s/it]
Epoch loss is 2.3753121296564736
Epoch 438/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:42, 59.41s/it]  7%|▋         | 2/30 [01:00<11:37, 24.90s/it] 10%|█         | 3/30 [01:00<06:14, 13.87s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.69s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.83s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.3186031699180605
Epoch 439/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:19, 58.60s/it]  7%|▋         | 2/30 [00:59<11:29, 24.64s/it] 10%|█         | 3/30 [01:00<06:10, 13.73s/it] 13%|█▎        | 4/30 [01:00<03:43,  8.60s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.77s/it] 20%|██        | 6/30 [01:02<01:37,  4.06s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.342426856358846
Epoch 440/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:12, 60.43s/it]  7%|▋         | 2/30 [01:01<11:59, 25.70s/it] 10%|█         | 3/30 [01:02<06:26, 14.31s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.96s/it] 17%|█▋        | 5/30 [01:04<02:29,  6.00s/it] 20%|██        | 6/30 [01:04<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.3456054369608563
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0236,  0.0010,  0.0131,  ..., -0.0057,  0.0173,  0.0040],
        [-0.0022,  0.0097,  0.0285,  ...,  0.0171,  0.0040, -0.0149],
        [-0.0293, -0.0380,  0.0248,  ...,  0.0832, -0.0045, -0.0145],
        ...,
        [ 0.0061, -0.0034,  0.0097,  ..., -0.0241, -0.0053, -0.0030],
        [-0.0332,  0.0097, -0.0036,  ...,  0.0050,  0.0168, -0.0106],
        [-0.0328, -0.0138,  0.0192,  ...,  0.0412,  0.0359, -0.0287]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8749, 0.8508, 0.8381, 0.8368, 0.8308, 0.8283, 0.8263, 0.8219, 0.8201,
         0.8174],
        [0.9417, 0.9291, 0.9279, 0.9273, 0.9261, 0.9220, 0.9219, 0.9207, 0.9184,
         0.9178],
        [0.9050, 0.8970, 0.8753, 0.8708, 0.8562, 0.8484, 0.8461, 0.8374, 0.8351,
         0.8278],
        [0.9560, 0.8862, 0.8825, 0.8792, 0.8787, 0.8749, 0.8653, 0.8635, 0.8616,
         0.8606],
        [0.8427, 0.8315, 0.8308, 0.8141, 0.8056, 0.8052, 0.8014, 0.7962, 0.7885,
         0.7811],
        [0.8526, 0.8177, 0.8085, 0.7965, 0.7931, 0.7882, 0.7828, 0.7776, 0.7686,
         0.7657],
        [0.8647, 0.8620, 0.8538, 0.8518, 0.8483, 0.8466, 0.8453, 0.8410, 0.8391,
         0.8366],
        [0.8920, 0.8721, 0.8669, 0.8559, 0.8487, 0.8302, 0.8258, 0.8172, 0.8054,
         0.8018],
        [0.9077, 0.9016, 0.8871, 0.8838, 0.8836, 0.8829, 0.8826, 0.8793, 0.8786,
         0.8785],
        [0.9488, 0.8832, 0.8821, 0.8801, 0.8731, 0.8713, 0.8672, 0.8671, 0.8609,
         0.8589],
        [0.9446, 0.9036, 0.8928, 0.8892, 0.8891, 0.8865, 0.8835, 0.8820, 0.8793,
         0.8774],
        [0.8870, 0.8718, 0.8551, 0.8470, 0.8241, 0.7979, 0.7973, 0.7944, 0.7821,
         0.7772],
        [0.9127, 0.9057, 0.9018, 0.8900, 0.8796, 0.8760, 0.8680, 0.8635, 0.8617,
         0.8508],
        [0.9566, 0.9545, 0.9511, 0.9509, 0.9481, 0.9462, 0.9435, 0.9417, 0.9396,
         0.9345],
        [0.9418, 0.9280, 0.9269, 0.9256, 0.9227, 0.9182, 0.9176, 0.9145, 0.9138,
         0.9123],
        [0.9260, 0.9215, 0.9205, 0.9133, 0.9114, 0.9101, 0.9081, 0.9047, 0.9040,
         0.9022],
        [0.9517, 0.9459, 0.9345, 0.9318, 0.9254, 0.9247, 0.9176, 0.9173, 0.9139,
         0.9133],
        [0.9615, 0.9555, 0.9513, 0.9396, 0.9391, 0.9347, 0.9337, 0.9322, 0.9318,
         0.9301],
        [0.9253, 0.9133, 0.9000, 0.8977, 0.8929, 0.8920, 0.8903, 0.8890, 0.8859,
         0.8839],
        [0.9027, 0.8979, 0.8965, 0.8964, 0.8945, 0.8938, 0.8911, 0.8893, 0.8881,
         0.8869],
        [0.9402, 0.9322, 0.9321, 0.9300, 0.9295, 0.9292, 0.9258, 0.9242, 0.9240,
         0.9228],
        [0.9253, 0.9107, 0.9101, 0.9077, 0.9053, 0.9043, 0.9039, 0.9017, 0.8886,
         0.8845],
        [0.9674, 0.9483, 0.9417, 0.9384, 0.9332, 0.9268, 0.9249, 0.9174, 0.9125,
         0.9114],
        [0.9406, 0.9335, 0.9296, 0.9262, 0.9238, 0.9229, 0.9223, 0.9220, 0.9218,
         0.9202],
        [0.9532, 0.8989, 0.8883, 0.8791, 0.8648, 0.8604, 0.8601, 0.8579, 0.8577,
         0.8567],
        [0.9354, 0.9313, 0.9293, 0.9229, 0.9215, 0.9173, 0.9133, 0.9096, 0.9031,
         0.9028],
        [0.9231, 0.9213, 0.9189, 0.9143, 0.9069, 0.8991, 0.8969, 0.8963, 0.8953,
         0.8942],
        [0.9595, 0.9479, 0.9421, 0.9254, 0.9242, 0.9235, 0.9188, 0.9176, 0.9175,
         0.9161],
        [0.9425, 0.9356, 0.9259, 0.9247, 0.9240, 0.9219, 0.9202, 0.9171, 0.9133,
         0.9119],
        [0.9308, 0.9261, 0.9249, 0.9245, 0.9212, 0.9179, 0.9170, 0.9168, 0.9132,
         0.9132],
        [0.9444, 0.9444, 0.9441, 0.9433, 0.9418, 0.9411, 0.9335, 0.9309, 0.9298,
         0.9277],
        [0.9368, 0.9205, 0.9192, 0.9091, 0.9015, 0.9012, 0.8968, 0.8964, 0.8958,
         0.8952],
        [0.8740, 0.8685, 0.8654, 0.8578, 0.8565, 0.8497, 0.8488, 0.8487, 0.8478,
         0.8477],
        [0.9002, 0.8898, 0.8848, 0.8828, 0.8801, 0.8786, 0.8786, 0.8772, 0.8756,
         0.8746],
        [0.9182, 0.9140, 0.9131, 0.9077, 0.9003, 0.8999, 0.8962, 0.8894, 0.8866,
         0.8861],
        [0.9051, 0.8949, 0.8923, 0.8844, 0.8809, 0.8756, 0.8755, 0.8692, 0.8586,
         0.8575],
        [0.9473, 0.8673, 0.8638, 0.8505, 0.8463, 0.8301, 0.8206, 0.8182, 0.8155,
         0.8142],
        [0.8994, 0.8806, 0.8788, 0.8752, 0.8729, 0.8693, 0.8549, 0.8545, 0.8540,
         0.8520],
        [0.8928, 0.8891, 0.8684, 0.8609, 0.8485, 0.8450, 0.8419, 0.8365, 0.8349,
         0.8331],
        [0.9342, 0.9306, 0.9290, 0.9288, 0.9276, 0.9256, 0.9246, 0.9239, 0.9236,
         0.9219],
        [0.8800, 0.8764, 0.8763, 0.8652, 0.8603, 0.8552, 0.8528, 0.8393, 0.8306,
         0.8235],
        [0.8996, 0.8607, 0.8602, 0.8573, 0.8514, 0.8502, 0.8479, 0.8460, 0.8454,
         0.8451],
        [0.8842, 0.8728, 0.8694, 0.8602, 0.8511, 0.8465, 0.8403, 0.8383, 0.8337,
         0.8336],
        [0.8752, 0.8714, 0.8565, 0.8507, 0.8476, 0.8436, 0.8426, 0.8414, 0.8403,
         0.8338],
        [0.8506, 0.8364, 0.8199, 0.8180, 0.8091, 0.8090, 0.7973, 0.7812, 0.7738,
         0.7712],
        [0.8291, 0.8051, 0.7896, 0.7855, 0.7797, 0.7796, 0.7760, 0.7637, 0.7591,
         0.7577],
        [0.8988, 0.8817, 0.8780, 0.8747, 0.8738, 0.8730, 0.8618, 0.8614, 0.8609,
         0.8589],
        [0.8048, 0.7839, 0.7789, 0.7723, 0.7629, 0.7594, 0.7412, 0.7397, 0.7305,
         0.7205],
        [0.8314, 0.8285, 0.8101, 0.8093, 0.8031, 0.8026, 0.8018, 0.7969, 0.7889,
         0.7847],
        [0.8905, 0.8615, 0.8437, 0.8321, 0.8311, 0.8290, 0.8208, 0.8199, 0.8153,
         0.8124],
        [0.8752, 0.8531, 0.8471, 0.8465, 0.8353, 0.8263, 0.8207, 0.8102, 0.8093,
         0.8044],
        [0.8872, 0.8843, 0.8439, 0.8148, 0.8112, 0.8056, 0.8022, 0.7975, 0.7974,
         0.7928],
        [0.9236, 0.9067, 0.9009, 0.8923, 0.8636, 0.8586, 0.8558, 0.8515, 0.8417,
         0.8373],
        [0.9113, 0.8738, 0.8695, 0.8594, 0.8527, 0.8514, 0.8426, 0.8415, 0.8382,
         0.8374],
        [0.9231, 0.9004, 0.8974, 0.8969, 0.8950, 0.8917, 0.8917, 0.8890, 0.8885,
         0.8874],
        [0.8989, 0.8791, 0.8781, 0.8670, 0.8649, 0.8642, 0.8610, 0.8608, 0.8604,
         0.8587],
        [0.9043, 0.8760, 0.8671, 0.8650, 0.8587, 0.8555, 0.8418, 0.8283, 0.8245,
         0.8231],
        [0.9395, 0.8784, 0.8699, 0.8616, 0.8548, 0.8507, 0.8348, 0.8312, 0.8300,
         0.8185],
        [0.9055, 0.8591, 0.8544, 0.8437, 0.8305, 0.8289, 0.8232, 0.8230, 0.8159,
         0.8141],
        [0.8980, 0.8164, 0.8136, 0.8013, 0.7911, 0.7903, 0.7899, 0.7866, 0.7833,
         0.7750],
        [0.8834, 0.8657, 0.8562, 0.8537, 0.8467, 0.8430, 0.8405, 0.8401, 0.8398,
         0.8394],
        [0.8535, 0.7757, 0.7749, 0.7741, 0.7634, 0.7357, 0.7290, 0.7264, 0.7256,
         0.7234],
        [0.8825, 0.8553, 0.8414, 0.8216, 0.8121, 0.8113, 0.7930, 0.7912, 0.7896,
         0.7851],
        [0.8375, 0.8065, 0.7941, 0.7789, 0.7770, 0.7707, 0.7673, 0.7515, 0.7488,
         0.7441]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 267796.0938,  189859.4688,  158482.5938,  155547.4688,  142639.0312,
          137658.5938,  133812.9531,  125598.5156,  122446.7188,  117822.4609],
        [ 695582.6875,  580819.2500,  571576.1875,  566779.9375,  557080.4375,
          525398.9375,  524127.2812,  515685.0938,  499078.4688,  494428.7812],
        [ 412099.8125,  367262.5625,  269488.0000,  252691.8750,  205141.7969,
          183430.7812,  177614.5312,  156781.3438,  151775.6406,  136644.0000],
        [ 854082.0625,  314750.7812,  298664.3750,  284938.8750,  283103.7812,
          267980.7812,  233452.2969,  227615.0469,  221696.7656,  218397.7188],
        [ 169182.7031,  144214.8906,  142762.3438,  112495.1953,   99611.2422,
           99066.9844,   93750.1172,   87109.2188,   77978.1562,   70209.4922],
        [ 194871.7031,  118332.0156,  103822.4922,   87428.5547,   83251.2500,
           77672.4453,   71864.6641,   66739.6094,   58712.1562,   56312.0859],
        [ 231718.4062,  222904.7656,  198312.3125,  192544.4375,  183373.4219,
          178961.6562,  175431.7812,  165054.5781,  160685.5469,  155102.6250],
        [ 341887.1875,  257563.5000,  239094.2500,  204288.2656,  184214.9375,
          141561.9531,  132893.6094,  117452.9141,   99285.8438,   94339.5469],
        [ 427860.5938,  392660.2812,  319130.6875,  304173.5625,  303516.6875,
          300582.8438,  298912.8438,  285539.2188,  282381.1562,  282014.8750],
        [ 769690.1875,  301703.4688,  297057.1562,  288423.8750,  261261.7500,
          254457.4531,  240210.6875,  239576.5312,  219333.4531,  213211.8281],
        [ 725026.3125,  403845.5312,  345787.6562,  328856.2188,  328176.6562,
          316428.0625,  302923.2812,  296349.4375,  285373.1875,  277600.5312],
        [ 318335.8438,  256194.7969,  201810.0469,  179784.4531,  129671.6094,
           89135.5859,   88398.2500,   84897.1250,   71185.2812,   66333.1172],
        [ 459863.4375,  416040.0000,  393335.2812,  332627.3125,  286550.7500,
          272377.4375,  242924.5156,  227578.1562,  221963.7500,  189843.5312],
        [ 860706.8125,  835845.4375,  795536.4375,  793207.6250,  762861.0625,
          742170.3125,  713751.7500,  695426.7500,  674812.1875,  627807.5625],
        [ 696620.9375,  572222.0000,  563313.9375,  552526.5000,  530154.8750,
          497398.8125,  493129.5625,  471837.1562,  466789.0625,  457209.9375],
        [ 555766.0625,  521402.2188,  514216.2500,  463835.4062,  451050.5625,
          442892.5000,  430482.0938,  410446.2188,  406226.1250,  396036.3438],
        [ 802796.6250,  738555.2500,  628160.9375,  603650.6875,  551476.2500,
          546021.6875,  493211.3750,  491291.3125,  467740.4375,  463973.4375],
        [ 922949.3125,  847017.8125,  797757.1875,  675013.0000,  670601.1875,
          629708.3750,  620478.5625,  607451.7500,  603848.7500,  589972.0000],
        [ 550472.6250,  463666.9062,  383477.2812,  371322.5000,  346327.5625,
          342010.4688,  333655.7188,  327924.4688,  313571.5000,  304775.5000],
        [ 398614.0938,  371939.5312,  364690.5938,  364219.6562,  354406.2812,
          351245.7188,  337490.9062,  329027.1562,  323339.6562,  318146.1562],
        [ 681411.6875,  607598.3125,  606821.8125,  588962.9375,  584323.3125,
          581658.4375,  554168.2500,  542013.8125,  540021.7500,  531362.0625],
        [ 550845.5000,  446750.7188,  442960.9375,  428161.4062,  413473.3438,
          407942.4688,  405486.8438,  392667.7812,  325826.5312,  307335.2500],
        [1004846.1250,  764970.1875,  696284.1875,  663329.6875,  615927.6875,
          562726.0000,  547398.6250,  491971.6562,  458345.0000,  451049.3125],
        [ 685433.1250,  619054.1250,  585340.0000,  557549.2500,  538916.1250,
          531729.0625,  527502.1250,  525393.4375,  523727.5625,  512100.6250],
        [ 820311.3125,  377383.0000,  324612.5938,  284698.5000,  231969.3594,
          217703.5938,  216841.1875,  210128.8125,  209534.8906,  206714.2969],
        [ 635985.6875,  599778.3750,  582551.6875,  531674.8125,  521085.0938,
          490794.0000,  463463.0938,  439893.7188,  401129.8750,  399389.6250],
        [ 533449.8750,  519582.9688,  502395.4688,  470728.8438,  422948.0625,
          378843.3750,  367161.0000,  363904.4062,  358413.6250,  353017.5312],
        [ 897669.8750,  760583.7500,  699561.5625,  551663.5000,  542033.9375,
          536680.5000,  501326.2500,  493286.1875,  492136.3750,  482691.2188],
        [ 703891.3750,  637841.2500,  554850.9375,  545612.5000,  540002.1875,
          524419.8125,  511999.0938,  489766.3125,  463479.4375,  454759.0625],
        [ 595252.2500,  557218.6250,  547677.5000,  544360.4375,  519104.5312,
          495460.5938,  488872.1875,  487886.1562,  463429.5000,  463359.7188],
        [ 723223.1875,  722752.3125,  720380.3125,  712162.8125,  696713.8750,
          689718.2500,  618635.1250,  596648.6250,  586898.4375,  569546.6250],
        [ 649101.0625,  514053.9375,  504658.1875,  436696.9062,  391591.1250,
          390067.0938,  366179.4375,  364272.4375,  360991.2500,  357855.9062],
        [ 264457.9062,  244609.9375,  233980.7656,  209839.0469,  205985.5625,
          187029.3750,  184429.0312,  184352.5469,  181937.2031,  181606.9688],
        [ 384417.2188,  331650.7812,  308742.4375,  299807.2812,  288629.4062,
          282629.0000,  282474.3438,  277039.3125,  270743.5625,  266676.7500],
        [ 497323.8438,  468250.1562,  462447.6562,  427899.3750,  385230.1250,
          383163.2500,  363151.4062,  329605.9688,  316531.8750,  314408.4688],
        [ 412520.9375,  356452.2500,  343682.5312,  307043.1875,  291935.3438,
          270498.8750,  270297.2500,  246997.7188,  212270.2500,  209002.4219],
        [ 753721.9375,  240377.2969,  228517.9062,  189186.3750,  178208.5312,
          141260.4062,  123289.3281,  119189.2656,  114757.8438,  112508.8281],
        [ 380295.3125,  290794.1562,  283221.5312,  269204.1562,  260427.8906,
          247198.7031,  201219.1094,  200241.6406,  198692.2500,  193261.9062],
        [ 346168.7188,  328208.9062,  244332.7344,  219410.2344,  183870.5625,
          174733.6875,  167199.8594,  154926.1250,  151321.9688,  147518.7344],
        [ 624909.8750,  594055.0625,  580296.5625,  578724.8125,  568737.9375,
          552574.4375,  544755.6875,  539836.8750,  537005.0625,  524256.2500],
        [ 288351.8125,  273593.5000,  273304.2812,  233172.1562,  217426.3906,
          202228.1250,  195355.8594,  161173.8906,  142370.5000,  128564.6094],
        [ 381489.6875,  218823.8594,  217206.7188,  208400.1406,  191469.2031,
          188175.8281,  182190.1719,  177357.7500,  175793.3594,  175033.0469],
        [ 306096.1875,  260190.8125,  247612.5469,  217198.2188,  190652.5469,
          178624.8906,  163460.0625,  158795.3281,  148713.8906,  148435.4688],
        [ 269114.8125,  254717.9531,  206137.0781,  189653.3438,  181304.2969,
          171328.8594,  168981.6094,  165946.7969,  163358.1406,  148876.7969],
        [ 189272.4531,  154565.7344,  122156.2891,  118871.1953,  104664.4297,
          104478.5391,   88450.1172,   70294.8516,   63248.6250,   60948.4766],
        [ 139356.0781,   98848.3203,   79273.1562,   74760.1484,   68743.0000,
           68670.8594,   65191.3477,   54739.2109,   51262.0273,   50252.5547],
        [ 377209.5625,  295225.0938,  280186.3125,  267327.0938,  263611.8750,
          260843.2344,  222113.0312,  221009.8594,  219399.3594,  213282.8125],
        [  98476.3750,   73045.5312,   68014.4062,   61869.6797,   54111.0742,
           51483.1328,   39682.1719,   38843.8711,   34074.1250,   29506.4766],
        [ 143966.5781,  138165.7500,  106119.5859,  105027.3984,   96014.8984,
           95384.4297,   94326.5000,   87944.4375,   78474.9297,   73903.6797],
        [ 334847.2812,  221427.5781,  171476.6250,  145446.5156,  143260.9688,
          139093.5781,  123638.5547,  122123.4375,  114374.0234,  109705.8984],
        [ 269071.1875,  196217.9219,  180181.8281,  178672.5938,  152080.4844,
          133739.7344,  123521.5234,  106367.4219,  104991.7422,   97899.1875],
        [ 319569.5938,  306495.2188,  171994.5000,  113469.5547,  107819.5156,
           99533.1875,   94838.0156,   88638.2422,   88559.4141,   82944.4844],
        [ 537456.9375,  421877.6875,  388327.7812,  343614.0625,  228162.5156,
          212238.8750,  203984.7500,  191759.9531,  166731.2344,  156628.7656],
        [ 450738.8438,  263848.0312,  248126.2188,  214713.8438,  195019.6875,
          191474.8594,  168887.3594,  166343.5469,  158669.9688,  156757.1250],
        [ 533164.5625,  385441.4062,  369300.3438,  366679.5000,  356979.8750,
          340765.1562,  340765.1562,  327774.0938,  325326.3438,  320482.1250],
        [ 377587.1250,  284480.2812,  280603.7188,  239387.6406,  232310.0781,
          230116.7500,  219560.0938,  219043.0938,  217865.5938,  212597.6406],
        [ 408035.0625,  272269.6875,  239828.8906,  232609.7969,  212562.9844,
          202983.0312,  166949.0625,  137647.9531,  130485.5078,  127834.3516],
        [ 674567.0625,  281565.8125,  249512.7188,  221430.7344,  201201.0625,
          189576.6719,  151158.9844,  143566.9062,  141046.7656,  119756.4531],
        [ 414824.4688,  213671.2500,  199975.8125,  171527.8125,  142166.4375,
          138852.0938,  128051.6641,  127626.8984,  115268.8594,  112368.4609],
        [ 372927.2812,  116231.0000,  111594.8438,   93686.8281,   80979.5078,
           80059.9609,   79564.1562,   75893.5000,   72404.2031,   64336.5508],
        [ 302604.2500,  234880.4062,  205099.3594,  198017.1250,  179056.0625,
          169781.8594,  163826.6562,  162980.9375,  162317.6719,  161456.9531],
        [ 197353.7031,   64999.5234,   64188.1172,   63459.3672,   54474.6602,
           36683.6719,   33350.0312,   32099.5840,   31746.7969,   30754.8281],
        [ 298573.5312,  202424.1562,  166080.2812,  125061.6094,  109271.3125,
          107949.7734,   83148.8984,   81027.1719,   79212.7734,   74326.3516],
        [ 157103.7344,  100895.9297,   84492.4609,   67972.0547,   66132.5781,
           60511.3242,   57635.5586,   45983.2500,   44214.7852,   41328.9375]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[267796.0938,      0.0000],
         [189859.4688,      0.0000],
         [158482.5938,      0.0000],
         ...,
         [125598.5156,      0.0000],
         [122446.7188,      0.0000],
         [     0.0000, 117822.4609]],

        [[695582.6875,      0.0000],
         [580819.2500,      0.0000],
         [571576.1875,      0.0000],
         ...,
         [515685.0938,      0.0000],
         [499078.4688,      0.0000],
         [494428.7812,      0.0000]],

        [[412099.8125,      0.0000],
         [367262.5625,      0.0000],
         [269488.0000,      0.0000],
         ...,
         [156781.3438,      0.0000],
         [151775.6406,      0.0000],
         [136644.0000,      0.0000]],

        ...,

        [[     0.0000, 197353.7031],
         [ 64999.5234,      0.0000],
         [ 64188.1172,      0.0000],
         ...,
         [ 32099.5840,      0.0000],
         [ 31746.7969,      0.0000],
         [ 30754.8281,      0.0000]],

        [[298573.5312,      0.0000],
         [     0.0000, 202424.1562],
         [166080.2812,      0.0000],
         ...,
         [ 81027.1719,      0.0000],
         [ 79212.7734,      0.0000],
         [ 74326.3516,      0.0000]],

        [[     0.0000, 157103.7344],
         [     0.0000, 100895.9297],
         [     0.0000,  84492.4609],
         ...,
         [ 45983.2500,      0.0000],
         [     0.0000,  44214.7852],
         [     0.0000,  41328.9375]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1433841.5000,  117822.4609],
        [5530557.0000,       0.0000],
        [2312930.2500,       0.0000],
        [3204682.7500,       0.0000],
        [ 453499.9375,  642880.3750],
        [ 774594.9375,  144412.0625],
        [1703404.0000,  160685.5469],
        [1812582.0000,       0.0000],
        [3196772.7500,       0.0000],
        [3084926.5000,       0.0000],
        [3610367.0000,       0.0000],
        [1485746.0000,       0.0000],
        [3043104.2500,       0.0000],
        [7502125.5000,       0.0000],
        [5301202.5000,       0.0000],
        [4592354.0000,       0.0000],
        [5786878.0000,       0.0000],
        [6964798.0000,       0.0000],
        [3737204.5000,       0.0000],
        [3184092.7500,  329027.1562],
        [5818342.0000,       0.0000],
        [4121451.0000,       0.0000],
        [6256848.5000,       0.0000],
        [5606745.0000,       0.0000],
        [3099897.5000,       0.0000],
        [5065746.0000,       0.0000],
        [4270445.0000,       0.0000],
        [5957632.5000,       0.0000],
        [5426622.0000,       0.0000],
        [5162621.5000,       0.0000],
        [6636679.5000,       0.0000],
        [4335467.5000,       0.0000],
        [2078228.3750,       0.0000],
        [2395438.2500,  597371.8750],
        [1786145.5000, 2161866.7500],
        [1024722.6250, 1895978.0000],
        [1094409.6250, 1106608.0000],
        [1323835.1250, 1200721.5000],
        [ 709528.8750, 1408162.6250],
        [5645153.0000,       0.0000],
        [2115541.2500,       0.0000],
        [2115939.7500,       0.0000],
        [2019780.0000,       0.0000],
        [1919419.6250,       0.0000],
        [ 235907.7500,  841042.9375],
        [ 328627.3750,  422469.3125],
        [2242998.7500,  377209.5625],
        [ 345991.7500,  203115.0938],
        [ 451470.0312,  567858.1250],
        [ 444553.1875, 1180841.2500],
        [1304012.1250,  238731.4688],
        [ 675802.4375,  798059.3125],
        [1681292.0000, 1169490.5000],
        [2023104.6250,  191474.8594],
        [3666678.5000,       0.0000],
        [2513552.0000,       0.0000],
        [ 975996.0000, 1155210.3750],
        [1669185.6250,  704197.6250],
        [ 951664.3125,  812669.4375],
        [ 535747.6875,  611930.1875],
        [1223480.5000,  716540.7500],
        [ 411756.5625,  197353.7031],
        [ 907430.6250,  419645.2500],
        [ 106494.5781,  619776.0625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 441/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:15, 60.54s/it]  7%|▋         | 2/30 [01:02<12:09, 26.05s/it] 10%|█         | 3/30 [01:03<06:31, 14.50s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.07s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.07s/it] 20%|██        | 6/30 [01:05<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.3129512866338096
Epoch 442/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:35, 59.14s/it]  7%|▋         | 2/30 [00:59<11:34, 24.79s/it] 10%|█         | 3/30 [01:00<06:12, 13.81s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.66s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.80s/it] 20%|██        | 6/30 [01:02<01:38,  4.08s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 2.3249545017878215
Epoch 443/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:56, 61.96s/it]  7%|▋         | 2/30 [01:04<12:37, 27.06s/it] 10%|█         | 3/30 [01:05<06:46, 15.05s/it] 13%|█▎        | 4/30 [01:06<04:04,  9.40s/it] 17%|█▋        | 5/30 [01:06<02:37,  6.28s/it] 20%|██        | 6/30 [01:07<01:45,  4.40s/it] 23%|██▎       | 7/30 [01:08<01:13,  3.21s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.42s/it] 30%|███       | 9/30 [01:09<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:10<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:11<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:12<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:13<00:14,  1.07it/s] 50%|█████     | 15/30 [01:14<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:15<00:10,  1.23it/s] 60%|██████    | 18/30 [01:16<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:17<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:18<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:19<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:20<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:21<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:22<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:23<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  1.34it/s]100%|██████████| 30/30 [01:25<00:00,  2.86s/it]
Epoch loss is 2.3323914130528767
Epoch 444/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:06, 58.14s/it]  7%|▋         | 2/30 [01:00<11:49, 25.35s/it] 10%|█         | 3/30 [01:01<06:21, 14.12s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.84s/it] 17%|█▋        | 5/30 [01:02<02:28,  5.92s/it] 20%|██        | 6/30 [01:03<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:05<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.310451610883077
Epoch 445/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:31, 56.94s/it]  7%|▋         | 2/30 [00:58<11:16, 24.17s/it] 10%|█         | 3/30 [00:58<06:03, 13.48s/it] 13%|█▎        | 4/30 [00:59<03:40,  8.47s/it] 17%|█▋        | 5/30 [01:00<02:23,  5.75s/it] 20%|██        | 6/30 [01:01<01:37,  4.05s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.97s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.26s/it] 30%|███       | 9/30 [01:03<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.65s/it]
Epoch loss is 2.289411441485087
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0236,  0.0010,  0.0131,  ..., -0.0057,  0.0173,  0.0040],
        [-0.0022,  0.0096,  0.0285,  ...,  0.0171,  0.0040, -0.0149],
        [-0.0293, -0.0380,  0.0247,  ...,  0.0832, -0.0044, -0.0144],
        ...,
        [ 0.0061, -0.0034,  0.0098,  ..., -0.0241, -0.0053, -0.0030],
        [-0.0332,  0.0097, -0.0035,  ...,  0.0050,  0.0169, -0.0106],
        [-0.0328, -0.0138,  0.0192,  ...,  0.0413,  0.0359, -0.0286]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8747, 0.8508, 0.8379, 0.8367, 0.8306, 0.8281, 0.8260, 0.8217, 0.8199,
         0.8173],
        [0.9416, 0.9290, 0.9279, 0.9272, 0.9261, 0.9219, 0.9217, 0.9206, 0.9183,
         0.9177],
        [0.9050, 0.8969, 0.8751, 0.8707, 0.8560, 0.8482, 0.8460, 0.8372, 0.8350,
         0.8277],
        [0.9561, 0.8860, 0.8823, 0.8791, 0.8787, 0.8747, 0.8651, 0.8633, 0.8614,
         0.8605],
        [0.8426, 0.8315, 0.8306, 0.8141, 0.8055, 0.8051, 0.8013, 0.7961, 0.7883,
         0.7811],
        [0.8525, 0.8176, 0.8085, 0.7964, 0.7931, 0.7880, 0.7826, 0.7776, 0.7684,
         0.7656],
        [0.8645, 0.8618, 0.8536, 0.8516, 0.8482, 0.8465, 0.8451, 0.8408, 0.8388,
         0.8364],
        [0.8918, 0.8720, 0.8668, 0.8558, 0.8485, 0.8301, 0.8257, 0.8170, 0.8052,
         0.8016],
        [0.9076, 0.9016, 0.8870, 0.8837, 0.8836, 0.8828, 0.8825, 0.8793, 0.8785,
         0.8784],
        [0.9487, 0.8831, 0.8820, 0.8799, 0.8730, 0.8712, 0.8671, 0.8669, 0.8608,
         0.8588],
        [0.9445, 0.9036, 0.8926, 0.8891, 0.8889, 0.8864, 0.8834, 0.8818, 0.8792,
         0.8772],
        [0.8868, 0.8716, 0.8549, 0.8468, 0.8239, 0.7977, 0.7970, 0.7942, 0.7818,
         0.7769],
        [0.9125, 0.9056, 0.9016, 0.8899, 0.8794, 0.8759, 0.8679, 0.8633, 0.8616,
         0.8506],
        [0.9565, 0.9545, 0.9510, 0.9508, 0.9480, 0.9461, 0.9434, 0.9415, 0.9395,
         0.9344],
        [0.9417, 0.9280, 0.9268, 0.9255, 0.9226, 0.9181, 0.9175, 0.9144, 0.9137,
         0.9123],
        [0.9259, 0.9214, 0.9205, 0.9133, 0.9113, 0.9100, 0.9080, 0.9047, 0.9040,
         0.9022],
        [0.9516, 0.9458, 0.9344, 0.9316, 0.9253, 0.9246, 0.9175, 0.9172, 0.9138,
         0.9132],
        [0.9614, 0.9554, 0.9512, 0.9396, 0.9391, 0.9346, 0.9336, 0.9321, 0.9317,
         0.9301],
        [0.9252, 0.9131, 0.8998, 0.8977, 0.8928, 0.8919, 0.8902, 0.8889, 0.8858,
         0.8839],
        [0.9026, 0.8978, 0.8964, 0.8963, 0.8944, 0.8937, 0.8909, 0.8892, 0.8880,
         0.8868],
        [0.9401, 0.9321, 0.9320, 0.9299, 0.9294, 0.9290, 0.9257, 0.9241, 0.9238,
         0.9227],
        [0.9253, 0.9107, 0.9100, 0.9076, 0.9052, 0.9042, 0.9037, 0.9015, 0.8885,
         0.8843],
        [0.9674, 0.9483, 0.9417, 0.9383, 0.9331, 0.9267, 0.9248, 0.9174, 0.9124,
         0.9113],
        [0.9406, 0.9334, 0.9295, 0.9262, 0.9238, 0.9229, 0.9223, 0.9220, 0.9217,
         0.9202],
        [0.9531, 0.8987, 0.8881, 0.8790, 0.8647, 0.8601, 0.8599, 0.8577, 0.8575,
         0.8567],
        [0.9353, 0.9312, 0.9291, 0.9228, 0.9214, 0.9172, 0.9131, 0.9095, 0.9031,
         0.9027],
        [0.9230, 0.9211, 0.9188, 0.9142, 0.9067, 0.8990, 0.8968, 0.8962, 0.8952,
         0.8941],
        [0.9595, 0.9479, 0.9420, 0.9253, 0.9241, 0.9234, 0.9187, 0.9176, 0.9174,
         0.9160],
        [0.9424, 0.9355, 0.9258, 0.9246, 0.9238, 0.9218, 0.9201, 0.9170, 0.9131,
         0.9118],
        [0.9307, 0.9261, 0.9249, 0.9244, 0.9211, 0.9178, 0.9169, 0.9168, 0.9132,
         0.9132],
        [0.9444, 0.9443, 0.9440, 0.9433, 0.9417, 0.9410, 0.9334, 0.9309, 0.9297,
         0.9276],
        [0.9367, 0.9204, 0.9191, 0.9090, 0.9014, 0.9011, 0.8967, 0.8963, 0.8957,
         0.8951],
        [0.8738, 0.8684, 0.8653, 0.8577, 0.8564, 0.8497, 0.8486, 0.8486, 0.8476,
         0.8475],
        [0.9000, 0.8897, 0.8847, 0.8826, 0.8800, 0.8785, 0.8784, 0.8771, 0.8755,
         0.8744],
        [0.9181, 0.9139, 0.9130, 0.9075, 0.9003, 0.8998, 0.8961, 0.8893, 0.8864,
         0.8860],
        [0.9050, 0.8947, 0.8922, 0.8844, 0.8807, 0.8755, 0.8754, 0.8691, 0.8585,
         0.8574],
        [0.9473, 0.8673, 0.8637, 0.8504, 0.8462, 0.8299, 0.8204, 0.8180, 0.8155,
         0.8139],
        [0.8992, 0.8804, 0.8785, 0.8752, 0.8727, 0.8692, 0.8546, 0.8543, 0.8538,
         0.8518],
        [0.8927, 0.8890, 0.8683, 0.8608, 0.8484, 0.8447, 0.8417, 0.8363, 0.8348,
         0.8330],
        [0.9340, 0.9305, 0.9289, 0.9287, 0.9274, 0.9254, 0.9245, 0.9238, 0.9236,
         0.9218],
        [0.8799, 0.8762, 0.8762, 0.8650, 0.8602, 0.8551, 0.8526, 0.8392, 0.8305,
         0.8233],
        [0.8995, 0.8606, 0.8601, 0.8572, 0.8512, 0.8500, 0.8477, 0.8458, 0.8453,
         0.8450],
        [0.8841, 0.8727, 0.8692, 0.8601, 0.8509, 0.8463, 0.8401, 0.8381, 0.8336,
         0.8335],
        [0.8749, 0.8712, 0.8564, 0.8505, 0.8474, 0.8435, 0.8424, 0.8412, 0.8402,
         0.8337],
        [0.8504, 0.8363, 0.8197, 0.8177, 0.8089, 0.8088, 0.7971, 0.7809, 0.7737,
         0.7710],
        [0.8290, 0.8049, 0.7894, 0.7854, 0.7796, 0.7795, 0.7758, 0.7637, 0.7591,
         0.7577],
        [0.8987, 0.8814, 0.8778, 0.8745, 0.8735, 0.8727, 0.8615, 0.8611, 0.8606,
         0.8586],
        [0.8046, 0.7838, 0.7787, 0.7721, 0.7629, 0.7592, 0.7408, 0.7394, 0.7304,
         0.7204],
        [0.8311, 0.8283, 0.8100, 0.8092, 0.8029, 0.8025, 0.8018, 0.7967, 0.7888,
         0.7845],
        [0.8903, 0.8614, 0.8434, 0.8319, 0.8309, 0.8288, 0.8206, 0.8197, 0.8151,
         0.8123],
        [0.8750, 0.8530, 0.8470, 0.8463, 0.8351, 0.8262, 0.8206, 0.8100, 0.8092,
         0.8043],
        [0.8871, 0.8842, 0.8437, 0.8145, 0.8109, 0.8052, 0.8020, 0.7973, 0.7972,
         0.7925],
        [0.9235, 0.9066, 0.9007, 0.8922, 0.8635, 0.8585, 0.8557, 0.8512, 0.8416,
         0.8371],
        [0.9112, 0.8737, 0.8694, 0.8593, 0.8526, 0.8512, 0.8425, 0.8413, 0.8380,
         0.8372],
        [0.9229, 0.9002, 0.8972, 0.8967, 0.8949, 0.8916, 0.8916, 0.8889, 0.8884,
         0.8873],
        [0.8987, 0.8789, 0.8780, 0.8669, 0.8648, 0.8641, 0.8609, 0.8606, 0.8602,
         0.8585],
        [0.9041, 0.8759, 0.8669, 0.8648, 0.8586, 0.8553, 0.8417, 0.8281, 0.8242,
         0.8229],
        [0.9395, 0.8783, 0.8698, 0.8614, 0.8547, 0.8506, 0.8347, 0.8310, 0.8297,
         0.8183],
        [0.9053, 0.8590, 0.8543, 0.8435, 0.8305, 0.8287, 0.8230, 0.8228, 0.8156,
         0.8138],
        [0.8980, 0.8162, 0.8134, 0.8011, 0.7909, 0.7901, 0.7897, 0.7864, 0.7831,
         0.7748],
        [0.8834, 0.8655, 0.8560, 0.8536, 0.8465, 0.8429, 0.8403, 0.8399, 0.8395,
         0.8392],
        [0.8534, 0.7757, 0.7747, 0.7739, 0.7632, 0.7357, 0.7289, 0.7263, 0.7255,
         0.7232],
        [0.8824, 0.8552, 0.8412, 0.8215, 0.8120, 0.8110, 0.7926, 0.7909, 0.7893,
         0.7849],
        [0.8374, 0.8063, 0.7939, 0.7787, 0.7768, 0.7705, 0.7672, 0.7512, 0.7486,
         0.7438]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 267033.5312,  189871.2344,  157999.0781,  155190.5156,  142388.9688,
          137392.8750,  133313.1406,  125275.2812,  122198.3594,  117631.3828],
        [ 694988.5000,  579996.1250,  571152.8125,  565860.7500,  556814.8750,
          524185.7812,  523195.4062,  514863.5000,  498369.3438,  493577.4375],
        [ 411685.0000,  366695.9062,  268620.1875,  252239.2344,  204646.6562,
          183101.8438,  177226.5469,  156427.2344,  151519.5156,  136467.1562],
        [ 854146.4375,  314083.0000,  297903.4062,  284640.1250,  282825.2812,
          267306.9375,  233012.5469,  227134.5312,  221006.2812,  218015.2500],
        [ 168979.6875,  144086.5000,  142353.2500,  112348.6406,   99381.0469,
           98836.8203,   93631.4531,   86963.6250,   77790.0938,   70191.1484],
        [ 194617.0625,  118193.7422,  103751.4219,   87271.7812,   83242.9141,
           77395.3047,   71719.5781,   66710.8438,   58495.3633,   56235.7188],
        [ 231044.2812,  222201.6094,  197750.8438,  192220.2500,  182910.5781,
          178671.0781,  174959.7812,  164652.8750,  160050.0781,  154555.8594],
        [ 341179.7500,  257119.0312,  238718.9844,  203895.2812,  183894.2344,
          141193.4688,  132700.3438,  117155.6875,   99038.8359,   94071.0078],
        [ 427237.5938,  392339.1250,  318533.8438,  304030.5938,  303246.1562,
          299923.0938,  298557.2812,  285242.8750,  281981.2500,  281602.5938],
        [ 769413.5000,  301150.9688,  296463.0938,  287930.0000,  260692.0469,
          254026.1094,  239681.4219,  239057.2969,  219009.4688,  212816.5312],
        [ 724691.0625,  403677.6250,  345243.2812,  328092.1562,  327226.2812,
          315731.4375,  302603.6562,  295764.4375,  284922.8438,  276860.2500],
        [ 317417.5938,  255470.4219,  201405.1250,  179259.8906,  129304.8359,
           88982.3672,   88053.3672,   84554.6953,   70899.1016,   66100.9844],
        [ 458479.2500,  415421.0938,  392356.3438,  331825.0938,  285689.3125,
          271675.6875,  242381.6094,  227040.9844,  221473.4062,  189305.8438],
        [ 859452.6250,  835475.6875,  794322.0000,  791993.0000,  761867.2500,
          740986.3750,  712877.6250,  694259.1875,  673882.9375,  626830.6250],
        [ 696310.0625,  571753.4375,  562787.7500,  551984.0000,  529349.5625,
          496863.0938,  492676.8750,  471439.5625,  466534.0625,  457038.6250],
        [ 555527.0625,  521002.5938,  513747.6562,  463482.1250,  450849.7500,
          442479.6562,  430091.8750,  410016.2500,  405826.1250,  395816.2188],
        [ 801729.3125,  737584.5625,  627325.1875,  602397.5625,  550543.0000,
          545141.8125,  492511.9688,  490337.8750,  466974.7500,  463232.0000],
        [ 921922.6875,  846190.1875,  797308.4375,  674862.4375,  670131.2500,
          628911.9375,  619890.0625,  606600.7500,  603618.4375,  589451.8125],
        [ 549897.0625,  462588.7812,  382397.7500,  370936.0000,  345872.4062,
          341399.1250,  333256.0000,  327352.6562,  313214.3438,  304648.2500],
        [ 397921.7188,  371564.0938,  364292.5938,  363756.2500,  353885.1562,
          350692.7812,  336924.2500,  328759.2812,  322933.4688,  317515.0625],
        [ 679894.0625,  606342.8125,  605742.3125,  588269.1250,  583588.1875,
          580410.5625,  553429.3750,  541310.7500,  538793.8125,  530512.4375],
        [ 550443.2500,  446551.8125,  442318.0312,  427444.1875,  413215.1250,
          407203.5625,  404571.3750,  392104.5938,  325312.6875,  306537.8750],
        [1004052.8750,  764634.0000,  695866.5625,  662497.0625,  615462.1250,
          561957.5000,  546797.5625,  491619.4062,  458002.4688,  450430.7188],
        [ 684948.8750,  618368.5000,  584869.0625,  557256.3125,  538710.0625,
          531623.0625,  527078.1875,  524933.1875,  523019.3125,  511761.3438],
        [ 819288.7500,  376685.4375,  323755.8750,  284206.1562,  231534.8438,
          216937.5938,  216366.5000,  209538.0781,  209011.3906,  206559.1875],
        [ 635134.6250,  599192.3750,  581563.0625,  530914.7500,  520511.9375,
          490070.4375,  462663.7812,  439222.5938,  400655.0000,  398756.3125],
        [ 532712.1875,  518749.2188,  501334.8750,  470031.7188,  422179.5938,
          378225.0312,  366500.8438,  363336.3750,  357944.6562,  352463.7812],
        [ 896952.8125,  760140.6875,  698910.0625,  550837.1250,  541285.5000,
          536119.3125,  500898.5312,  493083.9375,  491722.1250,  482307.9062],
        [ 703049.4375,  637075.8750,  554552.0625,  544691.2500,  539202.4375,
          523691.5938,  511159.4375,  489228.5312,  462621.4375,  454007.6875],
        [ 594419.5000,  556654.0000,  547314.6250,  543823.4375,  518632.9688,
          494727.8125,  488451.8125,  487490.8438,  463261.5938,  462868.5625],
        [ 722697.8750,  722362.3125,  719364.1875,  711624.3750,  695863.9375,
          689045.6875,  617936.3750,  596054.3750,  586385.3750,  569062.3125],
        [ 648210.8750,  513615.8750,  504145.8750,  435958.7500,  391296.5938,
          389519.8750,  365655.2500,  363832.9062,  360419.8438,  357523.3125],
        [ 263884.2500,  244241.4062,  233512.2031,  209481.3438,  205754.0938,
          186858.0469,  184092.5156,  184019.3125,  181476.0625,  181160.1562],
        [ 383338.3125,  330872.3438,  308082.9688,  299215.7188,  288012.1250,
          282148.0312,  281692.8438,  276700.0312,  270214.7500,  266031.2812],
        [ 496425.9062,  467609.7812,  461808.6250,  426948.3750,  385061.5312,
          382539.6562,  362581.7812,  329002.0625,  315898.5938,  313852.7188],
        [ 412053.4375,  355744.1875,  342985.7812,  306703.4062,  291294.5938,
          270099.5938,  269949.4688,  246492.2344,  211888.5938,  208677.7656],
        [ 753582.5000,  240285.6094,  228221.2656,  188918.0938,  177934.4375,
          140812.3750,  122948.3516,  118938.5547,  114621.7812,  112055.8672],
        [ 379346.6562,  289882.3750,  282200.7812,  268996.5312,  259543.7344,
          246834.0469,  200548.3906,  199562.0156,  198293.9844,  192772.8125],
        [ 345714.7500,  327595.3438,  243704.8594,  218996.9375,  183512.6719,
          174101.7500,  166806.4688,  154430.4688,  151075.1094,  147252.1094],
        [ 623744.0625,  593154.3750,  579408.4375,  577983.5000,  567643.3125,
          551337.4375,  543935.4375,  539080.0625,  537024.0000,  523408.0312],
        [ 287784.5000,  272988.5625,  272828.5000,  232695.4375,  217068.7969,
          201827.9375,  194948.0938,  160836.5625,  142025.2344,  128236.0703],
        [ 381043.5625,  218592.1250,  216788.6875,  208031.5938,  190956.0625,
          187728.2500,  181769.8281,  176775.8594,  175434.4531,  174716.1875],
        [ 305558.0938,  259566.0156,  246997.2188,  216846.3750,  190275.6406,
          178199.8594,  162987.0000,  158418.5312,  148469.4375,  148233.1719],
        [ 268023.2188,  254302.6875,  205580.1094,  188972.3438,  180925.1719,
          171052.4531,  168370.1875,  165607.5312,  163115.1406,  148775.4531],
        [ 188802.4688,  154390.1250,  121815.7812,  118405.2734,  104420.9609,
          104184.6250,   88189.8438,   69998.6953,   63121.1211,   60704.0312],
        [ 139109.3594,   98600.9922,   79012.2422,   74625.6562,   68629.9375,
           68576.7500,   65085.9297,   54689.1719,   51204.4727,   50223.2344],
        [ 376389.9062,  294165.8125,  279317.9062,  266499.0312,  262644.2188,
          259685.6250,  221328.5625,  220073.2812,  218593.8125,  212398.6250],
        [  98201.1250,   72920.9453,   67836.0625,   61674.6211,   54075.2266,
           51348.1406,   39475.8203,   38651.5352,   33982.2227,   29493.1113],
        [ 143409.5469,  137746.3125,  105950.0000,  104805.7656,   95757.2891,
           95266.7891,   94263.7266,   87702.0469,   78324.3438,   73675.7422],
        [ 334089.7188,  221017.0156,  170968.1406,  144872.8281,  142917.4844,
          138772.6719,  123362.8281,  121826.0078,  114104.3672,  109505.5156],
        [ 268151.8125,  195945.6719,  179850.6562,  178197.4844,  151777.5156,
          133564.2031,  123366.6016,  106035.9297,  104845.6562,   97722.3203],
        [ 319039.4375,  306017.9688,  171533.2188,  113068.6953,  107409.3047,
           99027.4062,   94594.7656,   88454.4141,   88316.2656,   82619.7734],
        [ 536400.5625,  421230.8750,  387117.9375,  343003.1250,  227576.6250,
          211866.9688,  203586.1562,  191083.0469,  166435.5781,  156225.9688],
        [ 450194.5625,  263560.3438,  247778.3906,  214379.1094,  194736.0625,
          190945.3125,  168691.6250,  165745.4688,  158248.5000,  156373.9844],
        [ 532107.0000,  384643.8438,  368558.3125,  365959.5000,  356458.6875,
          340071.7188,  340071.7188,  327171.0312,  324861.2812,  319726.8750],
        [ 376690.0938,  283901.0938,  280098.9375,  238904.8281,  231801.7344,
          229632.7031,  219364.6250,  218409.1875,  217353.6250,  211901.1250],
        [ 406686.2500,  271830.4375,  239142.5938,  232022.8906,  212314.3750,
          202480.1406,  166866.4531,  137286.3906,  129935.3984,  127479.9531],
        [ 674018.5000,  281160.3438,  248953.2031,  220938.2031,  200793.3438,
          189295.7344,  150896.2656,  143090.2969,  140554.2344,  119347.0312],
        [ 413743.9062,  213379.8594,  199584.8594,  171112.6562,  142019.8281,
          138525.4062,  127610.8359,  127341.4375,  114884.6406,  111910.3047],
        [ 372470.2188,  115903.6797,  111288.3359,   93390.2188,   80720.2812,
           79750.6484,   79316.8672,   75639.7344,   72202.8594,   64105.2891],
        [ 302477.0000,  234346.3438,  204671.0625,  197633.2031,  178530.7188,
          169603.0469,  163408.1562,  162541.3594,  161642.1406,  160923.0781],
        [ 197164.0781,   64933.3516,   64014.3242,   63286.5781,   54332.8125,
           36686.5039,   33269.7852,   32089.7910,   31694.7949,   30674.2148],
        [ 298165.7500,  202222.5156,  165602.7969,  124942.5234,  109065.3828,
          107585.6406,   82730.5547,   80717.8203,   78915.1719,   74057.9062],
        [ 156890.6719,  100603.6562,   84277.7500,   67775.0859,   66012.6641,
           60266.3281,   57532.2578,   45802.3164,   44081.9922,   41191.4961]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[267033.5312,      0.0000],
         [189871.2344,      0.0000],
         [157999.0781,      0.0000],
         ...,
         [125275.2812,      0.0000],
         [122198.3594,      0.0000],
         [117631.3828,      0.0000]],

        [[694988.5000,      0.0000],
         [579996.1250,      0.0000],
         [571152.8125,      0.0000],
         ...,
         [514863.5000,      0.0000],
         [498369.3438,      0.0000],
         [493577.4375,      0.0000]],

        [[411685.0000,      0.0000],
         [366695.9062,      0.0000],
         [268620.1875,      0.0000],
         ...,
         [156427.2344,      0.0000],
         [151519.5156,      0.0000],
         [136467.1562,      0.0000]],

        ...,

        [[     0.0000, 197164.0781],
         [ 64933.3516,      0.0000],
         [ 64014.3242,      0.0000],
         ...,
         [ 32089.7910,      0.0000],
         [ 31694.7949,      0.0000],
         [ 30674.2148,      0.0000]],

        [[298165.7500,      0.0000],
         [     0.0000, 202222.5156],
         [165602.7969,      0.0000],
         ...,
         [ 80717.8203,      0.0000],
         [ 78915.1719,      0.0000],
         [ 74057.9062,      0.0000]],

        [[     0.0000, 156890.6719],
         [     0.0000, 100603.6562],
         [     0.0000,  84277.7500],
         ...,
         [ 45802.3164,      0.0000],
         [     0.0000,  44081.9922],
         [     0.0000,  41191.4961]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1548294.5000,       0.0000],
        [5523004.0000,       0.0000],
        [2308629.2500,       0.0000],
        [3200074.0000,       0.0000],
        [ 452798.1250,  641764.1250],
        [ 773527.6250,  144106.1562],
        [1698967.1250,  160050.0781],
        [1808966.5000,       0.0000],
        [3192694.5000,       0.0000],
        [3080240.2500,       0.0000],
        [3604813.0000,       0.0000],
        [1481448.3750,       0.0000],
        [3035648.5000,       0.0000],
        [7491947.0000,       0.0000],
        [5296737.0000,       0.0000],
        [4588839.5000,       0.0000],
        [5777778.0000,       0.0000],
        [6958888.0000,       0.0000],
        [3731562.5000,       0.0000],
        [3179485.2500,  328759.2812],
        [5808293.5000,       0.0000],
        [4115702.2500,       0.0000],
        [6251320.5000,       0.0000],
        [5602567.5000,       0.0000],
        [3093883.7500,       0.0000],
        [5058685.0000,       0.0000],
        [4263478.5000,       0.0000],
        [5952258.0000,       0.0000],
        [5419280.0000,       0.0000],
        [5157645.0000,       0.0000],
        [6630397.0000,       0.0000],
        [4330179.0000,       0.0000],
        [2074479.3750,       0.0000],
        [2390213.5000,  596095.1250],
        [1783143.7500, 2158585.2500],
        [1022802.7500, 1893086.2500],
        [1092394.8750, 1105924.0000],
        [1320212.0000, 1197769.2500],
        [ 708015.1875, 1405175.2500],
        [5636718.5000,       0.0000],
        [1983003.6250,  128236.0703],
        [2111836.5000,       0.0000],
        [2015551.3750,       0.0000],
        [1914724.1250,       0.0000],
        [ 235123.6875,  838909.2500],
        [ 328122.7500,  421635.0000],
        [2234707.0000,  376389.9062],
        [ 344906.2500,  202752.5312],
        [ 450457.9062,  566443.6875],
        [ 443595.2500, 1177841.3750],
        [1301048.0000,  238409.8594],
        [ 673490.6250,  796590.6250],
        [1677342.8750, 1167184.0000],
        [2019708.0000,  190945.3125],
        [3659630.0000,       0.0000],
        [2508058.0000,       0.0000],
        [ 973555.2500, 1152489.6250],
        [1666155.2500,  702891.8750],
        [ 949397.7500,  810716.0000],
        [ 534102.1250,  610686.0000],
        [1220422.0000,  715354.0625],
        [ 410982.1875,  197164.0781],
        [ 905132.5000,  418873.5312],
        [ 106068.6406,  618365.5625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 446/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:01, 60.07s/it]  7%|▋         | 2/30 [01:00<11:44, 25.17s/it] 10%|█         | 3/30 [01:01<06:18, 14.02s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.273213533560435
Epoch 447/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:33, 59.08s/it]  7%|▋         | 2/30 [01:01<11:57, 25.61s/it] 10%|█         | 3/30 [01:02<06:31, 14.51s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.08s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.07s/it] 20%|██        | 6/30 [01:04<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:07<00:38,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.334156886736552
Epoch 448/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:16, 56.43s/it]  7%|▋         | 2/30 [01:00<11:53, 25.48s/it] 10%|█         | 3/30 [01:01<06:28, 14.37s/it] 13%|█▎        | 4/30 [01:02<03:53,  8.99s/it] 17%|█▋        | 5/30 [01:02<02:30,  6.02s/it] 20%|██        | 6/30 [01:03<01:41,  4.23s/it] 23%|██▎       | 7/30 [01:04<01:11,  3.09s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.2809124549229938
Epoch 449/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:05, 58.13s/it]  7%|▋         | 2/30 [01:00<11:41, 25.05s/it] 10%|█         | 3/30 [01:00<06:18, 14.04s/it] 13%|█▎        | 4/30 [01:01<03:48,  8.79s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.3420997142791746
Epoch 450/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:35, 61.23s/it]  7%|▋         | 2/30 [01:01<11:58, 25.65s/it] 10%|█         | 3/30 [01:02<06:25, 14.28s/it] 13%|█▎        | 4/30 [01:03<03:52,  8.94s/it] 17%|█▋        | 5/30 [01:04<02:29,  5.99s/it] 20%|██        | 6/30 [01:04<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.307398541768392
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0236,  0.0009,  0.0131,  ..., -0.0058,  0.0174,  0.0040],
        [-0.0022,  0.0096,  0.0286,  ...,  0.0172,  0.0041, -0.0148],
        [-0.0293, -0.0380,  0.0247,  ...,  0.0833, -0.0044, -0.0144],
        ...,
        [ 0.0060, -0.0034,  0.0098,  ..., -0.0241, -0.0052, -0.0029],
        [-0.0332,  0.0098, -0.0035,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0327, -0.0139,  0.0192,  ...,  0.0413,  0.0359, -0.0285]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8745, 0.8508, 0.8378, 0.8365, 0.8305, 0.8280, 0.8258, 0.8215, 0.8198,
         0.8173],
        [0.9416, 0.9289, 0.9278, 0.9271, 0.9261, 0.9217, 0.9216, 0.9205, 0.9183,
         0.9176],
        [0.9049, 0.8968, 0.8749, 0.8706, 0.8559, 0.8481, 0.8458, 0.8371, 0.8349,
         0.8276],
        [0.9561, 0.8859, 0.8822, 0.8791, 0.8786, 0.8746, 0.8650, 0.8632, 0.8612,
         0.8604],
        [0.8426, 0.8314, 0.8304, 0.8140, 0.8053, 0.8050, 0.8012, 0.7960, 0.7882,
         0.7811],
        [0.8524, 0.8175, 0.8085, 0.7963, 0.7931, 0.7878, 0.7825, 0.7775, 0.7682,
         0.7655],
        [0.8644, 0.8616, 0.8535, 0.8516, 0.8480, 0.8465, 0.8449, 0.8407, 0.8386,
         0.8362],
        [0.8917, 0.8719, 0.8667, 0.8557, 0.8484, 0.8299, 0.8256, 0.8168, 0.8051,
         0.8015],
        [0.9075, 0.9016, 0.8869, 0.8837, 0.8835, 0.8827, 0.8824, 0.8792, 0.8784,
         0.8783],
        [0.9487, 0.8830, 0.8819, 0.8798, 0.8729, 0.8711, 0.8670, 0.8668, 0.8607,
         0.8587],
        [0.9445, 0.9036, 0.8926, 0.8889, 0.8887, 0.8863, 0.8834, 0.8817, 0.8791,
         0.8770],
        [0.8866, 0.8714, 0.8548, 0.8465, 0.8237, 0.7976, 0.7967, 0.7939, 0.7816,
         0.7767],
        [0.9123, 0.9055, 0.9014, 0.8897, 0.8792, 0.8757, 0.8677, 0.8632, 0.8614,
         0.8504],
        [0.9564, 0.9545, 0.9509, 0.9507, 0.9480, 0.9460, 0.9433, 0.9414, 0.9394,
         0.9343],
        [0.9417, 0.9279, 0.9268, 0.9254, 0.9225, 0.9181, 0.9175, 0.9144, 0.9137,
         0.9123],
        [0.9259, 0.9214, 0.9204, 0.9132, 0.9113, 0.9100, 0.9080, 0.9046, 0.9039,
         0.9022],
        [0.9515, 0.9457, 0.9344, 0.9315, 0.9252, 0.9245, 0.9174, 0.9171, 0.9137,
         0.9131],
        [0.9613, 0.9553, 0.9512, 0.9396, 0.9390, 0.9346, 0.9336, 0.9320, 0.9317,
         0.9300],
        [0.9252, 0.9130, 0.8996, 0.8976, 0.8927, 0.8918, 0.8901, 0.8888, 0.8857,
         0.8839],
        [0.9025, 0.8977, 0.8963, 0.8962, 0.8943, 0.8937, 0.8908, 0.8892, 0.8879,
         0.8867],
        [0.9400, 0.9319, 0.9319, 0.9299, 0.9293, 0.9289, 0.9256, 0.9241, 0.9237,
         0.9226],
        [0.9253, 0.9106, 0.9099, 0.9075, 0.9052, 0.9041, 0.9036, 0.9015, 0.8884,
         0.8842],
        [0.9673, 0.9483, 0.9417, 0.9382, 0.9331, 0.9267, 0.9248, 0.9173, 0.9124,
         0.9112],
        [0.9406, 0.9334, 0.9295, 0.9261, 0.9238, 0.9228, 0.9222, 0.9219, 0.9216,
         0.9202],
        [0.9531, 0.8986, 0.8880, 0.8789, 0.8646, 0.8599, 0.8598, 0.8575, 0.8574,
         0.8566],
        [0.9352, 0.9312, 0.9290, 0.9227, 0.9213, 0.9171, 0.9130, 0.9094, 0.9030,
         0.9026],
        [0.9229, 0.9210, 0.9186, 0.9141, 0.9066, 0.8989, 0.8967, 0.8961, 0.8951,
         0.8940],
        [0.9594, 0.9479, 0.9420, 0.9253, 0.9240, 0.9234, 0.9186, 0.9176, 0.9173,
         0.9160],
        [0.9423, 0.9355, 0.9258, 0.9245, 0.9238, 0.9217, 0.9200, 0.9170, 0.9130,
         0.9117],
        [0.9306, 0.9260, 0.9249, 0.9244, 0.9211, 0.9177, 0.9169, 0.9167, 0.9132,
         0.9131],
        [0.9443, 0.9443, 0.9439, 0.9432, 0.9416, 0.9410, 0.9333, 0.9308, 0.9297,
         0.9276],
        [0.9367, 0.9204, 0.9191, 0.9089, 0.9014, 0.9010, 0.8966, 0.8962, 0.8956,
         0.8950],
        [0.8737, 0.8683, 0.8651, 0.8576, 0.8563, 0.8496, 0.8485, 0.8485, 0.8475,
         0.8473],
        [0.8998, 0.8895, 0.8845, 0.8825, 0.8798, 0.8784, 0.8782, 0.8771, 0.8754,
         0.8743],
        [0.9180, 0.9138, 0.9129, 0.9074, 0.9003, 0.8997, 0.8960, 0.8892, 0.8863,
         0.8859],
        [0.9050, 0.8946, 0.8921, 0.8843, 0.8806, 0.8754, 0.8753, 0.8689, 0.8584,
         0.8573],
        [0.9473, 0.8672, 0.8636, 0.8503, 0.8461, 0.8297, 0.8202, 0.8179, 0.8154,
         0.8136],
        [0.8991, 0.8802, 0.8783, 0.8751, 0.8725, 0.8691, 0.8544, 0.8541, 0.8537,
         0.8517],
        [0.8927, 0.8889, 0.8681, 0.8607, 0.8483, 0.8445, 0.8416, 0.8361, 0.8347,
         0.8329],
        [0.9339, 0.9304, 0.9288, 0.9286, 0.9273, 0.9253, 0.9244, 0.9238, 0.9236,
         0.9217],
        [0.8798, 0.8761, 0.8761, 0.8649, 0.8601, 0.8549, 0.8525, 0.8390, 0.8303,
         0.8232],
        [0.8995, 0.8606, 0.8600, 0.8571, 0.8510, 0.8499, 0.8476, 0.8456, 0.8451,
         0.8449],
        [0.8840, 0.8725, 0.8691, 0.8600, 0.8508, 0.8462, 0.8399, 0.8380, 0.8335,
         0.8334],
        [0.8747, 0.8712, 0.8562, 0.8502, 0.8473, 0.8434, 0.8422, 0.8411, 0.8401,
         0.8337],
        [0.8502, 0.8362, 0.8195, 0.8175, 0.8088, 0.8086, 0.7969, 0.7807, 0.7736,
         0.7707],
        [0.8289, 0.8048, 0.7892, 0.7853, 0.7795, 0.7794, 0.7758, 0.7636, 0.7590,
         0.7577],
        [0.8986, 0.8812, 0.8776, 0.8743, 0.8733, 0.8724, 0.8613, 0.8609, 0.8604,
         0.8584],
        [0.8045, 0.7837, 0.7786, 0.7719, 0.7628, 0.7591, 0.7405, 0.7391, 0.7302,
         0.7204],
        [0.8309, 0.8281, 0.8099, 0.8091, 0.8027, 0.8024, 0.8017, 0.7965, 0.7887,
         0.7843],
        [0.8902, 0.8613, 0.8433, 0.8316, 0.8308, 0.8287, 0.8205, 0.8196, 0.8150,
         0.8122],
        [0.8748, 0.8529, 0.8469, 0.8462, 0.8350, 0.8261, 0.8205, 0.8098, 0.8091,
         0.8042],
        [0.8870, 0.8841, 0.8435, 0.8143, 0.8107, 0.8049, 0.8019, 0.7972, 0.7970,
         0.7923],
        [0.9234, 0.9065, 0.9005, 0.8921, 0.8633, 0.8584, 0.8556, 0.8510, 0.8415,
         0.8370],
        [0.9112, 0.8737, 0.8693, 0.8592, 0.8525, 0.8510, 0.8424, 0.8411, 0.8379,
         0.8371],
        [0.9228, 0.9001, 0.8971, 0.8966, 0.8948, 0.8915, 0.8915, 0.8888, 0.8883,
         0.8871],
        [0.8986, 0.8788, 0.8779, 0.8668, 0.8646, 0.8640, 0.8608, 0.8604, 0.8601,
         0.8583],
        [0.9039, 0.8758, 0.8668, 0.8647, 0.8585, 0.8551, 0.8417, 0.8279, 0.8240,
         0.8227],
        [0.9394, 0.8782, 0.8696, 0.8613, 0.8546, 0.8505, 0.8346, 0.8308, 0.8295,
         0.8181],
        [0.9052, 0.8589, 0.8542, 0.8433, 0.8304, 0.8286, 0.8228, 0.8227, 0.8154,
         0.8135],
        [0.8979, 0.8161, 0.8132, 0.8009, 0.7907, 0.7898, 0.7895, 0.7862, 0.7829,
         0.7746],
        [0.8833, 0.8654, 0.8559, 0.8535, 0.8463, 0.8428, 0.8401, 0.8397, 0.8393,
         0.8390],
        [0.8534, 0.7756, 0.7745, 0.7737, 0.7631, 0.7357, 0.7287, 0.7263, 0.7254,
         0.7230],
        [0.8823, 0.8552, 0.8410, 0.8214, 0.8119, 0.8108, 0.7923, 0.7907, 0.7891,
         0.7847],
        [0.8373, 0.8062, 0.7938, 0.7785, 0.7767, 0.7702, 0.7671, 0.7510, 0.7484,
         0.7436]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 266395.3750,  189885.7188,  157626.2969,  154907.5000,  142199.6562,
          137180.6406,  132923.3906,  124981.9609,  122002.8438,  117615.0000],
        [ 694521.4375,  579342.7500,  570826.1250,  565082.0625,  556632.8125,
          523122.5625,  522445.5000,  514185.3438,  497767.5312,  492858.2500],
        [ 411334.1562,  366164.0625,  267871.4375,  251823.8906,  204199.8281,
          182803.6719,  176855.4531,  156114.1250,  151267.1250,  136294.9375],
        [ 854284.0625,  313522.7812,  297274.2188,  284435.2500,  282617.4062,
          266769.3438,  232672.8125,  226774.5938,  220439.6250,  217718.1250],
        [ 168833.1094,  143980.4531,  141990.2969,  112214.1484,   99168.5938,
           98667.6797,   93550.5000,   86833.1016,   77636.2266,   70179.7656],
        [ 194425.2500,  118078.7109,  103705.2344,   87144.7812,   83250.8516,
           77170.8828,   71606.5391,   66691.8906,   58320.6211,   56175.0430],
        [ 230555.2031,  221692.5312,  197349.0000,  191984.8281,  182557.6719,
          178465.8594,  174623.8906,  164365.6094,  159554.9375,  154159.7188],
        [ 340515.9688,  256713.2969,  238374.3281,  203569.4531,  183587.0625,
          140888.4062,  132520.3750,  116907.0156,   98818.1562,   93838.5781],
        [ 426775.3750,  392120.3125,  318037.2500,  303926.5312,  303026.7188,
          299384.7188,  298269.2812,  285039.7188,  281659.8125,  281257.9688],
        [ 769138.3750,  300649.9375,  295978.0312,  287534.0625,  260246.6562,
          253638.7812,  239218.3125,  238609.0469,  218767.3125,  212484.3281],
        [ 724408.4375,  403551.0000,  344836.2500,  327489.7500,  326488.4375,
          315197.4375,  302349.8125,  295278.8438,  284581.5000,  276242.0312],
        [ 316568.4062,  254824.6250,  200973.2344,  178717.0781,  128933.2109,
           88837.7969,   87719.4453,   84222.9531,   70638.3203,   65869.4766],
        [ 457213.0000,  414908.7812,  391513.8438,  331107.8125,  284919.3125,
          271049.4375,  241877.7656,  226556.0312,  221088.0625,  188870.0000],
        [ 858451.6875,  835188.8750,  793369.5625,  791069.8125,  761091.6875,
          740019.6250,  712088.0625,  693290.5625,  673114.6875,  626052.1250],
        [ 696033.1875,  571344.5625,  562346.1875,  551515.1875,  528681.1250,
          496443.9062,  492304.8750,  471091.2500,  466313.4375,  456897.4062],
        [ 555345.3750,  520676.2500,  513355.8750,  463190.9375,  450695.4062,
          442149.3438,  429767.1250,  409657.4688,  405504.2500,  395643.0000],
        [ 800823.7500,  736788.0625,  626594.5000,  601375.2500,  549740.2500,
          544415.5000,  491974.0312,  489531.9062,  466385.9062,  462607.7500],
        [ 921063.3125,  845568.3125,  796939.7500,  674818.0000,  669710.2500,
          628325.6250,  619503.0000,  605922.5625,  603437.7500,  589048.3125],
        [ 549376.0625,  461676.5000,  381437.2812,  370601.5000,  345502.8438,
          340918.8750,  332907.8438,  326888.4688,  312866.8438,  304539.5938],
        [ 397378.6250,  371302.3125,  363979.6875,  363390.0938,  353502.6562,
          350265.6250,  336397.0625,  328575.9375,  322623.8125,  316999.2188],
        [ 678686.5000,  605308.0625,  604886.8125,  587723.5000,  582991.8125,
          579405.6875,  552873.8750,  540753.5000,  537810.1875,  529852.6250],
        [ 550148.8125,  446393.0000,  441770.4375,  426823.7812,  412937.4062,
          406573.7812,  403761.9688,  391645.2812,  324854.4688,  305858.3750],
        [1003370.4375,  764352.5625,  695540.8125,  661872.5000,  615122.3125,
          561306.7500,  546301.8750,  491302.5938,  457701.6250,  449902.2500],
        [ 684555.1250,  617813.8125,  584489.9375,  557038.0000,  538564.1875,
          531544.0000,  526775.6250,  524559.3125,  522477.9062,  511506.1562],
        [ 818433.6250,  376102.1250,  323029.5938,  283825.8750,  231181.8125,
          216257.7969,  215987.1562,  209027.1406,  208579.0938,  206436.5000],
        [ 634439.6250,  598713.7500,  580740.0625,  530287.8125,  520077.2500,
          489464.1875,  461985.2500,  438670.4375,  400263.5625,  398225.8125],
        [ 532016.6250,  518012.6562,  500472.5938,  469428.3125,  421535.0625,
          377721.0938,  366002.4062,  362880.0000,  357535.2500,  352019.3750],
        [ 896391.7500,  759784.8125,  698348.3750,  550125.7500,  540639.6250,
          535632.7500,  500541.3438,  492933.4688,  491368.6562,  481969.5312],
        [ 702305.5625,  636407.8750,  554283.9375,  543881.0000,  538489.6875,
          523073.1562,  510464.2812,  488799.4375,  461842.9688,  453389.8125],
        [ 593746.3750,  556183.8750,  547027.6250,  543383.8125,  518196.4375,
          494112.5000,  488049.0625,  487174.8125,  463156.0312,  462457.7812],
        [ 722263.7500,  722038.5625,  718514.6875,  711186.1250,  695131.0625,
          688509.6250,  617367.3750,  595615.6250,  585979.5625,  568628.3125],
        [ 647483.0625,  513240.8125,  503744.0938,  435356.7500,  391043.6562,
          389066.5625,  365259.0000,  363474.3125,  359971.9062,  357234.6250],
        [ 263392.7500,  243922.9688,  233089.2344,  209183.2812,  205565.4062,
          186715.0000,  183809.2031,  183728.7656,  181105.5625,  180768.9062],
        [ 382463.4062,  330206.9062,  307545.5000,  298746.6875,  287479.1875,
          281732.5938,  281052.5938,  276429.6562,  269816.4062,  265482.3438],
        [ 495689.3438,  467104.3438,  461343.7812,  426225.8750,  384969.0000,
          382035.7812,  362149.1250,  328520.5000,  315359.7812,  313407.9688],
        [ 411668.1250,  355145.5625,  342401.1250,  306410.1875,  290759.4688,
          269784.7500,  269667.9688,  246057.7344,  211560.4688,  208426.5781],
        [ 753440.8750,  240185.7188,  227977.4062,  188687.4531,  177690.7500,
          140418.5156,  122655.0938,  118719.1641,  114510.3359,  111658.6094],
        [ 378609.7188,  289140.5000,  281374.4062,  268835.4688,  258798.7812,
          246574.0625,  199950.4531,  198975.0000,  197986.5469,  192366.0469],
        [ 345335.1562,  327092.7188,  243162.0938,  218662.5938,  183240.3750,
          173590.1250,  166480.0312,  154018.2031,  150860.5781,  147045.5469],
        [ 622749.0625,  592433.5625,  578703.8125,  577428.7500,  566791.8750,
          550312.5625,  543275.5000,  538487.1250,  537036.8125,  522758.0000],
        [ 287295.3125,  272483.1875,  272420.8438,  232271.2969,  216772.3438,
          201501.3906,  194586.2500,  160529.6094,  141721.3438,  127972.3047],
        [ 380661.0625,  218389.3906,  216429.0312,  207732.2344,  190543.1250,
          187372.5000,  181408.7656,  176277.3594,  175131.5625,  174443.4688],
        [ 305155.0625,  259057.8125,  246518.5781,  216561.3750,  189965.0469,
          177850.8125,  162614.5469,  158118.6250,  148275.2969,  148099.7812],
        [ 267141.0312,  254011.5625,  205123.8125,  188413.2188,  180620.0156,
          170850.4531,  167863.8750,  165342.4062,  162915.6719,  148688.5000],
        [ 188401.9062,  154246.7812,  121521.4062,  118003.2891,  104206.1875,
          103939.2969,   87965.8203,   69747.3516,   63011.0547,   60488.1875],
        [ 138938.5938,   98402.3047,   78796.1328,   74526.0859,   68547.6562,
           68507.7891,   65004.1719,   54635.8398,   51154.2969,   50193.5469],
        [ 375702.4062,  293256.6250,  278572.5000,  265796.2188,  261801.9844,
          258695.6562,  220659.4219,  219265.4844,  217920.6562,  211629.0781],
        [  97978.4844,   72826.3594,   67691.4375,   61525.8750,   54048.3633,
           51232.6055,   39307.7461,   38494.9688,   33901.4609,   29486.7578],
        [ 142962.3438,  137403.4844,  105799.2578,  104630.8984,   95530.9062,
           95155.7422,   94198.8438,   87480.0156,   78204.0312,   73491.6016],
        [ 333404.1250,  220658.3594,  170535.7812,  144370.6719,  142617.1406,
          138478.2500,  123099.2344,  121561.5078,  113856.1094,  109335.7344],
        [ 267386.5000,  195711.4688,  179577.4688,  177808.0625,  151541.7656,
          133424.6875,  123242.0703,  105774.4297,  104721.2422,   97588.1250],
        [ 318581.2500,  305628.6250,  171138.4531,  112723.9453,  107044.1328,
           98600.6094,   94385.7969,   88292.6797,   88113.4297,   82338.4141],
        [ 535486.6875,  420652.4062,  386119.1250,  342472.0000,  227056.3438,
          211557.0469,  203241.8125,  190511.1250,  166202.9062,  155887.3906],
        [ 449769.6875,  263308.3438,  247494.5156,  214099.6094,  194462.3438,
          190511.1250,  168537.4219,  165268.2969,  157918.6406,  156065.8906],
        [ 531226.2500,  384006.4688,  367918.8125,  365317.5312,  356031.9688,
          339511.4375,  339511.4375,  326651.0000,  324446.4062,  319106.6875],
        [ 375980.1875,  283433.0938,  279682.8125,  238516.4531,  231392.8906,
          229225.9531,  219214.8750,  217899.4688,  216937.1719,  211332.8125],
        [ 405578.8750,  271460.4688,  238564.4531,  231514.0938,  212086.1094,
          202049.6094,  166767.1719,  136960.3750,  129472.5312,  127189.6016],
        [ 673487.1250,  280768.0625,  248480.9375,  220540.3281,  200426.2031,
          189041.9062,  150658.7188,  142697.1250,  140132.5000,  118995.8516],
        [ 412810.6250,  213102.4688,  199224.8594,  170731.8750,  141888.5000,
          138237.0469,  127217.1406,  127082.3047,  114565.0625,  111522.6016],
        [ 372065.4688,  115617.6406,  111013.0547,   93130.9688,   80487.7500,
           79488.6094,   79102.4141,   75421.0469,   72026.7266,   63918.3008],
        [ 302332.7812,  233880.3750,  204279.0938,  197287.6562,  178059.7188,
          169421.5000,  163020.4219,  162139.1250,  161042.8438,  160437.9375],
        [ 196987.7812,   64861.1914,   63863.3438,   63138.4609,   54219.6602,
           36687.0625,   33192.6484,   32075.8066,   31649.7617,   30603.3008],
        [ 297814.7812,  202087.7656,  165172.0469,  124820.3203,  108886.4297,
          107251.3672,   82373.3672,   80445.9297,   78637.8750,   73824.5703],
        [ 156702.1250,  100362.1641,   84089.0078,   67607.2422,   65899.0078,
           60063.4414,   57437.3125,   45651.9531,   43971.2734,   41078.2812]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[266395.3750,      0.0000],
         [189885.7188,      0.0000],
         [157626.2969,      0.0000],
         ...,
         [124981.9609,      0.0000],
         [122002.8438,      0.0000],
         [117615.0000,      0.0000]],

        [[694521.4375,      0.0000],
         [579342.7500,      0.0000],
         [570826.1250,      0.0000],
         ...,
         [514185.3438,      0.0000],
         [497767.5312,      0.0000],
         [492858.2500,      0.0000]],

        [[411334.1562,      0.0000],
         [366164.0625,      0.0000],
         [267871.4375,      0.0000],
         ...,
         [156114.1250,      0.0000],
         [151267.1250,      0.0000],
         [136294.9375,      0.0000]],

        ...,

        [[     0.0000, 196987.7812],
         [ 64861.1914,      0.0000],
         [ 63863.3438,      0.0000],
         ...,
         [ 32075.8066,      0.0000],
         [ 31649.7617,      0.0000],
         [ 30603.3008,      0.0000]],

        [[297814.7812,      0.0000],
         [     0.0000, 202087.7656],
         [165172.0469,      0.0000],
         ...,
         [ 80445.9297,      0.0000],
         [ 78637.8750,      0.0000],
         [ 73824.5703,      0.0000]],

        [[     0.0000, 156702.1250],
         [     0.0000, 100362.1641],
         [     0.0000,  84089.0078],
         ...,
         [ 45651.9531,      0.0000],
         [     0.0000,  43971.2734],
         [     0.0000,  41078.2812]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1545718.5000,       0.0000],
        [5516784.5000,       0.0000],
        [2304728.5000,       0.0000],
        [3196508.2500,       0.0000],
        [ 452248.3125,  640805.5625],
        [ 772707.0000,  143862.7812],
        [1695754.2500,  159554.9375],
        [1805732.7500,       0.0000],
        [3189497.7500,       0.0000],
        [3076264.5000,       0.0000],
        [3600423.5000,       0.0000],
        [1477304.5000,       0.0000],
        [3029104.0000,       0.0000],
        [7483736.5000,       0.0000],
        [5292971.0000,       0.0000],
        [4585985.0000,       0.0000],
        [5770236.5000,       0.0000],
        [6954336.5000,       0.0000],
        [3726716.0000,       0.0000],
        [3175838.7500,  328575.9375],
        [5800293.0000,       0.0000],
        [4110767.5000,       0.0000],
        [6246774.0000,       0.0000],
        [5599324.0000,       0.0000],
        [3088860.7500,       0.0000],
        [5052868.0000,       0.0000],
        [4257623.5000,       0.0000],
        [5947736.0000,       0.0000],
        [5412937.5000,       0.0000],
        [5153488.5000,       0.0000],
        [6625235.0000,       0.0000],
        [4325874.5000,       0.0000],
        [2071281.0000,       0.0000],
        [2385930.7500,  595024.6875],
        [1780781.1250, 2156024.5000],
        [1021190.3750, 1890691.7500],
        [1090658.7500, 1105285.2500],
        [1317226.5000, 1195384.5000],
        [ 706781.7500, 1402705.6250],
        [5629977.0000,       0.0000],
        [1979581.5000,  127972.3047],
        [2108388.5000,       0.0000],
        [2012216.8750,       0.0000],
        [1910970.6250,       0.0000],
        [ 234441.7188,  837089.5625],
        [ 327740.0000,  420966.4062],
        [2227597.7500,  375702.4062],
        [ 344034.8750,  202459.1562],
        [ 449605.7812,  565251.3125],
        [ 442739.8750, 1175177.0000],
        [1298629.8750,  238145.9375],
        [ 671499.0000,  795348.3125],
        [1673994.2500, 1165192.5000],
        [2016924.6250,  190511.1250],
        [3653728.0000,       0.0000],
        [2503615.7500,       0.0000],
        [ 971457.1250, 1150186.2500],
        [1663494.1250,  701734.5625],
        [ 947383.8750,  808998.5625],
        [ 532669.5000,  609602.5000],
        [1217628.5000,  714272.8750],
        [ 410291.2500,  196987.7812],
        [ 903088.8750,  418225.5625],
        [ 105715.3906,  617146.3750]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 451/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:36, 59.18s/it]  7%|▋         | 2/30 [01:00<11:45, 25.21s/it] 10%|█         | 3/30 [01:01<06:19, 14.04s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.79s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.311169083913167
Epoch 452/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:28, 58.91s/it]  7%|▋         | 2/30 [01:01<11:56, 25.58s/it] 10%|█         | 3/30 [01:02<06:25, 14.29s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.99s/it] 20%|██        | 6/30 [01:04<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3303813616434734
Epoch 453/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:40, 59.34s/it]  7%|▋         | 2/30 [01:00<11:36, 24.89s/it] 10%|█         | 3/30 [01:00<06:14, 13.87s/it] 13%|█▎        | 4/30 [01:01<03:45,  8.69s/it] 17%|█▋        | 5/30 [01:02<02:25,  5.82s/it] 20%|██        | 6/30 [01:03<01:38,  4.10s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.00s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.3241626103719075
Epoch 454/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:04<31:11, 64.54s/it]  7%|▋         | 2/30 [01:05<12:36, 27.02s/it] 10%|█         | 3/30 [01:06<06:45, 15.02s/it] 13%|█▎        | 4/30 [01:06<04:04,  9.39s/it] 17%|█▋        | 5/30 [01:07<02:36,  6.27s/it] 20%|██        | 6/30 [01:08<01:45,  4.39s/it] 23%|██▎       | 7/30 [01:09<01:13,  3.20s/it] 27%|██▋       | 8/30 [01:09<00:53,  2.42s/it] 30%|███       | 9/30 [01:10<00:39,  1.90s/it] 33%|███▎      | 10/30 [01:11<00:30,  1.54s/it] 37%|███▋      | 11/30 [01:12<00:24,  1.30s/it] 40%|████      | 12/30 [01:12<00:20,  1.13s/it] 43%|████▎     | 13/30 [01:13<00:17,  1.02s/it] 47%|████▋     | 14/30 [01:14<00:14,  1.07it/s] 50%|█████     | 15/30 [01:15<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:15<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:16<00:10,  1.23it/s] 60%|██████    | 18/30 [01:17<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:18<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:18<00:07,  1.30it/s] 70%|███████   | 21/30 [01:19<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:20<00:06,  1.31it/s] 77%|███████▋  | 23/30 [01:21<00:05,  1.32it/s] 80%|████████  | 24/30 [01:21<00:04,  1.32it/s] 83%|████████▎ | 25/30 [01:22<00:03,  1.32it/s] 87%|████████▋ | 26/30 [01:23<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:24<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:24<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:25<00:00,  1.33it/s]100%|██████████| 30/30 [01:26<00:00,  1.34it/s]100%|██████████| 30/30 [01:26<00:00,  2.88s/it]
Epoch loss is 2.3052971363067627
Epoch 455/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:38, 59.27s/it]  7%|▋         | 2/30 [01:00<11:47, 25.27s/it] 10%|█         | 3/30 [01:01<06:19, 14.07s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.81s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.90s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.2804179986317954
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0009,  0.0132,  ..., -0.0058,  0.0175,  0.0040],
        [-0.0022,  0.0095,  0.0286,  ...,  0.0172,  0.0041, -0.0148],
        [-0.0293, -0.0380,  0.0247,  ...,  0.0833, -0.0043, -0.0144],
        ...,
        [ 0.0060, -0.0033,  0.0098,  ..., -0.0241, -0.0052, -0.0028],
        [-0.0332,  0.0098, -0.0035,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0327, -0.0139,  0.0192,  ...,  0.0413,  0.0359, -0.0285]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8743, 0.8508, 0.8376, 0.8364, 0.8305, 0.8279, 0.8256, 0.8214, 0.8197,
         0.8172],
        [0.9415, 0.9288, 0.9278, 0.9271, 0.9261, 0.9216, 0.9216, 0.9204, 0.9182,
         0.9175],
        [0.9049, 0.8967, 0.8747, 0.8705, 0.8558, 0.8480, 0.8457, 0.8370, 0.8348,
         0.8275],
        [0.9561, 0.8858, 0.8820, 0.8790, 0.8786, 0.8745, 0.8649, 0.8631, 0.8611,
         0.8603],
        [0.8425, 0.8314, 0.8303, 0.8139, 0.8052, 0.8048, 0.8012, 0.7959, 0.7881,
         0.7811],
        [0.8524, 0.8175, 0.8084, 0.7962, 0.7931, 0.7876, 0.7824, 0.7775, 0.7680,
         0.7655],
        [0.8642, 0.8615, 0.8534, 0.8515, 0.8479, 0.8464, 0.8448, 0.8406, 0.8384,
         0.8360],
        [0.8916, 0.8718, 0.8666, 0.8556, 0.8483, 0.8298, 0.8255, 0.8167, 0.8050,
         0.8013],
        [0.9074, 0.9015, 0.8868, 0.8837, 0.8835, 0.8826, 0.8823, 0.8792, 0.8783,
         0.8782],
        [0.9487, 0.8829, 0.8818, 0.8798, 0.8727, 0.8710, 0.8669, 0.8667, 0.8606,
         0.8586],
        [0.9445, 0.9035, 0.8925, 0.8888, 0.8886, 0.8862, 0.8833, 0.8816, 0.8790,
         0.8769],
        [0.8864, 0.8712, 0.8546, 0.8464, 0.8235, 0.7975, 0.7965, 0.7937, 0.7814,
         0.7765],
        [0.9121, 0.9054, 0.9013, 0.8896, 0.8790, 0.8756, 0.8676, 0.8630, 0.8613,
         0.8503],
        [0.9563, 0.9545, 0.9508, 0.9506, 0.9479, 0.9459, 0.9433, 0.9414, 0.9393,
         0.9342],
        [0.9417, 0.9279, 0.9267, 0.9254, 0.9224, 0.9180, 0.9174, 0.9144, 0.9137,
         0.9122],
        [0.9259, 0.9214, 0.9204, 0.9132, 0.9113, 0.9099, 0.9079, 0.9046, 0.9039,
         0.9022],
        [0.9515, 0.9456, 0.9343, 0.9314, 0.9251, 0.9244, 0.9174, 0.9170, 0.9136,
         0.9130],
        [0.9613, 0.9553, 0.9512, 0.9395, 0.9390, 0.9345, 0.9335, 0.9319, 0.9317,
         0.9300],
        [0.9251, 0.9129, 0.8995, 0.8976, 0.8926, 0.8917, 0.8900, 0.8887, 0.8857,
         0.8838],
        [0.9024, 0.8977, 0.8963, 0.8962, 0.8942, 0.8936, 0.8907, 0.8891, 0.8878,
         0.8866],
        [0.9398, 0.9318, 0.9318, 0.9298, 0.9293, 0.9288, 0.9255, 0.9240, 0.9236,
         0.9226],
        [0.9252, 0.9106, 0.9098, 0.9074, 0.9051, 0.9040, 0.9035, 0.9014, 0.8883,
         0.8840],
        [0.9673, 0.9483, 0.9416, 0.9381, 0.9330, 0.9266, 0.9247, 0.9173, 0.9123,
         0.9111],
        [0.9405, 0.9333, 0.9295, 0.9261, 0.9237, 0.9228, 0.9222, 0.9219, 0.9216,
         0.9201],
        [0.9530, 0.8985, 0.8879, 0.8789, 0.8645, 0.8597, 0.8597, 0.8574, 0.8573,
         0.8566],
        [0.9352, 0.9311, 0.9290, 0.9226, 0.9213, 0.9170, 0.9129, 0.9093, 0.9029,
         0.9026],
        [0.9228, 0.9210, 0.9185, 0.9141, 0.9065, 0.8989, 0.8967, 0.8961, 0.8950,
         0.8939],
        [0.9594, 0.9478, 0.9419, 0.9252, 0.9240, 0.9233, 0.9186, 0.9175, 0.9173,
         0.9160],
        [0.9423, 0.9354, 0.9258, 0.9244, 0.9237, 0.9217, 0.9199, 0.9169, 0.9129,
         0.9116],
        [0.9305, 0.9260, 0.9248, 0.9243, 0.9210, 0.9177, 0.9168, 0.9167, 0.9132,
         0.9131],
        [0.9443, 0.9443, 0.9439, 0.9432, 0.9416, 0.9409, 0.9333, 0.9308, 0.9296,
         0.9275],
        [0.9366, 0.9204, 0.9190, 0.9088, 0.9013, 0.9009, 0.8965, 0.8962, 0.8955,
         0.8950],
        [0.8736, 0.8683, 0.8650, 0.8575, 0.8563, 0.8496, 0.8484, 0.8484, 0.8474,
         0.8472],
        [0.8997, 0.8894, 0.8844, 0.8824, 0.8797, 0.8783, 0.8781, 0.8770, 0.8753,
         0.8741],
        [0.9179, 0.9137, 0.9129, 0.9073, 0.9002, 0.8996, 0.8959, 0.8891, 0.8862,
         0.8858],
        [0.9049, 0.8945, 0.8920, 0.8842, 0.8805, 0.8753, 0.8753, 0.8688, 0.8583,
         0.8572],
        [0.9473, 0.8672, 0.8635, 0.8503, 0.8461, 0.8295, 0.8201, 0.8178, 0.8153,
         0.8134],
        [0.8990, 0.8801, 0.8781, 0.8751, 0.8723, 0.8690, 0.8542, 0.8539, 0.8536,
         0.8516],
        [0.8926, 0.8888, 0.8680, 0.8606, 0.8482, 0.8443, 0.8415, 0.8360, 0.8346,
         0.8328],
        [0.9338, 0.9304, 0.9287, 0.9286, 0.9273, 0.9252, 0.9243, 0.9237, 0.9236,
         0.9216],
        [0.8797, 0.8760, 0.8760, 0.8648, 0.8600, 0.8549, 0.8524, 0.8389, 0.8302,
         0.8231],
        [0.8994, 0.8605, 0.8599, 0.8570, 0.8509, 0.8497, 0.8475, 0.8454, 0.8450,
         0.8448],
        [0.8839, 0.8724, 0.8689, 0.8599, 0.8507, 0.8461, 0.8398, 0.8379, 0.8334,
         0.8333],
        [0.8745, 0.8711, 0.8561, 0.8501, 0.8472, 0.8433, 0.8420, 0.8410, 0.8400,
         0.8336],
        [0.8501, 0.8362, 0.8194, 0.8173, 0.8087, 0.8085, 0.7968, 0.7805, 0.7735,
         0.7705],
        [0.8288, 0.8046, 0.7891, 0.7853, 0.7794, 0.7794, 0.7757, 0.7635, 0.7589,
         0.7576],
        [0.8985, 0.8810, 0.8775, 0.8742, 0.8731, 0.8722, 0.8611, 0.8606, 0.8603,
         0.8582],
        [0.8043, 0.7836, 0.7785, 0.7718, 0.7628, 0.7590, 0.7403, 0.7388, 0.7301,
         0.7204],
        [0.8307, 0.8280, 0.8098, 0.8090, 0.8026, 0.8024, 0.8017, 0.7964, 0.7886,
         0.7842],
        [0.8901, 0.8612, 0.8431, 0.8314, 0.8306, 0.8286, 0.8203, 0.8195, 0.8149,
         0.8121],
        [0.8746, 0.8528, 0.8468, 0.8461, 0.8349, 0.8260, 0.8205, 0.8097, 0.8091,
         0.8041],
        [0.8869, 0.8840, 0.8434, 0.8141, 0.8105, 0.8047, 0.8017, 0.7971, 0.7969,
         0.7921],
        [0.9233, 0.9064, 0.9003, 0.8920, 0.8632, 0.8583, 0.8555, 0.8509, 0.8414,
         0.8369],
        [0.9111, 0.8736, 0.8693, 0.8591, 0.8524, 0.8509, 0.8424, 0.8409, 0.8378,
         0.8369],
        [0.9227, 0.9000, 0.8970, 0.8965, 0.8947, 0.8914, 0.8914, 0.8887, 0.8882,
         0.8870],
        [0.8985, 0.8787, 0.8778, 0.8667, 0.8645, 0.8639, 0.8608, 0.8603, 0.8600,
         0.8581],
        [0.9038, 0.8757, 0.8666, 0.8645, 0.8585, 0.8550, 0.8417, 0.8278, 0.8238,
         0.8226],
        [0.9394, 0.8781, 0.8695, 0.8612, 0.8545, 0.8504, 0.8345, 0.8306, 0.8293,
         0.8179],
        [0.9050, 0.8588, 0.8541, 0.8432, 0.8303, 0.8285, 0.8226, 0.8226, 0.8153,
         0.8133],
        [0.8978, 0.8159, 0.8131, 0.8008, 0.7906, 0.7896, 0.7893, 0.7860, 0.7828,
         0.7744],
        [0.8833, 0.8653, 0.8558, 0.8534, 0.8461, 0.8428, 0.8400, 0.8396, 0.8390,
         0.8388],
        [0.8533, 0.7755, 0.7744, 0.7736, 0.7629, 0.7357, 0.7286, 0.7263, 0.7253,
         0.7229],
        [0.8822, 0.8551, 0.8409, 0.8214, 0.8118, 0.8106, 0.7921, 0.7905, 0.7889,
         0.7845],
        [0.8373, 0.8060, 0.7937, 0.7784, 0.7766, 0.7700, 0.7670, 0.7508, 0.7483,
         0.7435]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 265836.5000,  189906.7188,  157299.2188,  154659.0625,  142009.2500,
          136998.7812,  132586.3750,  124730.2422,  121835.2969,  117585.9531],
        [ 694089.6875,  578735.3125,  570557.8125,  564451.8750,  556475.6875,
          522272.1562,  521799.1562,  513588.4688,  497214.3125,  492268.7500],
        [ 411063.1875,  365757.4688,  267287.0625,  251502.5312,  203831.5312,
          182569.8594,  176550.7500,  155874.3125,  151082.4375,  136150.3594],
        [ 854297.1250,  313069.2188,  296736.3438,  284209.1250,  282433.1250,
          266308.2500,  232369.8906,  226454.7188,  219953.0469,  217457.2969],
        [ 168711.2500,  143873.1094,  141675.2656,  112096.9141,   98999.1719,
           98497.0391,   93466.3203,   86708.3984,   77494.3516,   70171.0703],
        [ 194239.1875,  117973.3516,  103664.1953,   87032.9062,   83255.5391,
           76983.3672,   71488.9141,   66667.2109,   58176.4727,   56115.8711],
        [ 230074.1875,  221204.2656,  196966.0000,  191742.3906,  182222.1406,
          178268.3594,  174296.7969,  164084.1719,  159130.0625,  153788.9531],
        [ 340014.9688,  256417.2344,  238136.6562,  203288.5312,  183348.5781,
          140643.8125,  132379.2969,  116704.8359,   98659.0234,   93653.2500],
        [ 426414.5312,  391929.2500,  317596.8438,  303826.5625,  302846.1875,
          298979.5625,  298039.8125,  284823.9688,  281424.8438,  281007.5625],
        [ 768949.9375,  300211.8438,  295548.4688,  287185.1875,  259842.9062,
          253295.7969,  238857.4375,  238245.7031,  218536.4844,  212196.7656],
        [ 724154.2500,  403454.4062,  344450.6875,  326992.5938,  325872.5312,
          314732.7500,  302125.8438,  294910.2188,  284286.6562,  275713.8125],
        [ 315936.2500,  254307.7656,  200636.0156,  178284.0000,  128637.3359,
           88715.6250,   87453.9922,   83957.1875,   70429.7969,   65684.9844],
        [ 456146.8125,  414519.1875,  390837.5000,  330550.6562,  284303.7188,
          270530.6250,  241450.4844,  226122.1719,  220776.2344,  188503.8125],
        [ 857602.3125,  834899.0000,  792573.2500,  790259.2500,  760427.1250,
          739212.0000,  711414.7500,  692467.8750,  672446.1875,  625398.1250],
        [ 695770.3750,  571000.3125,  561960.1875,  551151.3750,  528134.8125,
          496089.4062,  492010.1250,  470823.1250,  466137.3750,  456758.0000],
        [ 555165.3750,  520423.5625,  513040.1562,  462935.6875,  450551.8750,
          441857.2188,  429490.1562,  409361.4375,  405237.1250,  395505.6562],
        [ 800094.7500,  736103.3125,  625968.5625,  600518.5000,  549082.1875,
          543796.4375,  491518.1562,  488828.3750,  465856.0625,  462080.8438],
        [ 920335.3750,  845033.8125,  796593.2500,  674765.2500,  669386.5000,
          627841.0625,  619172.7500,  605331.7500,  603237.5000,  588699.5625],
        [ 548971.7500,  460961.5938,  380647.6250,  370328.7500,  345185.3438,
          340525.4062,  332608.5938,  326489.6562,  312578.1562,  304439.7188],
        [ 396877.9375,  371036.1250,  363684.4062,  363049.2500,  353198.0000,
          349870.0000,  335969.7188,  328378.2812,  322344.5312,  316561.4688],
        [ 677658.1875,  604445.6250,  604148.3125,  587246.6875,  582484.4375,
          578609.5000,  552359.0000,  540245.3125,  537004.5625,  529306.1250],
        [ 549925.3750,  446316.7812,  441394.8125,  426340.1250,  412756.2812,
          406048.7188,  403160.9688,  391294.3438,  324468.3750,  305293.6250],
        [1002835.6250,  764171.7500,  695296.1250,  661366.4375,  614821.4375,
          560796.8125,  545913.3750,  491071.6250,  457439.8125,  449488.0000],
        [ 684214.3750,  617366.8125,  584131.6250,  556831.3750,  538431.6875,
          531458.8750,  526532.5625,  524223.7812,  522034.1250,  511323.7188],
        [ 817727.5625,  375641.5000,  322422.9688,  283532.3125,  230870.0625,
          215729.0156,  215657.8438,  208616.2969,  208239.6250,  206306.2188],
        [ 633850.6250,  598341.5625,  580079.1250,  529768.7500,  519699.9375,
          488969.1875,  461434.3750,  438200.9062,  399982.3438,  397807.0938],
        [ 531426.9375,  517380.6562,  499745.3125,  468911.9688,  421000.7500,
          377294.4688,  365606.8125,  362489.5000,  357169.5625,  351641.2188],
        [ 895953.3125,  759476.9375,  697830.4375,  549548.9375,  540140.1875,
          535256.9375,  500267.9062,  492779.2812,  491088.9688,  481690.0938],
        [ 701712.3750,  635855.2500,  554083.6250,  543252.1875,  537922.0000,
          522595.0000,  509907.6250,  488441.1250,  461235.9688,  452908.3750],
        [ 593175.3125,  555807.8750,  546736.5625,  542999.9375,  517868.8750,
          493587.8125,  487681.0625,  486879.8750,  463050.9062,  462127.1250],
        [ 721876.7500,  721739.7500,  717821.6250,  710807.1250,  694534.6250,
          688067.8750,  616907.1875,  595226.1250,  585640.4375,  568244.0000],
        [ 646896.6875,  512918.3438,  503391.1250,  434825.6250,  390849.0625,
          388700.1562,  364931.0312,  363171.8125,  359585.5625,  357006.7812],
        [ 262999.1562,  243671.1719,  232762.2500,  208948.0156,  205426.0625,
          186606.7812,  183576.7344,  183486.9375,  180797.7031,  180451.1250],
        [ 381724.4062,  329669.4688,  307086.5312,  298345.5312,  287034.5625,
          281409.5625,  280535.7500,  276208.3125,  269500.5938,  265044.4375],
        [ 495046.8438,  466682.6875,  460918.5000,  425595.5000,  384879.4375,
          381586.8438,  361777.0000,  328105.0000,  314928.8125,  313034.2812],
        [ 411344.3438,  354627.0625,  341908.3750,  306173.2812,  290330.0000,
          269499.0312,  269419.1250,  245701.3125,  211277.0000,  208211.4219],
        [ 753324.5625,  240111.5312,  227771.8281,  188496.2656,  177501.7500,
          140108.9844,  122420.9141,  118537.5859,  114418.1016,  111329.2031],
        [ 377939.0938,  288523.4375,  280662.8750,  268680.1562,  258188.1719,
          246325.6250,  199458.3125,  198492.0781,  197708.6094,  192017.6094],
        [ 345013.2188,  326667.5312,  242708.4688,  218355.0312,  183007.0469,
          173140.7500,  166200.2188,  153671.9531,  150686.3125,  146879.3125],
        [ 621944.8750,  591818.6875,  578116.3750,  576990.5000,  566142.5000,
          549453.5625,  542763.8125,  537920.0000,  537050.6250,  522195.4688],
        [ 286907.8750,  272118.0625,  272077.8438,  231932.1875,  216534.7344,
          201236.5625,  194307.5312,  160284.2500,  141490.1562,  127771.7031],
        [ 380339.2188,  218229.2969,  216126.2500,  207460.0312,  190167.8750,
          187062.2031,  181085.5312,  175860.7812,  174861.6875,  174209.7031],
        [ 304808.6562,  258623.3594,  246095.7500,  216312.2500,  189693.8594,
          177563.7031,  162302.1875,  157858.7188,  148113.6250,  147971.4531],
        [ 266376.3125,  253719.8438,  204743.8750,  187931.9375,  180361.8281,
          170641.7031,  167424.6875,  165094.2344,  162729.9688,  148591.6875],
        [ 188074.2969,  154129.2812,  121283.2500,  117680.1875,  104015.6562,
          103728.6719,   87776.3438,   69528.8516,   62916.2422,   60303.4688],
        [ 138780.4844,   98220.9766,   78624.0781,   74451.1406,   68481.2031,
           68440.3984,   64935.3359,   54589.5898,   51107.5859,   50168.4688],
        [ 375128.5000,  292503.2812,  277974.8750,  265228.2500,  261113.2969,
          257868.2500,  220091.3281,  218596.5156,  217362.7500,  211002.3438],
        [  97785.9062,   72744.7969,   67583.5938,   61397.4531,   54030.5312,
           51133.3711,   39164.8828,   38365.5938,   33836.6055,   29480.7949],
        [ 142593.6094,  137109.7500,  105684.1953,  104493.2812,   95351.9531,
           95057.9688,   94136.6094,   87305.0703,   78087.3984,   73335.8984],
        [ 332844.0312,  220370.6562,  170196.8750,  143974.2656,  142374.2969,
          138252.0781,  122901.3438,  121354.8672,  113661.5859,  109216.2031],
        [ 266734.2188,  195511.2969,  179352.5781,  177475.0000,  151322.5312,
          133324.1875,  123140.5625,  105549.6250,  104618.9297,   97478.7422],
        [ 318204.0938,  305300.6250,  170817.5469,  112431.2734,  106744.7344,
           98257.8047,   94201.4531,   88162.0156,   87952.3203,   82098.4844],
        [ 534747.2500,  420210.9062,  385309.5000,  342060.6875,  226651.5625,
          211289.2812,  202955.5469,  190069.4219,  166003.1562,  155621.7969],
        [ 449403.5312,  263116.3125,  247248.0000,  213890.2031,  194233.9844,
          190166.0625,  168420.1250,  164869.4062,  157642.6875,  155812.7656],
        [ 530519.5000,  383471.0625,  367399.1562,  364806.4375,  355678.3750,
          339035.5000,  339035.5000,  326237.2500,  324112.3750,  318589.7500],
        [ 375416.2500,  283069.7812,  279374.3750,  238212.7500,  231068.0781,
          228914.6562,  219105.1562,  217468.6875,  216577.0625,  210870.5781],
        [ 404669.0000,  271147.1562,  238079.4219,  231099.3750,  211898.9062,
          201707.1094,  166706.7656,  136684.0156,  129091.6875,  126959.5859],
        [ 673060.8125,  280456.5625,  248093.7969,  220191.8906,  200129.2031,
          188831.2812,  150464.1719,  142377.7031,  139780.8125,  118703.8750],
        [ 412089.1875,  212891.2188,  198945.2031,  170428.6406,  141778.6719,
          138011.7969,  126885.8750,  126874.0078,  114314.0469,  111202.0859],
        [ 371711.5312,  115394.2500,  110795.9141,   92923.0078,   80304.1250,
           79272.2578,   78933.5391,   75248.3359,   71894.7500,   63757.5195],
        [ 302233.0625,  233495.5000,  203955.0000,  196999.0469,  177686.1875,
          169279.6875,  162701.5625,  161808.1094,  160555.3438,  160048.5625],
        [ 196813.3438,   64805.4805,   63735.6328,   63014.9609,   54121.8633,
           36690.3516,   33133.3789,   32069.8438,   31613.0176,   30542.7422],
        [ 297497.6875,  201981.7969,  164842.5312,  124722.1562,  108743.5312,
          106980.5547,   82077.1875,   80206.9219,   78405.2109,   73623.7031],
        [ 156562.7500,  100169.1094,   83948.5391,   67473.9141,   65817.4219,
           59915.7266,   57363.4648,   45537.7266,   43886.6055,   40990.3828]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[265836.5000,      0.0000],
         [189906.7188,      0.0000],
         [157299.2188,      0.0000],
         ...,
         [124730.2422,      0.0000],
         [121835.2969,      0.0000],
         [117585.9531,      0.0000]],

        [[694089.6875,      0.0000],
         [578735.3125,      0.0000],
         [570557.8125,      0.0000],
         ...,
         [513588.4688,      0.0000],
         [497214.3125,      0.0000],
         [492268.7500,      0.0000]],

        [[411063.1875,      0.0000],
         [365757.4688,      0.0000],
         [267287.0625,      0.0000],
         ...,
         [155874.3125,      0.0000],
         [151082.4375,      0.0000],
         [136150.3594,      0.0000]],

        ...,

        [[     0.0000, 196813.3438],
         [ 64805.4805,      0.0000],
         [ 63735.6328,      0.0000],
         ...,
         [ 32069.8438,      0.0000],
         [ 31613.0176,      0.0000],
         [ 30542.7422,      0.0000]],

        [[297497.6875,      0.0000],
         [     0.0000, 201981.7969],
         [164842.5312,      0.0000],
         ...,
         [ 80206.9219,      0.0000],
         [ 78405.2109,      0.0000],
         [ 73623.7031,      0.0000]],

        [[     0.0000, 156562.7500],
         [     0.0000, 100169.1094],
         [     0.0000,  83948.5391],
         ...,
         [ 45537.7266,      0.0000],
         [     0.0000,  43886.6055],
         [     0.0000,  40990.3828]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1543447.5000,       0.0000],
        [5511453.5000,       0.0000],
        [2301669.5000,       0.0000],
        [3193288.0000,       0.0000],
        [ 451725.6875,  639967.1875],
        [ 771946.3750,  143650.5781],
        [1692647.2500,  159130.0625],
        [1803246.1250,       0.0000],
        [3186889.0000,       0.0000],
        [3072870.5000,       0.0000],
        [3596693.5000,       0.0000],
        [1474043.0000,       0.0000],
        [3023741.2500,       0.0000],
        [7476700.0000,       0.0000],
        [5289835.5000,       0.0000],
        [4583568.0000,       0.0000],
        [5763847.0000,       0.0000],
        [6950397.0000,       0.0000],
        [3722736.5000,       0.0000],
        [3172591.2500,  328378.2812],
        [5793508.0000,       0.0000],
        [4106999.2500,       0.0000],
        [6243201.0000,       0.0000],
        [5596549.0000,       0.0000],
        [3084743.5000,       0.0000],
        [5048134.0000,       0.0000],
        [4252667.0000,       0.0000],
        [5944033.0000,       0.0000],
        [5407913.5000,       0.0000],
        [5149915.5000,       0.0000],
        [6620865.0000,       0.0000],
        [4322276.0000,       0.0000],
        [2068726.0000,       0.0000],
        [2382438.0000,  594121.1250],
        [1778763.6250, 2153791.2500],
        [1019816.8125, 1888674.1250],
        [1089255.3750, 1104765.2500],
        [1314665.1250, 1193330.8750],
        [ 705720.3750, 1400609.5000],
        [5624396.5000,       0.0000],
        [1976889.2500,  127771.7031],
        [2105402.5000,       0.0000],
        [2009343.5000,       0.0000],
        [1907616.2500,       0.0000],
        [ 233847.9688,  835588.3125],
        [ 327415.6875,  420383.5938],
        [2221741.0000,  375128.5000],
        [ 343295.2500,  202228.2812],
        [ 448905.8438,  564249.8750],
        [ 442060.2500, 1173086.0000],
        [1296564.5000,  237943.1250],
        [ 669848.1250,  794322.3125],
        [1671288.7500, 1163630.2500],
        [2014637.0000,  190166.0625],
        [3648884.7500,       0.0000],
        [2500077.2500,       0.0000],
        [ 969729.3125, 1148313.6250],
        [1661312.5000,  700777.6250],
        [ 945792.9375,  807627.7500],
        [ 531539.0000,  608696.1875],
        [1215347.2500,  713414.7500],
        [ 409727.2500,  196813.3438],
        [ 901375.4375,  417705.8750],
        [ 105453.4531,  616212.1875]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 456/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:41, 57.30s/it]  7%|▋         | 2/30 [00:58<11:13, 24.04s/it] 10%|█         | 3/30 [00:59<06:06, 13.58s/it] 13%|█▎        | 4/30 [00:59<03:41,  8.51s/it] 17%|█▋        | 5/30 [01:00<02:22,  5.71s/it] 20%|██        | 6/30 [01:01<01:36,  4.04s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.96s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.26s/it] 30%|███       | 9/30 [01:03<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.46s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.65s/it]
Epoch loss is 2.319761308034261
Epoch 457/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:14, 58.43s/it]  7%|▋         | 2/30 [01:01<12:00, 25.73s/it] 10%|█         | 3/30 [01:02<06:29, 14.43s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.03s/it] 17%|█▋        | 5/30 [01:03<02:31,  6.04s/it] 20%|██        | 6/30 [01:04<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.35s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3210328698158262
Epoch 458/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:22, 58.70s/it]  7%|▋         | 2/30 [00:59<11:29, 24.61s/it] 10%|█         | 3/30 [01:00<06:10, 13.72s/it] 13%|█▎        | 4/30 [01:00<03:43,  8.60s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.77s/it] 20%|██        | 6/30 [01:02<01:37,  4.06s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:12,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.2988669236501056
Epoch 459/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:18, 58.58s/it]  7%|▋         | 2/30 [00:59<11:27, 24.56s/it] 10%|█         | 3/30 [01:00<06:09, 13.69s/it] 13%|█▎        | 4/30 [01:00<03:43,  8.58s/it] 17%|█▋        | 5/30 [01:01<02:23,  5.76s/it] 20%|██        | 6/30 [01:02<01:37,  4.05s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.97s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.26s/it] 30%|███       | 9/30 [01:04<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 2.3131290276845298
Epoch 460/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:15, 60.54s/it]  7%|▋         | 2/30 [01:01<11:50, 25.37s/it] 10%|█         | 3/30 [01:02<06:21, 14.13s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3470164855321247
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0009,  0.0132,  ..., -0.0058,  0.0175,  0.0040],
        [-0.0023,  0.0095,  0.0286,  ...,  0.0172,  0.0042, -0.0148],
        [-0.0293, -0.0380,  0.0247,  ...,  0.0834, -0.0043, -0.0144],
        ...,
        [ 0.0060, -0.0033,  0.0098,  ..., -0.0241, -0.0051, -0.0028],
        [-0.0332,  0.0098, -0.0035,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0327, -0.0139,  0.0192,  ...,  0.0414,  0.0360, -0.0285]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8742, 0.8508, 0.8375, 0.8363, 0.8304, 0.8279, 0.8255, 0.8213, 0.8196,
         0.8172],
        [0.9415, 0.9288, 0.9278, 0.9270, 0.9260, 0.9215, 0.9215, 0.9204, 0.9181,
         0.9174],
        [0.9048, 0.8966, 0.8746, 0.8704, 0.8557, 0.8480, 0.8456, 0.8369, 0.8347,
         0.8275],
        [0.9561, 0.8857, 0.8820, 0.8790, 0.8786, 0.8744, 0.8649, 0.8630, 0.8610,
         0.8602],
        [0.8425, 0.8313, 0.8302, 0.8138, 0.8051, 0.8047, 0.8011, 0.7958, 0.7880,
         0.7811],
        [0.8523, 0.8174, 0.8084, 0.7961, 0.7931, 0.7875, 0.7823, 0.7775, 0.7678,
         0.7654],
        [0.8641, 0.8614, 0.8532, 0.8514, 0.8478, 0.8463, 0.8447, 0.8405, 0.8383,
         0.8359],
        [0.8915, 0.8717, 0.8666, 0.8555, 0.8483, 0.8297, 0.8255, 0.8166, 0.8049,
         0.8012],
        [0.9074, 0.9015, 0.8867, 0.8837, 0.8834, 0.8825, 0.8823, 0.8791, 0.8783,
         0.8782],
        [0.9487, 0.8828, 0.8817, 0.8797, 0.8727, 0.8709, 0.8668, 0.8666, 0.8606,
         0.8585],
        [0.9445, 0.9035, 0.8924, 0.8888, 0.8885, 0.8861, 0.8833, 0.8815, 0.8790,
         0.8768],
        [0.8863, 0.8711, 0.8546, 0.8463, 0.8234, 0.7974, 0.7963, 0.7935, 0.7812,
         0.7763],
        [0.9120, 0.9054, 0.9012, 0.8895, 0.8789, 0.8755, 0.8675, 0.8629, 0.8613,
         0.8502],
        [0.9563, 0.9544, 0.9508, 0.9506, 0.9479, 0.9459, 0.9432, 0.9413, 0.9393,
         0.9342],
        [0.9417, 0.9278, 0.9267, 0.9253, 0.9223, 0.9180, 0.9174, 0.9143, 0.9136,
         0.9122],
        [0.9259, 0.9213, 0.9203, 0.9131, 0.9113, 0.9099, 0.9079, 0.9045, 0.9038,
         0.9021],
        [0.9514, 0.9456, 0.9342, 0.9313, 0.9251, 0.9244, 0.9173, 0.9169, 0.9136,
         0.9130],
        [0.9612, 0.9553, 0.9511, 0.9395, 0.9390, 0.9345, 0.9335, 0.9319, 0.9317,
         0.9300],
        [0.9251, 0.9128, 0.8994, 0.8975, 0.8926, 0.8916, 0.8900, 0.8887, 0.8856,
         0.8838],
        [0.9023, 0.8976, 0.8962, 0.8961, 0.8942, 0.8935, 0.8907, 0.8891, 0.8878,
         0.8865],
        [0.9398, 0.9318, 0.9317, 0.9298, 0.9292, 0.9287, 0.9255, 0.9239, 0.9235,
         0.9225],
        [0.9252, 0.9106, 0.9098, 0.9073, 0.9051, 0.9039, 0.9034, 0.9014, 0.8882,
         0.8839],
        [0.9673, 0.9482, 0.9416, 0.9381, 0.9330, 0.9266, 0.9247, 0.9173, 0.9123,
         0.9111],
        [0.9405, 0.9333, 0.9294, 0.9261, 0.9237, 0.9228, 0.9222, 0.9218, 0.9215,
         0.9201],
        [0.9530, 0.8985, 0.8878, 0.8788, 0.8644, 0.8596, 0.8596, 0.8573, 0.8572,
         0.8566],
        [0.9351, 0.9311, 0.9289, 0.9226, 0.9212, 0.9170, 0.9129, 0.9093, 0.9029,
         0.9025],
        [0.9228, 0.9209, 0.9185, 0.9140, 0.9065, 0.8988, 0.8966, 0.8960, 0.8950,
         0.8939],
        [0.9594, 0.9478, 0.9419, 0.9251, 0.9239, 0.9233, 0.9186, 0.9175, 0.9173,
         0.9159],
        [0.9422, 0.9353, 0.9257, 0.9243, 0.9236, 0.9216, 0.9199, 0.9169, 0.9128,
         0.9116],
        [0.9305, 0.9259, 0.9248, 0.9243, 0.9210, 0.9176, 0.9168, 0.9167, 0.9132,
         0.9130],
        [0.9442, 0.9442, 0.9438, 0.9432, 0.9415, 0.9409, 0.9332, 0.9307, 0.9296,
         0.9275],
        [0.9365, 0.9203, 0.9190, 0.9087, 0.9013, 0.9009, 0.8965, 0.8961, 0.8954,
         0.8950],
        [0.8735, 0.8682, 0.8650, 0.8574, 0.8563, 0.8495, 0.8484, 0.8483, 0.8473,
         0.8471],
        [0.8996, 0.8893, 0.8844, 0.8823, 0.8796, 0.8783, 0.8780, 0.8770, 0.8752,
         0.8740],
        [0.9178, 0.9137, 0.9128, 0.9072, 0.9002, 0.8996, 0.8959, 0.8890, 0.8861,
         0.8857],
        [0.9049, 0.8944, 0.8919, 0.8842, 0.8804, 0.8753, 0.8752, 0.8688, 0.8582,
         0.8572],
        [0.9473, 0.8672, 0.8635, 0.8502, 0.8460, 0.8294, 0.8200, 0.8177, 0.8153,
         0.8133],
        [0.8989, 0.8800, 0.8780, 0.8751, 0.8722, 0.8690, 0.8541, 0.8538, 0.8535,
         0.8515],
        [0.8925, 0.8887, 0.8679, 0.8605, 0.8481, 0.8442, 0.8414, 0.8359, 0.8345,
         0.8327],
        [0.9338, 0.9303, 0.9287, 0.9285, 0.9272, 0.9251, 0.9243, 0.9236, 0.9236,
         0.9215],
        [0.8796, 0.8759, 0.8759, 0.8647, 0.8599, 0.8548, 0.8523, 0.8388, 0.8301,
         0.8230],
        [0.8994, 0.8605, 0.8598, 0.8569, 0.8508, 0.8496, 0.8474, 0.8453, 0.8449,
         0.8447],
        [0.8839, 0.8723, 0.8688, 0.8598, 0.8506, 0.8460, 0.8397, 0.8378, 0.8333,
         0.8333],
        [0.8743, 0.8710, 0.8560, 0.8499, 0.8471, 0.8433, 0.8418, 0.8409, 0.8399,
         0.8336],
        [0.8500, 0.8361, 0.8193, 0.8171, 0.8086, 0.8084, 0.7967, 0.7803, 0.7734,
         0.7703],
        [0.8288, 0.8045, 0.7889, 0.7852, 0.7793, 0.7793, 0.7756, 0.7635, 0.7589,
         0.7576],
        [0.8984, 0.8809, 0.8773, 0.8741, 0.8729, 0.8720, 0.8610, 0.8605, 0.8601,
         0.8580],
        [0.8042, 0.7836, 0.7784, 0.7716, 0.7628, 0.7589, 0.7401, 0.7386, 0.7299,
         0.7204],
        [0.8306, 0.8279, 0.8097, 0.8089, 0.8025, 0.8023, 0.8016, 0.7963, 0.7885,
         0.7841],
        [0.8900, 0.8611, 0.8430, 0.8313, 0.8305, 0.8285, 0.8202, 0.8194, 0.8148,
         0.8120],
        [0.8744, 0.8528, 0.8467, 0.8460, 0.8348, 0.8260, 0.8204, 0.8096, 0.8090,
         0.8041],
        [0.8869, 0.8840, 0.8433, 0.8140, 0.8103, 0.8045, 0.8016, 0.7970, 0.7968,
         0.7919],
        [0.9232, 0.9063, 0.9002, 0.8919, 0.8631, 0.8582, 0.8554, 0.8507, 0.8413,
         0.8368],
        [0.9111, 0.8736, 0.8692, 0.8591, 0.8523, 0.8508, 0.8424, 0.8408, 0.8377,
         0.8369],
        [0.9226, 0.8999, 0.8969, 0.8964, 0.8947, 0.8913, 0.8913, 0.8886, 0.8882,
         0.8869],
        [0.8984, 0.8787, 0.8778, 0.8666, 0.8645, 0.8638, 0.8608, 0.8602, 0.8599,
         0.8580],
        [0.9036, 0.8757, 0.8665, 0.8644, 0.8584, 0.8549, 0.8417, 0.8277, 0.8236,
         0.8225],
        [0.9393, 0.8780, 0.8694, 0.8611, 0.8544, 0.8503, 0.8344, 0.8305, 0.8292,
         0.8178],
        [0.9049, 0.8587, 0.8540, 0.8431, 0.8303, 0.8284, 0.8225, 0.8224, 0.8151,
         0.8132],
        [0.8978, 0.8158, 0.8130, 0.8006, 0.7904, 0.7895, 0.7892, 0.7859, 0.7827,
         0.7743],
        [0.8833, 0.8652, 0.8557, 0.8533, 0.8460, 0.8427, 0.8399, 0.8395, 0.8389,
         0.8387],
        [0.8533, 0.7755, 0.7743, 0.7735, 0.7628, 0.7357, 0.7285, 0.7263, 0.7252,
         0.7228],
        [0.8822, 0.8551, 0.8408, 0.8213, 0.8117, 0.8105, 0.7919, 0.7903, 0.7887,
         0.7843],
        [0.8372, 0.8059, 0.7936, 0.7783, 0.7765, 0.7699, 0.7669, 0.7507, 0.7481,
         0.7434]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 265409.4375,  189926.2812,  157049.0469,  154480.4219,  141875.1094,
          136865.8438,  132326.5312,  124525.9297,  121691.1875,  117560.2734],
        [ 693769.3750,  578321.5000,  570362.4375,  563974.0625,  556350.4375,
          521551.9062,  521302.2812,  513132.1562,  496814.2812,  491829.9688],
        [ 410855.8438,  365444.3750,  266833.9688,  251242.9062,  203545.0000,
          182395.1250,  176332.0156,  155673.4531,  150928.9375,  136056.5000],
        [ 854315.0625,  312692.0625,  296353.4062,  284033.0000,  282303.3125,
          265935.9062,  232120.9375,  226176.5156,  219576.4375,  217267.2031],
        [ 168601.5625,  143788.2031,  141437.3906,  111995.3984,   98864.0703,
           98363.4609,   93406.2578,   86612.2031,   77382.6094,   70159.4922],
        [ 194108.4375,  117891.0312,  103618.1328,   86937.5859,   83259.5859,
           76834.3984,   71411.2266,   66645.6641,   58062.0664,   56065.7539],
        [ 229709.3750,  220833.9375,  196660.9844,  191568.7500,  181951.5938,
          178124.2500,  174032.5312,  163860.8750,  158802.5938,  153501.8906],
        [ 339581.6875,  256145.2031,  237930.7656,  203073.2500,  183162.1094,
          140431.3750,  132263.4531,  116547.5625,   98520.6250,   93490.3906],
        [ 426157.5938,  391798.4688,  317277.4688,  303784.2500,  302694.5625,
          298641.5625,  297894.3438,  284674.0625,  281249.9375,  280789.7500],
        [ 768775.3750,  299879.9062,  295215.5000,  286886.5312,  259530.3750,
          253036.2500,  238569.2500,  237942.1094,  218353.3750,  211967.6250],
        [ 723989.1875,  403375.9375,  344144.0312,  326581.5312,  325385.6250,
          314388.9375,  301942.9688,  294617.8438,  284072.5625,  275308.3750],
        [ 315419.9688,  253904.0469,  200383.2188,  177965.6562,  128405.5625,
           88614.0703,   87240.3203,   83745.6719,   70267.6406,   65548.8750],
        [ 455312.7812,  414187.2500,  390284.7500,  330105.1875,  283798.7812,
          270130.7500,  241113.8281,  225778.4844,  220530.6562,  188234.1562],
        [ 856900.8125,  834673.6875,  791917.4375,  789613.6250,  759894.2500,
          738561.6250,  710881.6875,  691842.7500,  671910.9375,  624870.5000],
        [ 695579.9375,  570736.8125,  561671.9375,  550863.9375,  527697.3125,
          495811.7812,  491769.4688,  470596.4062,  465991.0938,  456660.4688],
        [ 555044.6250,  520179.9688,  512759.3750,  462734.4062,  450438.9062,
          441625.9375,  429286.2188,  409127.2500,  405031.9375,  395409.4688],
        [ 799502.8125,  735572.7500,  625504.8750,  599831.0000,  548593.8750,
          543314.8750,  491160.1562,  488227.3438,  465474.5938,  461696.2812],
        [ 919767.6250,  844610.0000,  796344.8750,  674742.0625,  669127.3125,
          627426.2500,  618940.7500,  604840.6250,  603101.7500,  588442.5000],
        [ 548656.1250,  460400.5312,  380005.6562,  370105.6250,  344966.1562,
          340205.3438,  332389.8125,  326185.0000,  312364.5000,  304393.2500],
        [ 396466.7812,  370843.6875,  363465.3125,  362790.3750,  352927.9688,
          349576.5000,  335630.5938,  328229.8750,  322106.4062,  316215.9688],
        [ 676850.1875,  603753.1250,  603562.0625,  586877.7500,  582073.0000,
          577979.6250,  551952.9375,  539841.0000,  536378.0625,  528894.8750],
        [ 549747.0625,  446235.9062,  441047.2188,  425956.8750,  412591.7812,
          405630.7188,  402662.5625,  391029.8438,  324136.5312,  304825.8125],
        [1002457.0000,  764038.4375,  695045.5625,  660930.1875,  614568.2500,
          560431.1250,  545607.3750,  490865.1562,  457247.9062,  449149.8750],
        [ 683933.2500,  617015.3750,  583853.6875,  556680.5625,  538345.9375,
          531424.3750,  526356.3125,  523951.3750,  521696.6875,  511181.3750],
        [ 817185.0000,  375267.3125,  321951.3125,  283285.0000,  230636.5625,
          215409.1406,  215314.4531,  208299.0000,  207974.4688,  206208.2656],
        [ 633384.6875,  598035.2500,  579557.6875,  529354.0625,  519384.8438,
          488606.0625,  461024.0000,  437833.2812,  399755.0312,  397458.6250],
        [ 530998.3125,  516868.2812,  499197.5000,  468499.4062,  420605.4375,
          376964.3125,  365292.0938,  362163.6562,  356881.8438,  351345.2188],
        [ 895591.9375,  759227.1250,  697450.5000,  549086.9375,  539749.3750,
          534948.1875,  500020.8438,  492678.2812,  490842.2188,  481487.1250],
        [ 701218.6875,  635419.4375,  553905.0625,  542742.6250,  537478.5000,
          522195.4688,  509460.0000,  488128.1875,  460736.5625,  452512.9062],
        [ 592715.6250,  555493.6875,  546546.3125,  542704.8750,  517598.8125,
          493192.1250,  487414.5938,  486636.1875,  462968.7812,  461857.5000],
        [ 721576.6875,  721518.8125,  717276.9375,  710510.2500,  694060.6250,
          687709.0000,  616529.5000,  594912.8750,  585393.6250,  567931.9375],
        [ 646454.5625,  512690.9688,  503113.7188,  434426.8750,  390705.2188,
          388432.2188,  364692.7188,  362954.7500,  359285.2812,  356839.2812],
        [ 262700.3438,  243468.8594,  232521.5312,  208750.6250,  205307.1875,
          186522.9531,  183382.3281,  183293.1562,  180548.9062,  180205.2031],
        [ 381146.4062,  329216.7500,  306716.5625,  298013.9688,  286667.7500,
          281128.1875,  280120.0312,  276025.5938,  269231.0938,  264686.7812],
        [ 494541.9688,  466356.1250,  460617.5000,  425104.6562,  384806.4062,
          381240.9062,  361508.3438,  327824.7500,  314615.7188,  312759.1562],
        [ 411090.2188,  354205.9062,  341527.7500,  305986.4688,  289988.0000,
          269307.6562,  269238.0625,  245422.3906,  211063.1094,  208049.8594],
        [ 753250.5625,  240058.1875,  227618.9375,  188339.0469,  177367.2188,
          139853.3438,  122229.4688,  118403.0234,  114355.5859,  111078.3984],
        [ 377429.0625,  288019.0000,  280109.9062,  268567.1875,  257674.0625,
          246136.3750,  199091.9062,  198119.5156,  197492.2656,  191740.3750],
        [ 344784.9375,  326358.0000,  242350.6562,  218130.0469,  182812.5625,
          172788.2500,  165965.3125,  153404.5781,  150541.2344,  146732.5938],
        [ 621292.1875,  591340.8125,  577641.8750,  576634.6250,  565651.9375,
          548750.3125,  542318.3750,  537495.3750,  537041.4375,  521728.5625],
        [ 286611.1562,  271876.5625,  271770.8125,  231671.5625,  216357.0156,
          201043.2031,  194075.5000,  160098.7812,  141287.7500,  127607.7891],
        [ 380072.7188,  218087.8125,  215873.9062,  207283.6250,  189870.3281,
          186804.5781,  180850.4688,  175535.7031,  174667.0312,  174021.2344],
        [ 304520.1250,  258249.2344,  245754.9844,  216106.4531,  189469.4844,
          177325.4531,  162045.7656,  157642.2344,  147981.8906,  147865.0938],
        [ 265790.6250,  253506.7656,  204442.8125,  187570.5938,  180158.7969,
          170490.0938,  167082.5312,  164901.1719,  162592.3750,  148526.7969],
        [ 187802.7500,  154043.4688,  121116.8047,  117409.9219,  103870.5312,
          103557.8750,   87637.2500,   69370.4922,   62847.3945,   60167.4961],
        [ 138646.3438,   98074.6797,   78478.4453,   74386.0625,   68424.8672,
           68386.2422,   64886.8008,   54559.7695,   51068.9492,   50146.0352],
        [ 374695.1562,  291899.9688,  277490.6875,  264767.0625,  260565.2812,
          257222.0625,  219656.2344,  218082.0000,  216895.3750,  210488.8281],
        [  97637.1797,   72681.7578,   67494.6406,   61296.4102,   54013.2188,
           51060.6250,   39048.5195,   38258.6523,   33779.1484,   29471.3496],
        [ 142308.2031,  136881.9062,  105590.7031,  104379.2422,   95211.4766,
           94981.7500,   94090.7422,   87159.6562,   77998.6094,   73202.7812],
        [ 332372.3750,  220128.0781,  169929.0938,  143660.0469,  142172.5469,
          138086.3281,  122733.8516,  121195.4922,  113514.0469,  109116.6719],
        [ 266209.7188,  195354.7500,  179169.3125,  177220.7969,  151163.8750,
          133230.9062,  123063.3125,  105381.5547,  104549.4062,   97398.3594],
        [ 317906.8438,  305043.0312,  170554.9844,  112209.2266,  106516.8438,
           97990.8203,   94072.2656,   88063.5234,   87829.1094,   81921.1797],
        [ 534162.5625,  419836.8125,  384674.6562,  341751.5938,  226329.9375,
          211091.9062,  202745.2500,  189718.1094,  165855.6719,  155405.2656],
        [ 449102.3438,  262980.8438,  247061.0625,  213709.5781,  194054.9531,
          189891.8750,  168326.6719,  164557.4375,  157430.1094,  155616.9062],
        [ 529934.5000,  383051.4375,  366985.6250,  364390.9375,  355423.3750,
          338658.3750,  338658.3750,  325888.0625,  323867.6562,  318161.6250],
        [ 374945.3750,  282772.4375,  279113.4062,  237959.8125,  230800.5000,
          228658.2812,  219006.9531,  217123.6719,  216286.0469,  210500.2656],
        [ 403917.5625,  270890.5000,  237704.4062,  230778.5000,  211759.7188,
          201437.9844,  166659.3750,  136466.8906,  128789.4219,  126783.5391],
        [ 672750.8125,  280211.4062,  247798.7031,  219928.7188,  199899.9219,
          188663.8750,  150310.8438,  142131.8594,  139512.4531,  118479.6016],
        [ 411526.4062,  212718.7031,  198715.0000,  170194.1094,  141688.3750,
          137836.5938,  126709.4453,  126636.9609,  114111.2344,  110955.0547],
        [ 371419.1875,  115224.6875,  110612.8438,   92753.2812,   80156.1484,
           79101.1250,   78795.5234,   75109.6719,   71789.9922,   63637.4219],
        [ 302162.1562,  233209.0781,  203698.2031,  196758.1719,  177391.4062,
          169163.3438,  162450.2344,  161548.1406,  160169.6406,  159743.8750],
        [ 196678.9844,   64755.6836,   63628.9219,   62909.8828,   54038.9805,
           36693.9258,   33085.9258,   32063.4199,   31586.7109,   30493.0312],
        [ 297256.6562,  201888.9688,  164576.2812,  124633.9375,  108616.4609,
          106773.4453,   81841.5312,   80026.0703,   78223.1953,   73464.0625],
        [ 156444.2344,  100013.6016,   83828.8594,   67369.1719,   65739.8828,
           59794.2578,   57304.7969,   45442.0664,   43814.9258,   40918.0117]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[265409.4375,      0.0000],
         [189926.2812,      0.0000],
         [157049.0469,      0.0000],
         ...,
         [124525.9297,      0.0000],
         [121691.1875,      0.0000],
         [117560.2734,      0.0000]],

        [[693769.3750,      0.0000],
         [578321.5000,      0.0000],
         [570362.4375,      0.0000],
         ...,
         [513132.1562,      0.0000],
         [496814.2812,      0.0000],
         [491829.9688,      0.0000]],

        [[410855.8438,      0.0000],
         [365444.3750,      0.0000],
         [266833.9688,      0.0000],
         ...,
         [155673.4531,      0.0000],
         [150928.9375,      0.0000],
         [136056.5000,      0.0000]],

        ...,

        [[     0.0000, 196678.9844],
         [ 64755.6836,      0.0000],
         [ 63628.9219,      0.0000],
         ...,
         [ 32063.4199,      0.0000],
         [ 31586.7109,      0.0000],
         [ 30493.0312,      0.0000]],

        [[297256.6562,      0.0000],
         [     0.0000, 201888.9688],
         [164576.2812,      0.0000],
         ...,
         [ 80026.0703,      0.0000],
         [ 78223.1953,      0.0000],
         [ 73464.0625,      0.0000]],

        [[     0.0000, 156444.2344],
         [     0.0000, 100013.6016],
         [     0.0000,  83828.8594],
         ...,
         [ 45442.0664,      0.0000],
         [     0.0000,  43814.9258],
         [     0.0000,  40918.0117]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1541710.1250,       0.0000],
        [5507408.0000,       0.0000],
        [2299308.0000,       0.0000],
        [3190774.0000,       0.0000],
        [ 451307.2188,  639303.3750],
        [ 771353.8125,  143480.0625],
        [1690244.1250,  158802.5938],
        [1801146.5000,       0.0000],
        [3184961.7500,       0.0000],
        [3070156.0000,       0.0000],
        [3593807.0000,       0.0000],
        [1471495.1250,       0.0000],
        [3019476.7500,       0.0000],
        [7471067.5000,       0.0000],
        [5287379.5000,       0.0000],
        [4581638.0000,       0.0000],
        [5758878.5000,       0.0000],
        [6947344.0000,       0.0000],
        [3719672.0000,       0.0000],
        [3170023.5000,  328229.8750],
        [5788163.0000,       0.0000],
        [4103864.5000,       0.0000],
        [6240341.0000,       0.0000],
        [5594439.0000,       0.0000],
        [3081530.5000,       0.0000],
        [5044393.5000,       0.0000],
        [4248816.0000,       0.0000],
        [5941083.0000,       0.0000],
        [5403797.5000,       0.0000],
        [5147128.5000,       0.0000],
        [6617420.0000,       0.0000],
        [4319595.5000,       0.0000],
        [2066701.1250,       0.0000],
        [2379568.7500,  593384.3125],
        [1777325.5000, 2152050.0000],
        [1018741.2500, 1887138.2500],
        [1088166.6250, 1104387.1250],
        [1312690.2500, 1191689.2500],
        [ 704888.4375, 1398979.7500],
        [5619895.5000,       0.0000],
        [1974792.3750,  127607.7891],
        [2103067.5000,       0.0000],
        [2006960.7500,       0.0000],
        [1905062.6250,       0.0000],
        [ 233408.5312,  834415.5000],
        [ 327152.9375,  419905.2812],
        [2217067.5000,  374695.1562],
        [ 342701.6875,  202039.8281],
        [ 448331.0000,  563474.0625],
        [ 441489.0625, 1171419.5000],
        [1294961.7500,  237780.3125],
        [ 668602.9375,  793504.8750],
        [1669173.3750, 1162398.5000],
        [2012840.0000,  189891.8750],
        [3645020.0000,       0.0000],
        [2497167.0000,       0.0000],
        [ 968363.3125, 1146824.6250],
        [1659648.1250,  700040.0000],
        [ 944530.7500,  806561.1250],
        [ 530633.8750,  607966.0000],
        [1213531.6250,  712762.6250],
        [ 409256.5000,  196678.9844],
        [ 900021.7500,  417278.8750],
        [ 105236.3281,  615433.5625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 461/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:33, 61.15s/it]  7%|▋         | 2/30 [01:02<12:06, 25.95s/it] 10%|█         | 3/30 [01:03<06:29, 14.44s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.03s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.05s/it] 20%|██        | 6/30 [01:05<01:41,  4.24s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.3235919157663982
Epoch 462/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:49, 57.56s/it]  7%|▋         | 2/30 [01:01<12:10, 26.09s/it] 10%|█         | 3/30 [01:02<06:31, 14.52s/it] 13%|█▎        | 4/30 [01:03<03:56,  9.08s/it] 17%|█▋        | 5/30 [01:03<02:31,  6.08s/it] 20%|██        | 6/30 [01:04<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:06<00:38,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.3493457714716595
Epoch 463/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:46, 57.45s/it]  7%|▋         | 2/30 [00:58<11:14, 24.10s/it] 10%|█         | 3/30 [00:58<06:02, 13.44s/it] 13%|█▎        | 4/30 [01:00<03:50,  8.86s/it] 17%|█▋        | 5/30 [01:02<02:32,  6.11s/it] 20%|██        | 6/30 [01:02<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:03<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:04<00:52,  2.37s/it] 30%|███       | 9/30 [01:05<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:05<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.28s/it] 40%|████      | 12/30 [01:07<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:07<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 2.304975724220276
Epoch 464/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:17, 60.59s/it]  7%|▋         | 2/30 [01:01<11:50, 25.39s/it] 10%|█         | 3/30 [01:02<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3264297962188722
Epoch 465/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:42, 59.38s/it]  7%|▋         | 2/30 [01:02<12:16, 26.30s/it] 10%|█         | 3/30 [01:03<06:35, 14.63s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.15s/it] 17%|█▋        | 5/30 [01:04<02:33,  6.12s/it] 20%|██        | 6/30 [01:05<01:43,  4.29s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:07<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.30786558787028
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0009,  0.0132,  ..., -0.0058,  0.0175,  0.0040],
        [-0.0023,  0.0095,  0.0286,  ...,  0.0172,  0.0042, -0.0148],
        [-0.0293, -0.0380,  0.0247,  ...,  0.0834, -0.0043, -0.0143],
        ...,
        [ 0.0059, -0.0033,  0.0099,  ..., -0.0241, -0.0051, -0.0028],
        [-0.0333,  0.0099, -0.0035,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0327, -0.0139,  0.0193,  ...,  0.0414,  0.0360, -0.0284]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8741, 0.8508, 0.8374, 0.8363, 0.8303, 0.8278, 0.8254, 0.8212, 0.8196,
         0.8172],
        [0.9415, 0.9287, 0.9278, 0.9269, 0.9260, 0.9214, 0.9214, 0.9203, 0.9181,
         0.9174],
        [0.9048, 0.8966, 0.8745, 0.8703, 0.8556, 0.8479, 0.8455, 0.8368, 0.8347,
         0.8274],
        [0.9561, 0.8856, 0.8819, 0.8789, 0.8785, 0.8743, 0.8648, 0.8630, 0.8609,
         0.8602],
        [0.8424, 0.8313, 0.8301, 0.8138, 0.8050, 0.8047, 0.8011, 0.7958, 0.7879,
         0.7811],
        [0.8523, 0.8174, 0.8084, 0.7960, 0.7931, 0.7874, 0.7823, 0.7775, 0.7677,
         0.7654],
        [0.8640, 0.8613, 0.8532, 0.8514, 0.8477, 0.8463, 0.8446, 0.8404, 0.8382,
         0.8358],
        [0.8914, 0.8717, 0.8665, 0.8554, 0.8482, 0.8296, 0.8254, 0.8166, 0.8048,
         0.8011],
        [0.9073, 0.9015, 0.8867, 0.8837, 0.8834, 0.8824, 0.8823, 0.8791, 0.8783,
         0.8781],
        [0.9487, 0.8827, 0.8816, 0.8796, 0.8726, 0.8708, 0.8667, 0.8665, 0.8605,
         0.8584],
        [0.9445, 0.9035, 0.8924, 0.8887, 0.8884, 0.8860, 0.8832, 0.8815, 0.8789,
         0.8767],
        [0.8862, 0.8710, 0.8545, 0.8461, 0.8233, 0.7974, 0.7962, 0.7933, 0.7811,
         0.7762],
        [0.9119, 0.9053, 0.9012, 0.8894, 0.8788, 0.8754, 0.8674, 0.8628, 0.8612,
         0.8501],
        [0.9562, 0.9544, 0.9507, 0.9505, 0.9478, 0.9458, 0.9432, 0.9412, 0.9392,
         0.9341],
        [0.9417, 0.9278, 0.9267, 0.9253, 0.9223, 0.9179, 0.9174, 0.9143, 0.9136,
         0.9122],
        [0.9259, 0.9213, 0.9203, 0.9131, 0.9112, 0.9098, 0.9079, 0.9045, 0.9038,
         0.9021],
        [0.9514, 0.9456, 0.9342, 0.9312, 0.9250, 0.9243, 0.9173, 0.9168, 0.9135,
         0.9129],
        [0.9612, 0.9552, 0.9511, 0.9395, 0.9389, 0.9344, 0.9335, 0.9318, 0.9317,
         0.9299],
        [0.9250, 0.9127, 0.8993, 0.8975, 0.8925, 0.8916, 0.8899, 0.8886, 0.8856,
         0.8838],
        [0.9023, 0.8976, 0.8962, 0.8961, 0.8941, 0.8935, 0.8906, 0.8891, 0.8877,
         0.8864],
        [0.9397, 0.9317, 0.9317, 0.9297, 0.9292, 0.9287, 0.9254, 0.9239, 0.9234,
         0.9225],
        [0.9252, 0.9106, 0.9097, 0.9073, 0.9051, 0.9039, 0.9033, 0.9013, 0.8882,
         0.8838],
        [0.9672, 0.9482, 0.9416, 0.9381, 0.9330, 0.9265, 0.9246, 0.9173, 0.9123,
         0.9110],
        [0.9405, 0.9333, 0.9294, 0.9261, 0.9237, 0.9228, 0.9221, 0.9218, 0.9215,
         0.9201],
        [0.9529, 0.8984, 0.8877, 0.8788, 0.8644, 0.8596, 0.8595, 0.8572, 0.8571,
         0.8566],
        [0.9351, 0.9311, 0.9289, 0.9225, 0.9212, 0.9169, 0.9128, 0.9092, 0.9029,
         0.9024],
        [0.9227, 0.9208, 0.9184, 0.9140, 0.9064, 0.8987, 0.8965, 0.8959, 0.8949,
         0.8938],
        [0.9593, 0.9478, 0.9418, 0.9251, 0.9239, 0.9233, 0.9185, 0.9175, 0.9172,
         0.9159],
        [0.9422, 0.9353, 0.9257, 0.9243, 0.9236, 0.9216, 0.9198, 0.9168, 0.9128,
         0.9115],
        [0.9304, 0.9259, 0.9248, 0.9243, 0.9210, 0.9176, 0.9167, 0.9166, 0.9132,
         0.9130],
        [0.9442, 0.9442, 0.9438, 0.9431, 0.9415, 0.9409, 0.9332, 0.9307, 0.9296,
         0.9275],
        [0.9365, 0.9203, 0.9190, 0.9087, 0.9013, 0.9008, 0.8964, 0.8961, 0.8954,
         0.8949],
        [0.8734, 0.8681, 0.8649, 0.8574, 0.8562, 0.8495, 0.8483, 0.8483, 0.8472,
         0.8471],
        [0.8995, 0.8892, 0.8843, 0.8823, 0.8796, 0.8782, 0.8779, 0.8769, 0.8752,
         0.8740],
        [0.9177, 0.9137, 0.9128, 0.9071, 0.9002, 0.8995, 0.8958, 0.8890, 0.8861,
         0.8857],
        [0.9048, 0.8944, 0.8918, 0.8842, 0.8804, 0.8752, 0.8752, 0.8687, 0.8581,
         0.8571],
        [0.9472, 0.8672, 0.8634, 0.8502, 0.8460, 0.8293, 0.8199, 0.8177, 0.8153,
         0.8131],
        [0.8988, 0.8799, 0.8779, 0.8750, 0.8721, 0.8689, 0.8540, 0.8537, 0.8535,
         0.8514],
        [0.8925, 0.8887, 0.8678, 0.8604, 0.8481, 0.8441, 0.8413, 0.8358, 0.8345,
         0.8327],
        [0.9337, 0.9303, 0.9286, 0.9285, 0.9271, 0.9250, 0.9242, 0.9236, 0.9236,
         0.9215],
        [0.8796, 0.8759, 0.8758, 0.8646, 0.8599, 0.8547, 0.8522, 0.8388, 0.8300,
         0.8229],
        [0.8993, 0.8604, 0.8597, 0.8569, 0.8507, 0.8496, 0.8473, 0.8452, 0.8449,
         0.8446],
        [0.8838, 0.8722, 0.8688, 0.8598, 0.8506, 0.8459, 0.8396, 0.8377, 0.8333,
         0.8332],
        [0.8742, 0.8710, 0.8559, 0.8498, 0.8470, 0.8432, 0.8417, 0.8409, 0.8399,
         0.8336],
        [0.8499, 0.8361, 0.8192, 0.8170, 0.8085, 0.8083, 0.7966, 0.7802, 0.7733,
         0.7702],
        [0.8287, 0.8045, 0.7888, 0.7852, 0.7793, 0.7793, 0.7756, 0.7635, 0.7588,
         0.7576],
        [0.8983, 0.8808, 0.8772, 0.8740, 0.8728, 0.8719, 0.8609, 0.8603, 0.8600,
         0.8579],
        [0.8042, 0.7835, 0.7783, 0.7716, 0.7628, 0.7588, 0.7399, 0.7385, 0.7299,
         0.7204],
        [0.8305, 0.8278, 0.8097, 0.8088, 0.8024, 0.8023, 0.8016, 0.7962, 0.7885,
         0.7840],
        [0.8899, 0.8611, 0.8429, 0.8311, 0.8305, 0.8284, 0.8202, 0.8193, 0.8147,
         0.8120],
        [0.8743, 0.8527, 0.8467, 0.8459, 0.8348, 0.8260, 0.8204, 0.8095, 0.8090,
         0.8040],
        [0.8868, 0.8839, 0.8432, 0.8139, 0.8102, 0.8043, 0.8015, 0.7969, 0.7967,
         0.7918],
        [0.9231, 0.9063, 0.9001, 0.8919, 0.8630, 0.8581, 0.8553, 0.8506, 0.8413,
         0.8367],
        [0.9110, 0.8736, 0.8692, 0.8590, 0.8523, 0.8507, 0.8423, 0.8407, 0.8376,
         0.8368],
        [0.9226, 0.8999, 0.8969, 0.8964, 0.8946, 0.8912, 0.8912, 0.8885, 0.8881,
         0.8869],
        [0.8983, 0.8786, 0.8777, 0.8665, 0.8644, 0.8637, 0.8608, 0.8601, 0.8598,
         0.8579],
        [0.9035, 0.8756, 0.8664, 0.8644, 0.8584, 0.8549, 0.8416, 0.8276, 0.8235,
         0.8224],
        [0.9393, 0.8780, 0.8694, 0.8610, 0.8543, 0.8503, 0.8344, 0.8304, 0.8291,
         0.8177],
        [0.9049, 0.8587, 0.8539, 0.8431, 0.8303, 0.8283, 0.8224, 0.8223, 0.8150,
         0.8131],
        [0.8977, 0.8157, 0.8129, 0.8005, 0.7903, 0.7894, 0.7891, 0.7858, 0.7826,
         0.7741],
        [0.8833, 0.8651, 0.8556, 0.8532, 0.8459, 0.8427, 0.8398, 0.8394, 0.8387,
         0.8386],
        [0.8532, 0.7754, 0.7742, 0.7734, 0.7627, 0.7357, 0.7284, 0.7263, 0.7252,
         0.7227],
        [0.8821, 0.8551, 0.8407, 0.8213, 0.8116, 0.8104, 0.7917, 0.7902, 0.7886,
         0.7842],
        [0.8372, 0.8058, 0.7935, 0.7782, 0.7765, 0.7698, 0.7669, 0.7506, 0.7480,
         0.7433]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 265058.0938,  189952.0000,  156838.9219,  154331.3906,  141767.4531,
          136766.6875,  132127.9219,  124367.4922,  121580.4141,  117550.8594],
        [ 693528.6250,  577982.9375,  570194.4375,  563554.6875,  556245.3750,
          521022.9688,  520918.6250,  512771.1250,  496508.7812,  491468.9375],
        [ 410672.5312,  365199.4375,  266449.2188,  251021.1250,  203302.8906,
          182244.5625,  176134.5312,  155507.5625,  150792.1094,  135977.2500],
        [ 854330.5000,  312383.5625,  296039.3125,  283885.6875,  282225.0000,
          265647.4688,  231923.7969,  225952.5312,  219275.2969,  217122.2188],
        [ 168540.9688,  143741.3125,  141262.2969,  111929.4141,   98755.6953,
           98257.2344,   93367.0781,   86551.2656,   77300.8203,   70162.5703],
        [ 194021.6406,  117833.7109,  103591.2578,   86863.3359,   83270.0703,
           76743.0078,   71359.6875,   66633.3984,   57975.8086,   56028.3867],
        [ 229446.6406,  220576.5156,  196454.0312,  191454.9688,  181759.7812,
          178025.9219,  173844.0938,  163701.7031,  158571.0469,  153295.7656],
        [ 339217.5625,  255922.7656,  237762.4531,  202898.4531,  182993.9531,
          140259.5156,  132165.8594,  116425.2578,   98416.3828,   93365.2031],
        [ 425909.3125,  391652.4062,  317001.0312,  303700.8438,  302563.2500,
          298344.9375,  297734.6875,  284558.9688,  281108.3438,  280602.6562],
        [ 768620.0000,  299609.1875,  294951.8438,  286677.0312,  259294.3594,
          252835.0625,  238337.0625,  237698.7500,  218215.5469,  211801.7188],
        [ 723834.5625,  403308.6250,  343913.6875,  326271.7812,  325019.6562,
          314116.2500,  301789.5000,  294384.7188,  283885.6875,  274990.5938],
        [ 315003.3125,  253569.3750,  200168.9062,  177687.7188,  128207.2109,
           88538.1328,   87065.0312,   83569.1172,   70137.6172,   65432.3359],
        [ 454682.7500,  413943.5938,  389890.0625,  329756.2500,  283398.7500,
          269833.8750,  240838.0469,  225482.3906,  220340.6250,  188004.8906],
        [ 856338.0000,  834518.5000,  791426.6875,  789136.3125,  759486.3750,
          738050.4375,  710437.0625,  691346.8125,  671525.3125,  624498.1875],
        [ 695396.9375,  570504.5000,  561443.2500,  550621.7500,  527345.1875,
          495571.6562,  491553.3438,  470418.7500,  465872.9062,  456575.5312],
        [ 554948.8125,  519999.9062,  512565.2812,  462594.9688,  450352.9688,
          441452.0312,  429120.0312,  408932.6250,  404838.4688,  395336.7188],
        [ 799069.8750,  735183.5625,  625123.8125,  599319.3125,  548180.1875,
          542950.1875,  490864.2188,  487817.3125,  465145.2812,  461382.0312],
        [ 919330.8750,  844273.3750,  796160.3125,  674706.0625,  668928.2500,
          627112.2500,  618736.0000,  604482.5625,  602993.6250,  588236.0000],
        [ 548412.3750,  459945.8750,  379500.0938,  369940.4688,  344767.5312,
          339982.5312,  332205.6875,  325964.8438,  312197.4062,  304329.6875],
        [ 396178.3438,  370700.1250,  363284.4375,  362590.7812,  352728.4375,
          349359.2188,  335364.0625,  328116.2500,  321923.6875,  315951.9375],
        [ 676192.7500,  603235.1875,  603158.1250,  586603.5625,  581768.3125,
          577495.3125,  551654.0625,  539564.1250,  535912.2500,  528577.7500],
        [ 549648.0000,  446200.5938,  440818.4688,  425682.3750,  412467.8438,
          405312.8750,  402299.4688,  390840.0938,  323895.7812,  304464.6875],
        [1002088.0625,  763924.0000,  694845.3750,  660622.0000,  614371.3750,
          560110.5000,  545377.9375,  490702.2812,  457110.0938,  448879.6875],
        [ 683703.0000,  616734.1250,  583642.1250,  556553.1250,  538265.3125,
          531366.6250,  526189.2500,  523740.5312,  521410.6562,  511056.5625],
        [ 816731.5625,  375001.1562,  321596.5625,  283120.2500,  230469.4844,
          215218.9844,  214986.9844,  208048.8594,  207771.0625,  206168.9375],
        [ 633040.5000,  597781.5000,  579144.9375,  529017.0000,  519102.0938,
          488323.2812,  460722.9375,  437546.5312,  399549.2500,  397165.3125],
        [ 530631.3125,  516445.5000,  498745.9062,  468147.0312,  420237.0000,
          376685.8125,  365034.0312,  361881.9062,  356651.1562,  351117.0938],
        [ 895312.6875,  759055.5625,  697130.6250,  548747.1250,  539426.1875,
          534720.6875,  499826.7812,  492604.9688,  490633.4688,  481328.7188],
        [ 700840.9375,  635081.9375,  553757.7500,  542325.6250,  537114.6250,
          521850.4375,  509107.3438,  487872.2188,  460338.2188,  452191.9375],
        [ 592369.1875,  555276.0000,  546396.7500,  542486.4375,  517405.3125,
          492863.4375,  487188.3125,  486443.6250,  462892.4062,  461656.2188],
        [ 721350.3125,  721305.6250,  716861.1250,  710234.5000,  693705.8750,
          687440.8125,  616240.2500,  594662.1875,  585210.5000,  567686.0625],
        [ 646095.8125,  512491.5312,  502893.0625,  434091.8438,  390580.0312,
          388194.8438,  364491.0312,  362755.0625,  359049.2500,  356688.2500],
        [ 262448.4375,  243310.0938,  232321.3750,  208609.3281,  205215.1719,
          186455.9062,  183238.4531,  183143.4062,  180350.6406,  180015.7344],
        [ 380687.9375,  328859.6250,  306421.0000,  297763.9375,  286377.2812,
          280919.6875,  279783.9062,  275897.9375,  269012.7188,  264410.7500],
        [ 494154.4375,  466095.1250,  460383.4062,  424692.9375,  384753.9062,
          380955.5938,  361276.0312,  327580.6562,  314313.7188,  312510.2188],
        [ 410889.9375,  353895.9375,  341211.0000,  305837.9688,  289705.7500,
          269147.4375,  269092.0000,  245204.8281,  210887.4688,  207913.7812],
        [ 753165.0625,  240013.5469,  227493.7344,  188225.0156,  177256.2969,
          139651.4375,  122083.0312,  118297.9375,  114289.8438,  110869.6875],
        [ 377044.8438,  287644.2812,  279670.0000,  268476.2812,  257273.5625,
          245975.6250,  198789.3125,  197816.6719,  197321.1406,  191522.3594],
        [ 344612.6875,  326140.8125,  242070.0000,  217964.3125,  182679.2344,
          172531.0469,  165789.7188,  153189.0938,  150426.8594,  146646.8281],
        [ 620776.8750,  590946.1875,  577234.8750,  576333.3125,  565235.6250,
          548172.8750,  541964.1875,  537132.0625,  537052.1875,  521362.9375],
        [ 286374.5625,  271656.5312,  271518.2188,  231433.5156,  216196.9531,
          200873.0156,  193879.4062,  159934.4219,  141112.8281,  127465.7344],
        [ 379834.6250,  217964.1094,  215663.0000,  207111.9062,  189630.7344,
          186599.4688,  180642.9219,  175257.7031,  174503.3750,  173870.7812],
        [ 304292.2500,  257962.2188,  245479.2812,  215914.8594,  189280.5781,
          177114.5312,  161843.2812,  157457.2812,  147867.4844,  147781.7812],
        [ 265311.4688,  253332.7500,  204201.2031,  187273.3438,  179985.5156,
          170369.0156,  166789.1250,  164751.0469,  162471.6250,  148483.4688],
        [ 187587.5938,  153958.5781,  120962.2422,  117192.1094,  103751.3281,
          103412.7969,   87526.5781,   69239.4219,   62787.7891,   60053.5352],
        [ 138551.8281,   97969.5156,   78371.4219,   74347.5469,   68395.2422,
           68346.9297,   64850.2422,   54530.7422,   51043.4336,   50129.2070],
        [ 374333.3438,  291435.2188,  277098.0000,  264391.5938,  260132.0156,
          256704.4844,  219292.0312,  217652.5156,  216524.4062,  210096.7500],
        [  97531.2734,   72631.7969,   67428.2422,   61219.9375,   54001.7383,
           51010.3438,   38964.6367,   38180.5039,   33741.0273,   29467.5566],
        [ 142082.1406,  136708.9219,  105514.8047,  104288.0078,   95102.0312,
           94931.5000,   94054.8594,   87052.0000,   77933.0312,   73116.1953],
        [ 332017.8750,  219953.0469,  169714.8438,  143419.3906,  142017.9219,
          137940.3438,  122593.3516,  121069.6875,  113395.7812,  109033.4609],
        [ 265843.8750,  195230.1406,  179036.0938,  177024.1875,  151029.7188,
          133166.1094,  123003.2422,  105251.4922,  104486.1094,   97339.2969],
        [ 317691.6562,  304842.9688,  170352.5938,  112028.8359,  106332.5312,
           97774.5312,   93964.3125,   87977.7422,   87719.9453,   81778.7969],
        [ 533676.3125,  419533.4062,  384184.8438,  341476.2812,  226094.3594,
          210929.1094,  202559.9219,  189436.2344,  165741.0312,  155234.9219],
        [ 448873.6875,  262879.8125,  246919.7344,  213554.5156,  193920.6250,
          189673.2344,  168249.6250,  164318.9219,  157270.2812,  155471.6875],
        [ 529478.3125,  382712.9688,  366656.7812,  364083.5000,  355188.5312,
          338363.6250,  338363.6250,  325627.4062,  323646.9062,  317835.9375],
        [ 374562.2188,  282507.4688,  278901.3125,  237747.7188,  230578.9531,
          228442.2969,  218914.2500,  216836.6562,  216048.1406,  210205.1719],
        [ 403353.2500,  270688.5625,  237430.2812,  230516.5156,  211637.9688,
          201219.6719,  166631.0938,  136285.3281,  128545.0000,  126634.7969],
        [ 672469.8750,  280008.6562,  247564.3906,  219717.8125,  199704.8125,
          188531.6719,  150189.0469,  141917.8750,  139285.3906,  118291.3984],
        [ 411087.4688,  212600.6719,  198537.5156,  170013.0625,  141622.7188,
          137696.7969,  126577.4375,  126442.6719,  113953.3203,  110758.7188],
        [ 371170.9688,  115080.0547,  110469.5781,   92621.9297,   80035.0000,
           78967.4922,   78678.3828,   74994.4375,   71705.9062,   63527.2461],
        [ 302113.1875,  232961.6719,  203488.9062,  196575.1094,  177145.7812,
          169071.8906,  162250.9688,  161338.2969,  159865.6562,  159510.0469],
        [ 196567.2344,   64719.2617,   63539.9062,   62821.2695,   53980.6758,
           36700.3984,   33053.0000,   32060.9121,   31563.7344,   30456.1797],
        [ 297064.7812,  201814.4688,  164367.3438,  124571.0703,  108527.1016,
          106606.9922,   81650.4453,   79872.8125,   78072.8047,   73332.6797],
        [ 156353.4062,   99890.3594,   83738.2500,   67285.8984,   65682.6016,
           59695.4609,   57254.8125,   45367.9375,   43754.9648,   40860.1836]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[265058.0938,      0.0000],
         [189952.0000,      0.0000],
         [156838.9219,      0.0000],
         ...,
         [124367.4922,      0.0000],
         [121580.4141,      0.0000],
         [117550.8594,      0.0000]],

        [[693528.6250,      0.0000],
         [577982.9375,      0.0000],
         [570194.4375,      0.0000],
         ...,
         [512771.1250,      0.0000],
         [496508.7812,      0.0000],
         [491468.9375,      0.0000]],

        [[410672.5312,      0.0000],
         [365199.4375,      0.0000],
         [266449.2188,      0.0000],
         ...,
         [155507.5625,      0.0000],
         [150792.1094,      0.0000],
         [135977.2500,      0.0000]],

        ...,

        [[     0.0000, 196567.2344],
         [ 64719.2617,      0.0000],
         [ 63539.9062,      0.0000],
         ...,
         [ 32060.9121,      0.0000],
         [ 31563.7344,      0.0000],
         [ 30456.1797,      0.0000]],

        [[297064.7812,      0.0000],
         [     0.0000, 201814.4688],
         [164367.3438,      0.0000],
         ...,
         [ 79872.8125,      0.0000],
         [ 78072.8047,      0.0000],
         [ 73332.6797,      0.0000]],

        [[     0.0000, 156353.4062],
         [     0.0000,  99890.3594],
         [     0.0000,  83738.2500],
         ...,
         [ 45367.9375,      0.0000],
         [     0.0000,  43754.9648],
         [     0.0000,  40860.1836]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1540341.2500,       0.0000],
        [5504196.0000,       0.0000],
        [2297301.2500,       0.0000],
        [3188785.2500,       0.0000],
        [ 451017.0938,  638851.5000],
        [ 770943.8750,  143376.4062],
        [1688559.5000,  158571.0469],
        [1799427.5000,       0.0000],
        [3183176.2500,       0.0000],
        [3068040.5000,       0.0000],
        [3591515.2500,       0.0000],
        [1469378.8750,       0.0000],
        [3016171.0000,       0.0000],
        [7466764.0000,       0.0000],
        [5285303.5000,       0.0000],
        [4580142.0000,       0.0000],
        [5755035.5000,       0.0000],
        [6944959.5000,       0.0000],
        [3717246.2500,       0.0000],
        [3168081.0000,  328116.2500],
        [5784162.0000,       0.0000],
        [4101630.5000,       0.0000],
        [6238031.0000,       0.0000],
        [5592661.0000,       0.0000],
        [3079114.0000,       0.0000],
        [5041393.5000,       0.0000],
        [4245577.0000,       0.0000],
        [5938787.0000,       0.0000],
        [5400481.0000,       0.0000],
        [5144977.5000,       0.0000],
        [6614697.0000,       0.0000],
        [4317331.0000,       0.0000],
        [2065108.5000,       0.0000],
        [2377336.5000,  592798.2500],
        [1776064.0000, 2150652.0000],
        [1017902.0625, 1885884.1250],
        [1087297.2500, 1104048.2500],
        [1311095.1250, 1190438.8750],
        [ 704259.5000, 1397791.1250],
        [5616211.5000,       0.0000],
        [1972979.5000,  127465.7344],
        [2101078.5000,       0.0000],
        [2004993.3750,       0.0000],
        [1902968.6250,       0.0000],
        [ 233044.2812,  833427.6875],
        [ 326983.3750,  419552.7500],
        [2213327.0000,  374333.3438],
        [ 342269.1875,  201907.8750],
        [ 447904.0312,  562879.4375],
        [ 441051.3438, 1170104.2500],
        [1293758.0000,  237652.2188],
        [ 667576.6875,  792887.2500],
        [1667491.3750, 1161375.0000],
        [2011459.0000,  189673.2344],
        [3641957.5000,       0.0000],
        [2494744.0000,       0.0000],
        [ 967255.1250, 1145687.5000],
        [1658249.6250,  699431.2500],
        [ 943560.0625,  805730.3125],
        [ 529895.8750,  607355.1250],
        [1212100.8750,  712220.6250],
        [ 408895.3438,  196567.2344],
        [ 898931.9375,  416948.5625],
        [ 105063.3984,  614820.4375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 466/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:53, 59.76s/it]  7%|▋         | 2/30 [01:00<11:41, 25.05s/it] 10%|█         | 3/30 [01:01<06:16, 13.95s/it] 13%|█▎        | 4/30 [01:02<03:47,  8.74s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.86s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.2747436205546063
Epoch 467/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:16, 60.58s/it]  7%|▋         | 2/30 [01:01<11:50, 25.39s/it] 10%|█         | 3/30 [01:02<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:39,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.326499350865682
Epoch 468/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:57, 59.93s/it]  7%|▋         | 2/30 [01:00<11:43, 25.12s/it] 10%|█         | 3/30 [01:01<06:19, 14.05s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.80s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.2970951318740847
Epoch 469/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:42, 57.34s/it]  7%|▋         | 2/30 [00:58<11:13, 24.05s/it] 10%|█         | 3/30 [01:00<06:21, 14.13s/it] 13%|█▎        | 4/30 [01:01<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:01<02:28,  5.93s/it] 20%|██        | 6/30 [01:02<01:39,  4.17s/it] 23%|██▎       | 7/30 [01:03<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.32s/it] 30%|███       | 9/30 [01:04<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Epoch loss is 2.3008816719055174
Epoch 470/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:37, 59.24s/it]  7%|▋         | 2/30 [01:00<11:51, 25.40s/it] 10%|█         | 3/30 [01:02<06:26, 14.30s/it] 13%|█▎        | 4/30 [01:02<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:03<02:29,  5.99s/it] 20%|██        | 6/30 [01:04<01:40,  4.21s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.33s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.2952772696812946
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0008,  0.0132,  ..., -0.0058,  0.0176,  0.0040],
        [-0.0023,  0.0095,  0.0286,  ...,  0.0172,  0.0042, -0.0148],
        [-0.0293, -0.0379,  0.0247,  ...,  0.0834, -0.0043, -0.0143],
        ...,
        [ 0.0059, -0.0033,  0.0099,  ..., -0.0241, -0.0051, -0.0027],
        [-0.0333,  0.0099, -0.0035,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0327, -0.0139,  0.0193,  ...,  0.0414,  0.0360, -0.0284]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8741, 0.8508, 0.8373, 0.8362, 0.8303, 0.8278, 0.8253, 0.8211, 0.8195,
         0.8172],
        [0.9414, 0.9287, 0.9277, 0.9269, 0.9260, 0.9214, 0.9214, 0.9203, 0.9180,
         0.9173],
        [0.9048, 0.8965, 0.8744, 0.8703, 0.8555, 0.8479, 0.8455, 0.8368, 0.8346,
         0.8274],
        [0.9561, 0.8856, 0.8818, 0.8789, 0.8785, 0.8742, 0.8647, 0.8629, 0.8608,
         0.8601],
        [0.8424, 0.8313, 0.8300, 0.8138, 0.8050, 0.8046, 0.8011, 0.7958, 0.7878,
         0.7811],
        [0.8523, 0.8174, 0.8084, 0.7960, 0.7931, 0.7873, 0.7822, 0.7775, 0.7677,
         0.7653],
        [0.8640, 0.8612, 0.8531, 0.8513, 0.8477, 0.8462, 0.8445, 0.8403, 0.8381,
         0.8357],
        [0.8914, 0.8716, 0.8665, 0.8554, 0.8482, 0.8295, 0.8254, 0.8165, 0.8047,
         0.8010],
        [0.9073, 0.9015, 0.8866, 0.8837, 0.8834, 0.8824, 0.8823, 0.8791, 0.8782,
         0.8781],
        [0.9487, 0.8827, 0.8816, 0.8796, 0.8726, 0.8708, 0.8666, 0.8665, 0.8605,
         0.8584],
        [0.9444, 0.9035, 0.8923, 0.8886, 0.8883, 0.8860, 0.8832, 0.8814, 0.8789,
         0.8766],
        [0.8862, 0.8710, 0.8544, 0.8461, 0.8232, 0.7973, 0.7961, 0.7932, 0.7810,
         0.7761],
        [0.9118, 0.9053, 0.9011, 0.8894, 0.8788, 0.8753, 0.8674, 0.8628, 0.8612,
         0.8500],
        [0.9562, 0.9544, 0.9507, 0.9505, 0.9478, 0.9458, 0.9431, 0.9412, 0.9392,
         0.9341],
        [0.9416, 0.9278, 0.9267, 0.9253, 0.9223, 0.9179, 0.9173, 0.9143, 0.9136,
         0.9122],
        [0.9259, 0.9213, 0.9203, 0.9131, 0.9112, 0.9098, 0.9078, 0.9045, 0.9038,
         0.9021],
        [0.9514, 0.9455, 0.9342, 0.9312, 0.9250, 0.9243, 0.9172, 0.9168, 0.9135,
         0.9129],
        [0.9612, 0.9552, 0.9511, 0.9395, 0.9389, 0.9344, 0.9335, 0.9318, 0.9317,
         0.9299],
        [0.9250, 0.9127, 0.8992, 0.8975, 0.8925, 0.8915, 0.8899, 0.8886, 0.8856,
         0.8838],
        [0.9022, 0.8976, 0.8962, 0.8960, 0.8941, 0.8934, 0.8906, 0.8891, 0.8877,
         0.8864],
        [0.9396, 0.9317, 0.9317, 0.9297, 0.9291, 0.9286, 0.9254, 0.9239, 0.9234,
         0.9224],
        [0.9252, 0.9106, 0.9097, 0.9073, 0.9051, 0.9038, 0.9033, 0.9013, 0.8881,
         0.8838],
        [0.9672, 0.9482, 0.9416, 0.9380, 0.9330, 0.9265, 0.9246, 0.9172, 0.9123,
         0.9110],
        [0.9405, 0.9332, 0.9294, 0.9261, 0.9237, 0.9228, 0.9221, 0.9218, 0.9215,
         0.9201],
        [0.9529, 0.8984, 0.8876, 0.8787, 0.8643, 0.8595, 0.8594, 0.8571, 0.8570,
         0.8565],
        [0.9350, 0.9310, 0.9288, 0.9225, 0.9212, 0.9169, 0.9128, 0.9092, 0.9028,
         0.9024],
        [0.9227, 0.9208, 0.9183, 0.9139, 0.9064, 0.8987, 0.8965, 0.8959, 0.8949,
         0.8938],
        [0.9593, 0.9478, 0.9418, 0.9250, 0.9238, 0.9232, 0.9185, 0.9175, 0.9172,
         0.9159],
        [0.9422, 0.9353, 0.9257, 0.9242, 0.9235, 0.9215, 0.9198, 0.9168, 0.9127,
         0.9115],
        [0.9304, 0.9259, 0.9248, 0.9243, 0.9209, 0.9175, 0.9167, 0.9166, 0.9132,
         0.9130],
        [0.9442, 0.9442, 0.9438, 0.9431, 0.9415, 0.9408, 0.9332, 0.9307, 0.9296,
         0.9274],
        [0.9365, 0.9203, 0.9189, 0.9086, 0.9013, 0.9008, 0.8964, 0.8961, 0.8953,
         0.8949],
        [0.8734, 0.8681, 0.8649, 0.8573, 0.8562, 0.8495, 0.8483, 0.8482, 0.8471,
         0.8470],
        [0.8994, 0.8892, 0.8842, 0.8822, 0.8795, 0.8782, 0.8779, 0.8769, 0.8751,
         0.8739],
        [0.9177, 0.9136, 0.9128, 0.9071, 0.9002, 0.8995, 0.8958, 0.8889, 0.8860,
         0.8856],
        [0.9048, 0.8943, 0.8918, 0.8841, 0.8803, 0.8752, 0.8752, 0.8686, 0.8581,
         0.8571],
        [0.9472, 0.8672, 0.8634, 0.8501, 0.8459, 0.8292, 0.8198, 0.8176, 0.8152,
         0.8130],
        [0.8988, 0.8798, 0.8778, 0.8750, 0.8720, 0.8689, 0.8539, 0.8536, 0.8534,
         0.8513],
        [0.8925, 0.8886, 0.8677, 0.8604, 0.8480, 0.8440, 0.8412, 0.8357, 0.8344,
         0.8327],
        [0.9337, 0.9302, 0.9286, 0.9285, 0.9271, 0.9250, 0.9242, 0.9236, 0.9235,
         0.9215],
        [0.8795, 0.8758, 0.8758, 0.8646, 0.8598, 0.8547, 0.8522, 0.8387, 0.8300,
         0.8228],
        [0.8993, 0.8604, 0.8597, 0.8568, 0.8506, 0.8495, 0.8472, 0.8451, 0.8448,
         0.8446],
        [0.8838, 0.8722, 0.8687, 0.8597, 0.8505, 0.8459, 0.8395, 0.8376, 0.8332,
         0.8332],
        [0.8741, 0.8709, 0.8558, 0.8497, 0.8470, 0.8432, 0.8416, 0.8408, 0.8398,
         0.8336],
        [0.8499, 0.8361, 0.8192, 0.8169, 0.8084, 0.8082, 0.7965, 0.7801, 0.7733,
         0.7701],
        [0.8287, 0.8044, 0.7888, 0.7851, 0.7793, 0.7792, 0.7756, 0.7634, 0.7588,
         0.7575],
        [0.8983, 0.8807, 0.8772, 0.8739, 0.8727, 0.8718, 0.8608, 0.8603, 0.8599,
         0.8578],
        [0.8041, 0.7835, 0.7783, 0.7715, 0.7628, 0.7587, 0.7398, 0.7384, 0.7298,
         0.7204],
        [0.8304, 0.8277, 0.8096, 0.8088, 0.8023, 0.8022, 0.8016, 0.7961, 0.7884,
         0.7839],
        [0.8899, 0.8610, 0.8429, 0.8311, 0.8304, 0.8284, 0.8201, 0.8192, 0.8147,
         0.8119],
        [0.8743, 0.8527, 0.8466, 0.8458, 0.8347, 0.8259, 0.8204, 0.8094, 0.8090,
         0.8040],
        [0.8868, 0.8839, 0.8431, 0.8138, 0.8101, 0.8042, 0.8015, 0.7969, 0.7967,
         0.7917],
        [0.9231, 0.9062, 0.9001, 0.8918, 0.8630, 0.8581, 0.8553, 0.8505, 0.8412,
         0.8366],
        [0.9110, 0.8735, 0.8691, 0.8590, 0.8522, 0.8507, 0.8423, 0.8406, 0.8375,
         0.8367],
        [0.9225, 0.8998, 0.8968, 0.8963, 0.8946, 0.8912, 0.8912, 0.8885, 0.8881,
         0.8868],
        [0.8983, 0.8786, 0.8777, 0.8665, 0.8643, 0.8637, 0.8607, 0.8600, 0.8598,
         0.8578],
        [0.9035, 0.8756, 0.8664, 0.8643, 0.8584, 0.8548, 0.8416, 0.8275, 0.8234,
         0.8224],
        [0.9393, 0.8779, 0.8693, 0.8610, 0.8543, 0.8503, 0.8343, 0.8303, 0.8290,
         0.8176],
        [0.9048, 0.8587, 0.8539, 0.8430, 0.8302, 0.8282, 0.8223, 0.8222, 0.8150,
         0.8130],
        [0.8977, 0.8157, 0.8128, 0.8005, 0.7902, 0.7893, 0.7890, 0.7857, 0.7826,
         0.7741],
        [0.8833, 0.8651, 0.8556, 0.8532, 0.8459, 0.8426, 0.8397, 0.8393, 0.8387,
         0.8385],
        [0.8532, 0.7754, 0.7741, 0.7733, 0.7627, 0.7357, 0.7283, 0.7263, 0.7251,
         0.7226],
        [0.8821, 0.8550, 0.8406, 0.8212, 0.8116, 0.8103, 0.7916, 0.7901, 0.7885,
         0.7841],
        [0.8372, 0.8058, 0.7934, 0.7781, 0.7764, 0.7697, 0.7668, 0.7505, 0.7480,
         0.7432]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 264802.6562,  189965.5938,  156682.9844,  154221.2031,  141695.4062,
          136687.2812,  131984.8594,  124246.4531,  121490.3516,  117540.7734],
        [ 693323.0000,  577714.5625,  570027.5000,  563232.8125,  556136.6250,
          520592.8438,  520584.8750,  512490.5000,  496261.1875,  491189.1875],
        [ 410564.0625,  365016.6562,  266194.9688,  250878.7344,  203138.7188,
          182127.4531,  175999.7031,  155397.7188,  150711.4531,  135917.4844],
        [ 854324.0000,  312156.3438,  295805.3750,  283765.5000,  282136.1875,
          265419.7812,  231767.2344,  225793.1250,  219034.3281,  216997.5938],
        [ 168491.7812,  143695.0000,  141124.1406,  111874.5547,   98672.4766,
           98177.7188,   93331.1953,   86500.0234,   77238.7734,   70158.8281],
        [ 193942.2812,  117787.8672,  103568.6406,   86813.5625,   83269.4375,
           76667.7344,   71312.4844,   66622.8516,   57908.5039,   56002.6406],
        [ 229196.2344,  220322.9531,  196257.9688,  191348.9062,  181594.6562,
          177924.0938,  173669.9375,  163567.5000,  158391.7969,  153126.7188],
        [ 338996.6875,  255793.1875,  237663.3750,  202785.4844,  182894.5156,
          140157.0938,  132101.0781,  116345.2266,   98344.2344,   93279.0547],
        [ 425712.4062,  391546.7188,  316793.7188,  303654.7812,  302468.9375,
          298133.3438,  297635.6250,  284456.6875,  280985.3125,  280445.3438],
        [ 768510.0625,  299416.4062,  294761.4375,  286502.9375,  259113.6562,
          252682.7344,  238158.2344,  237521.3281,  218113.1875,  211665.2188],
        [ 723691.6875,  403250.9375,  343719.9062,  326004.0000,  324711.0625,
          313880.2812,  301661.1562,  294186.3125,  283734.6562,  274734.2500],
        [ 314754.0625,  253358.1250,  200058.5938,  177532.2031,  128092.2109,
           88483.6094,   86955.9922,   83461.2812,   70051.8516,   65367.0312],
        [ 454201.7188,  413739.5625,  389568.1875,  329489.6875,  283107.5625,
          269614.7188,  240658.7500,  225277.3438,  220190.4062,  187846.1094],
        [ 855909.3125,  834406.3125,  791037.3750,  788748.0625,  759131.5000,
          737643.6875,  710124.1250,  690985.5625,  671235.8750,  624184.3750],
        [ 695284.8125,  570347.2500,  561278.3750,  550459.5625,  527094.2500,
          495375.5312,  491389.2812,  470308.3438,  465802.2812,  456511.0938],
        [ 554880.0625,  519872.4688,  512404.0312,  462488.2188,  450271.8125,
          441316.9375,  428993.5938,  408798.8750,  404694.8750,  395251.9062],
        [ 798693.5000,  734844.2500,  624810.9375,  598887.9375,  547872.8750,
          542654.6250,  490642.3750,  487449.4688,  464898.2500,  461137.4688],
        [ 919018.0000,  844017.3750,  796022.1875,  674685.4375,  668740.7500,
          626868.8125,  618589.1250,  604204.7500,  602934.4375,  588109.1875],
        [ 548225.1250,  459638.9375,  379143.4062,  369818.0625,  344641.2812,
          339782.5312,  332080.5625,  325794.8125,  312082.5312,  304300.0625],
        [ 395930.2188,  370575.3438,  363155.2188,  362432.4375,  352549.1875,
          349177.3438,  335155.2812,  328034.9062,  321783.4062,  315821.1875],
        [ 675714.4375,  602838.9375,  602826.3125,  586378.1250,  581527.0000,
          577119.8750,  551392.6250,  539329.0000,  535548.0000,  528320.1875],
        [ 549551.5625,  446144.4688,  440626.8125,  425469.6875,  412399.4062,
          405089.5000,  402014.5312,  390675.0312,  323713.9062,  304232.1875],
        [1001840.6250,  763824.9375,  694677.7500,  660334.1250,  614231.9375,
          559880.8750,  545203.1875,  490580.6250,  456986.3438,  448683.6875],
        [ 683521.1250,  616503.6250,  583452.3750,  556448.0625,  538183.2500,
          531317.9375,  526053.2500,  523561.7500,  521211.3125,  510940.1250],
        [ 816407.5625,  374795.2188,  321328.9375,  282973.6875,  230333.0156,
          215067.1406,  214739.2344,  207858.8594,  207613.2188,  206108.5781],
        [ 632765.8750,  597583.1250,  578828.0000,  528740.6250,  518926.8750,
          488065.8125,  460484.4375,  437355.8750,  399407.1562,  396953.2812],
        [ 530357.6250,  516160.4062,  498415.4375,  467906.8750,  419988.9688,
          376468.1875,  364831.5000,  361677.6562,  356461.7500,  350943.0312],
        [ 895111.3125,  758912.1875,  696921.9375,  548490.2500,  539193.7500,
          534537.6875,  499675.2188,  492542.9688,  490486.5938,  481204.3438],
        [ 700546.2500,  634826.4375,  553638.8750,  542044.3125,  536826.3750,
          521612.5938,  508828.2812,  487668.9375,  460038.8750,  451928.5625],
        [ 592083.4375,  555075.8750,  546251.8750,  542309.0000,  517241.0625,
          492608.7500,  487054.5312,  486307.2500,  462832.3750,  461495.5625],
        [ 721225.1250,  721110.9375,  716511.8750,  710044.8750,  693416.1875,
          687231.7500,  615995.3125,  594463.1250,  585046.4375,  567506.8750],
        [ 645814.2500,  512334.6562,  502704.1562,  433833.5938,  390480.9688,
          388011.6250,  364339.5000,  362610.1562,  358862.7188,  356573.9688],
        [ 262247.0312,  243179.4844,  232161.0156,  208488.8125,  205146.0938,
          186407.0156,  183125.6094,  183032.3750,  180208.9844,  179865.0625],
        [ 380330.1250,  328584.0625,  306191.0625,  297570.9062,  286168.4375,
          280763.8125,  279541.4688,  275788.2188,  268833.4375,  264204.3438],
        [ 493867.5000,  465896.9375,  460156.4688,  424381.6250,  384668.0625,
          380735.1562,  361112.4062,  327404.5000,  314127.3438,  312338.8750],
        [ 410737.9062,  353648.3125,  340993.0312,  305730.0625,  289498.3438,
          269015.0000,  268976.5312,  245052.3906,  210757.3750,  207808.3281],
        [ 753116.9375,  239976.4531,  227401.3281,  188131.8750,  177180.9219,
          139509.1250,  121979.2188,  118214.5938,  114251.1641,  110719.0078],
        [ 376736.0938,  287332.5625,  279328.0312,  268403.8438,  256971.9531,
          245860.0000,  198560.4062,  197590.8125,  197195.2969,  191356.5781],
        [ 344474.0312,  325981.0000,  241857.9219,  217845.0156,  182567.2500,
          172323.8438,  165656.0156,  153021.4688,  150347.8281,  146572.7344],
        [ 620387.4375,  590658.8750,  576962.4375,  576108.5625,  564942.4375,
          547790.3125,  541705.8125,  537032.1875,  536871.8750,  521066.7188],
        [ 286199.0000,  271499.5625,  271333.9375,  231267.8125,  216086.8906,
          200755.8281,  193734.5000,  159822.5000,  140996.5938,  127369.0078],
        [ 379683.2500,  217881.6094,  215510.2188,  206990.8594,  189445.6562,
          186439.5469,  180490.7188,  175058.0938,  174382.4219,  173744.9688],
        [ 304090.3438,  257731.5625,  245274.0625,  215773.0469,  189140.3750,
          176960.3750,  161682.8438,  157325.9375,  147778.5312,  147715.5469],
        [ 264949.6562,  253189.2812,  204000.3281,  187029.5625,  179855.6250,
          170266.6719,  166568.6562,  164628.3750,  162383.1719,  148440.8438],
        [ 187423.9688,  153902.9531,  120853.1641,  117032.8516,  103654.0156,
          103318.7578,   87443.8125,   69139.3906,   62752.7734,   59966.2617],
        [ 138477.7188,   97880.2344,   78294.3281,   74308.0625,   68363.9453,
           68317.8672,   64823.6523,   54511.1914,   51023.6250,   50116.9219],
        [ 374077.0938,  291081.8750,  276815.0938,  264112.8750,  259817.1406,
          256318.9375,  219028.6719,  217356.5156,  216247.4688,  209807.8281],
        [  97438.5000,   72595.7891,   67373.6016,   61162.6367,   53991.8984,
           50961.0898,   38896.6602,   38119.1992,   33711.2734,   29462.7246],
        [ 141924.6406,  136591.8750,  105462.1953,  104215.8281,   95013.8281,
           94900.2656,   94026.4297,   86971.6719,   77879.3125,   73043.8594],
        [ 331764.6562,  219815.9219,  169567.2969,  143241.8438,  141914.4844,
          137836.3281,  122506.0547,  120974.0078,  113312.4297,  108967.1328],
        [ 265541.0938,  195137.6250,  178934.3594,  176869.9531,  150930.5156,
          133120.7812,  122957.3828,  105148.3516,  104447.2578,   97292.4375],
        [ 317534.7500,  304710.6875,  170202.2188,  111900.6016,  106197.6484,
           97616.7031,   93888.7031,   87918.3516,   87644.9453,   81674.8281],
        [ 533325.2500,  419300.2188,  383824.1250,  341276.0625,  225908.5625,
          210810.8594,  202426.2812,  189220.6562,  165656.0156,  155109.8750],
        [ 448681.9375,  262802.5938,  246799.2031,  213443.9531,  193814.3281,
          189513.4062,  168194.1250,  164130.6562,  157141.7969,  155356.5156],
        [ 529149.6875,  382455.3750,  366397.0312,  363828.0625,  355048.6875,
          338132.9688,  338132.9688,  325443.9375,  323493.2500,  317583.2188],
        [ 374271.5938,  282340.7500,  278754.2812,  237589.0625,  230419.5938,
          228288.0781,  218848.7031,  216634.0938,  215861.9531,  209977.1719],
        [ 402933.0312,  270539.3750,  237204.3906,  230310.1719,  211548.5625,
          201065.4531,  166605.2031,  136146.3281,  128366.0156,  126532.9062],
        [ 672288.4375,  279850.6250,  247392.5781,  219571.0000,  199568.3125,
          188448.8125,  150109.7188,  141781.9219,  139137.5000,  118168.1562],
        [ 410758.6875,  212485.3438,  198397.2656,  169864.7812,  141561.8125,
          137569.2188,  126469.2109,  126299.5000,  113829.9375,  110602.6094],
        [ 370979.8750,  114980.1094,  110364.8047,   92519.8750,   79943.9141,
           78866.2656,   78599.3438,   74916.5156,   71643.9766,   63449.3203],
        [ 302070.8125,  232783.7969,  203359.3125,  196456.6719,  176976.4219,
          169015.7812,  162118.8906,  161203.7188,  159651.8906,  159341.8906],
        [ 196496.2031,   64693.0352,   63472.3203,   62755.8828,   53938.7852,
           36700.7148,   33023.5703,   32054.6758,   31547.1836,   30428.1934],
        [ 296922.3125,  201760.4062,  164210.6719,  124509.8984,  108460.7734,
          106481.8047,   81504.4219,   79759.0156,   77963.8828,   73235.4688],
        [ 156287.8125,   99802.9453,   83665.8516,   67225.5391,   65635.9531,
           59620.4141,   57222.9336,   45310.5586,   43712.6758,   40820.2969]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[264802.6562,      0.0000],
         [189965.5938,      0.0000],
         [156682.9844,      0.0000],
         ...,
         [124246.4531,      0.0000],
         [121490.3516,      0.0000],
         [117540.7734,      0.0000]],

        [[693323.0000,      0.0000],
         [577714.5625,      0.0000],
         [570027.5000,      0.0000],
         ...,
         [512490.5000,      0.0000],
         [496261.1875,      0.0000],
         [491189.1875,      0.0000]],

        [[410564.0625,      0.0000],
         [365016.6562,      0.0000],
         [266194.9688,      0.0000],
         ...,
         [155397.7188,      0.0000],
         [150711.4531,      0.0000],
         [135917.4844,      0.0000]],

        ...,

        [[     0.0000, 196496.2031],
         [ 64693.0352,      0.0000],
         [ 63472.3203,      0.0000],
         ...,
         [ 32054.6758,      0.0000],
         [ 31547.1836,      0.0000],
         [ 30428.1934,      0.0000]],

        [[296922.3125,      0.0000],
         [     0.0000, 201760.4062],
         [164210.6719,      0.0000],
         ...,
         [ 79759.0156,      0.0000],
         [ 77963.8828,      0.0000],
         [ 73235.4688,      0.0000]],

        [[     0.0000, 156287.8125],
         [     0.0000,  99802.9453],
         [     0.0000,  83665.8516],
         ...,
         [ 45310.5586,      0.0000],
         [     0.0000,  43712.6758],
         [     0.0000,  40820.2969]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1539317.6250,       0.0000],
        [5501553.0000,       0.0000],
        [2295947.0000,       0.0000],
        [3187199.5000,       0.0000],
        [ 450781.0625,  638483.3750],
        [ 770605.4375,  143290.5938],
        [1687008.8750,  158391.7969],
        [1798360.0000,       0.0000],
        [3181833.0000,       0.0000],
        [3066445.2500,       0.0000],
        [3589574.2500,       0.0000],
        [1468115.0000,       0.0000],
        [3013694.0000,       0.0000],
        [7463406.0000,       0.0000],
        [5283850.5000,       0.0000],
        [4578972.5000,       0.0000],
        [5751891.5000,       0.0000],
        [6943190.0000,       0.0000],
        [3715507.5000,       0.0000],
        [3166579.5000,  328034.9062],
        [5780994.5000,       0.0000],
        [4099917.2500,       0.0000],
        [6236244.0000,       0.0000],
        [5591193.0000,       0.0000],
        [3077225.5000,       0.0000],
        [5039111.0000,       0.0000],
        [4243211.5000,       0.0000],
        [5937076.0000,       0.0000],
        [5397959.0000,       0.0000],
        [5143260.0000,       0.0000],
        [6612553.0000,       0.0000],
        [4315566.0000,       0.0000],
        [2063861.5000,       0.0000],
        [2375616.5000,  592359.5000],
        [1775139.6250, 2149549.5000],
        [1017266.3750, 1884950.7500],
        [1086668.2500, 1103812.3750],
        [1309891.0000, 1189444.5000],
        [ 703781.5625, 1396865.5000],
        [5613527.0000,       0.0000],
        [1971696.5000,  127369.0078],
        [2099627.5000,       0.0000],
        [2003472.6250,       0.0000],
        [1901312.2500,       0.0000],
        [ 232759.6719,  832728.3125],
        [ 326837.1562,  419280.3750],
        [2210586.5000,  374077.0938],
        [ 341924.0625,  201789.3125],
        [ 447572.8750,  562457.0625],
        [ 440731.7812, 1169168.3750],
        [1292811.7500,  237568.0312],
        [ 666841.7500,  792447.6250],
        [1666250.1250, 1160607.7500],
        [2010365.1250,  189513.4062],
        [3639665.2500,       0.0000],
        [2492985.2500,       0.0000],
        [ 966427.3125, 1144824.1250],
        [1657327.1250,  698989.9375],
        [ 942749.1250,  805089.1875],
        [ 529371.0000,  606893.0000],
        [1211148.1250,  711831.0625],
        [ 408614.3750,  196496.2031],
        [ 898105.6250,  416703.0000],
        [ 104930.9688,  614374.0625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 471/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:44, 61.54s/it]  7%|▋         | 2/30 [01:02<12:01, 25.78s/it] 10%|█         | 3/30 [01:03<06:27, 14.35s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.98s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.01s/it] 20%|██        | 6/30 [01:05<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:06<01:10,  3.09s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 2.2860339721043905
Epoch 472/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:08, 58.23s/it]  7%|▋         | 2/30 [00:58<11:23, 24.42s/it] 10%|█         | 3/30 [00:59<06:08, 13.64s/it] 13%|█▎        | 4/30 [01:01<03:52,  8.95s/it] 17%|█▋        | 5/30 [01:02<02:29,  5.99s/it] 20%|██        | 6/30 [01:03<01:41,  4.21s/it] 23%|██▎       | 7/30 [01:03<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:04<00:51,  2.34s/it] 30%|███       | 9/30 [01:05<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:06<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:06<00:24,  1.27s/it] 40%|████      | 12/30 [01:07<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:08<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.71s/it]
Epoch loss is 2.312609243392944
Epoch 473/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:02, 60.09s/it]  7%|▋         | 2/30 [01:00<11:47, 25.25s/it] 10%|█         | 3/30 [01:01<06:19, 14.06s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.81s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.90s/it] 20%|██        | 6/30 [01:03<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.3094921191533406
Epoch 474/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:26, 56.79s/it]  7%|▋         | 2/30 [00:58<11:18, 24.23s/it] 10%|█         | 3/30 [00:58<06:04, 13.51s/it] 13%|█▎        | 4/30 [00:59<03:42,  8.55s/it] 17%|█▋        | 5/30 [01:00<02:23,  5.74s/it] 20%|██        | 6/30 [01:01<01:37,  4.04s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.97s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.26s/it] 30%|███       | 9/30 [01:03<00:37,  1.79s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.65s/it]
Epoch loss is 2.288866980870565
Epoch 475/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:17, 60.60s/it]  7%|▋         | 2/30 [01:01<11:51, 25.39s/it] 10%|█         | 3/30 [01:02<06:21, 14.14s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.85s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.93s/it] 20%|██        | 6/30 [01:04<01:40,  4.17s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3294175704320272
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0008,  0.0132,  ..., -0.0058,  0.0176,  0.0040],
        [-0.0023,  0.0095,  0.0286,  ...,  0.0172,  0.0042, -0.0147],
        [-0.0293, -0.0379,  0.0247,  ...,  0.0834, -0.0043, -0.0143],
        ...,
        [ 0.0059, -0.0033,  0.0099,  ..., -0.0241, -0.0051, -0.0027],
        [-0.0333,  0.0099, -0.0035,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0326, -0.0139,  0.0193,  ...,  0.0414,  0.0360, -0.0284]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8740, 0.8508, 0.8373, 0.8362, 0.8303, 0.8278, 0.8253, 0.8210, 0.8195,
         0.8172],
        [0.9414, 0.9287, 0.9277, 0.9269, 0.9260, 0.9214, 0.9213, 0.9203, 0.9180,
         0.9173],
        [0.9048, 0.8965, 0.8744, 0.8703, 0.8555, 0.8478, 0.8454, 0.8367, 0.8346,
         0.8274],
        [0.9561, 0.8856, 0.8818, 0.8789, 0.8785, 0.8742, 0.8647, 0.8629, 0.8607,
         0.8601],
        [0.8424, 0.8313, 0.8300, 0.8137, 0.8049, 0.8046, 0.8011, 0.7957, 0.7878,
         0.7811],
        [0.8523, 0.8173, 0.8084, 0.7960, 0.7931, 0.7873, 0.7822, 0.7775, 0.7676,
         0.7653],
        [0.8639, 0.8612, 0.8531, 0.8513, 0.8476, 0.8462, 0.8445, 0.8403, 0.8380,
         0.8357],
        [0.8913, 0.8716, 0.8665, 0.8554, 0.8481, 0.8295, 0.8254, 0.8165, 0.8047,
         0.8010],
        [0.9073, 0.9014, 0.8866, 0.8836, 0.8834, 0.8823, 0.8822, 0.8791, 0.8782,
         0.8781],
        [0.9486, 0.8826, 0.8815, 0.8796, 0.8725, 0.8708, 0.8666, 0.8664, 0.8605,
         0.8584],
        [0.9444, 0.9035, 0.8923, 0.8886, 0.8883, 0.8859, 0.8832, 0.8814, 0.8789,
         0.8766],
        [0.8861, 0.8709, 0.8544, 0.8460, 0.8232, 0.7973, 0.7960, 0.7932, 0.7809,
         0.7761],
        [0.9118, 0.9053, 0.9011, 0.8893, 0.8787, 0.8753, 0.8673, 0.8627, 0.8611,
         0.8500],
        [0.9562, 0.9544, 0.9507, 0.9505, 0.9478, 0.9458, 0.9431, 0.9412, 0.9392,
         0.9341],
        [0.9416, 0.9278, 0.9266, 0.9253, 0.9222, 0.9179, 0.9173, 0.9143, 0.9136,
         0.9122],
        [0.9258, 0.9213, 0.9203, 0.9131, 0.9112, 0.9098, 0.9078, 0.9045, 0.9037,
         0.9021],
        [0.9513, 0.9455, 0.9341, 0.9312, 0.9249, 0.9243, 0.9172, 0.9168, 0.9134,
         0.9129],
        [0.9612, 0.9552, 0.9511, 0.9395, 0.9389, 0.9344, 0.9335, 0.9318, 0.9317,
         0.9299],
        [0.9250, 0.9126, 0.8991, 0.8974, 0.8925, 0.8915, 0.8899, 0.8886, 0.8856,
         0.8838],
        [0.9022, 0.8976, 0.8962, 0.8960, 0.8941, 0.8934, 0.8905, 0.8891, 0.8877,
         0.8864],
        [0.9396, 0.9316, 0.9316, 0.9297, 0.9291, 0.9286, 0.9254, 0.9238, 0.9233,
         0.9224],
        [0.9252, 0.9106, 0.9097, 0.9072, 0.9051, 0.9038, 0.9033, 0.9013, 0.8881,
         0.8837],
        [0.9672, 0.9482, 0.9416, 0.9380, 0.9330, 0.9265, 0.9246, 0.9172, 0.9123,
         0.9110],
        [0.9404, 0.9332, 0.9294, 0.9260, 0.9237, 0.9228, 0.9221, 0.9218, 0.9215,
         0.9201],
        [0.9529, 0.8984, 0.8876, 0.8787, 0.8643, 0.8595, 0.8593, 0.8571, 0.8570,
         0.8565],
        [0.9350, 0.9310, 0.9288, 0.9225, 0.9211, 0.9168, 0.9128, 0.9092, 0.9028,
         0.9024],
        [0.9227, 0.9208, 0.9183, 0.9139, 0.9063, 0.8987, 0.8965, 0.8959, 0.8949,
         0.8938],
        [0.9593, 0.9478, 0.9418, 0.9250, 0.9238, 0.9232, 0.9185, 0.9175, 0.9172,
         0.9159],
        [0.9422, 0.9353, 0.9257, 0.9242, 0.9235, 0.9215, 0.9198, 0.9168, 0.9127,
         0.9115],
        [0.9304, 0.9259, 0.9247, 0.9242, 0.9209, 0.9175, 0.9167, 0.9166, 0.9132,
         0.9129],
        [0.9442, 0.9442, 0.9437, 0.9431, 0.9414, 0.9408, 0.9332, 0.9307, 0.9295,
         0.9274],
        [0.9365, 0.9203, 0.9189, 0.9086, 0.9012, 0.9008, 0.8964, 0.8961, 0.8953,
         0.8949],
        [0.8734, 0.8681, 0.8648, 0.8573, 0.8562, 0.8495, 0.8482, 0.8482, 0.8471,
         0.8470],
        [0.8994, 0.8891, 0.8842, 0.8822, 0.8795, 0.8781, 0.8778, 0.8769, 0.8751,
         0.8739],
        [0.9177, 0.9136, 0.9127, 0.9071, 0.9002, 0.8995, 0.8958, 0.8889, 0.8860,
         0.8856],
        [0.9048, 0.8943, 0.8917, 0.8841, 0.8803, 0.8752, 0.8751, 0.8686, 0.8581,
         0.8571],
        [0.9472, 0.8672, 0.8634, 0.8501, 0.8459, 0.8292, 0.8198, 0.8176, 0.8152,
         0.8130],
        [0.8987, 0.8797, 0.8777, 0.8750, 0.8719, 0.8688, 0.8539, 0.8535, 0.8534,
         0.8513],
        [0.8925, 0.8886, 0.8677, 0.8604, 0.8480, 0.8439, 0.8412, 0.8356, 0.8344,
         0.8326],
        [0.9336, 0.9302, 0.9286, 0.9285, 0.9271, 0.9249, 0.9241, 0.9236, 0.9235,
         0.9214],
        [0.8795, 0.8758, 0.8757, 0.8646, 0.8598, 0.8547, 0.8522, 0.8387, 0.8299,
         0.8228],
        [0.8993, 0.8604, 0.8596, 0.8568, 0.8506, 0.8495, 0.8472, 0.8450, 0.8448,
         0.8445],
        [0.8837, 0.8721, 0.8687, 0.8597, 0.8505, 0.8458, 0.8395, 0.8376, 0.8332,
         0.8332],
        [0.8740, 0.8709, 0.8558, 0.8497, 0.8470, 0.8431, 0.8416, 0.8408, 0.8398,
         0.8335],
        [0.8498, 0.8361, 0.8191, 0.8168, 0.8084, 0.8081, 0.7965, 0.7800, 0.7732,
         0.7700],
        [0.8287, 0.8044, 0.7887, 0.7851, 0.7793, 0.7792, 0.7755, 0.7634, 0.7588,
         0.7575],
        [0.8982, 0.8806, 0.8771, 0.8738, 0.8727, 0.8717, 0.8607, 0.8602, 0.8598,
         0.8577],
        [0.8040, 0.7835, 0.7782, 0.7714, 0.7628, 0.7587, 0.7397, 0.7383, 0.7297,
         0.7204],
        [0.8304, 0.8277, 0.8096, 0.8088, 0.8023, 0.8022, 0.8016, 0.7961, 0.7884,
         0.7839],
        [0.8898, 0.8610, 0.8428, 0.8310, 0.8304, 0.8283, 0.8201, 0.8192, 0.8146,
         0.8119],
        [0.8742, 0.8527, 0.8466, 0.8458, 0.8347, 0.8259, 0.8204, 0.8094, 0.8089,
         0.8040],
        [0.8868, 0.8839, 0.8431, 0.8137, 0.8100, 0.8041, 0.8014, 0.7969, 0.7966,
         0.7917],
        [0.9230, 0.9062, 0.9000, 0.8918, 0.8629, 0.8581, 0.8552, 0.8505, 0.8412,
         0.8366],
        [0.9110, 0.8735, 0.8691, 0.8590, 0.8522, 0.8506, 0.8423, 0.8405, 0.8375,
         0.8367],
        [0.9225, 0.8998, 0.8968, 0.8963, 0.8946, 0.8911, 0.8911, 0.8885, 0.8881,
         0.8868],
        [0.8983, 0.8785, 0.8776, 0.8664, 0.8643, 0.8637, 0.8607, 0.8600, 0.8597,
         0.8578],
        [0.9034, 0.8755, 0.8663, 0.8643, 0.8583, 0.8548, 0.8416, 0.8275, 0.8233,
         0.8223],
        [0.9393, 0.8779, 0.8693, 0.8609, 0.8542, 0.8502, 0.8343, 0.8303, 0.8290,
         0.8175],
        [0.9048, 0.8586, 0.8538, 0.8430, 0.8302, 0.8282, 0.8223, 0.8222, 0.8149,
         0.8129],
        [0.8976, 0.8156, 0.8128, 0.8004, 0.7902, 0.7892, 0.7890, 0.7856, 0.7825,
         0.7740],
        [0.8833, 0.8650, 0.8556, 0.8531, 0.8458, 0.8426, 0.8397, 0.8393, 0.8386,
         0.8385],
        [0.8532, 0.7754, 0.7740, 0.7732, 0.7627, 0.7357, 0.7283, 0.7263, 0.7251,
         0.7226],
        [0.8821, 0.8550, 0.8406, 0.8212, 0.8116, 0.8102, 0.7915, 0.7900, 0.7884,
         0.7840],
        [0.8371, 0.8057, 0.7934, 0.7781, 0.7764, 0.7696, 0.7668, 0.7504, 0.7479,
         0.7431]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 264620.6562,  189964.5156,  156572.0000,  154142.3750,  141632.0312,
          136634.2188,  131877.9062,  124151.2266,  121423.6328,  117530.3516],
        [ 693180.1875,  577509.1250,  569904.1250,  563016.4375,  556067.1875,
          520377.4062,  520285.0938,  512287.7500,  496083.2812,  490988.3125],
        [ 410471.2500,  364896.5625,  266009.7188,  250756.2656,  203017.6875,
          182034.7344,  175896.6719,  155314.2969,  150646.3750,  135877.3125],
        [ 854352.5625,  311986.7188,  295629.0938,  283682.1562,  282069.1875,
          265271.7500,  231663.5938,  225681.5938,  218869.1562,  216912.7656],
        [ 168451.4531,  143662.5156,  141023.0938,  111837.5469,   98610.7656,
           98120.2422,   93311.4375,   86466.1172,   77193.7812,   70159.0938],
        [ 193902.3281,  117761.3594,  103555.7031,   86778.6328,   83276.6562,
           76609.4844,   71281.1328,   66623.4922,   57862.9570,   55985.7656],
        [ 229065.1250,  220191.0469,  196160.2969,  191298.1875,  181502.3906,
          177879.7969,  173586.9844,  163492.3281,  158265.4219,  153023.2188],
        [ 338803.0938,  255667.3594,  237581.3438,  202689.7812,  182801.5781,
          140067.1719,  132051.4688,  116280.8828,   98280.1016,   93210.3984],
        [ 425586.1250,  391475.0312,  316630.0000,  303613.6562,  302401.4375,
          297964.5000,  297548.2188,  284376.4062,  280891.0000,  280332.2188],
        [ 768436.0000,  299265.0938,  294605.7500,  286362.8125,  258977.0469,
          252562.2656,  238029.4844,  237386.8125,  218030.0000,  211567.9375],
        [ 723602.6875,  403207.4688,  343586.8438,  325833.3750,  324503.6250,
          313720.7500,  301567.9688,  294055.0312,  283628.5938,  274567.1562],
        [ 314507.7188,  253148.9688,  199933.1094,  177376.3594,  127978.2891,
           88447.0781,   86861.2578,   83366.5391,   69979.0078,   65300.3008],
        [ 453870.4375,  413608.2188,  389342.7188,  329289.9062,  282876.0000,
          269445.3438,  240508.2344,  225118.2031,  220082.9375,  187729.5156],
        [ 855606.5000,  834290.8750,  790781.6250,  788485.6250,  758914.3750,
          737361.6250,  709875.6250,  690704.8750,  671039.3750,  623970.7500],
        [ 695178.1250,  570229.7500,  561162.2500,  550320.4375,  526908.8125,
          495234.3125,  491256.6562,  470208.8125,  465737.4375,  456463.6562],
        [ 554824.5000,  519767.3750,  512284.3125,  462399.5625,  450223.3125,
          441232.7500,  428908.0938,  408695.9688,  404592.2188,  395207.0625],
        [ 798451.3125,  734596.2500,  624600.0000,  598590.4375,  547637.3125,
          542440.4375,  490466.4688,  487215.6875,  464713.8750,  460955.4062],
        [ 918773.5000,  843833.9375,  795890.8125,  674659.0625,  668631.7500,
          626699.6875,  618494.6875,  604009.4375,  602880.3750,  587989.1875],
        [ 548082.4375,  459393.1250,  378884.5938,  369705.5625,  344526.2812,
          339654.8750,  331974.7812,  325652.2500,  311987.5938,  304262.0938],
        [ 395770.5625,  370478.1875,  363049.2500,  362314.2500,  352445.2812,
          349048.1562,  335014.9688,  327988.2812,  321678.4688,  315763.0625],
        [ 675342.0625,  602592.9375,  602549.8125,  586214.3125,  581360.1250,
          576839.2500,  551219.1875,  539163.8750,  535287.0625,  528127.7500],
        [ 549449.9375,  446123.5938,  440473.8750,  425294.0000,  412316.0312,
          404918.7812,  401802.9375,  390546.8750,  323583.6562,  304021.9062],
        [1001661.0000,  763748.5000,  694557.8125,  660147.7500,  614100.1250,
          559712.6875,  545061.2500,  490497.8125,  456907.8750,  448544.1875],
        [ 683393.3750,  616343.1250,  583332.7500,  556376.4375,  538127.7500,
          531277.4375,  525956.4375,  523440.4688,  521056.7500,  510855.8125],
        [ 816151.5000,  374620.8125,  321110.2188,  282877.3750,  230228.5000,
          214943.2969,  214552.7656,  207710.4531,  207484.1562,  206071.6250],
        [ 632561.9375,  597449.7500,  578597.3125,  528547.0000,  518777.4062,
          487893.1875,  460303.5312,  437196.5625,  399304.2812,  396796.2188],
        [ 530183.1875,  515941.4062,  498173.0938,  467724.4062,  419820.7812,
          376329.6250,  364684.6875,  361534.5625,  356329.2188,  350823.2500],
        [ 894967.8750,  758817.3750,  696740.5000,  548303.0000,  539027.6875,
          534406.6875,  499547.0625,  492510.5938,  490369.1875,  481089.6562],
        [ 700329.8750,  634638.1250,  553543.8750,  541830.3750,  536632.8750,
          521446.4688,  508633.7188,  487522.0000,  459814.3125,  451759.5938],
        [ 591880.1875,  554947.1875,  546159.1875,  542190.6250,  517130.0625,
          492431.6875,  486934.1875,  486201.5000,  462788.2500,  461387.3125],
        [ 721159.0625,  720986.4375,  716265.9375,  709917.6250,  693212.5000,
          687075.7500,  615833.1875,  594329.3750,  584941.0625,  567372.1250],
        [ 645612.9375,  512222.7812,  502583.8125,  433654.4688,  390425.1250,
          387893.6250,  364235.2812,  362505.0312,  358727.2188,  356493.4062],
        [ 262116.0000,  243083.0156,  232053.2188,  208399.3438,  205086.0469,
          186360.4531,  183046.5000,  182946.8438,  180110.3594,  179750.5156],
        [ 380068.3438,  328378.2812,  306030.2500,  297440.4062,  286023.0312,
          280648.9375,  279362.1250,  275713.2812,  268715.5312,  264048.9062],
        [ 493639.6250,  465750.3125,  460021.7812,  424159.8750,  384636.1562,
          380595.0312,  360984.0000,  327265.2812,  313989.5625,  312204.2812],
        [ 410623.5625,  353455.7812,  340820.0938,  305648.7500,  289341.8438,
          268921.9062,  268876.5000,  244931.8438,  210657.7188,  207736.7969],
        [ 753064.5000,  239942.8281,  227342.3594,  188076.0781,  177121.4531,
          139403.3906,  121901.4297,  118161.2812,  114216.5078,  110606.9297],
        [ 376511.5938,  287133.1250,  279086.7812,  268347.7812,  256754.6875,
          245761.7812,  198408.2344,  197425.0469,  197095.6562,  191238.0000],
        [ 344375.4688,  325850.7500,  241697.2188,  217758.2031,  182492.2188,
          172173.0469,  165554.2969,  152895.8750,  150285.7656,  146526.7500],
        [ 620095.2500,  590424.0000,  576754.5000,  575952.0000,  564750.7500,
          547494.1875,  541495.6250,  537027.0625,  536660.0000,  520845.6250],
        [ 286069.1250,  271384.6562,  271195.0000,  231137.9375,  216005.9062,
          200670.6406,  193630.5000,  159734.8906,  140895.7969,  127293.1094],
        [ 379557.9688,  217810.1250,  215407.4844,  206898.0938,  189313.7969,
          186324.7344,  180388.6719,  174903.7344,  174294.4531,  173652.0469],
        [ 303967.4062,  257584.3750,  245127.8906,  215667.7188,  189041.3750,
          176842.1250,  161572.3281,  157231.7500,  147718.9375,  147668.3750],
        [ 264684.0000,  253077.9844,  203857.5625,  186866.5938,  179757.5469,
          170192.9844,  166420.5156,  164536.2656,  162318.0000,  148419.8906],
        [ 187306.5781,  153865.2344,  120775.0391,  116916.8359,  103582.6641,
          103244.8828,   87380.9609,   69070.7188,   62719.5625,   59903.2734],
        [ 138421.6094,   97810.3438,   78239.3125,   74282.9844,   68345.6875,
           68298.5859,   64807.8906,   54495.4453,   51007.4727,   50108.4141],
        [ 373883.0938,  290834.9375,  276593.6875,  263905.6562,  259586.3125,
          256046.0469,  218832.0000,  217130.2812,  216045.2500,  209591.0469],
        [  97375.5156,   72569.2109,   67334.6172,   61120.3008,   53983.6094,
           50929.8008,   38847.3164,   38074.1836,   33689.4531,   29463.1445],
        [ 141813.4219,  136503.0781,  105423.1797,  104164.3516,   94957.6641,
           94876.5547,   94008.8594,   86914.3750,   77838.9922,   72993.0938],
        [ 331562.8438,  219722.0156,  169465.9375,  143113.9062,  141829.2500,
          137758.6719,  122429.0859,  120905.8438,  113249.4531,  108919.8672],
        [ 265334.5312,  195077.9062,  178864.0625,  176756.4688,  150860.4375,
          133084.2188,  122926.4297,  105082.0938,  104411.9062,   97261.4531],
        [ 317400.3438,  304591.8750,  170082.9531,  111798.2969,  106092.9766,
           97495.6641,   93832.4922,   87873.0000,   87584.7812,   81598.0625],
        [ 533065.4375,  419114.3438,  383559.9062,  341141.0312,  225767.0625,
          210723.0312,  202335.9688,  189067.3281,  165594.2500,  155019.0781],
        [ 448553.5938,  262738.1875,  246721.0781,  213365.4062,  193739.1250,
          189392.7031,  168142.1562,  163998.6094,  157051.3125,  155272.9844],
        [ 528910.0625,  382274.1562,  366210.5000,  363648.3750,  354918.6875,
          337965.3125,  337965.3125,  325300.9062,  323371.4062,  317403.3438],
        [ 374055.7188,  282200.5000,  278640.8125,  237478.2812,  230303.1562,
          228173.1562,  218799.6562,  216476.5156,  215729.6250,  209815.2344],
        [ 402619.1875,  270420.7500,  237053.1094,  230161.0781,  211479.9844,
          200937.9844,  166585.4844,  136050.4062,  128230.1953,  126450.3906],
        [ 672131.3750,  279754.2812,  247261.4375,  219450.1875,  199468.7812,
          188371.5469,  150040.4375,  141670.1406,  139011.0938,  118060.9141],
        [ 410517.8438,  212412.0000,  198306.4688,  169764.2188,  141520.3750,
          137480.0312,  126390.9531,  126199.3359,  113737.4922,  110500.9766],
        [ 370850.4062,  114902.3984,  110283.2578,   92439.8828,   79877.9922,
           78791.6953,   78535.2734,   74852.9531,   71592.2734,   63395.0625],
        [ 302027.3125,  232649.7344,  203251.1094,  196363.7500,  176848.8594,
          168967.4375,  162016.2500,  161099.6719,  159490.2812,  159212.3281],
        [ 196451.0312,   64677.1172,   63428.6914,   62711.5469,   53906.8516,
           36704.9492,   33004.8398,   32053.8496,   31535.3027,   30406.5527],
        [ 296811.9062,  201725.0000,  164097.9531,  124472.6250,  108404.6250,
          106386.1875,   81405.5312,   79681.6953,   77883.1797,   73166.7031],
        [ 156235.5000,   99740.0469,   83619.6641,   67180.2891,   65605.2266,
           59563.2422,   57194.8359,   45268.5312,   43680.8750,   40789.1250]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[264620.6562,      0.0000],
         [189964.5156,      0.0000],
         [156572.0000,      0.0000],
         ...,
         [124151.2266,      0.0000],
         [121423.6328,      0.0000],
         [117530.3516,      0.0000]],

        [[693180.1875,      0.0000],
         [577509.1250,      0.0000],
         [569904.1250,      0.0000],
         ...,
         [512287.7500,      0.0000],
         [496083.2812,      0.0000],
         [490988.3125,      0.0000]],

        [[410471.2500,      0.0000],
         [364896.5625,      0.0000],
         [266009.7188,      0.0000],
         ...,
         [155314.2969,      0.0000],
         [150646.3750,      0.0000],
         [135877.3125,      0.0000]],

        ...,

        [[     0.0000, 196451.0312],
         [ 64677.1172,      0.0000],
         [ 63428.6914,      0.0000],
         ...,
         [ 32053.8496,      0.0000],
         [ 31535.3027,      0.0000],
         [ 30406.5527,      0.0000]],

        [[296811.9062,      0.0000],
         [     0.0000, 201725.0000],
         [164097.9531,      0.0000],
         ...,
         [ 79681.6953,      0.0000],
         [ 77883.1797,      0.0000],
         [ 73166.7031,      0.0000]],

        [[     0.0000, 156235.5000],
         [     0.0000,  99740.0469],
         [     0.0000,  83619.6641],
         ...,
         [ 45268.5312,      0.0000],
         [     0.0000,  43680.8750],
         [     0.0000,  40789.1250]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1538548.8750,       0.0000],
        [5499699.0000,       0.0000],
        [2294921.0000,       0.0000],
        [3186118.5000,       0.0000],
        [ 450622.1250,  638214.0000],
        [ 770404.5625,  143232.9688],
        [1686199.3750,  158265.4219],
        [1797433.1250,       0.0000],
        [3180818.5000,       0.0000],
        [3065223.0000,       0.0000],
        [3588273.5000,       0.0000],
        [1466898.6250,       0.0000],
        [3011871.5000,       0.0000],
        [7461031.5000,       0.0000],
        [5282700.0000,       0.0000],
        [4578135.0000,       0.0000],
        [5749667.0000,       0.0000],
        [6941862.0000,       0.0000],
        [3714123.7500,       0.0000],
        [3165562.2500,  327988.2812],
        [5778696.0000,       0.0000],
        [4098531.5000,       0.0000],
        [6234939.0000,       0.0000],
        [5590160.0000,       0.0000],
        [3075750.7500,       0.0000],
        [5037427.5000,       0.0000],
        [4241544.0000,       0.0000],
        [5935779.5000,       0.0000],
        [5396151.5000,       0.0000],
        [5142050.0000,       0.0000],
        [6611093.0000,       0.0000],
        [4314353.5000,       0.0000],
        [2062952.5000,       0.0000],
        [2374375.7500,  592053.2500],
        [1774464.8750, 2148781.0000],
        [1016782.1250, 1884232.6250],
        [1086222.5000, 1103614.2500],
        [1309015.5000, 1188747.1250],
        [ 703432.0625, 1396177.5000],
        [5611499.0000,       0.0000],
        [1970724.5000,  127293.1094],
        [2098551.0000,       0.0000],
        [2002422.3750,       0.0000],
        [1900131.3750,       0.0000],
        [ 232556.6562,  832209.0625],
        [ 326742.6250,  419075.1250],
        [2208565.2500,  373883.0938],
        [ 341676.0000,  201711.1875],
        [ 447334.0000,  562159.5625],
        [ 440482.7188, 1168474.1250],
        [1292163.5000,  237496.1250],
        [ 666275.2500,  792075.1875],
        [1665341.7500, 1160045.7500],
        [2009582.5000,  189392.7031],
        [3637968.0000,       0.0000],
        [2491672.7500,       0.0000],
        [ 965800.4375, 1144188.1250],
        [1656547.0000,  698673.2500],
        [ 942201.2500,  804628.5000],
        [ 528957.8750,  606563.3750],
        [1210400.7500,  711525.9375],
        [ 408429.7188,  196451.0312],
        [ 897519.6250,  416515.8125],
        [ 104831.7734,  614045.5625]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 476/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:32, 59.05s/it]  7%|▋         | 2/30 [00:59<11:33, 24.75s/it] 10%|█         | 3/30 [01:00<06:12, 13.80s/it] 13%|█▎        | 4/30 [01:01<03:44,  8.65s/it] 17%|█▋        | 5/30 [01:02<02:24,  5.80s/it] 20%|██        | 6/30 [01:02<01:37,  4.08s/it] 23%|██▎       | 7/30 [01:03<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.28s/it] 30%|███       | 9/30 [01:05<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.08it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 2.3012446959813437
Epoch 477/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:25, 56.73s/it]  7%|▋         | 2/30 [00:57<11:14, 24.10s/it] 10%|█         | 3/30 [00:58<06:02, 13.44s/it] 13%|█▎        | 4/30 [00:59<03:39,  8.43s/it] 17%|█▋        | 5/30 [01:00<02:21,  5.66s/it] 20%|██        | 6/30 [01:01<01:35,  3.99s/it] 23%|██▎       | 7/30 [01:01<01:07,  2.93s/it] 27%|██▋       | 8/30 [01:02<00:49,  2.24s/it] 30%|███       | 9/30 [01:03<00:37,  1.77s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.46s/it] 37%|███▋      | 11/30 [01:04<00:23,  1.24s/it] 40%|████      | 12/30 [01:05<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:07<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:10<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:13<00:05,  1.32it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:16<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  2.64s/it]
Epoch loss is 2.345927532513936
Epoch 478/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:40, 57.27s/it]  7%|▋         | 2/30 [00:58<11:19, 24.26s/it] 10%|█         | 3/30 [00:59<06:12, 13.80s/it] 13%|█▎        | 4/30 [01:00<03:44,  8.65s/it] 17%|█▋        | 5/30 [01:01<02:25,  5.80s/it] 20%|██        | 6/30 [01:02<01:37,  4.08s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.99s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.28s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.23it/s] 60%|██████    | 18/30 [01:11<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.32it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.67s/it]
Epoch loss is 2.3361639261245726
Epoch 479/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:51, 59.70s/it]  7%|▋         | 2/30 [01:00<11:40, 25.02s/it] 10%|█         | 3/30 [01:01<06:22, 14.17s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:03<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.3163852373758953
Epoch 480/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:21, 58.68s/it]  7%|▋         | 2/30 [00:59<11:28, 24.60s/it] 10%|█         | 3/30 [01:00<06:10, 13.71s/it] 13%|█▎        | 4/30 [01:01<03:47,  8.75s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.87s/it] 20%|██        | 6/30 [01:02<01:39,  4.13s/it] 23%|██▎       | 7/30 [01:03<01:09,  3.02s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:06<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:09<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:12<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:15<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:18<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.70s/it]
Epoch loss is 2.3151058753331504
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0008,  0.0132,  ..., -0.0058,  0.0176,  0.0040],
        [-0.0023,  0.0095,  0.0286,  ...,  0.0172,  0.0042, -0.0147],
        [-0.0293, -0.0379,  0.0247,  ...,  0.0834, -0.0043, -0.0143],
        ...,
        [ 0.0059, -0.0033,  0.0099,  ..., -0.0241, -0.0051, -0.0027],
        [-0.0333,  0.0099, -0.0035,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0326, -0.0139,  0.0193,  ...,  0.0414,  0.0360, -0.0284]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8740, 0.8508, 0.8373, 0.8362, 0.8302, 0.8277, 0.8252, 0.8210, 0.8195,
         0.8172],
        [0.9414, 0.9286, 0.9277, 0.9269, 0.9260, 0.9213, 0.9213, 0.9202, 0.9180,
         0.9173],
        [0.9047, 0.8965, 0.8744, 0.8702, 0.8554, 0.8478, 0.8454, 0.8367, 0.8346,
         0.8273],
        [0.9561, 0.8855, 0.8818, 0.8789, 0.8785, 0.8742, 0.8647, 0.8629, 0.8607,
         0.8601],
        [0.8424, 0.8313, 0.8299, 0.8137, 0.8049, 0.8046, 0.8010, 0.7957, 0.7878,
         0.7811],
        [0.8522, 0.8173, 0.8083, 0.7960, 0.7931, 0.7872, 0.7822, 0.7775, 0.7676,
         0.7653],
        [0.8639, 0.8611, 0.8530, 0.8513, 0.8476, 0.8462, 0.8445, 0.8403, 0.8380,
         0.8356],
        [0.8913, 0.8716, 0.8665, 0.8553, 0.8481, 0.8295, 0.8253, 0.8164, 0.8047,
         0.8009],
        [0.9073, 0.9014, 0.8866, 0.8836, 0.8834, 0.8823, 0.8822, 0.8791, 0.8782,
         0.8780],
        [0.9486, 0.8826, 0.8815, 0.8795, 0.8725, 0.8707, 0.8666, 0.8664, 0.8605,
         0.8583],
        [0.9444, 0.9035, 0.8923, 0.8886, 0.8883, 0.8859, 0.8832, 0.8814, 0.8789,
         0.8766],
        [0.8861, 0.8709, 0.8544, 0.8460, 0.8231, 0.7973, 0.7960, 0.7931, 0.7809,
         0.7760],
        [0.9118, 0.9053, 0.9010, 0.8893, 0.8787, 0.8753, 0.8673, 0.8627, 0.8611,
         0.8500],
        [0.9562, 0.9544, 0.9506, 0.9504, 0.9478, 0.9457, 0.9431, 0.9412, 0.9391,
         0.9341],
        [0.9416, 0.9278, 0.9266, 0.9253, 0.9222, 0.9179, 0.9173, 0.9143, 0.9136,
         0.9122],
        [0.9258, 0.9213, 0.9203, 0.9131, 0.9112, 0.9098, 0.9078, 0.9044, 0.9037,
         0.9021],
        [0.9513, 0.9455, 0.9341, 0.9311, 0.9249, 0.9243, 0.9172, 0.9167, 0.9134,
         0.9129],
        [0.9611, 0.9552, 0.9511, 0.9395, 0.9389, 0.9344, 0.9334, 0.9318, 0.9317,
         0.9299],
        [0.9250, 0.9126, 0.8991, 0.8974, 0.8925, 0.8915, 0.8899, 0.8885, 0.8855,
         0.8838],
        [0.9022, 0.8976, 0.8961, 0.8960, 0.8941, 0.8934, 0.8905, 0.8890, 0.8877,
         0.8864],
        [0.9396, 0.9316, 0.9316, 0.9297, 0.9291, 0.9286, 0.9254, 0.9238, 0.9233,
         0.9224],
        [0.9252, 0.9106, 0.9097, 0.9072, 0.9051, 0.9038, 0.9032, 0.9013, 0.8881,
         0.8837],
        [0.9672, 0.9482, 0.9416, 0.9380, 0.9329, 0.9264, 0.9246, 0.9172, 0.9123,
         0.9109],
        [0.9404, 0.9332, 0.9293, 0.9260, 0.9237, 0.9228, 0.9221, 0.9218, 0.9214,
         0.9201],
        [0.9529, 0.8983, 0.8875, 0.8787, 0.8643, 0.8594, 0.8593, 0.8570, 0.8570,
         0.8565],
        [0.9350, 0.9310, 0.9288, 0.9224, 0.9211, 0.9168, 0.9128, 0.9092, 0.9028,
         0.9024],
        [0.9226, 0.9207, 0.9183, 0.9139, 0.9063, 0.8987, 0.8965, 0.8959, 0.8948,
         0.8937],
        [0.9593, 0.9478, 0.9418, 0.9250, 0.9238, 0.9232, 0.9185, 0.9175, 0.9172,
         0.9159],
        [0.9421, 0.9352, 0.9257, 0.9242, 0.9235, 0.9215, 0.9197, 0.9168, 0.9127,
         0.9114],
        [0.9304, 0.9259, 0.9247, 0.9242, 0.9209, 0.9175, 0.9167, 0.9166, 0.9131,
         0.9129],
        [0.9442, 0.9442, 0.9437, 0.9431, 0.9414, 0.9408, 0.9331, 0.9307, 0.9295,
         0.9274],
        [0.9364, 0.9202, 0.9189, 0.9086, 0.9012, 0.9008, 0.8964, 0.8960, 0.8953,
         0.8949],
        [0.8733, 0.8681, 0.8648, 0.8573, 0.8562, 0.8495, 0.8482, 0.8482, 0.8471,
         0.8469],
        [0.8993, 0.8891, 0.8842, 0.8822, 0.8794, 0.8781, 0.8778, 0.8769, 0.8751,
         0.8738],
        [0.9176, 0.9136, 0.9127, 0.9070, 0.9002, 0.8994, 0.8957, 0.8889, 0.8860,
         0.8856],
        [0.9048, 0.8943, 0.8917, 0.8841, 0.8802, 0.8751, 0.8751, 0.8686, 0.8580,
         0.8571],
        [0.9472, 0.8672, 0.8634, 0.8501, 0.8459, 0.8291, 0.8197, 0.8176, 0.8152,
         0.8129],
        [0.8987, 0.8797, 0.8777, 0.8750, 0.8719, 0.8688, 0.8538, 0.8535, 0.8534,
         0.8513],
        [0.8925, 0.8886, 0.8676, 0.8604, 0.8480, 0.8439, 0.8412, 0.8356, 0.8344,
         0.8326],
        [0.9336, 0.9302, 0.9285, 0.9285, 0.9271, 0.9249, 0.9241, 0.9236, 0.9235,
         0.9214],
        [0.8795, 0.8758, 0.8757, 0.8645, 0.8598, 0.8546, 0.8521, 0.8387, 0.8299,
         0.8228],
        [0.8993, 0.8604, 0.8596, 0.8568, 0.8506, 0.8494, 0.8472, 0.8450, 0.8448,
         0.8445],
        [0.8837, 0.8721, 0.8686, 0.8597, 0.8505, 0.8458, 0.8395, 0.8376, 0.8332,
         0.8332],
        [0.8740, 0.8709, 0.8557, 0.8496, 0.8469, 0.8431, 0.8415, 0.8407, 0.8398,
         0.8335],
        [0.8498, 0.8361, 0.8191, 0.8168, 0.8083, 0.8081, 0.7964, 0.7800, 0.7732,
         0.7700],
        [0.8286, 0.8043, 0.7887, 0.7851, 0.7793, 0.7792, 0.7755, 0.7634, 0.7588,
         0.7575],
        [0.8982, 0.8806, 0.8771, 0.8738, 0.8726, 0.8717, 0.8607, 0.8601, 0.8598,
         0.8577],
        [0.8040, 0.7834, 0.7782, 0.7714, 0.7627, 0.7586, 0.7397, 0.7383, 0.7297,
         0.7204],
        [0.8303, 0.8277, 0.8096, 0.8087, 0.8023, 0.8022, 0.8016, 0.7961, 0.7883,
         0.7838],
        [0.8898, 0.8610, 0.8428, 0.8310, 0.8303, 0.8283, 0.8200, 0.8192, 0.8146,
         0.8119],
        [0.8742, 0.8527, 0.8466, 0.8457, 0.8347, 0.8259, 0.8203, 0.8093, 0.8089,
         0.8039],
        [0.8867, 0.8839, 0.8431, 0.8137, 0.8100, 0.8041, 0.8014, 0.7968, 0.7966,
         0.7916],
        [0.9230, 0.9062, 0.9000, 0.8918, 0.8629, 0.8581, 0.8552, 0.8505, 0.8412,
         0.8366],
        [0.9110, 0.8735, 0.8691, 0.8589, 0.8522, 0.8506, 0.8423, 0.8405, 0.8375,
         0.8367],
        [0.9225, 0.8998, 0.8967, 0.8963, 0.8946, 0.8911, 0.8911, 0.8885, 0.8880,
         0.8867],
        [0.8982, 0.8785, 0.8776, 0.8664, 0.8643, 0.8636, 0.8607, 0.8599, 0.8597,
         0.8577],
        [0.9034, 0.8755, 0.8663, 0.8642, 0.8583, 0.8547, 0.8416, 0.8274, 0.8233,
         0.8223],
        [0.9393, 0.8779, 0.8693, 0.8609, 0.8542, 0.8502, 0.8343, 0.8302, 0.8289,
         0.8175],
        [0.9047, 0.8586, 0.8538, 0.8429, 0.8302, 0.8282, 0.8223, 0.8222, 0.8149,
         0.8128],
        [0.8976, 0.8156, 0.8127, 0.8004, 0.7901, 0.7892, 0.7890, 0.7856, 0.7825,
         0.7740],
        [0.8833, 0.8650, 0.8555, 0.8531, 0.8458, 0.8426, 0.8397, 0.8393, 0.8385,
         0.8384],
        [0.8532, 0.7754, 0.7740, 0.7732, 0.7626, 0.7358, 0.7283, 0.7263, 0.7251,
         0.7225],
        [0.8820, 0.8550, 0.8405, 0.8212, 0.8115, 0.8102, 0.7914, 0.7900, 0.7884,
         0.7840],
        [0.8371, 0.8057, 0.7934, 0.7780, 0.7764, 0.7696, 0.7668, 0.7504, 0.7479,
         0.7431]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 264491.7188,  189974.4844,  156490.7969,  154087.4062,  141591.9219,
          136598.9219,  131805.3594,  124097.1250,  121384.2734,  117529.8984],
        [ 693089.6250,  577392.8750,  569839.4375,  562875.7500,  556038.5625,
          520238.4688,  520096.6250,  512157.2812,  495982.9688,  490860.0000],
        [ 410396.5000,  364811.3125,  265875.3125,  250687.4062,  202934.8281,
          181979.3594,  175827.0625,  155254.7656,  150600.2500,  135846.3281],
        [ 854348.4375,  311886.1562,  295521.1562,  283641.5938,  282027.5000,
          265173.6250,  231602.8750,  225608.2344,  218762.9375,  216861.8906],
        [ 168417.2344,  143651.6875,  140955.5938,  111820.2656,   98566.6719,
           98084.5938,   93294.9766,   86445.2578,   77171.4766,   70158.7578],
        [ 193857.7656,  117735.6406,  103543.6562,   86751.9062,   83274.6797,
           76575.8828,   71262.7891,   66617.5156,   57826.3320,   55970.8711],
        [ 228956.3594,  220090.5000,  196078.0000,  191257.5156,  181427.9688,
          177839.4375,  173511.6719,  163429.0312,  158181.6562,  152935.9844],
        [ 338680.9688,  255581.7969,  237514.5156,  202630.8281,  182739.8594,
          140007.8594,  132020.1094,  116238.8672,   98245.9922,   93166.4062],
        [ 425468.0625,  391423.8438,  316533.4062,  303584.7188,  302362.8125,
          297856.5625,  297488.3438,  284340.8750,  280836.3750,  280256.0625],
        [ 768375.1875,  299177.7812,  294516.4375,  286290.7188,  258897.0312,
          252492.9062,  237945.2812,  237301.6875,  217978.8594,  211500.3594],
        [ 723535.0625,  403175.1875,  343491.1875,  325712.5000,  324364.4062,
          313614.2500,  301511.0625,  293962.2188,  283561.2500,  274455.6250],
        [ 314346.0938,  253021.5312,  199866.3750,  177290.2812,  127909.4688,
           88419.5000,   86803.1328,   83309.8594,   69930.3047,   65259.4023],
        [ 453682.1875,  413515.9062,  389202.0312,  329156.7812,  282725.7812,
          269341.2812,  240406.6406,  225016.4531,  220005.7188,  187642.7031],
        [ 855395.1875,  834232.8750,  790599.8750,  788296.8750,  758764.5625,
          737174.6250,  709713.8750,  690529.0625,  670918.4375,  623846.3750],
        [ 695122.4375,  570145.5000,  561081.9375,  550231.1875,  526778.6875,
          495157.3438,  491192.0000,  470135.2500,  465685.0312,  456430.5625],
        [ 554792.1875,  519697.5000,  512205.1562,  462351.0625,  450191.9375,
          441163.7188,  428845.9062,  408630.5000,  404525.8750,  395180.6875],
        [ 798283.0625,  734444.9375,  624461.2500,  598410.0625,  547476.4375,
          542300.2500,  490343.0000,  487074.9375,  464601.3125,  460838.0625],
        [ 918621.9375,  843693.8750,  795823.3125,  674651.3125,  668539.9375,
          626568.8125,  618406.8125,  603873.5000,  602828.0000,  587905.1250],
        [ 547990.9375,  459220.5312,  378707.9375,  369643.1875,  344443.4688,
          339580.0625,  331901.0312,  325570.2812,  311917.0938,  304226.9688],
        [ 395659.2188,  370428.3438,  362979.6562,  362231.6875,  352362.3125,
          348963.9375,  334915.3125,  327943.2500,  321605.7500,  315719.0938],
        [ 675107.6875,  602452.7500,  602359.1250,  586098.0625,  581248.1250,
          576667.0625,  551093.5000,  539061.6250,  535108.9375,  528005.8750],
        [ 549404.8125,  446102.3125,  440379.3750,  425186.1250,  412276.7188,
          404800.6562,  401661.5625,  390463.0938,  323497.5625,  303888.8750],
        [1001546.3750,  763697.5000,  694497.5625,  660032.5625,  614044.4375,
          559607.0625,  544978.6250,  490430.4688,  456869.0938,  448440.2188],
        [ 683315.1250,  616236.1875,  583248.8125,  556324.9375,  538090.3125,
          531248.0000,  525885.6875,  523350.1250,  520957.4062,  510797.3438],
        [ 815986.4375,  374515.0938,  320972.7500,  282782.4062,  230163.9375,
          214863.1875,  214429.2031,  207623.2969,  207398.8906,  206058.6562],
        [ 632435.8750,  597354.0625,  578452.8125,  528427.0625,  518676.0000,
          487774.0625,  460168.3438,  437094.0312,  399227.0312,  396691.4062],
        [ 530033.5000,  515810.5312,  498026.3125,  467616.9062,  419693.5000,
          376231.2812,  364585.9375,  361443.5312,  356238.4688,  350733.2500],
        [ 894864.5625,  758747.9375,  696632.1875,  548180.6875,  538915.6250,
          534311.3750,  499471.8125,  492474.4062,  490307.9375,  481027.2500],
        [ 700214.3125,  634528.0625,  553491.6250,  541678.9375,  536483.4375,
          521331.6250,  508525.0625,  487435.5625,  459677.5312,  451648.9062],
        [ 591758.2500,  554877.8750,  546094.0625,  542087.7500,  517055.1250,
          492319.0000,  486870.5938,  486142.1875,  462756.0000,  461305.0312],
        [ 721106.8125,  720900.5625,  716122.5000,  709839.1250,  693096.1875,
          686976.1875,  615729.8125,  594225.0625,  584879.1250,  567292.6250],
        [ 645471.9375,  512150.9375,  502502.3438,  433527.9688,  390383.4062,
          387806.3438,  364150.5312,  362439.7188,  358638.6250,  356438.3125],
        [ 262015.5469,  243023.4531,  231971.1406,  208341.5156,  205045.7500,
          186323.4844,  182989.4219,  182893.1094,  180037.0312,  179684.5469],
        [ 379905.6250,  328240.1875,  305915.2812,  297361.5312,  285916.0938,
          280571.3438,  279241.7188,  275663.8438,  268634.0625,  263939.6250],
        [ 493503.5625,  465656.1562,  459940.1875,  424029.2188,  384606.0625,
          380486.5000,  360901.0312,  327171.9688,  313887.1562,  312104.8438],
        [ 410541.7188,  353355.6875,  340699.5000,  305593.6562,  289232.8750,
          268864.7188,  268822.9375,  244851.2656,  210590.6250,  207685.8906],
        [ 753039.3750,  239919.9531,  227297.2500,  188034.2969,  177077.7188,
          139327.2344,  121843.2031,  118121.3984,  114187.9766,  110528.6953],
        [ 376369.0625,  286990.7812,  278935.1250,  268310.4062,  256599.2344,
          245688.4219,  198298.1406,  197306.4688,  197024.4062,  191163.4062],
        [ 344313.0938,  325778.3750,  241589.1406,  217699.4531,  182439.8438,
          172072.7500,  165491.4844,  152810.7500,  150247.0625,  146486.5156],
        [ 619913.1250,  590278.6875,  576611.0000,  575841.6250,  564593.4375,
          547319.3125,  541338.6250,  537026.0625,  536535.1250,  520699.0938],
        [ 285986.1875,  271301.0625,  271106.0312,  231048.2500,  215948.0312,
          200606.5312,  193545.2031,  159673.5156,  140833.7344,  127238.0078],
        [ 379484.8750,  217771.9219,  215334.5781,  206841.4844,  189230.0469,
          186245.3281,  180324.8594,  174803.6719,  174238.2812,  173595.5938],
        [ 303881.3125,  257479.0156,  245026.9219,  215596.1562,  188974.3281,
          176760.1719,  161499.9219,  157164.7344,  147678.9219,  147635.8438],
        [ 264512.9062,  253008.9688,  203759.6094,  186756.5000,  179690.0156,
          170139.0938,  166315.4844,  164482.7656,  162276.8125,  148402.0625],
        [ 187225.8438,  153835.2969,  120717.7031,  116833.3438,  103534.8672,
          103191.9297,   87337.3047,   69027.4531,   62696.9570,   59861.2969],
        [ 138384.6406,   97768.0938,   78204.1094,   74268.2500,   68335.1953,
           68286.1406,   64796.0234,   54487.1836,   50997.1602,   50103.9219],
        [ 373758.3125,  290689.8750,  276442.3125,  263761.7500,  259436.8281,
          255861.5156,  218699.0938,  216984.1562,  215910.5469,  209443.5781],
        [  97335.4062,   72548.0391,   67309.7031,   61092.9141,   53979.6445,
           50906.5391,   38815.2461,   38044.0234,   33679.0781,   29462.7793],
        [ 141735.5469,  136442.1719,  105402.7734,  104135.7500,   94923.0703,
           94866.8750,   93999.8984,   86884.6250,   77817.1719,   72963.5156],
        [ 331445.2500,  219656.4531,  169389.1875,  143034.2188,  141777.8438,
          137703.7656,  122384.6016,  120857.5391,  113200.8594,  108886.5234],
        [ 265211.3125,  195036.4375,  178817.5000,  176681.2969,  150809.2188,
          133057.0625,  122908.4922,  105034.7031,  104387.7031,   97243.0859],
        [ 317319.5312,  304510.2812,  170004.6406,  111734.5547,  106029.0547,
           97413.2188,   93792.4141,   87842.8359,   87541.5234,   81546.4062],
        [ 532898.1875,  418998.0312,  383380.3438,  341037.2812,  225675.3750,
          210668.5625,  202276.7188,  188961.8906,  165547.3750,  154954.2031],
        [ 448468.4688,  262687.5625,  246666.0156,  213303.1562,  193687.7500,
          189306.3906,  168109.6094,  163911.3594,  156989.9219,  155217.1562],
        [ 528746.6250,  382159.6875,  366087.9375,  363522.5000,  354835.4375,
          337848.0312,  337848.0312,  325209.0938,  323299.2500,  317280.1875],
        [ 373921.2500,  282105.7812,  278561.8750,  237396.7656,  230223.4375,
          228097.6719,  218766.4844,  216372.0781,  215644.2656,  209708.8125],
        [ 402426.1250,  270345.4375,  236935.5938,  230064.9688,  211439.8594,
          200869.3906,  166579.1406,  135982.1719,  128146.4609,  126397.2188],
        [ 672019.1875,  279675.0625,  247178.4375,  219362.7344,  199396.5000,
          188316.2344,  149994.2344,  141589.6250,  138926.1406,  117987.1953],
        [ 410356.1875,  212365.0000,  198239.7031,  169701.0781,  141492.2969,
          137421.7031,  126342.5000,  126130.3828,  113677.8438,  110428.7109],
        [ 370769.0625,  114843.8984,  110225.6406,   92391.8516,   79838.2422,
           78741.3672,   78489.3828,   74810.5703,   71557.5938,   63354.3281],
        [ 302011.1875,  232563.4375,  203184.2656,  196303.2812,  176756.1406,
          168932.4688,  161944.7344,  161026.5469,  159385.2031,  159124.7500],
        [ 196420.3281,   64670.5820,   63401.2344,   62683.8672,   53888.8125,
           36707.7461,   32992.3750,   32052.3223,   31528.4766,   30393.7090],
        [ 296752.4688,  201693.4531,  164035.8281,  124454.5859,  108381.8828,
          106326.6484,   81341.4375,   79631.7891,   77835.0625,   73128.6172],
        [ 156202.5781,   99698.2969,   83586.4922,   67148.7109,   65587.4609,
           59526.1016,   57180.8203,   45241.9883,   43660.4297,   40768.6328]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[264491.7188,      0.0000],
         [189974.4844,      0.0000],
         [156490.7969,      0.0000],
         ...,
         [124097.1250,      0.0000],
         [121384.2734,      0.0000],
         [117529.8984,      0.0000]],

        [[693089.6250,      0.0000],
         [577392.8750,      0.0000],
         [569839.4375,      0.0000],
         ...,
         [512157.2812,      0.0000],
         [495982.9688,      0.0000],
         [490860.0000,      0.0000]],

        [[410396.5000,      0.0000],
         [364811.3125,      0.0000],
         [265875.3125,      0.0000],
         ...,
         [155254.7656,      0.0000],
         [150600.2500,      0.0000],
         [135846.3281,      0.0000]],

        ...,

        [[     0.0000, 196420.3281],
         [ 64670.5820,      0.0000],
         [ 63401.2344,      0.0000],
         ...,
         [ 32052.3223,      0.0000],
         [ 31528.4766,      0.0000],
         [ 30393.7090,      0.0000]],

        [[296752.4688,      0.0000],
         [     0.0000, 201693.4531],
         [164035.8281,      0.0000],
         ...,
         [ 79631.7891,      0.0000],
         [ 77835.0625,      0.0000],
         [ 73128.6172,      0.0000]],

        [[     0.0000, 156202.5781],
         [     0.0000,  99698.2969],
         [     0.0000,  83586.4922],
         ...,
         [ 45241.9883,      0.0000],
         [     0.0000,  43660.4297],
         [     0.0000,  40768.6328]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1538051.8750,       0.0000],
        [5498571.5000,       0.0000],
        [2294213.0000,       0.0000],
        [3185434.2500,       0.0000],
        [ 450530.0625,  638036.4375],
        [ 770223.6250,  143193.4062],
        [1685526.5000,  158181.6562],
        [1796827.2500,       0.0000],
        [3180151.0000,       0.0000],
        [3064476.5000,       0.0000],
        [3587382.7500,       0.0000],
        [1466155.8750,       0.0000],
        [3010695.5000,       0.0000],
        [7459472.0000,       0.0000],
        [5281960.0000,       0.0000],
        [4577584.5000,       0.0000],
        [5748233.0000,       0.0000],
        [6940913.0000,       0.0000],
        [3713201.5000,       0.0000],
        [3164865.2500,  327943.2500],
        [5777203.0000,       0.0000],
        [4097661.2500,       0.0000],
        [6234144.0000,       0.0000],
        [5589454.0000,       0.0000],
        [3074793.7500,       0.0000],
        [5036300.5000,       0.0000],
        [4240413.0000,       0.0000],
        [5934934.0000,       0.0000],
        [5395015.5000,       0.0000],
        [5141266.0000,       0.0000],
        [6610168.0000,       0.0000],
        [4313510.0000,       0.0000],
        [2062325.1250,       0.0000],
        [2373558.0000,  591831.3750],
        [1774005.2500, 2148281.5000],
        [1016483.4375, 1883755.3750],
        [1085889.0000, 1103488.0000],
        [1308416.0000, 1188269.5000],
        [ 703197.1250, 1395731.3750],
        [5610156.0000,       0.0000],
        [1970048.5000,  127238.0078],
        [2097870.7500,       0.0000],
        [2001697.2500,       0.0000],
        [1899344.2500,       0.0000],
        [ 232423.6094,  831838.3750],
        [ 326682.7812,  418947.9375],
        [2207229.5000,  373758.3125],
        [ 341514.7188,  201658.6719],
        [ 447203.8438,  561967.5625],
        [ 440331.7812, 1168004.5000],
        [1291742.0000,  237444.7656],
        [ 665900.0000,  791834.4375],
        [1664732.2500, 1159665.7500],
        [2009041.0000,  189306.3906],
        [3636837.0000,       0.0000],
        [2490798.5000,       0.0000],
        [ 965408.3750, 1143778.0000],
        [1656011.1250,  698434.3125],
        [ 941834.0000,  804321.3750],
        [ 528668.5000,  606353.5000],
        [1209901.2500,  711330.7500],
        [ 408319.1250,  196420.3281],
        [ 897179.8125,  416401.9688],
        [ 104768.0938,  613833.4375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 481/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:50, 59.66s/it]  7%|▋         | 2/30 [01:00<11:40, 25.01s/it] 10%|█         | 3/30 [01:01<06:16, 13.93s/it] 13%|█▎        | 4/30 [01:01<03:46,  8.73s/it] 17%|█▋        | 5/30 [01:02<02:26,  5.85s/it] 20%|██        | 6/30 [01:03<01:38,  4.12s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.29s/it] 30%|███       | 9/30 [01:05<00:38,  1.81s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.32it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.33it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.3290959358215333
Epoch 482/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:01, 62.11s/it]  7%|▋         | 2/30 [01:02<12:08, 26.01s/it] 10%|█         | 3/30 [01:03<06:30, 14.48s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.2693301836649575
Epoch 483/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:45, 59.48s/it]  7%|▋         | 2/30 [01:00<11:46, 25.23s/it] 10%|█         | 3/30 [01:01<06:22, 14.17s/it] 13%|█▎        | 4/30 [01:02<03:50,  8.87s/it] 17%|█▋        | 5/30 [01:03<02:28,  5.94s/it] 20%|██        | 6/30 [01:03<01:40,  4.18s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.06s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.32s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.3006083647410076
Epoch 484/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:58, 57.88s/it]  7%|▋         | 2/30 [00:58<11:19, 24.27s/it] 10%|█         | 3/30 [00:59<06:05, 13.53s/it] 13%|█▎        | 4/30 [01:00<03:40,  8.49s/it] 17%|█▋        | 5/30 [01:00<02:22,  5.70s/it] 20%|██        | 6/30 [01:01<01:36,  4.01s/it] 23%|██▎       | 7/30 [01:02<01:07,  2.95s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.25s/it] 30%|███       | 9/30 [01:03<00:37,  1.78s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.46s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.24s/it] 40%|████      | 12/30 [01:06<00:19,  1.09s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:12,  1.16it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.21it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.66s/it]
Epoch loss is 2.288381576538086
Epoch 485/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:19, 58.61s/it]  7%|▋         | 2/30 [01:01<12:00, 25.74s/it] 10%|█         | 3/30 [01:02<06:26, 14.33s/it] 13%|█▎        | 4/30 [01:02<03:53,  8.97s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.00s/it] 20%|██        | 6/30 [01:04<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.3350252151489257
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0008,  0.0132,  ..., -0.0058,  0.0176,  0.0040],
        [-0.0023,  0.0095,  0.0286,  ...,  0.0172,  0.0042, -0.0147],
        [-0.0293, -0.0379,  0.0247,  ...,  0.0834, -0.0043, -0.0143],
        ...,
        [ 0.0059, -0.0033,  0.0099,  ..., -0.0241, -0.0051, -0.0027],
        [-0.0333,  0.0099, -0.0034,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0326, -0.0139,  0.0193,  ...,  0.0414,  0.0360, -0.0284]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8740, 0.8508, 0.8372, 0.8362, 0.8302, 0.8277, 0.8252, 0.8210, 0.8195,
         0.8172],
        [0.9414, 0.9286, 0.9277, 0.9268, 0.9260, 0.9213, 0.9213, 0.9202, 0.9180,
         0.9173],
        [0.9047, 0.8965, 0.8743, 0.8702, 0.8554, 0.8478, 0.8454, 0.8367, 0.8346,
         0.8273],
        [0.9561, 0.8855, 0.8817, 0.8789, 0.8785, 0.8742, 0.8647, 0.8628, 0.8607,
         0.8601],
        [0.8424, 0.8313, 0.8299, 0.8137, 0.8049, 0.8045, 0.8010, 0.7957, 0.7877,
         0.7811],
        [0.8522, 0.8173, 0.8083, 0.7959, 0.7931, 0.7872, 0.7822, 0.7775, 0.7675,
         0.7653],
        [0.8639, 0.8611, 0.8530, 0.8513, 0.8476, 0.8462, 0.8445, 0.8403, 0.8380,
         0.8356],
        [0.8913, 0.8716, 0.8664, 0.8553, 0.8481, 0.8294, 0.8253, 0.8164, 0.8047,
         0.8009],
        [0.9073, 0.9014, 0.8865, 0.8836, 0.8833, 0.8823, 0.8822, 0.8790, 0.8782,
         0.8780],
        [0.9486, 0.8826, 0.8815, 0.8795, 0.8725, 0.8707, 0.8666, 0.8664, 0.8604,
         0.8583],
        [0.9444, 0.9035, 0.8923, 0.8885, 0.8883, 0.8859, 0.8832, 0.8814, 0.8789,
         0.8766],
        [0.8861, 0.8709, 0.8544, 0.8460, 0.8231, 0.7973, 0.7960, 0.7931, 0.7808,
         0.7760],
        [0.9117, 0.9053, 0.9010, 0.8893, 0.8786, 0.8752, 0.8673, 0.8627, 0.8611,
         0.8499],
        [0.9561, 0.9544, 0.9506, 0.9504, 0.9478, 0.9457, 0.9431, 0.9412, 0.9391,
         0.9340],
        [0.9416, 0.9277, 0.9266, 0.9253, 0.9222, 0.9179, 0.9173, 0.9142, 0.9136,
         0.9122],
        [0.9258, 0.9213, 0.9202, 0.9131, 0.9112, 0.9098, 0.9078, 0.9044, 0.9037,
         0.9021],
        [0.9513, 0.9455, 0.9341, 0.9311, 0.9249, 0.9242, 0.9172, 0.9167, 0.9134,
         0.9128],
        [0.9611, 0.9552, 0.9511, 0.9395, 0.9389, 0.9344, 0.9334, 0.9318, 0.9317,
         0.9299],
        [0.9250, 0.9126, 0.8991, 0.8974, 0.8925, 0.8915, 0.8899, 0.8885, 0.8855,
         0.8838],
        [0.9022, 0.8976, 0.8961, 0.8960, 0.8941, 0.8934, 0.8905, 0.8890, 0.8877,
         0.8864],
        [0.9396, 0.9316, 0.9316, 0.9297, 0.9291, 0.9285, 0.9254, 0.9238, 0.9233,
         0.9224],
        [0.9252, 0.9106, 0.9097, 0.9072, 0.9051, 0.9038, 0.9032, 0.9012, 0.8881,
         0.8837],
        [0.9672, 0.9482, 0.9416, 0.9380, 0.9329, 0.9264, 0.9246, 0.9172, 0.9122,
         0.9109],
        [0.9404, 0.9332, 0.9293, 0.9260, 0.9237, 0.9228, 0.9221, 0.9218, 0.9214,
         0.9201],
        [0.9528, 0.8983, 0.8875, 0.8787, 0.8642, 0.8594, 0.8593, 0.8570, 0.8570,
         0.8565],
        [0.9350, 0.9310, 0.9288, 0.9224, 0.9211, 0.9168, 0.9127, 0.9091, 0.9028,
         0.9024],
        [0.9226, 0.9207, 0.9183, 0.9139, 0.9063, 0.8986, 0.8964, 0.8958, 0.8948,
         0.8937],
        [0.9593, 0.9478, 0.9418, 0.9250, 0.9238, 0.9232, 0.9185, 0.9175, 0.9172,
         0.9159],
        [0.9421, 0.9352, 0.9257, 0.9242, 0.9235, 0.9215, 0.9197, 0.9168, 0.9127,
         0.9114],
        [0.9303, 0.9258, 0.9247, 0.9242, 0.9209, 0.9175, 0.9167, 0.9166, 0.9131,
         0.9129],
        [0.9442, 0.9442, 0.9437, 0.9431, 0.9414, 0.9408, 0.9331, 0.9306, 0.9295,
         0.9274],
        [0.9364, 0.9202, 0.9189, 0.9086, 0.9012, 0.9008, 0.8964, 0.8960, 0.8953,
         0.8949],
        [0.8733, 0.8681, 0.8648, 0.8573, 0.8562, 0.8495, 0.8482, 0.8482, 0.8470,
         0.8469],
        [0.8993, 0.8891, 0.8842, 0.8822, 0.8794, 0.8781, 0.8778, 0.8769, 0.8751,
         0.8738],
        [0.9176, 0.9136, 0.9127, 0.9070, 0.9002, 0.8994, 0.8957, 0.8889, 0.8860,
         0.8856],
        [0.9048, 0.8943, 0.8917, 0.8841, 0.8802, 0.8751, 0.8751, 0.8686, 0.8580,
         0.8571],
        [0.9472, 0.8672, 0.8634, 0.8501, 0.8459, 0.8291, 0.8197, 0.8175, 0.8152,
         0.8129],
        [0.8987, 0.8797, 0.8777, 0.8750, 0.8718, 0.8688, 0.8538, 0.8535, 0.8534,
         0.8512],
        [0.8924, 0.8886, 0.8676, 0.8603, 0.8480, 0.8439, 0.8412, 0.8356, 0.8344,
         0.8326],
        [0.9336, 0.9302, 0.9285, 0.9284, 0.9271, 0.9249, 0.9241, 0.9236, 0.9235,
         0.9214],
        [0.8794, 0.8758, 0.8757, 0.8645, 0.8598, 0.8546, 0.8521, 0.8386, 0.8299,
         0.8227],
        [0.8992, 0.8604, 0.8596, 0.8568, 0.8505, 0.8494, 0.8472, 0.8450, 0.8448,
         0.8445],
        [0.8837, 0.8721, 0.8686, 0.8597, 0.8504, 0.8458, 0.8394, 0.8375, 0.8332,
         0.8332],
        [0.8740, 0.8709, 0.8557, 0.8496, 0.8469, 0.8431, 0.8415, 0.8407, 0.8398,
         0.8335],
        [0.8498, 0.8360, 0.8191, 0.8168, 0.8083, 0.8081, 0.7964, 0.7799, 0.7732,
         0.7700],
        [0.8286, 0.8043, 0.7887, 0.7851, 0.7792, 0.7792, 0.7755, 0.7634, 0.7588,
         0.7575],
        [0.8982, 0.8806, 0.8771, 0.8738, 0.8726, 0.8716, 0.8607, 0.8601, 0.8598,
         0.8576],
        [0.8040, 0.7834, 0.7782, 0.7714, 0.7627, 0.7586, 0.7396, 0.7382, 0.7297,
         0.7204],
        [0.8303, 0.8276, 0.8096, 0.8087, 0.8022, 0.8022, 0.8016, 0.7960, 0.7883,
         0.7838],
        [0.8898, 0.8610, 0.8428, 0.8309, 0.8303, 0.8283, 0.8200, 0.8192, 0.8146,
         0.8119],
        [0.8742, 0.8527, 0.8466, 0.8457, 0.8347, 0.8259, 0.8203, 0.8093, 0.8089,
         0.8039],
        [0.8867, 0.8838, 0.8430, 0.8137, 0.8100, 0.8040, 0.8014, 0.7968, 0.7966,
         0.7916],
        [0.9230, 0.9062, 0.9000, 0.8918, 0.8629, 0.8580, 0.8552, 0.8504, 0.8412,
         0.8365],
        [0.9109, 0.8735, 0.8691, 0.8589, 0.8522, 0.8506, 0.8423, 0.8405, 0.8375,
         0.8367],
        [0.9225, 0.8997, 0.8967, 0.8962, 0.8945, 0.8911, 0.8911, 0.8884, 0.8880,
         0.8867],
        [0.8982, 0.8785, 0.8776, 0.8664, 0.8643, 0.8636, 0.8607, 0.8599, 0.8597,
         0.8577],
        [0.9033, 0.8755, 0.8663, 0.8642, 0.8583, 0.8547, 0.8416, 0.8274, 0.8232,
         0.8223],
        [0.9393, 0.8779, 0.8692, 0.8609, 0.8542, 0.8502, 0.8343, 0.8302, 0.8289,
         0.8175],
        [0.9047, 0.8586, 0.8538, 0.8429, 0.8302, 0.8281, 0.8223, 0.8221, 0.8149,
         0.8128],
        [0.8976, 0.8156, 0.8127, 0.8003, 0.7901, 0.7891, 0.7889, 0.7856, 0.7825,
         0.7739],
        [0.8833, 0.8650, 0.8555, 0.8531, 0.8458, 0.8426, 0.8396, 0.8392, 0.8385,
         0.8384],
        [0.8532, 0.7754, 0.7740, 0.7732, 0.7626, 0.7358, 0.7283, 0.7263, 0.7251,
         0.7225],
        [0.8820, 0.8550, 0.8405, 0.8212, 0.8115, 0.8102, 0.7914, 0.7899, 0.7883,
         0.7840],
        [0.8371, 0.8057, 0.7933, 0.7780, 0.7764, 0.7696, 0.7668, 0.7504, 0.7479,
         0.7431]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 264418.8125,  189984.9844,  156446.1719,  154060.5156,  141572.7500,
          136576.9062,  131759.6250,  124071.8125,  121369.3359,  117535.5078],
        [ 693042.0000,  577329.5625,  569810.1250,  562787.7500,  556039.1250,
          520156.6250,  519987.0000,  512080.6250,  495932.3750,  490800.5312],
        [ 410358.1562,  364762.9688,  265788.3438,  250646.5156,  202882.2031,
          181945.3438,  175789.6719,  155212.5781,  150568.0938,  135831.7031],
        [ 854341.9375,  311821.6250,  295452.3750,  283602.9062,  282019.1562,
          265106.6250,  231557.1406,  225554.4375,  218701.6094,  216831.0625],
        [ 168401.8281,  143641.6875,  140911.9219,  111804.4844,   98539.4141,
           98055.8750,   93285.2812,   86431.0000,   77151.3828,   70161.3672],
        [ 193837.2500,  117726.4297,  103535.2578,   86736.5156,   83277.3750,
           76554.2734,   71251.1641,   66612.9453,   57806.6992,   55962.6523],
        [ 228899.3750,  220030.8906,  196026.2188,  191226.1406,  181380.7344,
          177811.4531,  173468.8125,  163391.6406,  158126.4688,  152887.7031],
        [ 338605.4062,  255538.1719,  237484.1719,  202593.9219,  182704.4844,
          139966.6094,  131998.7031,  116211.2656,   98224.7188,   93140.8281],
        [ 425423.8438,  391402.9688,  316473.9375,  303565.9062,  302330.7812,
          297796.6250,  297467.6250,  284302.6562,  280798.3438,  280214.0938],
        [ 768356.1250,  299116.7188,  294466.7188,  286253.0312,  258856.0625,
          252446.9219,  237902.8594,  237260.2812,  217952.4531,  211468.2969],
        [ 723513.6250,  403148.6562,  343437.1562,  325641.6875,  324266.6875,
          313548.7812,  301473.6562,  293905.8750,  283522.3125,  274383.1562],
        [ 314265.7500,  252958.7969,  199829.5938,  177239.9062,  127871.1719,
           88405.0781,   86767.2109,   83275.7891,   69902.8359,   65236.5000],
        [ 453543.3438,  413463.4688,  389115.1875,  329086.4688,  282642.4688,
          269278.1250,  240351.8594,  224959.3906,  219962.2812,  187599.9375],
        [ 855275.3125,  834201.7500,  790495.1250,  788188.6250,  758668.3750,
          737056.5000,  709646.8750,  690423.6875,  670838.5000,  623761.3125],
        [ 695091.9375,  570085.6875,  561046.0625,  550178.1875,  526703.8125,
          495104.4375,  491154.0625,  470105.7188,  465663.2812,  456417.9375],
        [ 554767.8750,  519663.7812,  512164.1562,  462311.8125,  450169.6250,
          441126.7500,  428816.4688,  408597.3750,  404493.4688,  395163.6875],
        [ 798193.1875,  734359.5000,  624377.8750,  598287.9375,  547388.7500,
          542223.2500,  490278.9688,  486980.1875,  464533.9688,  460773.4688],
        [ 918529.0625,  843631.9375,  795788.3750,  674635.8750,  668495.3125,
          626514.4375,  618343.1250,  603805.0000,  602809.0625,  587865.8750],
        [ 547928.2500,  459123.3125,  378598.9062,  369604.7500,  344397.4688,
          339520.4688,  331866.8750,  325513.7812,  311887.0312,  304210.7188],
        [ 395590.5312,  370402.5938,  362932.9375,  362190.5625,  352313.5625,
          348916.3750,  334858.1250,  327915.7188,  321563.1562,  315689.0000],
        [ 674949.9375,  602366.0000,  602237.8750,  586037.1250,  581175.5000,
          576543.3125,  551033.0625,  538990.1250,  535000.2500,  527922.8125],
        [ 549379.6875,  446098.5000,  440327.3125,  425127.3750,  412247.2188,
          404730.7812,  401580.3750,  390419.8750,  323446.0625,  303817.8750],
        [1001465.1875,  763672.6875,  694438.6250,  659943.8125,  614001.1250,
          559541.3750,  544926.6250,  490392.5625,  456836.4062,  448382.5000],
        [ 683264.3125,  616176.8125,  583204.8750,  556300.5625,  538068.2500,
          531230.8125,  525853.6250,  523295.1875,  520886.3438,  510770.5625],
        [ 815884.5000,  374458.3125,  320891.0312,  282748.9688,  230123.1250,
          214822.0000,  214354.9844,  207570.2500,  207351.4219,  206050.4062],
        [ 632356.3125,  597300.5625,  578370.5625,  528355.5000,  518629.5000,
          487712.6562,  460097.2500,  437029.4062,  399183.9688,  396626.7188],
        [ 529951.6250,  515717.5938,  497926.1250,  467544.6562,  419616.6562,
          376177.8125,  364518.8438,  361393.9062,  356185.5000,  350676.0625],
        [ 894810.8125,  758703.7500,  696557.8125,  548103.8750,  538848.7500,
          534254.3125,  499437.0312,  492461.7188,  490273.3438,  480991.9375],
        [ 700140.8750,  634463.3125,  553462.5625,  541598.8750,  536407.7500,
          521267.0000,  508462.0312,  487391.8438,  459600.8125,  451577.4062],
        [ 591671.9375,  554822.8750,  546063.3125,  542052.5625,  517009.2812,
          492248.0938,  486825.0938,  486098.1250,  462741.9062,  461263.2500],
        [ 721070.3750,  720854.4375,  716029.6250,  709786.3125,  693005.6875,
          686925.0625,  615665.1875,  594171.2500,  584845.1250,  567241.8125],
        [ 645394.3750,  512118.6875,  502467.3438,  433458.5000,  390363.3125,
          387760.4688,  364113.7188,  362402.7188,  358582.8750,  356410.7812],
        [ 261954.0938,  242989.1562,  231928.4375,  208308.1406,  205021.7188,
          186302.5156,  182951.5625,  182857.0156,  179988.9531,  179637.5781],
        [ 379803.1250,  328174.1562,  305844.6875,  297303.4062,  285854.1875,
          280527.7188,  279179.9375,  275639.9375,  268596.1562,  263881.2500],
        [ 493422.1562,  465604.2188,  459899.4062,  423939.0625,  384592.5000,
          380431.0000,  360850.4375,  327123.0000,  313830.5938,  312062.0000],
        [ 410508.0312,  353285.2500,  340641.3438,  305577.0312,  289171.9062,
          268831.1250,  268798.3125,  244805.2656,  210563.3125,  207654.0000],
        [ 753027.1875,  239918.7969,  227273.0000,  188012.6094,  177056.0938,
          139282.2031,  121809.3906,  118097.1797,  114179.0469,  110481.4844],
        [ 376285.4688,  286901.5625,  278834.8438,  268285.0938,  256508.4688,
          245643.6719,  198234.0312,  197236.4844,  196981.7812,  191113.6562],
        [ 344271.0625,  325719.9688,  241529.0000,  217651.6875,  182409.9219,
          172009.2656,  165456.1250,  152766.7344,  150219.7031,  146463.1719],
        [ 619792.5000,  590177.9375,  576527.3750,  575767.4375,  564485.7500,
          547195.6250,  541262.2500,  537026.0625,  536458.3750,  520608.7188],
        [ 285934.9375,  271255.2812,  271052.5312,  230994.5000,  215914.2656,
          200569.6250,  193500.5469,  159634.0781,  140796.0000,  127204.3984],
        [ 379429.5000,  217742.2031,  215282.0000,  206802.8125,  189178.4375,
          186201.8125,  180277.5625,  174741.6719,  174201.2500,  173568.4375],
        [ 303823.0938,  257418.6094,  244969.2344,  215557.9219,  188934.1406,
          176718.3750,  161456.0469,  157132.0469,  147659.7812,  147620.5000],
        [ 264412.0312,  252969.8906,  203706.9688,  186686.3438,  179648.3750,
          170109.8906,  166253.7969,  164447.1562,  162249.2656,  148391.8594],
        [ 187172.6562,  153819.0156,  120683.8594,  116786.1094,  103504.9531,
          103162.7031,   87310.9922,   68998.8203,   62684.7617,   59835.7266],
        [ 138355.2188,   97742.9219,   78179.3516,   74255.5703,   68325.7500,
           68280.3438,   64790.8945,   54480.7344,   50992.5430,   50101.2969],
        [ 373687.0312,  290574.3125,  276357.7188,  263674.2188,  259341.8438,
          255746.1250,  218622.5781,  216884.2188,  215826.5469,  209350.7188],
        [  97306.8203,   72535.5859,   67293.5312,   61075.9609,   53974.2383,
           50892.5586,   38796.2227,   38026.5781,   33670.9180,   29461.2344],
        [ 141686.6250,  136398.1875,  105388.8984,  104112.0234,   94897.5547,
           94853.5781,   93987.1641,   86862.4219,   77793.9453,   72943.6875],
        [ 331375.3750,  219618.3125,  169335.0781,  142981.9844,  141741.3438,
          137666.7344,  122354.0312,  120833.4531,  113177.1172,  108875.0000],
        [ 265122.5625,  195003.3125,  178785.2656,  176636.4844,  150780.1719,
          133043.2344,  122897.0078,  105002.8594,  104375.6641,   97228.6172],
        [ 317256.2812,  304460.6250,  169961.6719,  111701.4219,  105990.0234,
           97365.8516,   93768.9766,   87825.5000,   87520.4922,   81514.6016],
        [ 532797.0625,  418960.8750,  383263.7500,  340977.4375,  225616.8281,
          210624.9688,  202232.3594,  188892.1562,  165518.1562,  154912.6719],
        [ 448428.2500,  262660.2812,  246630.9844,  213268.1562,  193655.6250,
          189252.7812,  168089.7344,  163862.5938,  156957.1250,  155190.5156],
        [ 528637.2500,  382082.0625,  366011.8438,  363452.4688,  354781.9688,
          337777.1562,  337777.1562,  325148.2812,  323253.3125,  317198.5000],
        [ 373846.7500,  282055.2188,  278516.4688,  237346.5156,  230175.3594,
          228047.2188,  218745.2031,  216311.6250,  215588.3438,  209653.0156],
        [ 402296.7812,  270300.3125,  236861.7031,  230002.8906,  211414.6406,
          200814.9844,  166564.3594,  135942.5000,  128093.9141,  126358.2891],
        [ 671962.8125,  279628.9375,  247125.8750,  219309.8125,  199353.3594,
          188295.5781,  149971.0625,  141545.6094,  138876.8594,  117946.2500],
        [ 410265.4062,  212339.0781,  198203.9844,  169651.4062,  141472.0625,
          137384.0938,  126317.3281,  126085.2812,  113636.8750,  110379.1094],
        [ 370704.0312,  114813.1250,  110192.7422,   92359.2500,   79808.3984,
           78709.9766,   78459.4453,   74786.9531,   71536.3047,   63330.7695],
        [ 301991.9062,  232504.6719,  203137.7656,  196263.2188,  176703.3750,
          168909.9219,  161899.4844,  160979.8750,  159314.6875,  159073.6094],
        [ 196404.2031,   64666.8164,   63381.9492,   62668.7461,   53877.8672,
           36710.8984,   32985.8945,   32051.3438,   31523.4844,   30385.9141],
        [ 296712.0000,  201680.7656,  163995.7812,  124438.0938,  108363.2812,
          106289.9453,   81304.2891,   79598.3828,   77803.5938,   73103.5781],
        [ 156175.7656,   99672.8203,   83564.5781,   67130.9141,   65573.8906,
           59503.6289,   57168.4414,   45224.6523,   43645.4805,   40755.2617]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[264418.8125,      0.0000],
         [189984.9844,      0.0000],
         [156446.1719,      0.0000],
         ...,
         [124071.8125,      0.0000],
         [121369.3359,      0.0000],
         [117535.5078,      0.0000]],

        [[693042.0000,      0.0000],
         [577329.5625,      0.0000],
         [569810.1250,      0.0000],
         ...,
         [512080.6250,      0.0000],
         [495932.3750,      0.0000],
         [490800.5312,      0.0000]],

        [[410358.1562,      0.0000],
         [364762.9688,      0.0000],
         [265788.3438,      0.0000],
         ...,
         [155212.5781,      0.0000],
         [150568.0938,      0.0000],
         [135831.7031,      0.0000]],

        ...,

        [[     0.0000, 196404.2031],
         [ 64666.8164,      0.0000],
         [ 63381.9492,      0.0000],
         ...,
         [ 32051.3438,      0.0000],
         [ 31523.4844,      0.0000],
         [ 30385.9141,      0.0000]],

        [[296712.0000,      0.0000],
         [     0.0000, 201680.7656],
         [163995.7812,      0.0000],
         ...,
         [ 79598.3828,      0.0000],
         [ 77803.5938,      0.0000],
         [ 73103.5781,      0.0000]],

        [[     0.0000, 156175.7656],
         [     0.0000,  99672.8203],
         [     0.0000,  83564.5781],
         ...,
         [ 45224.6523,      0.0000],
         [     0.0000,  43645.4805],
         [     0.0000,  40755.2617]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1537796.2500,       0.0000],
        [5497966.0000,       0.0000],
        [2293785.5000,       0.0000],
        [3184988.7500,       0.0000],
        [ 450458.3750,  637925.8750],
        [ 770133.3125,  143167.2188],
        [1685123.0000,  158126.4688],
        [1796468.2500,       0.0000],
        [3179776.7500,       0.0000],
        [3064079.2500,       0.0000],
        [3586841.5000,       0.0000],
        [1465752.7500,       0.0000],
        [3010002.5000,       0.0000],
        [7458556.0000,       0.0000],
        [5281551.0000,       0.0000],
        [4577275.0000,       0.0000],
        [5747397.0000,       0.0000],
        [6940418.5000,       0.0000],
        [3712651.5000,       0.0000],
        [3164456.7500,  327915.7188],
        [5776256.0000,       0.0000],
        [4097175.0000,       0.0000],
        [6233600.5000,       0.0000],
        [5589051.0000,       0.0000],
        [3074255.0000,       0.0000],
        [5035662.5000,       0.0000],
        [4239709.0000,       0.0000],
        [5934443.5000,       0.0000],
        [5394372.0000,       0.0000],
        [5140796.5000,       0.0000],
        [6609594.5000,       0.0000],
        [4313073.0000,       0.0000],
        [2061939.1250,       0.0000],
        [2373105.5000,  591698.8750],
        [1773765.5000, 2147989.0000],
        [1016307.8125, 1883527.7500],
        [1085709.5000, 1103427.5000],
        [1308044.5000, 1187980.6250],
        [ 703048.0625, 1395448.6250],
        [5609302.0000,       0.0000],
        [1969651.7500,  127204.3984],
        [2097425.7500,       0.0000],
        [2001289.7500,       0.0000],
        [1898875.5000,       0.0000],
        [ 232339.5000,  831620.1250],
        [ 326645.0938,  418859.5000],
        [2206378.2500,  373687.0312],
        [ 341412.0625,  201621.5625],
        [ 447100.9688,  561823.1250],
        [ 440250.3750, 1167708.0000],
        [1291456.3750,  237418.9062],
        [ 665686.8750,  791678.5625],
        [1664340.7500, 1159455.5000],
        [2008743.2500,  189252.7812],
        [3636120.0000,       0.0000],
        [2490286.0000,       0.0000],
        [ 965154.6250, 1143495.7500],
        [1655724.1250,  698292.1250],
        [ 941594.6250,  804140.1250],
        [ 528498.5625,  606202.5000],
        [1209578.6250,  711199.9375],
        [ 408252.9062,  196404.2031],
        [ 896955.7500,  416334.0000],
        [ 104728.2812,  613687.1250]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 486/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:58<28:15, 58.47s/it]  7%|▋         | 2/30 [01:00<11:44, 25.17s/it] 10%|█         | 3/30 [01:01<06:18, 14.02s/it] 13%|█▎        | 4/30 [01:01<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:02<02:27,  5.88s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:04<00:50,  2.30s/it] 30%|███       | 9/30 [01:05<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:07<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:08<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.09it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:10<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:11<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:13<00:07,  1.30it/s] 70%|███████   | 21/30 [01:14<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:16<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:17<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:19<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Epoch loss is 2.2989943623542786
Epoch 487/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:46, 57.47s/it]  7%|▋         | 2/30 [01:01<12:01, 25.78s/it] 10%|█         | 3/30 [01:01<06:27, 14.35s/it] 13%|█▎        | 4/30 [01:02<03:53,  8.98s/it] 17%|█▋        | 5/30 [01:03<02:30,  6.01s/it] 20%|██        | 6/30 [01:04<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.09s/it] 27%|██▋       | 8/30 [01:05<00:51,  2.34s/it] 30%|███       | 9/30 [01:06<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.3035187164942426
Epoch 488/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:01<29:42, 61.45s/it]  7%|▋         | 2/30 [01:02<12:00, 25.74s/it] 10%|█         | 3/30 [01:02<06:26, 14.33s/it] 13%|█▎        | 4/30 [01:03<03:53,  8.97s/it] 17%|█▋        | 5/30 [01:04<02:30,  6.00s/it] 20%|██        | 6/30 [01:05<01:41,  4.22s/it] 23%|██▎       | 7/30 [01:05<01:10,  3.08s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.34s/it] 30%|███       | 9/30 [01:07<00:38,  1.84s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.50s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.27s/it] 40%|████      | 12/30 [01:09<00:20,  1.11s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.78s/it]
Epoch loss is 2.297413404782613
Epoch 489/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:00, 60.01s/it]  7%|▋         | 2/30 [01:00<11:44, 25.16s/it] 10%|█         | 3/30 [01:02<06:30, 14.48s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.06s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.06s/it] 20%|██        | 6/30 [01:04<01:42,  4.26s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.36s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.77s/it]
Epoch loss is 2.2777726570765178
Epoch 490/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:17, 56.45s/it]  7%|▋         | 2/30 [00:57<11:03, 23.68s/it] 10%|█         | 3/30 [00:58<06:08, 13.65s/it] 13%|█▎        | 4/30 [01:00<03:49,  8.81s/it] 17%|█▋        | 5/30 [01:01<02:27,  5.90s/it] 20%|██        | 6/30 [01:01<01:39,  4.15s/it] 23%|██▎       | 7/30 [01:02<01:09,  3.04s/it] 27%|██▋       | 8/30 [01:03<00:50,  2.31s/it] 30%|███       | 9/30 [01:04<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.26s/it] 40%|████      | 12/30 [01:06<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:19<00:00,  2.67s/it]
Epoch loss is 2.2790141979853313
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0008,  0.0132,  ..., -0.0058,  0.0176,  0.0040],
        [-0.0023,  0.0095,  0.0286,  ...,  0.0172,  0.0042, -0.0147],
        [-0.0293, -0.0379,  0.0247,  ...,  0.0834, -0.0043, -0.0143],
        ...,
        [ 0.0059, -0.0033,  0.0099,  ..., -0.0241, -0.0051, -0.0027],
        [-0.0333,  0.0099, -0.0034,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0326, -0.0139,  0.0193,  ...,  0.0414,  0.0360, -0.0284]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8740, 0.8508, 0.8372, 0.8361, 0.8302, 0.8277, 0.8252, 0.8210, 0.8195,
         0.8172],
        [0.9414, 0.9286, 0.9277, 0.9268, 0.9260, 0.9213, 0.9213, 0.9202, 0.9180,
         0.9173],
        [0.9047, 0.8965, 0.8743, 0.8702, 0.8554, 0.8478, 0.8454, 0.8367, 0.8345,
         0.8273],
        [0.9561, 0.8855, 0.8817, 0.8789, 0.8785, 0.8741, 0.8647, 0.8628, 0.8607,
         0.8601],
        [0.8424, 0.8313, 0.8299, 0.8137, 0.8049, 0.8045, 0.8010, 0.7957, 0.7877,
         0.7811],
        [0.8522, 0.8173, 0.8083, 0.7959, 0.7931, 0.7872, 0.7822, 0.7775, 0.7675,
         0.7653],
        [0.8639, 0.8611, 0.8530, 0.8513, 0.8476, 0.8462, 0.8444, 0.8403, 0.8380,
         0.8356],
        [0.8913, 0.8716, 0.8664, 0.8553, 0.8481, 0.8294, 0.8253, 0.8164, 0.8046,
         0.8009],
        [0.9073, 0.9014, 0.8865, 0.8836, 0.8833, 0.8823, 0.8822, 0.8790, 0.8782,
         0.8780],
        [0.9486, 0.8826, 0.8815, 0.8795, 0.8725, 0.8707, 0.8666, 0.8664, 0.8604,
         0.8583],
        [0.9444, 0.9035, 0.8923, 0.8885, 0.8882, 0.8859, 0.8831, 0.8814, 0.8788,
         0.8765],
        [0.8861, 0.8709, 0.8544, 0.8460, 0.8231, 0.7973, 0.7960, 0.7931, 0.7808,
         0.7760],
        [0.9117, 0.9053, 0.9010, 0.8893, 0.8786, 0.8752, 0.8673, 0.8626, 0.8611,
         0.8499],
        [0.9561, 0.9544, 0.9506, 0.9504, 0.9477, 0.9457, 0.9431, 0.9411, 0.9391,
         0.9340],
        [0.9416, 0.9277, 0.9266, 0.9253, 0.9222, 0.9179, 0.9173, 0.9142, 0.9136,
         0.9122],
        [0.9258, 0.9213, 0.9202, 0.9131, 0.9112, 0.9098, 0.9078, 0.9044, 0.9037,
         0.9021],
        [0.9513, 0.9455, 0.9341, 0.9311, 0.9249, 0.9242, 0.9172, 0.9167, 0.9134,
         0.9128],
        [0.9611, 0.9552, 0.9511, 0.9395, 0.9389, 0.9344, 0.9334, 0.9318, 0.9317,
         0.9299],
        [0.9250, 0.9126, 0.8991, 0.8974, 0.8925, 0.8915, 0.8899, 0.8885, 0.8855,
         0.8838],
        [0.9022, 0.8976, 0.8961, 0.8960, 0.8941, 0.8934, 0.8905, 0.8890, 0.8877,
         0.8864],
        [0.9396, 0.9316, 0.9316, 0.9297, 0.9291, 0.9285, 0.9254, 0.9238, 0.9233,
         0.9224],
        [0.9252, 0.9106, 0.9097, 0.9072, 0.9051, 0.9038, 0.9032, 0.9012, 0.8881,
         0.8837],
        [0.9672, 0.9482, 0.9416, 0.9380, 0.9329, 0.9264, 0.9246, 0.9172, 0.9122,
         0.9109],
        [0.9404, 0.9332, 0.9293, 0.9260, 0.9237, 0.9228, 0.9221, 0.9217, 0.9214,
         0.9201],
        [0.9528, 0.8983, 0.8875, 0.8787, 0.8642, 0.8594, 0.8593, 0.8570, 0.8569,
         0.8565],
        [0.9350, 0.9310, 0.9288, 0.9224, 0.9211, 0.9168, 0.9127, 0.9091, 0.9028,
         0.9023],
        [0.9226, 0.9207, 0.9183, 0.9139, 0.9063, 0.8986, 0.8964, 0.8958, 0.8948,
         0.8937],
        [0.9593, 0.9478, 0.9418, 0.9250, 0.9238, 0.9232, 0.9185, 0.9175, 0.9172,
         0.9158],
        [0.9421, 0.9352, 0.9257, 0.9242, 0.9235, 0.9215, 0.9197, 0.9168, 0.9127,
         0.9114],
        [0.9303, 0.9258, 0.9247, 0.9242, 0.9209, 0.9175, 0.9167, 0.9166, 0.9131,
         0.9129],
        [0.9442, 0.9442, 0.9437, 0.9431, 0.9414, 0.9408, 0.9331, 0.9306, 0.9295,
         0.9274],
        [0.9364, 0.9202, 0.9189, 0.9086, 0.9012, 0.9008, 0.8964, 0.8960, 0.8953,
         0.8949],
        [0.8733, 0.8680, 0.8648, 0.8573, 0.8562, 0.8495, 0.8482, 0.8481, 0.8470,
         0.8469],
        [0.8993, 0.8891, 0.8842, 0.8822, 0.8794, 0.8781, 0.8778, 0.8769, 0.8751,
         0.8738],
        [0.9176, 0.9136, 0.9127, 0.9070, 0.9002, 0.8994, 0.8957, 0.8889, 0.8860,
         0.8856],
        [0.9048, 0.8942, 0.8917, 0.8841, 0.8802, 0.8751, 0.8751, 0.8686, 0.8580,
         0.8570],
        [0.9472, 0.8672, 0.8634, 0.8501, 0.8459, 0.8291, 0.8197, 0.8175, 0.8152,
         0.8129],
        [0.8987, 0.8797, 0.8777, 0.8750, 0.8718, 0.8688, 0.8538, 0.8534, 0.8534,
         0.8512],
        [0.8924, 0.8886, 0.8676, 0.8603, 0.8480, 0.8439, 0.8411, 0.8356, 0.8344,
         0.8326],
        [0.9336, 0.9302, 0.9285, 0.9284, 0.9271, 0.9249, 0.9241, 0.9236, 0.9235,
         0.9214],
        [0.8794, 0.8758, 0.8757, 0.8645, 0.8598, 0.8546, 0.8521, 0.8386, 0.8298,
         0.8227],
        [0.8992, 0.8604, 0.8596, 0.8568, 0.8505, 0.8494, 0.8471, 0.8450, 0.8448,
         0.8445],
        [0.8837, 0.8721, 0.8686, 0.8597, 0.8504, 0.8458, 0.8394, 0.8375, 0.8332,
         0.8332],
        [0.8740, 0.8709, 0.8557, 0.8496, 0.8469, 0.8431, 0.8415, 0.8407, 0.8398,
         0.8335],
        [0.8498, 0.8360, 0.8191, 0.8168, 0.8083, 0.8081, 0.7964, 0.7799, 0.7732,
         0.7699],
        [0.8286, 0.8043, 0.7887, 0.7851, 0.7792, 0.7792, 0.7755, 0.7634, 0.7588,
         0.7575],
        [0.8982, 0.8806, 0.8771, 0.8738, 0.8726, 0.8716, 0.8606, 0.8601, 0.8597,
         0.8576],
        [0.8040, 0.7834, 0.7782, 0.7714, 0.7627, 0.7586, 0.7396, 0.7382, 0.7297,
         0.7204],
        [0.8303, 0.8276, 0.8096, 0.8087, 0.8022, 0.8022, 0.8016, 0.7960, 0.7883,
         0.7838],
        [0.8898, 0.8610, 0.8428, 0.8309, 0.8303, 0.8283, 0.8200, 0.8191, 0.8146,
         0.8119],
        [0.8741, 0.8526, 0.8466, 0.8457, 0.8346, 0.8259, 0.8203, 0.8093, 0.8089,
         0.8039],
        [0.8867, 0.8838, 0.8430, 0.8136, 0.8100, 0.8040, 0.8014, 0.7968, 0.7966,
         0.7916],
        [0.9230, 0.9062, 0.8999, 0.8918, 0.8629, 0.8580, 0.8552, 0.8504, 0.8412,
         0.8365],
        [0.9109, 0.8735, 0.8691, 0.8589, 0.8522, 0.8506, 0.8423, 0.8405, 0.8375,
         0.8367],
        [0.9225, 0.8997, 0.8967, 0.8962, 0.8945, 0.8911, 0.8911, 0.8884, 0.8880,
         0.8867],
        [0.8982, 0.8785, 0.8776, 0.8664, 0.8643, 0.8636, 0.8607, 0.8599, 0.8597,
         0.8577],
        [0.9033, 0.8755, 0.8663, 0.8642, 0.8583, 0.8547, 0.8416, 0.8274, 0.8232,
         0.8223],
        [0.9393, 0.8779, 0.8692, 0.8609, 0.8542, 0.8502, 0.8343, 0.8302, 0.8289,
         0.8174],
        [0.9047, 0.8586, 0.8538, 0.8429, 0.8302, 0.8281, 0.8223, 0.8221, 0.8148,
         0.8128],
        [0.8976, 0.8156, 0.8127, 0.8003, 0.7901, 0.7891, 0.7889, 0.7856, 0.7824,
         0.7739],
        [0.8833, 0.8650, 0.8555, 0.8531, 0.8457, 0.8426, 0.8396, 0.8392, 0.8385,
         0.8384],
        [0.8532, 0.7754, 0.7740, 0.7732, 0.7626, 0.7358, 0.7283, 0.7263, 0.7251,
         0.7225],
        [0.8820, 0.8550, 0.8405, 0.8212, 0.8115, 0.8102, 0.7914, 0.7899, 0.7883,
         0.7840],
        [0.8371, 0.8057, 0.7933, 0.7780, 0.7764, 0.7696, 0.7668, 0.7503, 0.7479,
         0.7431]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 264375.9688,  189986.7812,  156426.3281,  154044.3438,  141560.0625,
          136562.3125,  131737.6250,  124053.8203,  121356.9531,  117535.9531],
        [ 693020.8125,  577301.5000,  569788.9375,  562742.6875,  556022.6250,
          520106.5000,  519930.4688,  512038.6562,  495896.9375,  490767.3125],
        [ 410349.5312,  364733.0312,  265749.8438,  250630.2656,  202862.4531,
          181930.4219,  175770.8906,  155199.0938,  150558.6094,  135826.3906],
        [ 854354.1250,  311786.2500,  295412.9375,  283581.2500,  282005.7188,
          265072.5000,  231534.1719,  225535.7188,  218668.8750,  216807.0781],
        [ 168392.3438,  143633.4688,  140890.9531,  111794.0391,   98529.2656,
           98038.4922,   93277.9844,   86420.1250,   77142.4844,   70161.1016],
        [ 193823.7656,  117718.0156,  103531.7109,   86729.8125,   83277.7734,
           76543.8281,   71243.4141,   66610.9688,   57797.2734,   55958.8633],
        [ 228861.1719,  219992.2812,  195990.3281,  191205.3438,  181354.1094,
          177790.9375,  173436.7344,  163368.5781,  158100.0781,  152860.7344],
        [ 338576.6562,  255530.3750,  237471.2656,  202580.5938,  182694.7344,
          139955.7969,  131991.5312,  116201.6250,   98216.1094,   93132.9219],
        [ 425405.5625,  391388.4062,  316454.0000,  303563.0000,  302319.8438,
          297773.0625,  297458.8438,  284287.1875,  280786.5625,  280195.9062],
        [ 768340.0000,  299091.3438,  294437.7812,  286230.6562,  258829.6406,
          252423.8125,  237880.6250,  237233.8125,  217941.0156,  211449.5312],
        [ 723503.3125,  403144.4375,  343422.0938,  325602.8750,  324222.1562,
          313519.1562,  301459.0000,  293882.3438,  283504.4688,  274343.3438],
        [ 314241.1562,  252937.8125,  199819.2969,  177224.8438,  127855.9297,
           88397.8281,   86754.9688,   83262.8438,   69891.9688,   65230.5273],
        [ 453467.2188,  413433.0938,  389066.2188,  329047.8750,  282598.2812,
          269245.5000,  240329.6094,  224930.4219,  219936.7031,  187579.3750],
        [ 855214.1250,  834194.6875,  790439.3125,  788134.5000,  758616.9375,
          736991.8750,  709605.5625,  690371.6875,  670796.2500,  623715.5000],
        [ 695084.6250,  570062.3125,  561023.6250,  550160.3750,  526674.6875,
          495077.0625,  491131.5625,  470095.8438,  465654.8125,  456415.3438],
        [ 554761.0000,  519645.9375,  512146.0938,  462297.2812,  450167.4688,
          441115.3750,  428805.8438,  408581.4062,  404476.5000,  395159.5625],
        [ 798140.0000,  734319.5625,  624327.2500,  598227.4375,  547356.8750,
          542185.4375,  490257.4375,  486921.6875,  464500.7188,  460734.3750],
        [ 918488.7500,  843602.1875,  795773.1875,  674632.0625,  668471.6875,
          626483.3750,  618321.8750,  603771.0000,  602802.1250,  587850.7500],
        [ 547903.1875,  459083.9062,  378546.5312,  369595.2500,  344380.4062,
          339488.4375,  331857.6875,  325487.4062,  311875.7500,  304213.3438],
        [ 395552.0625,  370389.8438,  362911.8438,  362167.7812,  352283.0000,
          348889.0938,  334823.3438,  327905.4062,  321540.1562,  315672.4375],
        [ 674886.8750,  602322.8750,  602176.4375,  585999.6875,  581137.8125,
          576494.3750,  550992.1250,  538956.2500,  534945.6250,  527879.5000],
        [ 549380.7500,  446092.0938,  440305.8750,  425102.2188,  412246.4375,
          404694.8750,  401536.3438,  390397.1875,  323414.5625,  303786.5625],
        [1001428.8750,  763665.4375,  694414.7500,  659908.5625,  613980.0625,
          559503.5000,  544902.1875,  490366.8438,  456822.4688,  448353.0000],
        [ 683238.2500,  616145.1250,  583177.0625,  556284.1250,  538054.8750,
          531214.0625,  525838.0625,  523270.2500,  520862.5000,  510749.6250],
        [ 815838.6250,  374422.2500,  320850.9375,  282729.0312,  230101.6250,
          214801.1094,  214316.3594,  207545.1094,  207331.6562,  206039.2031],
        [ 632317.6875,  597272.0000,  578324.2500,  528318.1875,  518590.9375,
          487681.5000,  460059.9375,  437001.0625,  399162.2812,  396598.3438],
        [ 529908.6875,  515674.2812,  497880.5312,  467508.5625,  419573.0312,
          376144.4375,  364486.5000,  361372.8750,  356151.8438,  350650.9688],
        [ 894783.5625,  758687.1875,  696518.5625,  548059.9375,  538822.0625,
          534223.2500,  499417.5000,  492456.5312,  490253.6875,  480970.8438],
        [ 700102.8125,  634422.6875,  553446.2500,  541555.5625,  536371.4375,
          521234.1875,  508426.6562,  487365.7812,  459549.9688,  451541.6562],
        [ 591629.5625,  554797.0000,  546046.6875,  542030.3750,  516991.0312,
          492214.7500,  486805.5938,  486082.8438,  462733.0625,  461239.9375],
        [ 721054.5625,  720820.8125,  715980.4375,  709760.5625,  692960.6875,
          686900.1875,  615622.9375,  594142.3125,  584829.5000,  567219.6250],
        [ 645355.0000,  512101.1562,  502437.6562,  433418.8125,  390346.1875,
          387736.0625,  364091.1562,  362385.7812,  358561.0000,  356395.5000],
        [ 261921.6094,  242971.3125,  231902.7656,  208290.8594,  205013.7031,
          186297.1875,  182932.3750,  182838.8906,  179968.7031,  179610.6875],
        [ 379753.8438,  328129.7188,  305814.6562,  297272.2500,  285820.6562,
          280503.3750,  279139.5000,  275620.7500,  268571.5625,  263843.7500],
        [ 493380.2500,  465581.1250,  459871.7812,  423898.6250,  384582.9688,
          380397.2500,  360832.8750,  327098.0312,  313803.6562,  312042.0312],
        [ 410482.2188,  353249.2188,  340610.4688,  305560.7188,  289145.7188,
          268808.3125,  268784.2188,  244781.7031,  210545.8438,  207639.3438],
        [ 753025.0000,  239916.5000,  227259.7656,  187997.8906,  177044.4375,
          139263.8750,  121794.6328,  118086.8203,  114178.2891,  110458.4062],
        [ 376244.9062,  286848.5000,  278785.3750,  268278.9688,  256467.3750,
          245629.6094,  198196.9844,  197200.9375,  196963.5469,  191081.4062],
        [ 344253.6562,  325692.3125,  241495.8281,  217628.4375,  182393.9062,
          171976.1250,  165433.0938,  152742.4219,  150205.0938,  146450.1875],
        [ 619742.2500,  590141.9375,  576496.0625,  575741.6875,  564450.2500,
          547143.4375,  541234.3750,  537027.0625,  536428.6875,  520576.9688],
        [ 285911.7500,  271232.2500,  271028.5000,  230970.6875,  215901.0781,
          200557.0000,  193483.0312,  159617.6406,  140781.0938,  127191.5469],
        [ 379407.4375,  217731.4219,  215258.6094,  206787.2344,  189154.0781,
          186183.5156,  180256.7500,  174712.5156,  174184.1250,  173554.2031],
        [ 303795.5625,  257384.4844,  244941.4219,  215535.9219,  188911.7969,
          176696.7969,  161433.7031,  157112.8750,  147644.7188,  147608.3906],
        [ 264365.3750,  252952.5312,  203682.8594,  186652.8750,  179627.1406,
          170093.0156,  166221.7656,  164428.9531,  162235.1875,  148385.6406],
        [ 187149.8125,  153812.5625,  120670.1562,  116763.8359,  103491.2344,
          103147.5547,   87297.2422,   68982.3750,   62678.4219,   59822.7734],
        [ 138343.3594,   97730.8125,   78166.9688,   74248.6328,   68321.2500,
           68275.7891,   64788.7344,   54479.0234,   50988.4570,   50098.0469],
        [ 373652.1250,  290525.2812,  276319.2188,  263637.2500,  259295.5938,
          255691.0000,  218580.0469,  216839.5469,  215787.2500,  209306.4062],
        [  97291.4141,   72530.5312,   67284.7422,   61066.4102,   53974.7031,
           50882.7578,   38785.3125,   38017.2578,   33667.4180,   29461.1211],
        [ 141662.8281,  136377.5000,  105384.9766,  104100.8984,   94886.3359,
           94850.8672,   93983.2188,   86851.8984,   77786.1562,   72934.5781],
        [ 331348.2188,  219592.9844,  169310.2031,  142952.1250,  141725.5469,
          137653.2188,  122346.5625,  120817.6719,  113162.5469,  108866.5938],
        [ 265076.7812,  194989.5469,  178769.7500,  176613.4062,  150766.6562,
          133034.7344,  122890.7891,  104985.5312,  104369.0938,   97222.9609],
        [ 317236.3125,  304442.5938,  169939.9688,  111684.7031,  105970.5156,
           97344.2266,   93758.2500,   87816.8750,   87509.3047,   81501.3906],
        [ 532748.3125,  418931.6875,  383207.4688,  340954.6562,  225592.9375,
          210610.5000,  202213.0781,  188860.4688,  165507.1094,  154895.2344],
        [ 448412.8750,  262648.5000,  246611.4531,  213254.7500,  193641.7656,
          189231.6719,  168082.5312,  163835.7188,  156939.6250,  155172.4688],
        [ 528594.8750,  382048.9062,  365971.0000,  363424.7500,  354762.3750,
          337749.7812,  337749.7812,  325120.6875,  323238.2188,  317158.8750],
        [ 373817.5000,  282035.5625,  278499.7188,  237329.2969,  230157.5781,
          228028.7344,  218737.9062,  216286.0469,  215566.3438,  209627.8281],
        [ 402237.3125,  270282.7812,  236836.6250,  229976.1250,  211401.3438,
          200789.9062,  166561.3438,  135919.4219,  128068.8750,  126344.4375],
        [ 671944.2500,  279607.5938,  247098.3125,  219291.4062,  199329.7656,
          188280.8438,  149956.4688,  141524.4219,  138854.6094,  117926.6719],
        [ 410225.9062,  212326.9375,  198188.1094,  169629.7344,  141467.3438,
          137368.1094,  126305.5234,  126064.4766,  113622.8906,  110357.5312],
        [ 370682.0938,  114801.4141,  110181.2891,   92345.4219,   79797.1328,
           78695.3438,   78449.1172,   74776.1875,   71528.0547,   63319.1172],
        [ 301991.9062,  232482.2812,  203121.2969,  196247.6719,  176680.7812,
          168900.5781,  161881.8906,  160959.6094,  159287.2031,  159051.3125],
        [ 196395.9688,   64664.1055,   63373.9102,   62660.5586,   53871.2891,
           36711.6680,   32981.2695,   32050.9141,   31521.5605,   30382.2617],
        [ 296689.6562,  201671.1406,  163973.8906,  124432.1484,  108353.1562,
          106272.6172,   81285.4453,   79581.3828,   77787.4922,   73090.4688],
        [ 156167.5781,   99661.9844,   83553.3438,   67122.7812,   65567.7578,
           59494.7227,   57163.8086,   45217.1445,   43639.0742,   40749.6250]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[264375.9688,      0.0000],
         [189986.7812,      0.0000],
         [156426.3281,      0.0000],
         ...,
         [124053.8203,      0.0000],
         [121356.9531,      0.0000],
         [117535.9531,      0.0000]],

        [[693020.8125,      0.0000],
         [577301.5000,      0.0000],
         [569788.9375,      0.0000],
         ...,
         [512038.6562,      0.0000],
         [495896.9375,      0.0000],
         [490767.3125,      0.0000]],

        [[410349.5312,      0.0000],
         [364733.0312,      0.0000],
         [265749.8438,      0.0000],
         ...,
         [155199.0938,      0.0000],
         [150558.6094,      0.0000],
         [135826.3906,      0.0000]],

        ...,

        [[     0.0000, 196395.9688],
         [ 64664.1055,      0.0000],
         [ 63373.9102,      0.0000],
         ...,
         [ 32050.9141,      0.0000],
         [ 31521.5605,      0.0000],
         [ 30382.2617,      0.0000]],

        [[296689.6562,      0.0000],
         [     0.0000, 201671.1406],
         [163973.8906,      0.0000],
         ...,
         [ 79581.3828,      0.0000],
         [ 77787.4922,      0.0000],
         [ 73090.4688,      0.0000]],

        [[     0.0000, 156167.5781],
         [     0.0000,  99661.9844],
         [     0.0000,  83553.3438],
         ...,
         [ 45217.1445,      0.0000],
         [     0.0000,  43639.0742],
         [     0.0000,  40749.6250]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1537640.1250,       0.0000],
        [5497616.5000,       0.0000],
        [2293610.5000,       0.0000],
        [3184758.5000,       0.0000],
        [ 450414.0938,  637866.1875],
        [ 770080.6250,  143154.7969],
        [1684860.2500,  158100.0781],
        [1796351.7500,       0.0000],
        [3179632.2500,       0.0000],
        [3063858.0000,       0.0000],
        [3586603.2500,       0.0000],
        [1465617.2500,       0.0000],
        [3009634.2500,       0.0000],
        [7458080.0000,       0.0000],
        [5281380.0000,       0.0000],
        [4577156.5000,       0.0000],
        [5746971.0000,       0.0000],
        [6940197.0000,       0.0000],
        [3712432.0000,       0.0000],
        [3164229.5000,  327905.4062],
        [5775791.5000,       0.0000],
        [4096957.0000,       0.0000],
        [6233346.0000,       0.0000],
        [5588834.0000,       0.0000],
        [3073976.0000,       0.0000],
        [5035326.0000,       0.0000],
        [4239352.0000,       0.0000],
        [5934193.0000,       0.0000],
        [5394017.0000,       0.0000],
        [5140571.0000,       0.0000],
        [6609292.0000,       0.0000],
        [4312828.5000,       0.0000],
        [2061748.1250,       0.0000],
        [2372834.7500,  591635.3125],
        [1773648.2500, 2147840.2500],
        [1016216.0625, 1883391.7500],
        [1085625.7500, 1103399.8750],
        [1307858.0000, 1187839.7500],
        [ 702969.8750, 1395301.2500],
        [5608983.0000,       0.0000],
        [1969483.0000,  127191.5469],
        [2097230.0000,       0.0000],
        [2001065.7500,       0.0000],
        [1898645.2500,       0.0000],
        [ 232296.3750,  831519.5625],
        [ 326622.8750,  418818.2188],
        [2205981.5000,  373652.1250],
        [ 341358.3438,  201603.3125],
        [ 447058.5000,  561760.7500],
        [ 440214.8125, 1167560.7500],
        [1291315.5000,  237403.8281],
        [ 665585.2500,  791618.8750],
        [1664164.2500, 1159357.2500],
        [2008599.7500,  189231.6719],
        [3635819.2500,       0.0000],
        [2490086.5000,       0.0000],
        [ 965037.1250, 1143381.0000],
        [1655585.5000,  698228.7500],
        [ 941493.3750,  804063.2500],
        [ 528431.3750,  606143.7500],
        [1209449.5000,  711155.0000],
        [ 408217.5312,  196395.9688],
        [ 896840.5000,  416296.9062],
        [ 104711.8672,  613625.8750]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 491/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:56, 57.81s/it]  7%|▋         | 2/30 [01:01<12:07, 25.99s/it] 10%|█         | 3/30 [01:02<06:30, 14.47s/it] 13%|█▎        | 4/30 [01:03<03:55,  9.05s/it] 17%|█▋        | 5/30 [01:03<02:31,  6.06s/it] 20%|██        | 6/30 [01:04<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:06<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Epoch loss is 2.279329299926758
Epoch 492/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:55<26:54, 55.68s/it]  7%|▋         | 2/30 [00:57<11:04, 23.72s/it] 10%|█         | 3/30 [00:58<06:01, 13.38s/it] 13%|█▎        | 4/30 [00:59<03:46,  8.72s/it] 17%|█▋        | 5/30 [01:00<02:26,  5.84s/it] 20%|██        | 6/30 [01:01<01:38,  4.11s/it] 23%|██▎       | 7/30 [01:01<01:09,  3.01s/it] 27%|██▋       | 8/30 [01:02<00:50,  2.29s/it] 30%|███       | 9/30 [01:03<00:37,  1.81s/it] 33%|███▎      | 10/30 [01:04<00:29,  1.48s/it] 37%|███▋      | 11/30 [01:04<00:23,  1.26s/it] 40%|████      | 12/30 [01:05<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:06<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:07<00:14,  1.09it/s] 50%|█████     | 15/30 [01:07<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:08<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:09<00:10,  1.24it/s] 60%|██████    | 18/30 [01:10<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:10<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:11<00:07,  1.30it/s] 70%|███████   | 21/30 [01:12<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:13<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:13<00:05,  1.32it/s] 80%|████████  | 24/30 [01:14<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:15<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:16<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:16<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:17<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:18<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  1.33it/s]100%|██████████| 30/30 [01:19<00:00,  2.64s/it]
Epoch loss is 2.3160898129145306
Epoch 493/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:11, 60.40s/it]  7%|▋         | 2/30 [01:01<11:48, 25.31s/it] 10%|█         | 3/30 [01:01<06:20, 14.10s/it] 13%|█▎        | 4/30 [01:02<03:49,  8.83s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.91s/it] 20%|██        | 6/30 [01:04<01:39,  4.16s/it] 23%|██▎       | 7/30 [01:04<01:10,  3.05s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.31s/it] 30%|███       | 9/30 [01:06<00:38,  1.83s/it] 33%|███▎      | 10/30 [01:07<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:24,  1.27s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.74s/it]
Epoch loss is 2.272679885228475
Epoch 494/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:57<27:52, 57.68s/it]  7%|▋         | 2/30 [00:59<11:28, 24.60s/it] 10%|█         | 3/30 [00:59<06:11, 13.75s/it] 13%|█▎        | 4/30 [01:00<03:44,  8.62s/it] 17%|█▋        | 5/30 [01:01<02:24,  5.78s/it] 20%|██        | 6/30 [01:02<01:37,  4.07s/it] 23%|██▎       | 7/30 [01:02<01:08,  2.98s/it] 27%|██▋       | 8/30 [01:03<00:49,  2.27s/it] 30%|███       | 9/30 [01:04<00:37,  1.80s/it] 33%|███▎      | 10/30 [01:05<00:29,  1.47s/it] 37%|███▋      | 11/30 [01:05<00:23,  1.25s/it] 40%|████      | 12/30 [01:06<00:19,  1.10s/it] 43%|████▎     | 13/30 [01:07<00:16,  1.01it/s] 47%|████▋     | 14/30 [01:08<00:14,  1.09it/s] 50%|█████     | 15/30 [01:08<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:09<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:10<00:10,  1.24it/s] 60%|██████    | 18/30 [01:11<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:11<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:12<00:07,  1.30it/s] 70%|███████   | 21/30 [01:13<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:14<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:14<00:05,  1.33it/s] 80%|████████  | 24/30 [01:15<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:16<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:17<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:17<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:18<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:19<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  1.34it/s]100%|██████████| 30/30 [01:20<00:00,  2.68s/it]
Epoch loss is 2.2494162400563558
Epoch 495/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<29:59, 62.05s/it]  7%|▋         | 2/30 [01:02<12:07, 25.99s/it] 10%|█         | 3/30 [01:03<06:30, 14.46s/it] 13%|█▎        | 4/30 [01:04<03:55,  9.05s/it] 17%|█▋        | 5/30 [01:05<02:31,  6.06s/it] 20%|██        | 6/30 [01:05<01:42,  4.25s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.11s/it] 27%|██▋       | 8/30 [01:07<00:51,  2.36s/it] 30%|███       | 9/30 [01:08<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.00s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.80s/it]
Epoch loss is 2.323559308052063
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0008,  0.0132,  ..., -0.0058,  0.0176,  0.0040],
        [-0.0023,  0.0095,  0.0286,  ...,  0.0172,  0.0042, -0.0147],
        [-0.0293, -0.0379,  0.0247,  ...,  0.0834, -0.0043, -0.0143],
        ...,
        [ 0.0059, -0.0033,  0.0099,  ..., -0.0241, -0.0051, -0.0027],
        [-0.0333,  0.0099, -0.0034,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0326, -0.0139,  0.0193,  ...,  0.0414,  0.0360, -0.0284]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8740, 0.8508, 0.8372, 0.8361, 0.8302, 0.8277, 0.8252, 0.8210, 0.8195,
         0.8172],
        [0.9414, 0.9286, 0.9277, 0.9268, 0.9260, 0.9213, 0.9213, 0.9202, 0.9180,
         0.9173],
        [0.9047, 0.8965, 0.8743, 0.8702, 0.8554, 0.8478, 0.8454, 0.8367, 0.8345,
         0.8273],
        [0.9561, 0.8855, 0.8817, 0.8789, 0.8785, 0.8741, 0.8647, 0.8628, 0.8607,
         0.8601],
        [0.8424, 0.8313, 0.8299, 0.8137, 0.8049, 0.8045, 0.8010, 0.7957, 0.7877,
         0.7811],
        [0.8522, 0.8173, 0.8083, 0.7959, 0.7931, 0.7872, 0.7822, 0.7775, 0.7675,
         0.7653],
        [0.8639, 0.8611, 0.8530, 0.8513, 0.8476, 0.8462, 0.8444, 0.8403, 0.8380,
         0.8356],
        [0.8913, 0.8716, 0.8664, 0.8553, 0.8481, 0.8294, 0.8253, 0.8164, 0.8046,
         0.8009],
        [0.9073, 0.9014, 0.8865, 0.8836, 0.8833, 0.8823, 0.8822, 0.8790, 0.8782,
         0.8780],
        [0.9486, 0.8826, 0.8815, 0.8795, 0.8725, 0.8707, 0.8666, 0.8664, 0.8604,
         0.8583],
        [0.9444, 0.9035, 0.8923, 0.8885, 0.8882, 0.8859, 0.8831, 0.8814, 0.8788,
         0.8765],
        [0.8861, 0.8709, 0.8544, 0.8460, 0.8231, 0.7973, 0.7960, 0.7931, 0.7808,
         0.7760],
        [0.9117, 0.9053, 0.9010, 0.8893, 0.8786, 0.8752, 0.8673, 0.8626, 0.8611,
         0.8499],
        [0.9561, 0.9544, 0.9506, 0.9504, 0.9477, 0.9457, 0.9431, 0.9411, 0.9391,
         0.9340],
        [0.9416, 0.9277, 0.9266, 0.9253, 0.9222, 0.9179, 0.9173, 0.9142, 0.9136,
         0.9122],
        [0.9258, 0.9213, 0.9202, 0.9131, 0.9112, 0.9098, 0.9078, 0.9044, 0.9037,
         0.9021],
        [0.9513, 0.9455, 0.9341, 0.9311, 0.9249, 0.9242, 0.9172, 0.9167, 0.9134,
         0.9128],
        [0.9611, 0.9552, 0.9511, 0.9395, 0.9389, 0.9344, 0.9334, 0.9318, 0.9317,
         0.9299],
        [0.9250, 0.9126, 0.8991, 0.8974, 0.8925, 0.8915, 0.8899, 0.8885, 0.8855,
         0.8838],
        [0.9022, 0.8976, 0.8961, 0.8960, 0.8941, 0.8934, 0.8905, 0.8890, 0.8877,
         0.8864],
        [0.9396, 0.9316, 0.9316, 0.9297, 0.9291, 0.9285, 0.9254, 0.9238, 0.9233,
         0.9224],
        [0.9252, 0.9106, 0.9097, 0.9072, 0.9051, 0.9038, 0.9032, 0.9012, 0.8881,
         0.8837],
        [0.9672, 0.9482, 0.9416, 0.9380, 0.9329, 0.9264, 0.9246, 0.9172, 0.9122,
         0.9109],
        [0.9404, 0.9332, 0.9293, 0.9260, 0.9237, 0.9228, 0.9221, 0.9217, 0.9214,
         0.9201],
        [0.9528, 0.8983, 0.8875, 0.8787, 0.8642, 0.8594, 0.8593, 0.8570, 0.8569,
         0.8565],
        [0.9350, 0.9310, 0.9288, 0.9224, 0.9211, 0.9168, 0.9127, 0.9091, 0.9028,
         0.9023],
        [0.9226, 0.9207, 0.9183, 0.9139, 0.9063, 0.8986, 0.8964, 0.8958, 0.8948,
         0.8937],
        [0.9593, 0.9478, 0.9418, 0.9250, 0.9238, 0.9232, 0.9185, 0.9175, 0.9172,
         0.9158],
        [0.9421, 0.9352, 0.9257, 0.9242, 0.9235, 0.9215, 0.9197, 0.9168, 0.9127,
         0.9114],
        [0.9303, 0.9258, 0.9247, 0.9242, 0.9209, 0.9175, 0.9167, 0.9166, 0.9131,
         0.9129],
        [0.9442, 0.9442, 0.9437, 0.9431, 0.9414, 0.9408, 0.9331, 0.9306, 0.9295,
         0.9274],
        [0.9364, 0.9202, 0.9189, 0.9086, 0.9012, 0.9008, 0.8964, 0.8960, 0.8953,
         0.8949],
        [0.8733, 0.8680, 0.8648, 0.8573, 0.8562, 0.8495, 0.8482, 0.8481, 0.8470,
         0.8469],
        [0.8993, 0.8891, 0.8841, 0.8822, 0.8794, 0.8781, 0.8778, 0.8769, 0.8751,
         0.8738],
        [0.9176, 0.9136, 0.9127, 0.9070, 0.9002, 0.8994, 0.8957, 0.8889, 0.8860,
         0.8856],
        [0.9048, 0.8942, 0.8917, 0.8841, 0.8802, 0.8751, 0.8751, 0.8686, 0.8580,
         0.8570],
        [0.9472, 0.8672, 0.8634, 0.8501, 0.8459, 0.8291, 0.8197, 0.8175, 0.8152,
         0.8129],
        [0.8987, 0.8797, 0.8777, 0.8750, 0.8718, 0.8688, 0.8538, 0.8534, 0.8534,
         0.8512],
        [0.8924, 0.8886, 0.8676, 0.8603, 0.8480, 0.8439, 0.8411, 0.8356, 0.8344,
         0.8326],
        [0.9336, 0.9302, 0.9285, 0.9284, 0.9271, 0.9249, 0.9241, 0.9236, 0.9235,
         0.9214],
        [0.8794, 0.8757, 0.8757, 0.8645, 0.8598, 0.8546, 0.8521, 0.8386, 0.8298,
         0.8227],
        [0.8992, 0.8604, 0.8596, 0.8568, 0.8505, 0.8494, 0.8471, 0.8450, 0.8447,
         0.8445],
        [0.8837, 0.8721, 0.8686, 0.8597, 0.8504, 0.8457, 0.8394, 0.8375, 0.8332,
         0.8332],
        [0.8740, 0.8709, 0.8557, 0.8496, 0.8469, 0.8431, 0.8415, 0.8407, 0.8398,
         0.8335],
        [0.8498, 0.8360, 0.8191, 0.8167, 0.8083, 0.8081, 0.7964, 0.7799, 0.7732,
         0.7699],
        [0.8286, 0.8043, 0.7887, 0.7851, 0.7792, 0.7792, 0.7755, 0.7634, 0.7588,
         0.7575],
        [0.8982, 0.8806, 0.8770, 0.8738, 0.8726, 0.8716, 0.8606, 0.8601, 0.8597,
         0.8576],
        [0.8040, 0.7834, 0.7782, 0.7714, 0.7627, 0.7586, 0.7396, 0.7382, 0.7297,
         0.7204],
        [0.8303, 0.8276, 0.8096, 0.8087, 0.8022, 0.8022, 0.8016, 0.7960, 0.7883,
         0.7838],
        [0.8898, 0.8610, 0.8428, 0.8309, 0.8303, 0.8283, 0.8200, 0.8191, 0.8146,
         0.8118],
        [0.8741, 0.8526, 0.8466, 0.8457, 0.8346, 0.8259, 0.8203, 0.8093, 0.8089,
         0.8039],
        [0.8867, 0.8838, 0.8430, 0.8136, 0.8100, 0.8040, 0.8014, 0.7968, 0.7966,
         0.7916],
        [0.9230, 0.9062, 0.8999, 0.8918, 0.8629, 0.8580, 0.8552, 0.8504, 0.8412,
         0.8365],
        [0.9109, 0.8735, 0.8691, 0.8589, 0.8522, 0.8505, 0.8423, 0.8405, 0.8374,
         0.8367],
        [0.9225, 0.8997, 0.8967, 0.8962, 0.8945, 0.8911, 0.8911, 0.8884, 0.8880,
         0.8867],
        [0.8982, 0.8785, 0.8776, 0.8664, 0.8643, 0.8636, 0.8607, 0.8599, 0.8597,
         0.8577],
        [0.9033, 0.8755, 0.8663, 0.8642, 0.8583, 0.8547, 0.8416, 0.8274, 0.8232,
         0.8223],
        [0.9393, 0.8779, 0.8692, 0.8609, 0.8542, 0.8502, 0.8343, 0.8302, 0.8289,
         0.8174],
        [0.9047, 0.8586, 0.8538, 0.8429, 0.8302, 0.8281, 0.8222, 0.8221, 0.8148,
         0.8128],
        [0.8976, 0.8156, 0.8127, 0.8003, 0.7901, 0.7891, 0.7889, 0.7856, 0.7824,
         0.7739],
        [0.8833, 0.8650, 0.8555, 0.8531, 0.8457, 0.8426, 0.8396, 0.8392, 0.8385,
         0.8384],
        [0.8532, 0.7754, 0.7740, 0.7732, 0.7626, 0.7358, 0.7283, 0.7263, 0.7251,
         0.7225],
        [0.8820, 0.8550, 0.8405, 0.8212, 0.8115, 0.8102, 0.7914, 0.7899, 0.7883,
         0.7840],
        [0.8371, 0.8057, 0.7933, 0.7780, 0.7764, 0.7695, 0.7668, 0.7503, 0.7479,
         0.7431]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 264366.1250,  189986.4219,  156415.2969,  154037.1562,  141551.5625,
          136559.4531,  131728.8281,  124048.1406,  121351.5078,  117535.2812],
        [ 693008.3125,  577286.0625,  569781.3125,  562721.1875,  556020.0000,
          520090.1250,  519896.7500,  512016.6562,  495883.6562,  490750.5000],
        [ 410347.5625,  364724.3438,  265737.9062,  250621.1875,  202851.0625,
          181927.1250,  175760.8438,  155194.0781,  150553.0156,  135822.8906],
        [ 854351.6875,  311771.6562,  295396.0312,  283574.7812,  282002.2188,
          265062.8750,  231524.0156,  225528.6250,  218654.2656,  216799.4219],
        [ 168390.5781,  143631.6875,  140882.3594,  111791.5781,   98524.9375,
           98035.2188,   93277.8047,   86418.2188,   77140.5000,   70162.6406],
        [ 193821.5312,  117717.4531,  103531.9062,   86726.4297,   83279.4375,
           76537.9141,   71241.1094,   66611.0312,   57792.4766,   55957.1016],
        [ 228854.6406,  219985.9844,  195983.6094,  191203.3438,  181347.3594,
          177787.0312,  173430.1094,  163362.9688,  158091.9375,  152851.7031],
        [ 338564.4062,  255525.5000,  237465.3750,  202578.4688,  182689.5156,
          139950.8594,  131990.2812,  116197.9688,   98213.0156,   93128.5625],
        [ 425395.4375,  391385.7812,  316442.2188,  303560.3750,  302316.3438,
          297755.7188,  297448.9062,  284282.5938,  280783.3438,  280188.9688],
        [ 768332.6875,  299080.2188,  294426.8438,  286222.7500,  258824.9375,
          252413.4531,  237870.4219,  237225.0000,  217938.3125,  211443.8906],
        [ 723499.8125,  403141.7500,  343409.6250,  325592.0000,  324205.4375,
          313507.8125,  301450.6562,  293873.0938,  283497.1875,  274330.5625],
        [ 314224.4062,  252922.1250,  199808.6250,  177211.1719,  127845.5625,
           88393.5312,   86747.6016,   83254.9062,   69886.6328,   65224.8047],
        [ 453447.7500,  413427.5938,  389052.5000,  329031.2500,  282579.9375,
          269235.2188,  240315.6250,  224918.6250,  219929.5625,  187570.7812],
        [ 855195.4375,  834185.8750,  790427.2500,  788118.0000,  758608.3125,
          736973.6250,  709577.8750,  690353.2500,  670786.0000,  623706.0000],
        [ 695078.6875,  570056.8750,  561017.1875,  550152.5000,  526662.6875,
          495067.6250,  491123.6250,  470092.6875,  465648.6250,  456408.3438],
        [ 554764.1250,  519641.4688,  512138.7500,  462293.2812,  450162.3438,
          441109.9062,  428801.7500,  408576.6875,  404471.8438,  395158.4375],
        [ 798127.8125,  734299.9375,  624311.7500,  598204.6250,  547336.5625,
          542173.5625,  490245.2812,  486911.0000,  464482.5625,  460727.3125],
        [ 918471.2500,  843591.7500,  795766.3750,  674635.8750,  668467.2500,
          626472.0000,  618321.8750,  603755.4375,  602792.9375,  587844.5625],
        [ 547896.9375,  459062.0000,  378529.1875,  369591.3750,  344376.7812,
          339483.5938,  331855.1562,  325479.3125,  311870.9688,  304210.7188],
        [ 395541.5000,  370382.0625,  362906.9688,  362160.8750,  352282.3438,
          348881.7812,  334811.2188,  327903.5312,  321533.0938,  315667.0000],
        [ 674858.5000,  602307.3750,  602154.0625,  585993.5625,  581127.8750,
          576477.3750,  550978.9375,  538950.5625,  534922.6875,  527873.0000],
        [ 549382.3125,  446096.3750,  440295.4062,  425094.5312,  412242.9062,
          404684.4688,  401525.2188,  390390.1250,  323406.8750,  303772.0938],
        [1001408.8125,  763659.6250,  694417.4375,  659894.6875,  613970.6875,
          559484.8750,  544887.1875,  490359.3750,  456818.5625,  448337.5938],
        [ 683231.0625,  616132.7500,  583171.4375,  556282.5000,  538049.2500,
          531215.1250,  525833.0000,  523263.2812,  520847.5938,  510742.8125],
        [ 815819.9375,  374412.6250,  320835.3125,  282723.8750,  230094.3750,
          214795.3594,  214299.7969,  207535.2031,  207323.9375,  206043.1250],
        [ 632299.0000,  597260.6875,  578308.8125,  528300.0625,  518577.5938,
          487662.4375,  460046.7812,  436991.4688,  399154.2812,  396585.8750],
        [ 529890.0000,  515655.1250,  497856.8125,  467486.2812,  419556.6562,
          376128.3125,  364475.3750,  361359.7812,  356138.9688,  350637.5938],
        [ 894774.1875,  758683.5000,  696504.0000,  548052.1250,  538812.3125,
          534218.1250,  499414.1562,  492457.9688,  490251.8125,  480964.8750],
        [ 700082.8125,  634411.2500,  553438.3125,  541534.8750,  536354.0000,
          521216.3125,  508411.1250,  487351.4062,  459532.0000,  451529.5938],
        [ 591610.9375,  554790.0625,  546037.3125,  542014.3125,  516977.7188,
          492200.6562,  486794.0000,  486072.1562,  462728.2188,  461227.1875],
        [ 721046.2500,  720813.8750,  715959.3125,  709749.0625,  692944.1875,
          686889.6875,  615610.0000,  594127.0000,  584818.3125,  567205.5625],
        [ 645339.6250,  512096.2188,  502427.1250,  433404.7500,  390341.3438,
          387728.2812,  364081.4375,  362377.1562,  358548.6875,  356392.4375],
        [ 261909.6094,  242963.4375,  231893.2656,  208283.7188,  205008.2344,
          186292.2188,  182925.9062,  182832.0938,  179960.4688,  179601.2656],
        [ 379731.4062,  328115.0000,  305801.8125,  297261.7500,  285808.9375,
          280491.6250,  279123.2500,  275616.0000,  268563.5938,  263830.4062],
        [ 493363.3438,  465570.0312,  459864.7500,  423877.6250,  384582.5625,
          380383.8438,  360821.8750,  327085.8438,  313787.1875,  312033.1250],
        [ 410474.7812,  353233.0312,  340600.7500,  305555.1875,  289133.8750,
          268803.4375,  268779.5938,  244770.5000,  210536.2031,  207634.2031],
        [ 753017.1250,  239912.1562,  227255.0000,  187992.8750,  177038.0156,
          139254.8438,  121789.5312,  118082.4297,  114173.3906,  110448.7109],
        [ 376238.4375,  286836.7500,  278763.3125,  268276.3750,  256450.7344,
          245620.7188,  198185.0781,  197187.2031,  196954.1562,  191069.9219],
        [ 344247.7500,  325682.3750,  241483.4062,  217619.7344,  182388.0000,
          171965.6250,  165426.6250,  152732.2188,  150200.5156,  146446.4062],
        [ 619716.8750,  590122.2500,  576477.9375,  575730.6875,  564434.6875,
          547118.9375,  541215.8125,  537031.1875,  536414.3750,  520562.5625],
        [ 285900.2812,  271222.9375,  271016.0938,  230959.4688,  215893.8750,
          200548.7812,  193474.3438,  159608.3594,  140770.2188,  127183.6641],
        [ 379398.3750,  217725.3906,  215251.0156,  206781.3281,  189142.7188,
          186174.2812,  180248.6875,  174700.0156,  174177.4844,  173546.9219],
        [ 303788.3125,  257371.7344,  244927.6406,  215526.6562,  188903.8594,
          176686.8594,  161425.2500,  157104.1875,  147641.4688,  147604.4375],
        [ 264346.7188,  252942.8750,  203674.3281,  186639.5156,  179620.2969,
          170087.8281,  166208.7812,  164422.6875,  162230.5625,  148382.2344],
        [ 187140.8906,  153809.1875,  120663.4844,  116752.9297,  103486.3984,
          103142.3359,   87292.5859,   68975.9297,   62676.5742,   59817.6445],
        [ 138338.4688,   97726.5234,   78162.2031,   74247.2188,   68320.1406,
           68273.3125,   64786.2031,   54477.2539,   50986.7070,   50097.5664],
        [ 373635.0000,  290503.0938,  276308.4375,  263623.4375,  259273.5938,
          255665.9062,  218562.5469,  216823.2031,  215768.7344,  209291.4375],
        [  97286.4062,   72529.4219,   67281.9844,   61063.8477,   53974.6016,
           50880.6211,   38781.9062,   38013.5234,   33665.8164,   29461.3184],
        [ 141653.6562,  136368.0156,  105381.7578,  104098.2109,   94882.9844,
           94850.1406,   93982.6875,   86846.8516,   77783.4844,   72930.5391],
        [ 331331.4688,  219586.0781,  169301.0156,  142941.0781,  141716.7500,
          137646.7812,  122341.7812,  120813.1797,  113157.6797,  108862.3359],
        [ 265062.3750,  194985.8281,  178766.0000,  176605.3281,  150761.0625,
          133032.5781,  122888.3359,  104978.6250,  104365.3125,   97221.0156],
        [ 317226.9375,  304434.1875,  169927.9688,  111676.1875,  105960.9219,
           97334.5703,   93753.6016,   87811.9375,   87505.1328,   81494.0000],
        [ 532729.0000,  418918.1250,  383181.8750,  340941.0000,  225584.5625,
          210604.6875,  202203.4375,  188846.2344,  165501.2656,  154887.5625],
        [ 448405.6250,  262641.5000,  246603.7031,  213246.6094,  193633.2656,
          189216.8594,  168076.7500,  163824.3125,  156930.6250,  155164.6250],
        [ 528575.1875,  382035.0625,  365952.1562,  363409.5000,  354747.1250,
          337734.9688,  337734.9688,  325108.0000,  323226.5000,  317145.5625],
        [ 373802.1562,  282025.6250,  278486.9688,  237318.4375,  230147.0469,
          228019.8125,  218734.5625,  216276.5625,  215557.9219,  209615.2188],
        [ 402208.5625,  270274.2812,  236823.7500,  229961.4375,  211396.5156,
          200782.2344,  166559.9062,  135912.4375,  128054.7188,  126337.4531],
        [ 671923.0625,  279599.8750,  247088.4062,  219283.6719,  199318.5469,
          188277.2500,  149950.8906,  141512.8281,  138843.3594,  117916.4375],
        [ 410207.9062,  212321.0625,  198183.5781,  169624.2344,  141464.1094,
          137363.5156,  126300.1016,  126059.6797,  113616.9297,  110348.7031],
        [ 370675.0000,  114791.4531,  110173.5156,   92341.2891,   79793.7812,
           78689.6406,   78443.4297,   74770.1953,   71523.2812,   63314.0430],
        [ 301988.7500,  232473.4062,  203112.1875,  196239.8281,  176669.0000,
          168897.8438,  161872.6094,  160950.5469,  159275.3594,  159042.3594],
        [ 196392.7812,   64663.2422,   63370.1602,   62656.6680,   53868.9258,
           36712.5469,   32980.0430,   32050.7930,   31520.5391,   30381.0469],
        [ 296684.5625,  201668.0625,  163965.2969,  124432.2734,  108350.0469,
          106266.2344,   81277.3906,   79575.3047,   77781.6328,   73086.4297],
        [ 156165.0469,   99655.7188,   83546.9609,   67119.3281,   65564.2578,
           59489.4414,   57161.9023,   45213.6094,   43635.9492,   40746.6719]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[264366.1250,      0.0000],
         [189986.4219,      0.0000],
         [156415.2969,      0.0000],
         ...,
         [124048.1406,      0.0000],
         [121351.5078,      0.0000],
         [117535.2812,      0.0000]],

        [[693008.3125,      0.0000],
         [577286.0625,      0.0000],
         [569781.3125,      0.0000],
         ...,
         [512016.6562,      0.0000],
         [495883.6562,      0.0000],
         [490750.5000,      0.0000]],

        [[410347.5625,      0.0000],
         [364724.3438,      0.0000],
         [265737.9062,      0.0000],
         ...,
         [155194.0781,      0.0000],
         [150553.0156,      0.0000],
         [135822.8906,      0.0000]],

        ...,

        [[     0.0000, 196392.7812],
         [ 64663.2422,      0.0000],
         [ 63370.1602,      0.0000],
         ...,
         [ 32050.7930,      0.0000],
         [ 31520.5391,      0.0000],
         [ 30381.0469,      0.0000]],

        [[296684.5625,      0.0000],
         [     0.0000, 201668.0625],
         [163965.2969,      0.0000],
         ...,
         [ 79575.3047,      0.0000],
         [ 77781.6328,      0.0000],
         [ 73086.4297,      0.0000]],

        [[     0.0000, 156165.0469],
         [     0.0000,  99655.7188],
         [     0.0000,  83546.9609],
         ...,
         [ 45213.6094,      0.0000],
         [     0.0000,  43635.9492],
         [     0.0000,  40746.6719]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1537579.7500,       0.0000],
        [5497454.5000,       0.0000],
        [2293540.0000,       0.0000],
        [3184665.5000,       0.0000],
        [ 450407.7500,  637847.7500],
        [ 770067.4375,  143148.9375],
        [1684806.7500,  158091.9375],
        [1796304.0000,       0.0000],
        [3179559.7500,       0.0000],
        [3063778.5000,       0.0000],
        [3586508.0000,       0.0000],
        [1465519.3750,       0.0000],
        [3009509.0000,       0.0000],
        [7457931.0000,       0.0000],
        [5281309.0000,       0.0000],
        [4577118.5000,       0.0000],
        [5746820.5000,       0.0000],
        [6940119.0000,       0.0000],
        [3712356.2500,       0.0000],
        [3164167.0000,  327903.5312],
        [5775644.0000,       0.0000],
        [4096890.0000,       0.0000],
        [6233238.5000,       0.0000],
        [5588769.0000,       0.0000],
        [3073883.5000,       0.0000],
        [5035187.0000,       0.0000],
        [4239185.0000,       0.0000],
        [5934133.0000,       0.0000],
        [5393862.0000,       0.0000],
        [5140453.0000,       0.0000],
        [6609163.0000,       0.0000],
        [4312737.0000,       0.0000],
        [2061670.2500,       0.0000],
        [2372733.0000,  591610.7500],
        [1773592.8750, 2147777.5000],
        [1016173.9375, 1883347.7500],
        [1085586.0000, 1103378.0000],
        [1307780.3750, 1187802.2500],
        [ 702940.4375, 1395252.1250],
        [5608825.0000,       0.0000],
        [1969394.2500,  127183.6641],
        [2097146.3750,       0.0000],
        [2000980.3750,       0.0000],
        [1898555.7500,       0.0000],
        [ 232279.9688,  831478.0000],
        [ 326613.5625,  418802.0000],
        [2205820.5000,  373635.0000],
        [ 341340.9375,  201598.5156],
        [ 447040.8438,  561737.5000],
        [ 440193.8125, 1167504.3750],
        [1291268.5000,  237397.8906],
        [ 665536.3750,  791589.1250],
        [1664087.7500, 1159310.0000],
        [2008527.0000,  189216.8594],
        [3635669.0000,       0.0000],
        [2489984.2500,       0.0000],
        [ 964985.1250, 1143326.1250],
        [1655512.3750,  698202.1250],
        [ 941458.0000,  804031.8125],
        [ 528391.5000,  606124.1250],
        [1209390.7500,  711131.1250],
        [ 408203.9688,  196392.7812],
        [ 896802.8750,  416284.3750],
        [ 104703.0469,  613595.8125]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
Epoch 496/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:38, 59.24s/it]  7%|▋         | 2/30 [01:01<12:06, 25.94s/it] 10%|█         | 3/30 [01:02<06:29, 14.44s/it] 13%|█▎        | 4/30 [01:03<03:54,  9.04s/it] 17%|█▋        | 5/30 [01:04<02:31,  6.05s/it] 20%|██        | 6/30 [01:04<01:41,  4.25s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.10s/it] 27%|██▋       | 8/30 [01:06<00:51,  2.35s/it] 30%|███       | 9/30 [01:07<00:38,  1.85s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.51s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:09<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:12<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:15<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:18<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:21<00:01,  1.33it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:22<00:00,  1.33it/s]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Epoch loss is 2.2815874576568604
Epoch 497/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:59<28:47, 59.56s/it]  7%|▋         | 2/30 [01:02<12:16, 26.29s/it] 10%|█         | 3/30 [01:03<06:34, 14.63s/it] 13%|█▎        | 4/30 [01:04<03:57,  9.15s/it] 17%|█▋        | 5/30 [01:04<02:32,  6.12s/it] 20%|██        | 6/30 [01:05<01:43,  4.29s/it] 23%|██▎       | 7/30 [01:06<01:12,  3.13s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:07<00:39,  1.87s/it] 33%|███▎      | 10/30 [01:08<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:10<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:11<00:14,  1.07it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.19it/s] 57%|█████▋    | 17/30 [01:13<00:10,  1.23it/s] 60%|██████    | 18/30 [01:14<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.28it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:16<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:17<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.32it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:19<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:20<00:03,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:23<00:00,  2.79s/it]
Epoch loss is 2.277734883626302
Epoch 498/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:00<29:02, 60.07s/it]  7%|▋         | 2/30 [01:00<11:44, 25.18s/it] 10%|█         | 3/30 [01:01<06:18, 14.02s/it] 13%|█▎        | 4/30 [01:02<03:48,  8.78s/it] 17%|█▋        | 5/30 [01:03<02:27,  5.89s/it] 20%|██        | 6/30 [01:03<01:39,  4.14s/it] 23%|██▎       | 7/30 [01:04<01:09,  3.03s/it] 27%|██▋       | 8/30 [01:05<00:50,  2.30s/it] 30%|███       | 9/30 [01:06<00:38,  1.82s/it] 33%|███▎      | 10/30 [01:06<00:29,  1.49s/it] 37%|███▋      | 11/30 [01:07<00:23,  1.26s/it] 40%|████      | 12/30 [01:08<00:19,  1.11s/it] 43%|████▎     | 13/30 [01:09<00:16,  1.00it/s] 47%|████▋     | 14/30 [01:09<00:14,  1.08it/s] 50%|█████     | 15/30 [01:10<00:13,  1.15it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:12<00:09,  1.27it/s] 63%|██████▎   | 19/30 [01:13<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:15<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:16<00:05,  1.33it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:18<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:19<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:21<00:00,  2.73s/it]
Epoch loss is 2.276638452212016
Epoch 499/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:56<27:11, 56.25s/it]  7%|▋         | 2/30 [01:01<12:14, 26.23s/it] 10%|█         | 3/30 [01:02<06:34, 14.60s/it] 13%|█▎        | 4/30 [01:02<03:57,  9.13s/it] 17%|█▋        | 5/30 [01:03<02:32,  6.11s/it] 20%|██        | 6/30 [01:04<01:42,  4.28s/it] 23%|██▎       | 7/30 [01:05<01:11,  3.13s/it] 27%|██▋       | 8/30 [01:05<00:52,  2.37s/it] 30%|███       | 9/30 [01:06<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:07<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:08<00:24,  1.28s/it] 40%|████      | 12/30 [01:08<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:09<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:10<00:14,  1.08it/s] 50%|█████     | 15/30 [01:11<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:11<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:12<00:10,  1.24it/s] 60%|██████    | 18/30 [01:13<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:14<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:14<00:07,  1.30it/s] 70%|███████   | 21/30 [01:15<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:16<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:17<00:05,  1.32it/s] 80%|████████  | 24/30 [01:17<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:18<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:19<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:20<00:02,  1.34it/s] 93%|█████████▎| 28/30 [01:20<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:21<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  1.34it/s]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Epoch loss is 2.332702962557475
Epoch 500/501
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [01:02<30:11, 62.48s/it]  7%|▋         | 2/30 [01:03<12:12, 26.17s/it] 10%|█         | 3/30 [01:03<06:33, 14.56s/it] 13%|█▎        | 4/30 [01:04<03:56,  9.11s/it] 17%|█▋        | 5/30 [01:05<02:32,  6.09s/it] 20%|██        | 6/30 [01:06<01:42,  4.27s/it] 23%|██▎       | 7/30 [01:06<01:11,  3.12s/it] 27%|██▋       | 8/30 [01:07<00:52,  2.37s/it] 30%|███       | 9/30 [01:08<00:39,  1.86s/it] 33%|███▎      | 10/30 [01:09<00:30,  1.52s/it] 37%|███▋      | 11/30 [01:09<00:24,  1.28s/it] 40%|████      | 12/30 [01:10<00:20,  1.12s/it] 43%|████▎     | 13/30 [01:11<00:17,  1.01s/it] 47%|████▋     | 14/30 [01:12<00:14,  1.08it/s] 50%|█████     | 15/30 [01:12<00:13,  1.14it/s] 53%|█████▎    | 16/30 [01:13<00:11,  1.20it/s] 57%|█████▋    | 17/30 [01:14<00:10,  1.24it/s] 60%|██████    | 18/30 [01:15<00:09,  1.26it/s] 63%|██████▎   | 19/30 [01:15<00:08,  1.29it/s] 67%|██████▋   | 20/30 [01:16<00:07,  1.30it/s] 70%|███████   | 21/30 [01:17<00:06,  1.31it/s] 73%|███████▎  | 22/30 [01:18<00:06,  1.32it/s] 77%|███████▋  | 23/30 [01:18<00:05,  1.33it/s] 80%|████████  | 24/30 [01:19<00:04,  1.33it/s] 83%|████████▎ | 25/30 [01:20<00:03,  1.33it/s] 87%|████████▋ | 26/30 [01:21<00:02,  1.33it/s] 90%|█████████ | 27/30 [01:21<00:02,  1.33it/s] 93%|█████████▎| 28/30 [01:22<00:01,  1.34it/s] 97%|█████████▋| 29/30 [01:23<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  1.34it/s]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Epoch loss is 2.306527288754781
saving checkpoint
Extracting features for train set...
feature output is dim torch.Size([7530, 384])
label output is dim torch.Size([7530, 1])
Extracting features for val set...
feature output is dim torch.Size([64, 384])
label output is dim torch.Size([64, 1])
TRAIN FEATURES ARE SIZE
 torch.Size([384, 7530])
TEST FEATURES ARE
 torch.Size([64, 384])
tensor([[ 0.0235,  0.0008,  0.0132,  ..., -0.0058,  0.0176,  0.0041],
        [-0.0023,  0.0095,  0.0286,  ...,  0.0172,  0.0042, -0.0147],
        [-0.0293, -0.0379,  0.0247,  ...,  0.0834, -0.0043, -0.0143],
        ...,
        [ 0.0059, -0.0033,  0.0099,  ..., -0.0241, -0.0051, -0.0027],
        [-0.0333,  0.0099, -0.0034,  ...,  0.0050,  0.0169, -0.0105],
        [-0.0326, -0.0139,  0.0193,  ...,  0.0414,  0.0360, -0.0284]],
       device='cuda:0')
TEST LABELS ARE
 torch.Size([64, 1])
tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [0],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
DOT PROD OF TEST FEATURES x TRAIN FEATURES
 torch.Size([64, 7530])
TOP K DOT PROD VALS ARE
 torch.Size([64, 10])
tensor([[0.8740, 0.8508, 0.8372, 0.8361, 0.8302, 0.8277, 0.8252, 0.8210, 0.8194,
         0.8172],
        [0.9414, 0.9286, 0.9277, 0.9268, 0.9260, 0.9213, 0.9213, 0.9202, 0.9180,
         0.9173],
        [0.9047, 0.8965, 0.8743, 0.8702, 0.8554, 0.8478, 0.8454, 0.8367, 0.8345,
         0.8273],
        [0.9561, 0.8855, 0.8817, 0.8789, 0.8785, 0.8741, 0.8647, 0.8628, 0.8607,
         0.8601],
        [0.8424, 0.8313, 0.8299, 0.8137, 0.8049, 0.8045, 0.8010, 0.7957, 0.7877,
         0.7811],
        [0.8522, 0.8173, 0.8083, 0.7959, 0.7931, 0.7872, 0.7822, 0.7775, 0.7675,
         0.7653],
        [0.8639, 0.8611, 0.8530, 0.8513, 0.8476, 0.8462, 0.8444, 0.8403, 0.8380,
         0.8356],
        [0.8913, 0.8716, 0.8664, 0.8553, 0.8481, 0.8294, 0.8253, 0.8164, 0.8046,
         0.8009],
        [0.9073, 0.9014, 0.8865, 0.8836, 0.8833, 0.8823, 0.8822, 0.8790, 0.8782,
         0.8780],
        [0.9486, 0.8826, 0.8815, 0.8795, 0.8725, 0.8707, 0.8666, 0.8664, 0.8604,
         0.8583],
        [0.9444, 0.9035, 0.8923, 0.8885, 0.8882, 0.8859, 0.8831, 0.8814, 0.8788,
         0.8765],
        [0.8861, 0.8709, 0.8544, 0.8460, 0.8231, 0.7973, 0.7960, 0.7931, 0.7808,
         0.7760],
        [0.9117, 0.9053, 0.9010, 0.8893, 0.8786, 0.8752, 0.8673, 0.8626, 0.8611,
         0.8499],
        [0.9561, 0.9544, 0.9506, 0.9504, 0.9477, 0.9457, 0.9431, 0.9411, 0.9391,
         0.9340],
        [0.9416, 0.9277, 0.9266, 0.9253, 0.9222, 0.9179, 0.9173, 0.9142, 0.9136,
         0.9122],
        [0.9258, 0.9213, 0.9202, 0.9131, 0.9112, 0.9098, 0.9078, 0.9044, 0.9037,
         0.9021],
        [0.9513, 0.9455, 0.9341, 0.9311, 0.9249, 0.9242, 0.9172, 0.9167, 0.9134,
         0.9128],
        [0.9611, 0.9552, 0.9511, 0.9395, 0.9389, 0.9344, 0.9334, 0.9318, 0.9317,
         0.9299],
        [0.9250, 0.9126, 0.8991, 0.8974, 0.8925, 0.8915, 0.8899, 0.8885, 0.8855,
         0.8838],
        [0.9022, 0.8976, 0.8961, 0.8960, 0.8941, 0.8934, 0.8905, 0.8890, 0.8877,
         0.8864],
        [0.9396, 0.9316, 0.9316, 0.9297, 0.9291, 0.9285, 0.9254, 0.9238, 0.9233,
         0.9224],
        [0.9252, 0.9106, 0.9097, 0.9072, 0.9051, 0.9038, 0.9032, 0.9012, 0.8881,
         0.8837],
        [0.9672, 0.9482, 0.9416, 0.9380, 0.9329, 0.9264, 0.9246, 0.9172, 0.9122,
         0.9109],
        [0.9404, 0.9332, 0.9293, 0.9260, 0.9237, 0.9228, 0.9221, 0.9217, 0.9214,
         0.9201],
        [0.9528, 0.8983, 0.8875, 0.8787, 0.8642, 0.8594, 0.8593, 0.8570, 0.8569,
         0.8565],
        [0.9350, 0.9310, 0.9288, 0.9224, 0.9211, 0.9168, 0.9127, 0.9091, 0.9028,
         0.9023],
        [0.9226, 0.9207, 0.9183, 0.9139, 0.9063, 0.8986, 0.8964, 0.8958, 0.8948,
         0.8937],
        [0.9593, 0.9478, 0.9418, 0.9250, 0.9238, 0.9232, 0.9185, 0.9175, 0.9172,
         0.9158],
        [0.9421, 0.9352, 0.9257, 0.9242, 0.9235, 0.9215, 0.9197, 0.9168, 0.9127,
         0.9114],
        [0.9303, 0.9258, 0.9247, 0.9242, 0.9209, 0.9175, 0.9167, 0.9166, 0.9131,
         0.9129],
        [0.9442, 0.9442, 0.9437, 0.9431, 0.9414, 0.9408, 0.9331, 0.9306, 0.9295,
         0.9274],
        [0.9364, 0.9202, 0.9189, 0.9086, 0.9012, 0.9008, 0.8964, 0.8960, 0.8953,
         0.8949],
        [0.8733, 0.8680, 0.8648, 0.8573, 0.8562, 0.8495, 0.8482, 0.8481, 0.8470,
         0.8469],
        [0.8993, 0.8891, 0.8841, 0.8822, 0.8794, 0.8781, 0.8778, 0.8769, 0.8751,
         0.8738],
        [0.9176, 0.9136, 0.9127, 0.9070, 0.9002, 0.8994, 0.8957, 0.8889, 0.8860,
         0.8856],
        [0.9048, 0.8942, 0.8917, 0.8841, 0.8802, 0.8751, 0.8751, 0.8686, 0.8580,
         0.8570],
        [0.9472, 0.8672, 0.8634, 0.8501, 0.8459, 0.8291, 0.8197, 0.8175, 0.8152,
         0.8129],
        [0.8987, 0.8797, 0.8777, 0.8750, 0.8718, 0.8688, 0.8538, 0.8534, 0.8534,
         0.8512],
        [0.8924, 0.8886, 0.8676, 0.8603, 0.8480, 0.8439, 0.8411, 0.8355, 0.8344,
         0.8326],
        [0.9336, 0.9302, 0.9285, 0.9284, 0.9271, 0.9249, 0.9241, 0.9236, 0.9235,
         0.9214],
        [0.8794, 0.8757, 0.8757, 0.8645, 0.8598, 0.8546, 0.8521, 0.8386, 0.8298,
         0.8227],
        [0.8992, 0.8604, 0.8596, 0.8568, 0.8505, 0.8494, 0.8471, 0.8450, 0.8447,
         0.8445],
        [0.8837, 0.8721, 0.8686, 0.8597, 0.8504, 0.8457, 0.8394, 0.8375, 0.8332,
         0.8332],
        [0.8740, 0.8709, 0.8557, 0.8496, 0.8469, 0.8431, 0.8415, 0.8407, 0.8398,
         0.8335],
        [0.8498, 0.8360, 0.8191, 0.8167, 0.8083, 0.8081, 0.7964, 0.7799, 0.7732,
         0.7699],
        [0.8286, 0.8043, 0.7887, 0.7851, 0.7792, 0.7792, 0.7755, 0.7634, 0.7588,
         0.7575],
        [0.8982, 0.8806, 0.8770, 0.8738, 0.8726, 0.8716, 0.8606, 0.8601, 0.8597,
         0.8576],
        [0.8040, 0.7834, 0.7782, 0.7714, 0.7627, 0.7586, 0.7396, 0.7382, 0.7297,
         0.7204],
        [0.8303, 0.8276, 0.8096, 0.8087, 0.8022, 0.8022, 0.8016, 0.7960, 0.7883,
         0.7838],
        [0.8898, 0.8610, 0.8428, 0.8309, 0.8303, 0.8283, 0.8200, 0.8191, 0.8146,
         0.8118],
        [0.8741, 0.8526, 0.8466, 0.8457, 0.8346, 0.8259, 0.8203, 0.8093, 0.8089,
         0.8039],
        [0.8867, 0.8838, 0.8430, 0.8136, 0.8100, 0.8040, 0.8014, 0.7968, 0.7966,
         0.7916],
        [0.9230, 0.9062, 0.8999, 0.8918, 0.8629, 0.8580, 0.8552, 0.8504, 0.8412,
         0.8365],
        [0.9109, 0.8735, 0.8691, 0.8589, 0.8522, 0.8505, 0.8423, 0.8405, 0.8374,
         0.8367],
        [0.9225, 0.8997, 0.8967, 0.8962, 0.8945, 0.8911, 0.8911, 0.8884, 0.8880,
         0.8867],
        [0.8982, 0.8785, 0.8776, 0.8664, 0.8643, 0.8636, 0.8607, 0.8599, 0.8597,
         0.8577],
        [0.9033, 0.8755, 0.8663, 0.8642, 0.8583, 0.8547, 0.8416, 0.8274, 0.8232,
         0.8223],
        [0.9393, 0.8779, 0.8692, 0.8609, 0.8542, 0.8502, 0.8343, 0.8302, 0.8289,
         0.8174],
        [0.9047, 0.8586, 0.8538, 0.8429, 0.8302, 0.8281, 0.8222, 0.8221, 0.8148,
         0.8128],
        [0.8976, 0.8156, 0.8127, 0.8003, 0.7901, 0.7891, 0.7889, 0.7856, 0.7824,
         0.7739],
        [0.8833, 0.8650, 0.8555, 0.8531, 0.8457, 0.8426, 0.8396, 0.8392, 0.8385,
         0.8384],
        [0.8532, 0.7754, 0.7740, 0.7732, 0.7626, 0.7358, 0.7283, 0.7263, 0.7251,
         0.7225],
        [0.8820, 0.8550, 0.8405, 0.8212, 0.8115, 0.8102, 0.7914, 0.7899, 0.7883,
         0.7840],
        [0.8371, 0.8057, 0.7933, 0.7780, 0.7764, 0.7695, 0.7668, 0.7503, 0.7479,
         0.7431]], device='cuda:0')
TRAIN LABEL CANDIDATES ARE
 torch.Size([64, 7530])
RETRIEVED CANDIDATES ARE
 torch.Size([64, 10])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0, 0, 0, 0],
        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 1],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 1, 1, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 1, 1, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],
        [0, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 1, 0, 0],
        [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]], device='cuda:0')
ONE HOT RETRIEVED CANDIATES ARE
 torch.Size([640, 2])
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        ...,
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
EXP(TOP K VALS / TEMP)
 torch.Size([64, 10])
tensor([[ 264364.8750,  189980.6250,  156412.9219,  154033.3438,  141549.2656,
          136557.8906,  131725.5781,  124046.1328,  121349.0781,  117533.8125],
        [ 693009.0000,  577287.1875,  569781.3125,  562720.1250,  556023.6875,
          520089.6562,  519890.2812,  512015.2188,  495883.6562,  490749.5312],
        [ 410353.8438,  364725.0312,  265738.9062,  250621.6719,  202851.8281,
          181925.2188,  175760.3438,  155194.5156,  150555.1719,  135824.9688],
        [ 854348.4375,  311773.4375,  295395.1875,  283576.6562,  282002.7500,
          265058.5938,  231525.5625,  225530.3594,  218654.4688,  216796.9531],
        [ 168388.3281,  143631.9531,  140879.2656,  111791.9062,   98522.9688,
           98033.4375,   93275.4062,   86417.3125,   77139.6094,   70161.9688],
        [ 193820.6094,  117716.4453,  103531.6094,   86725.8438,   83278.8828,
           76537.7031,   71242.1953,   66611.9219,   57792.0391,   55956.6758],
        [ 228852.4375,  219983.2656,  195980.9844,  191200.4219,  181345.4531,
          177785.6875,  173427.2969,  163361.0938,  158091.3281,  152850.5312],
        [ 338572.7812,  255530.3750,  237471.5000,  202580.2031,  182691.9531,
          139955.0000,  131992.2969,  116199.0781,   98215.1719,   93131.0469],
        [ 425396.2500,  391388.4062,  316444.0312,  303562.4062,  302322.7188,
          297753.7500,  297448.6250,  284284.7500,  280785.5000,  280186.8438],
        [ 768331.9375,  299081.9062,  294423.7500,  286221.3750,  258822.4844,
          252412.9688,  237869.9688,  237223.8594,  217938.7344,  211443.8906],
        [ 723498.4375,  403144.4375,  343410.2812,  325590.1562,  324200.8125,
          313506.9062,  301449.2188,  293870.2812,  283492.8438,  274330.2812],
        [ 314225.9062,  252925.5000,  199814.1562,  177216.4062,  127848.9766,
           88393.5312,   86749.5938,   83257.2031,   69887.4375,   65226.2383],
        [ 453445.1562,  413424.8125,  389045.8125,  329029.6875,  282579.1562,
          269234.7188,  240317.7031,  224918.6250,  219928.5156,  187569.3438],
        [ 855192.9375,  834181.9375,  790424.2500,  788111.1875,  758603.2500,
          736968.6875,  709576.5000,  690350.6250,  670782.7500,  623700.6250],
        [ 695082.6250,  570063.4375,  561017.1875,  550153.5625,  526662.6875,
          495067.6250,  491127.8438,  470093.5938,  465648.6250,  456412.6875],
        [ 554763.6250,  519639.0000,  512138.7500,  462292.4062,  450163.6562,
          441109.0625,  428801.7500,  408577.8750,  404471.8438,  395158.0625],
        [ 798129.3125,  734298.5625,  624310.0000,  598202.9375,  547332.3125,
          542168.9375,  490247.5938,  486912.8438,  464485.2188,  460726.0312],
        [ 918473.8750,  843590.8750,  795770.1875,  674639.7500,  668470.3750,
          626470.8125,  618321.8750,  603746.2500,  602794.6875,  587845.6875],
        [ 547895.3125,  459060.6875,  378525.9375,  369593.8125,  344376.7812,
          339478.7188,  331856.4062,  325478.0938,  311871.8750,  304206.0625],
        [ 395539.2500,  370380.3438,  362906.6562,  362161.5625,  352282.0000,
          348879.7500,  334810.8750,  327904.1562,  321531.5625,  315668.8125],
        [ 674855.3750,  602307.3750,  602153.5000,  585990.1250,  581123.9375,
          576472.9375,  550975.2500,  538945.3750,  534919.1250,  527867.9375],
        [ 549383.3750,  446097.6250,  440294.1250,  425093.7188,  412246.0625,
          404684.4688,  401525.6250,  390383.4062,  323407.5000,  303774.4062],
        [1001408.8125,  763663.2500,  694416.7500,  659897.1875,  613971.8750,
          559487.0000,  544886.6250,  490362.6562,  456821.1562,  448339.3438],
        [ 683229.7500,  616132.7500,  583172.5625,  556279.8750,  538047.7500,
          531210.0625,  525832.0625,  523258.7812,  520849.0938,  510743.7812],
        [ 815816.8125,  374412.6250,  320834.7188,  282720.9375,  230093.0625,
          214793.9375,  214299.7969,  207539.7656,  207322.5625,  206039.9844],
        [ 632300.1875,  597257.8125,  578307.6875,  528298.5625,  518577.0938,
          487658.7188,  460043.7188,  436992.2812,  399153.5312,  396580.9375],
        [ 529888.0000,  515655.1250,  497851.5625,  467487.1875,  419554.6250,
          376127.9375,  364474.3438,  361356.6875,  356137.6250,  350634.5938],
        [ 894773.2500,  758684.2500,  696502.0000,  548050.5625,  538813.8750,
          534220.1875,  499413.2188,  492459.8438,  490254.1562,  480966.7188],
        [ 700080.0625,  634408.8125,  553433.5625,  541531.2500,  536350.9375,
          521216.3125,  508407.7188,  487347.6875,  459532.0000,  451529.5938],
        [ 591608.6875,  554792.6875,  546037.8125,  542013.8125,  516981.1875,
          492199.2500,  486797.7188,  486075.8750,  462731.3125,  461224.0938],
        [ 721048.3750,  720817.3750,  715964.0625,  709746.3750,  692944.1875,
          686892.3125,  615607.1250,  594128.1250,  584816.1250,  567206.6250],
        [ 645335.3125,  512093.3125,  502425.2188,  433399.8125,  390339.4688,
          387730.8750,  364082.8125,  362376.1250,  358544.5938,  356393.0938],
        [ 261906.8594,  242960.8906,  231890.8281,  208281.9219,  205007.0469,
          186290.4375,  182925.4062,  182830.8594,  179957.8906,  179601.2656],
        [ 379724.1875,  328113.1250,  305798.0000,  297260.3438,  285807.8438,
          280490.0000,  279120.0312,  275613.6562,  268561.3125,  263829.1562],
        [ 493362.8438,  465565.5938,  459868.2812,  423879.6562,  384576.3438,
          380381.6250,  360820.1562,  327086.4688,  313789.0000,  312035.7812],
        [ 410472.0625,  353228.3438,  340598.8125,  305558.6562,  289130.0000,
          268801.1250,  268777.2812,  244771.1875,  210532.7969,  207632.4062],
        [ 753018.5625,  239909.4219,  227254.5781,  187991.4531,  177037.6875,
          139255.3750,  121789.8750,  118082.2031,  114175.1328,  110447.4531],
        [ 376236.3125,  286834.0000,  278758.5312,  268272.0312,  256446.0938,
          245619.0781,  198184.7031,  197184.7500,  196952.6562,  191069.0000],
        [ 344246.1250,  325679.5938,  241480.8750,  217617.6562,  182386.9688,
          171961.5312,  165425.6875,  152727.9844,  150202.0781,  146444.1719],
        [ 619717.5000,  590120.5625,  576476.8125,  575729.0625,  564435.7500,
          547118.9375,  541213.2500,  537024.0000,  536415.3750,  520562.5625],
        [ 285898.3750,  271221.6250,  271014.7812,  230961.0156,  215893.0469,
          200548.3906,  193472.8750,  159608.0469,  140769.6875,  127183.4219],
        [ 379398.3750,  217725.3906,  215251.2031,  206783.1094,  189142.8906,
          186171.6250,  180247.1406,  174700.6719,  174177.8125,  173545.7656],
        [ 303785.1250,  257368.5312,  244926.2344,  215523.3906,  188901.1719,
          176685.8594,  161422.7812,  157102.0781,  147640.9062,  147601.6250],
        [ 264342.4375,  252939.0000,  203670.0469,  186635.9531,  179618.7500,
          170085.4062,  166204.9688,  164420.8125,  162228.3906,  148380.6875],
        [ 187138.0312,  153810.7969,  120663.4844,  116750.0312,  103486.2031,
          103142.3359,   87291.7578,   68975.8594,   62677.0508,   59816.8438],
        [ 138334.5000,   97725.5859,   78160.7109,   74246.5078,   68319.2969,
           68273.1172,   64786.0117,   54475.8555,   50985.6875,   50096.9453],
        [ 373633.2188,  290503.9375,  276308.4375,  263622.9375,  259274.3281,
          255662.7344,  218562.5469,  216821.7656,  215767.5000,  209291.0469],
        [  97283.8984,   72528.1094,   67281.6562,   61062.8555,   53973.9297,
           50879.3594,   38781.3555,   38013.0859,   33666.3594,   29461.1504],
        [ 141650.2656,  136363.4688,  105381.0625,  104096.5312,   94882.7969,
           94849.3281,   93982.0547,   86845.9375,   77782.5938,   72928.2422],
        [ 331330.2188,  219586.2656,  169301.4844,  142943.1250,  141718.1094,
          137648.2188,  122344.1094,  120815.4844,  113158.6562,  108863.0625],
        [ 265057.0938,  194984.8906,  178764.6406,  176601.9531,  150758.1719,
          133031.4375,  122888.4453,  104976.2266,  104363.5234,   97221.2031],
        [ 317225.4375,  304432.1562,  169926.5156,  111675.2266,  105961.2188,
           97333.1797,   93753.5078,   87811.7656,   87503.7891,   81492.6016],
        [ 532729.0000,  418912.1562,  383180.0625,  340939.0625,  225584.1250,
          210605.0938,  202200.9375,  188844.2500,  165500.4844,  154884.5938],
        [ 448409.0312,  262641.2188,  246601.3594,  213245.5938,  193631.2344,
          189214.8750,  168075.6406,  163823.3750,  156930.4844,  155164.9062],
        [ 528576.2500,  382038.0000,  365951.8125,  363411.5625,  354748.1562,
          337733.6875,  337733.6875,  325107.6562,  323225.5625,  317144.9688],
        [ 373804.6562,  282027.7812,  278485.1250,  237316.6406,  230146.6094,
          228021.9844,  218736.6562,  216278.4062,  215555.6562,  209612.0312],
        [ 402207.8125,  270272.5000,  236824.2188,  229960.7812,  211396.9062,
          200784.9219,  166558.7969,  135908.5312,  128053.2500,  126338.0469],
        [ 671922.4375,  279600.9375,  247086.5312,  219283.0312,  199319.3281,
          188275.1094,  149950.8906,  141512.0156,  138842.1562,  117915.8750],
        [ 410204.7812,  212320.2656,  198179.9688,  169622.9375,  141463.4375,
          137360.7812,  126298.2969,  126058.9609,  113615.5234,  110346.3828],
        [ 370676.0938,  114791.9922,  110173.3047,   92342.1641,   79792.7969,
           78689.0391,   78443.7266,   74768.9844,   71522.9375,   63313.6211],
        [ 301989.5938,  232474.0781,  203114.3125,  196241.6875,  176669.3438,
          168899.6094,  161873.2344,  160951.9375,  159276.8594,  159042.5156],
        [ 196392.2188,   64664.1641,   63370.2812,   62656.2539,   53868.9258,
           36713.0000,   32980.0156,   32051.2227,   31521.1406,   30380.8438],
        [ 296684.5625,  201669.0312,  163963.8906,  124431.0859,  108348.2969,
          106266.4375,   81276.1406,   79574.6250,   77781.4844,   73086.1562],
        [ 156164.7500,   99655.3359,   83546.3281,   67118.6875,   65563.2578,
           59488.8164,   57161.7930,   45213.1758,   43634.4102,   40745.8945]],
       device='cuda:0')
ONE HOT x EXP(TOP K VALS)
 torch.Size([64, 10, 2])
tensor([[[264364.8750,      0.0000],
         [189980.6250,      0.0000],
         [156412.9219,      0.0000],
         ...,
         [124046.1328,      0.0000],
         [121349.0781,      0.0000],
         [117533.8125,      0.0000]],

        [[693009.0000,      0.0000],
         [577287.1875,      0.0000],
         [569781.3125,      0.0000],
         ...,
         [512015.2188,      0.0000],
         [495883.6562,      0.0000],
         [490749.5312,      0.0000]],

        [[410353.8438,      0.0000],
         [364725.0312,      0.0000],
         [265738.9062,      0.0000],
         ...,
         [155194.5156,      0.0000],
         [150555.1719,      0.0000],
         [135824.9688,      0.0000]],

        ...,

        [[     0.0000, 196392.2188],
         [ 64664.1641,      0.0000],
         [ 63370.2812,      0.0000],
         ...,
         [ 32051.2227,      0.0000],
         [ 31521.1406,      0.0000],
         [ 30380.8438,      0.0000]],

        [[296684.5625,      0.0000],
         [     0.0000, 201669.0312],
         [163963.8906,      0.0000],
         ...,
         [ 79574.6250,      0.0000],
         [ 77781.4844,      0.0000],
         [ 73086.1562,      0.0000]],

        [[     0.0000, 156164.7500],
         [     0.0000,  99655.3359],
         [     0.0000,  83546.3281],
         ...,
         [ 45213.1758,      0.0000],
         [     0.0000,  43634.4102],
         [     0.0000,  40745.8945]]], device='cuda:0')
PROBS ARE
 torch.Size([64, 2])
tensor([[1537553.5000,       0.0000],
        [5497450.0000,       0.0000],
        [2293551.5000,       0.0000],
        [3184662.5000,       0.0000],
        [ 450402.3438,  637839.8125],
        [ 770064.3750,  143149.6250],
        [1684787.1250,  158091.3281],
        [1796339.5000,       0.0000],
        [3179573.0000,       0.0000],
        [3063770.7500,       0.0000],
        [3586493.7500,       0.0000],
        [1465545.0000,       0.0000],
        [3009493.5000,       0.0000],
        [7457893.0000,       0.0000],
        [5281329.5000,       0.0000],
        [4577116.0000,       0.0000],
        [5746814.0000,       0.0000],
        [6940124.5000,       0.0000],
        [3712344.0000,       0.0000],
        [3164160.7500,  327904.1562],
        [5775610.5000,       0.0000],
        [4096890.5000,       0.0000],
        [6233255.0000,       0.0000],
        [5588756.0000,       0.0000],
        [3073874.2500,       0.0000],
        [5035170.5000,       0.0000],
        [4239168.0000,       0.0000],
        [5934138.5000,       0.0000],
        [5393838.0000,       0.0000],
        [5140462.5000,       0.0000],
        [6609170.5000,       0.0000],
        [4312720.5000,       0.0000],
        [2061653.3750,       0.0000],
        [2372711.7500,  591605.8750],
        [1773599.7500, 2147766.2500],
        [1016164.7500, 1883337.8750],
        [1085586.2500, 1103375.5000],
        [1307768.7500, 1187788.3750],
        [ 702934.6875, 1395238.0000],
        [5608813.5000,       0.0000],
        [1969387.7500,  127183.4219],
        [2097143.8750,       0.0000],
        [2000957.7500,       0.0000],
        [1898526.5000,       0.0000],
        [ 232278.9062,  831473.5000],
        [ 326610.6250,  418793.5938],
        [2205815.2500,  373633.2188],
        [ 341335.6875,  201596.0938],
        [ 447034.3750,  561727.9375],
        [ 440193.2812, 1167515.5000],
        [1291252.7500,  237394.9688],
        [ 665531.2500,  791584.1250],
        [1664079.2500, 1159300.5000],
        [2008522.8750,  189214.8750],
        [3635671.2500,       0.0000],
        [2489985.5000,       0.0000],
        [ 964980.0000, 1143325.7500],
        [1655505.0000,  698203.2500],
        [ 941445.8125,  804025.5000],
        [ 528390.0000,  606124.6875],
        [1209400.2500,  711133.0000],
        [ 408205.8438,  196392.2188],
        [ 896798.0000,  416283.7500],
        [ 104701.9922,  613590.4375]], device='cuda:0')
PREDICTION IDX/CLASS SORTED
tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [1, 0],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0]], device='cuda:0')
CORRECT PREDS
tensor([[ True],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [False],
        [ True],
        [ True],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [ True],
        [False],
        [False],
        [False],
        [ True]], device='cuda:0')
Top 1 acc 67.1875
Top1 accuracy for validation set is 67.1875 size is torch.Size([64, 1])
